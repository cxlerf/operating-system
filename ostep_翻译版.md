**Preface**
==前言==

**To Everyone**
==致每一个人==

Welcome to this book!
==欢迎阅读本书！==

We hope you'll enjoy reading it as much as we enjoyed writing it.
==我们希望您阅读本书的乐趣能像我们撰写本书时一样多。==

The book is called **Operating Systems: Three Easy Pieces** (available at [http://ostep.org](http://ostep.org)), and the title is obviously an homage to one of the greatest sets of lecture notes ever created, by one Richard Feynman on the topic of Physics.
==本书名为《**操作系统导论**》（**Operating Systems: Three Easy Pieces**，可在 [http://ostep.org](http://ostep.org) 获取），这个标题显然是在向理查德·费曼（Richard Feynman）关于物理学的伟大讲义致敬，那是通过史以来最伟大的讲义之一。==

While this book will undoubtedly fall short of the high standard set by that famous physicist, perhaps it will be good enough for you in your quest to understand what operating systems (and more generally, systems) are all about.
==虽然本书无疑难以企及那位著名物理学家设立的高标准，但也许它足够帮助您探索操作系统（以及更广泛的系统）的奥秘。==

The three easy pieces refer to the three major thematic elements the book is organized around: virtualization, concurrency, and persistence.
==这“三个简单的部分”指的是本书围绕组织的三个主要主题元素：虚拟化、并发和持久性。==

In discussing these concepts, we'll end up discussing most of the important things an operating system does.
==在讨论这些概念时，我们将最终探讨操作系统所做的大部分重要工作。==

Hopefully, you'll also have some fun along the way.
==希望您在此过程中也能获得一些乐趣。==

Learning new things is fun, right?
==学习新事物很有趣，对吧？==

At least, it (usually) should be.
==至少，它（通常）应该是这样的。==

Each major concept is divided into a set of chapters, most of which present a particular problem and then show how to solve it.
==每个主要概念都分为一组章节，其中大多数章节会提出一个特定问题，然后展示如何解决它。==

The chapters are short, and try (as best as possible) to reference the source material where the ideas really came from.
==章节都很短，并尽可能地引用思想真正来源的原始资料。==

One of our goals in writing this book is to make the paths of history as clear as possible, as we think that helps a student understand what is, what was, and what will be more clearly.
==我们要编写这本书的目标之一是尽可能清晰地呈现历史的脉络，因为我们认为这有助于学生更清楚地理解现状、过去和未来。==

In this case, seeing how the sausage was made is nearly as important as understanding what the sausage is good for.
==在这种情况下，了解“香肠是如何制作的”几乎与了解“香肠有什么用处”一样重要。==

Hint: eating!
==提示：吃！==

Or if you're a vegetarian, running away from..
==或者如果您是素食主义者，那就逃离……==

There are a couple devices we use throughout the book which are probably worth introducing here.
==我们在整本书中使用了一些手段，也许值得在这里介绍一下。==

The first is the **crux of the problem**.
==第一点是**问题的关键 (crux of the problem)**。==

Anytime we are trying to solve a problem, we first try to state what the most important issue is.
==每当我们试图解决一个问题时，我们首先尝试陈述最重要的问题是什么。==

Such a crux of the problem is explicitly called out in the text, and hopefully solved via the techniques, algorithms, and ideas presented in the rest of the text.
==这种“问题的关键”会在文中明确指出，并希望通过文中随后介绍的技术、算法和思想来解决。==

In many places, we'll explain how a system works by showing its behavior over time.
==在很多地方，我们将通过展示系统随时间变化的行为来解释它是如何工作的。==

These timelines are at the essence of understanding; if you know what happens, for example, when a process page faults, you are on your way to truly understanding how virtual memory operates.
==这些时间线是理解的本质；例如，如果您知道当进程发生页面错误（page fault）时会发生什么，您就开始真正理解虚拟内存是如何运作的了。==

If you comprehend what takes place when a journaling file system writes a block to disk, you have taken the first steps towards mastery of storage systems.
==如果您理解当日志文件系统将一个块写入磁盘时发生了什么，您就迈出了精通存储系统的第一步。==

There are also numerous asides and tips throughout the text, adding a little color to the mainline presentation.
==文中还有许多旁白和提示，为主要内容的陈述增添了一些色彩。==

Asides tend to discuss something relevant (but perhaps not essential) to the main text.
==旁白倾向于讨论与正文相关（但也许不是必不可少）的内容。==

Tips tend to be general lessons that can be applied to systems you build.
==提示倾向于通用的经验教训，可应用于您构建的系统。==

An index at the end of the book lists all of these tips and asides (as well as cruces, the odd plural of crux) for your convenience.
==书末的索引列出了所有这些提示和旁白（以及 cruces，即 crux 的古怪复数形式），以方便您查阅。==

We use one of the oldest didactic methods, the dialogue, throughout the book, as a way of presenting some of the material in a different light.
==我们在整本书中使用了最古老的教学方法之一——对话，作为以不同角度呈现部分材料的一种方式。==

These are used to introduce the major thematic concepts (in a peachy way, as we will see), as well as to review material every now and then.
==这些对话用于介绍主要的主题概念（正如我们将看到的，以一种“桃子般”有趣的方式），以及不时地复习材料。==

They are also a chance to write in a more humorous style.
==这也是一个以更幽默的风格进行写作的机会。==

Whether you find them useful, or humorous, well, that's another matter entirely.
==无论您觉得它们是有用还是幽默，好吧，那完全是另一回事了。==

At the beginning of each major section, we'll first present an abstraction that an operating system provides, and then work in subsequent chapters on the mechanisms, policies, and other support needed to provide the abstraction.
==在每个主要部分的开头，我们将首先介绍操作系统提供的一个抽象，然后在随后的章节中研究提供该抽象所需的机制、策略和其他支持。==

Abstractions are fundamental to all aspects of Computer Science, so it is perhaps no surprise that they are also essential in operating systems.
==抽象是计算机科学各个方面的基础，因此它们在操作系统中也至关重要，这也许不足为奇。==

Throughout the chapters, we try to use real code (not pseudocode) where possible, so for virtually all examples, you should be able to type them up yourself and run them.
==在整章中，我们尽可能尝试使用真实代码（而不是伪代码），因此对于几乎所有的示例，您应该都能够自己输入并运行它们。==

Running real code on real systems is the best way to learn about operating systems, so we encourage you to do so when you can.
==在真实系统上运行真实代码是学习操作系统的最佳方式，因此我们鼓励您在可能的情况下这样做。==

We are also making code available for your viewing pleasure.
==我们还提供了代码供您查阅。==

In various parts of the text, we have sprinkled in a few homeworks to ensure that you are understanding what is going on.
==在文中的不同部分，我们穿插了一些家庭作业，以确保您理解正在发生的事情。==

Many of these homeworks are little simulations of pieces of the operating system.
==其中许多作业是操作系统组件的小型模拟。==

You should download the homeworks, and run them to quiz yourself.
==您应该下载这些作业，并运行它们来测验自己。==

The homework simulators have the following feature: by giving them a different random seed, you can generate a virtually infinite set of problems.
==这些作业模拟器具有以下特点：通过给它们不同的随机种子，您可以生成几乎无限的一组问题。==

The simulators can also be told to solve the problems for you.
==这些模拟器也可以被指令为您解决问题。==

Thus, you can test and re-test yourself until you have achieved a good level of understanding.
==因此，您可以反复测试自己，直到达到良好的理解水平。==

The most important addendum to this book is a set of projects in which you learn about how real systems work by designing, implementing, and testing your own code.
==本书最重要的附录是一组项目，在这些项目中，您将通过设计、实现和测试自己的代码来了解真实系统是如何工作的。==

All projects (as well as the code examples, mentioned above) are in the C programming language.
==所有项目（以及上面提到的代码示例）都是用 C 语言编写的。==

C is a simple and powerful language that underlies most operating systems, and thus worth adding to your tool-chest of languages.
==C 是一种简单而强大的语言，是大多数操作系统的基础，因此值得将其添加到您的语言工具箱中。==

Two types of projects are available (see the online appendix for ideas).
==有两种类型的项目可供选择（请参阅在线附录以获取思路）。==

The first type is systems programming projects.
==第一类是系统编程项目。==

These projects are great for those who are new to C and UNIX and want to learn how to do low-level C programming.
==这些项目非常适合那些刚接触 C 和 UNIX 并想学习如何进行低级 C 编程的人。==

The second type is based on a real operating system kernel developed at MIT called **xv6**.
==第二类项目基于麻省理工学院（MIT）开发的一个名为 **xv6** 的真实操作系统内核。==

These projects are great for students that already have some C and want to get their hands dirty inside the OS.
==这些项目非常适合那些已经掌握了一些 C 语言并想深入操作系统内部实践的学生。==

At Wisconsin, we've run the course in three different ways: either all systems programming, all xv6 programming, or a mix of both.
==在威斯康星大学，我们以三种不同的方式开设这门课程：要么全部是系统编程，要么全部是 xv6 编程，或者是两者的混合。==

We are slowly making project descriptions, and a testing framework, available.
==我们正在慢慢提供项目说明和测试框架。==

See our repository for more information.
==请查看我们的代码仓库以获取更多信息。==

If not part of a class, this will give you a chance to do these projects on your own, to better learn the material.
==如果您不是在参加课程，这将给您一个独自完成这些项目的机会，以便更好地学习材料。==

Unfortunately, you don't have a TA to bug when you get stuck, but not everything in life can be free (but books can be!).
==遗憾的是，当您卡住时没有助教可以打扰，但生活中并非所有东西都是免费的（但书可以是！）。==

**To Educators**
==致教育工作者==

If you are an instructor or professor who wishes to use this book, please feel free to do so.
==如果您是一位希望使用本书的讲师或教授，请随时使用。==

As you may have noticed, they are free and available on-line from the following web page: [http://www.ostep.org](http://www.ostep.org)
==正如您可能注意到的，它们是免费的，并可从以下网页在线获取：[http://www.ostep.org](http://www.ostep.org)==

You can also purchase a printed copy from [http://lulu.com](http://lulu.com) or [http://amazon.com](http://amazon.com).
==您也可以从 [http://lulu.com](http://lulu.com) 或 [http://amazon.com](http://amazon.com) 购买纸质版。==

The course divides fairly well across a 15-week semester, in which you can cover most of the topics within at a reasonable level of depth.
==这门课程可以很好地划分在 15 周的学期中，您可以以合理的深度涵盖其中的大部分主题。==

Cramming the course into a 10-week quarter probably requires dropping some detail from each of the pieces.
==将课程压缩到 10 周的学期可能需要从每个部分中删减一些细节。==

There are also a few chapters on virtual machine monitors, which we usually squeeze in sometime during the semester, either right at end of the large section on virtualization, or near the end as an aside.
==还有几章关于虚拟机监视器的内容，我们通常会在学期中的某个时间挤进去讲，要么是在虚拟化这一大章的最后，要么是在接近尾声时作为补充。==

One slightly unusual aspect of the book is that concurrency, a topic at the front of many OS books, is pushed off herein until the student has built an understanding of virtualization of the CPU and of memory.
==本书一个稍显不同寻常的地方是，并发（许多操作系统书籍开头就会讲的主题）在这里被推迟了，直到学生建立了对 CPU 和内存虚拟化的理解之后才讲。==

In our experience in teaching this course for nearly 20 years, students have a hard time understanding how the concurrency problem arises, or why they are trying to solve it, if they don't yet understand what an address space is, what a process is, or why context switches can occur at arbitrary points in time.
==根据我们要讲授这门课程近 20 年的经验，如果学生还不理解什么是地址空间、什么是进程，或者为什么上下文切换可能在任意时间点发生，他们就很难理解并发问题是如何产生的，或者为什么要试图解决它。==

Once they do understand these concepts, however, introducing the notion of threads and the problems that arise due to them becomes rather easy, or at least, easier.
==然而，一旦他们理解了这些概念，引入线程的概念以及由此产生的问题就变得相当容易，或者至少更容易了。==

As much as is possible, we use a chalkboard (or whiteboard) to deliver a lecture.
==我们要尽可能使用黑板（或白板）来讲课。==

On these more conceptual days, we come to class with a few major ideas and examples in mind and use the board to present them.
==在讲解这些概念性较强的日子里，我们会带着几个主要观点和例子来上课，并用黑板来演示它们。==

Handouts are useful to give the students concrete problems to solve based on the material.
==讲义对于给学生提供基于材料的具体问题来解决很有用。==

On more practical days, we simply plug a laptop into the projector and show real code.
==在讲解实践性较强的日子里，我们只需将笔记本电脑连接到投影仪上并展示真实代码。==

This style works particularly well for concurrency lectures as well as for any discussion sections where you show students code that is relevant for their projects.
==这种风格特别适合并发课程，以及任何向学生展示与其项目相关的代码的讨论环节。==

We don't generally use slides to present material, but have now made a set available for those who prefer that style of presentation.
==我们通常不使用幻灯片来展示材料，但现在已经为那些喜欢这种展示风格的人提供了一套幻灯片。==

If you'd like a copy of any of these materials, please drop us an email.
==如果您想要这些材料的副本，请给我们发电子邮件。==

One last request: if you use the free online chapters, please just link to them, instead of making a local copy.
==最后一个请求：如果您使用免费的在线章节，请直接链接到它们，而不是制作本地副本。==

This helps us track usage (million of chapters downloaded each month) and also ensures students get the latest (and greatest?) version.
==这有助于我们跟踪使用情况（每月数百万章的下载量），并确保学生获得最新（也是最好？）的版本。==

**To Students**
==致学生==

If you are a student reading this book, thank you!
==如果您是正在阅读本书的学生，谢谢您！==

It is an honor for us to provide some material to help you in your pursuit of knowledge about operating systems.
==我们很荣幸能提供一些材料，帮助您追求关于操作系统的知识。==

We both think back fondly towards some textbooks of our undergraduate days (e.g., Hennessy and Patterson, the classic book on computer architecture) and hope this book will become one of those positive memories for you.
==我们要深情地回想起我们本科时代的一些教科书（例如，Hennessy 和 Patterson 编写的关于计算机体系结构的经典著作），并希望这本书能成为您美好回忆的一部分。==

You may have noticed this book is free and available online.
==您可能已经注意到这本书是免费的，并且可以在线获取。==

There is one major reason for this: textbooks are generally too expensive.
==这其中有一个主要原因：教科书通常太贵了。==

This book, we hope, is the first of a new wave of free materials to help those in pursuit of their education, regardless of which part of the world they come from or how much they are willing to spend for a book.
==我们希望这本书能成为新一波免费教材的先驱，帮助那些追求教育的人，无论他们来自世界的哪个角落，也无论他们愿意为一本书花多少钱。==

Failing that, it is one free book, which is better than none.
==即便做不到这一点，这也算是一本免费的书，总比没有好。==

We also hope, where possible, to point you to the original sources of much of the material in the book: the great papers and persons who have shaped the field of operating systems over the years.
==我们还希望在可能的情况下，向您指出书中大部分材料的原始来源：那些多年来塑造了操作系统领域的伟大论文和人物。==

Ideas are not pulled out of the air; they come from smart and hard-working people (including numerous Turing-award winners), and thus we should strive to celebrate those ideas and people where possible.
==思想不是凭空产生的；它们来自聪明和勤奋的人（包括许多图灵奖得主），因此我们应该尽可能地赞美这些思想和人物。==

In doing so, we hopefully can better understand the revolutions that have taken place, instead of writing texts as if those thoughts have always been present.
==这样做，我们希望能更好地理解已经发生的革命，而不是把教科书写得好像这些思想一直存在一样。==

Further, perhaps such references will encourage you to dig deeper on your own.
==此外，也许这些参考文献会鼓励您自己进行更深入的挖掘。==

Reading the famous papers of our field is certainly one of the best ways to learn.
==阅读我们领域的著名论文无疑是最好的学习方式之一。==

A digression here: "free" in the way we use it here does not mean open source, and it does not mean the book is not copyrighted with the usual protections - it is!
==这里离题一下：我们在这里使用的“免费”并不意味着开源，也不意味着这本书没有受到通常的版权保护——它受保护！==

What it means is that you can download the chapters and use them to learn about operating systems.
==它的意思是您可以下载章节并使用它们来学习操作系统。==

Why not an open-source book, just like Linux is an open-source kernel?
==为什么不像 Linux 是开源内核那样做一本开源书呢？==

Well, we believe it is important for a book to have a single voice throughout, and have worked hard to provide such a voice.
==好吧，我们认为一本书在通篇保持统一的声音很重要，并且我们一直在努力提供这样的声音。==

When you're reading it, the book should kind of feel like a dialogue with the person explaining something to you.
==当您阅读它时，这本书应该感觉像是与向您解释某事的人进行的对话。==

Hence, our approach.
==因此，我们要采取了这种方法。==

The Turing Award is the highest award in Computer Science; it is like the Nobel Prize, except that you have never heard of it.
==图灵奖是计算机科学的最高奖项；它就像诺贝尔奖，除了您可能从未听说过它。==

**Acknowledgments**
==致谢==

This section will contain thanks to those who helped us put the book together.
==本节将包含对那些帮助我们编写本书的人的感谢。==

The important thing for now: your name could go here!
==现在重要的是：您的名字可能会出现在这里！==

But, you have to help.
==但是，您得提供帮助。==

So send us some feedback and help debug this book.
==所以给我们发送一些反馈，帮助调试这本书。==

And you could be famous!
==您可能会出名！==

Or, at least, have your name in some book.
==或者，至少，您的名字会出现在某本书里。==

The people who have helped so far include:
==到目前为止提供帮助的人包括：（以下为大量人名，此处保留原文以示尊重）==
Aaron Gember, Aashrith H Govindraj, Abdallah Ahmed, Abhinav Mehra, ... (and hundreds more students and contributors).

Special thanks to those marked with an asterisk above, who have gone above and beyond in their suggestions for improvement.
==特别感谢上面标有星号的人，他们在提出改进建议方面付出了额外的努力。==

In addition, a hearty thanks to Professor Joe Meehean (Lynchburg) for his detailed notes on each chapter.
==此外，衷心感谢 Joe Meehean 教授（Lynchburg）为每一章提供的详细笔记。==

To Professor Jerod Weinman (Grinnell) and his entire class for their incredible booklets.
==感谢 Jerod Weinman 教授（Grinnell）和他全班同学制作的令人难以置信的小册子。==

To Professor Chien-Chung Shen (Delaware) for his invaluable and detailed reading and comments.
==感谢 Chien-Chung Shen 教授（特拉华大学）提供的宝贵而详细的阅读和评论。==

To Adam Drescher (WUSTL) for his careful reading and suggestions.
==感谢 Adam Drescher (WUSTL) 的仔细阅读和建议。==

To Glen Granzow (College of Idaho) for his incredibly detailed comments and tips.
==感谢 Glen Granzow（爱达荷学院）提供的极其详细的评论和提示。==

To Michael Walfish (NYU) for his enthusiasm and detailed suggestions for improvement.
==感谢 Michael Walfish (NYU) 的热情和详细的改进建议。==

To Peter Peterson (UMD) for his many bits of useful feedback and commentary.
==感谢 Peter Peterson (UMD) 提供的许多有用的反馈和评论。==

To Mark Kampe (Pomona) for detailed criticism (we only wish we could fix all suggestions!).
==感谢 Mark Kampe (Pomona) 的详细批评（我们只希望我们能修正所有的建议！）。==

And to Youjip Won (Hanyang) for his translation work into Korean(!) and numerous insightful suggestions.
==感谢 Youjip Won (汉阳大学) 将本书翻译成韩语（！）以及无数富有洞察力的建议。==

To Terence Kelly for his sidebar on memory mapping.
==感谢 Terence Kelly 关于内存映射的侧边栏内容。==

All have helped these authors immeasurably in the refinement of the materials herein.
==所有这些人都对作者完善本书材料提供了不可估量的帮助。==

A special thank you to Professor Peter Reiher (UCLA) for writing a wonderful set of security chapters, all in the style of this book.
==特别感谢 Peter Reiher 教授（UCLA）撰写了一套精彩的安全章节，风格与本书完全一致。==

We had the fortune of meeting Peter many years ago, and little did we know that we would collaborate in this fashion two decades later.
==我们有幸在许多年前遇到了 Peter，当时完全没想到二十年后我们会以这种方式合作。==

Amazing work!
==了不起的工作！==

Also, many thanks to the hundreds of students who have taken 537 over the years.
==此外，非常感谢多年来选修 537 课程的数百名学生。==

In particular, the Fall '08 class who encouraged the first written form of these notes (they were sick of not having any kind of textbook to read pushy students!), and then praised them enough for us to keep going.
==特别是 08 年秋季班的学生，他们鼓励了这些笔记的最初书面形式（他们厌倦了没有任何教科书可读——爱催促的学生！），然后给予了足够的赞扬让我们坚持下去。==

A great debt of thanks is also owed to the brave few who took the xv6 project lab course, much of which is now incorporated into the main 537 course.
==还要非常感谢那些参加 xv6 项目实验课程的勇敢的少数人，其中大部分内容现在已并入主要的 537 课程中。==

Although they do not directly help with the book, our students have taught us much of what we know about systems.
==虽然他们不直接帮助编写本书，但我们的学生教会了我们许多关于系统的知识。==

We talk with them regularly while they are at Wisconsin, but they do all the real work and by telling us about what they are doing, we learn new things every week.
==当他们在威斯康星大学时，我们要定期与他们交谈，但他们做了所有实际的工作，通过告诉我们他们在做什么，我们每周都能学到新东西。==

Our graduate students have largely been funded by the National Science Foundation (NSF), the Department of Energy Office of Science (DOE), and by industry grants.
==我们的研究生主要由国家科学基金会 (NSF)、能源部科学办公室 (DOE) 和行业资助。==

We are especially grateful to the NSF for their support over many years, as our research has shaped the content of many chapters herein.
==我们要特别感谢 NSF 多年来的支持，因为我们的研究塑造了本书许多章节的内容。==

We thank Thomas Griebel, who demanded a better cover for the book.
==我们感谢 Thomas Griebel，他要求这本书有一个更好的封面。==

Although we didn't take his specific suggestion (a dinosaur, can you believe it?), the beautiful picture of Halley's comet would not be found on the cover without him.
==虽然我们要没有采纳他的具体建议（一只恐龙，你敢信？），但如果没有他，封面上就不会有哈雷彗星的美丽照片。==

A final debt of gratitude is also owed to Aaron Brown.
==最后还要感谢 Aaron Brown。==

His tireless work has vastly improved the state of the projects (particularly those in xv6 land) and thus has helped better the learning experience for countless undergraduates and graduates here at Wisconsin.
==他不懈的工作极大地改善了项目的状况（特别是在 xv6 领域），从而帮助改善了威斯康星大学无数本科生和研究生的学习体验。==

As Aaron would say (in his usual succinct manner): "Thx."
==正如 Aaron 会说的那样（以他一贯简洁的方式）：“谢了。”==

**Final Words**
==最后的话==

Yeats famously said "Education is not the filling of a pail but the lighting of a fire."
==叶芝有句名言：“教育不是注满一桶水，而是点燃一把火。”==

He was right but wrong at the same time.
==他是对的，但同时也错了。==

You do have to "fill the pail" a bit, and these notes are certainly here to help with that part of your education.
==你确实需要“注满桶”一点，而这些笔记当然是为了帮助你完成那部分教育；==

After all, when you go to interview at Google, and they ask you a trick question about how to use semaphores, it might be good to actually know what a semaphore is, right?
==毕竟，当你去 Google 面试时，如果他们问你一个关于如何使用信号量的刁钻问题，真正知道信号量是什么可能会很好，对吧？==

But Yeats's larger point is obviously on the mark: the real point of education is to get you interested in something, to learn something more about the subject matter on your own and not just what you have to digest to get a good grade in some class.
==但叶芝更宏大的观点显然是正确的：教育的真正意义在于让你对某事感兴趣，让你自己去学习更多关于该主题的知识，而不仅仅是为了在某门课上取得好成绩而必须消化的东西。==

As one of our fathers (Remzi's dad, Vedat Arpaci) used to say, "Learn beyond the classroom".
==正如我们要的一位父亲（Remzi 的父亲，Vedat Arpaci）常说的，“要在课堂之外学习”。==

We created these notes to spark your interest in operating systems, to read more about the topic on your own, to talk to your professor about all the exciting research that is going on in the field, and even to get involved with that research.
==我们创建这些笔记是为了激发您对操作系统的兴趣，让您自己阅读更多关于该主题的内容，与您的教授谈论该领域正在进行的所有令人兴奋的研究，甚至参与到该研究中去。==

It is a great field(!), full of exciting and wonderful ideas that have shaped computing history in profound and important ways.
==这是一个伟大的领域（！），充满了令人兴奋和奇妙的思想，这些思想以深刻而重要的方式塑造了计算历史。==

And while we understand this fire won't light for all of you, we hope it does for many, or even a few.
==虽然我们要明白这把火不会为你们所有人点燃，但我们希望它能为许多人，甚至只是少数人点燃。==

Because once that fire is lit, well, that is when you truly become capable of doing something great.
==因为一旦这把火被点燃，那么，那正是你真正有能力做一些伟大的事情的时候。==

And thus the real point of the educational process: to go forth, to study many new and fascinating topics, to learn, to mature, and most importantly, to find something that lights a fire for you.
==这就是教育过程的真正意义：勇往直前，研究许多新颖而迷人的话题，学习，成熟，最重要的是，找到能为你点燃心中之火的东西。==

Andrea and Remzi
==Andrea 和 Remzi==

Married couple
==已婚夫妇==

Professors of Computer Science at the University of Wisconsin
==威斯康星大学计算机科学教授==

Chief Lighters of Fires, hopefully
==希望能成为首席点火人==

**References**
==参考文献==

[CK+08] "The xv6 Operating System" by Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich.
==[CK+08] 《xv6 操作系统》，作者：Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich。==

xv6 was developed as a port of the original UNIX version 6 and represents a beautiful, clean, and simple way to understand a modern operating system.
==xv6 是作为原始 UNIX 第 6 版的移植版开发的，代表了一种理解现代操作系统的优美、干净且简单的方式。==

[F96] "Six Easy Pieces: Essentials Of Physics Explained By Its Most Brilliant Teacher" by Richard P. Feynman.
==[F96] 《物理之美：费曼物理学讲义入门选》（Six Easy Pieces），作者：Richard P. Feynman。==

Basic Books, 1996. This book reprints the six easiest chapters of Feynman's Lectures on Physics, from 1963.
==Basic Books 出版社，1996 年。本书重印了 1963 年《费曼物理学讲义》中最简单的六章。==

If you like Physics, it is a fantastic read.
==如果您喜欢物理，这是一本极好的读物。==

[HP90] "Computer Architecture a Quantitative Approach" (1st ed.) by David A. Patterson and John L. Hennessy.
==[HP90] 《计算机体系结构：量化研究方法》（第 1 版），作者：David A. Patterson 和 John L. Hennessy。==

A book that encouraged each of us at our undergraduate institutions to pursue graduate studies.
==这是一本鼓励我们在本科院校攻读研究生的书。==

We later both had the pleasure of working with Patterson, who greatly shaped the foundations of our research careers.
==我们要后来都有幸与 Patterson 共事，他极大地塑造了我们研究生涯的基础。==

[KR88] "The C Programming Language" by Brian Kernighan and Dennis Ritchie.
==[KR88] 《C 程序设计语言》，作者：Brian Kernighan 和 Dennis Ritchie。==

The C programming reference that everyone should have, by the people who invented the language.
==每个人都应该拥有的 C 编程参考书，由发明该语言的人编写。==

[K62] "The Structure of Scientific Revolutions" by Thomas S. Kuhn.
==[K62] 《科学革命的结构》，作者：Thomas S. Kuhn。==

A great and famous read about the fundamentals of the scientific process.
==关于科学过程基础的一本伟大而著名的读物。==

Mop-up work, anomaly, crisis, and revolution.
==扫尾工作、反常、危机和革命。==

We are mostly destined to do mop-up work, alas.
==唉，我们大多数人注定要做扫尾工作。==

**Contents**
==目录==

1 A Dialogue on the Book
==1 关于本书的对话==

2 Introduction to Operating Systems
==2 操作系统介绍==

2.1 Virtualizing The CPU
==2.1 CPU 虚拟化==

2.2 Virtualizing Memory
==2.2 内存虚拟化==

2.3 Concurrency
==2.3 并发==

2.4 Persistence
==2.4 持久性==

2.5 Design Goals
==2.5 设计目标==

2.6 Some History
==2.6 一些历史==

2.7 Summary
==2.7 总结==

**Part I: Virtualization**
==**第一部分：虚拟化**==

3 A Dialogue on Virtualization
==3 关于虚拟化的对话==

4 The Abstraction: The Process
==4 抽象：进程==

4.1 The Abstraction: A Process
==4.1 抽象：一个进程==

4.2 Process API
==4.2 进程 API==

4.3 Process Creation: A Little More Detail
==4.3 进程创建：更多细节==

4.4 Process States
==4.4 进程状态==

4.5 Data Structures
==4.5 数据结构==

4.6 Summary
==4.6 总结==

5 Interlude: Process API
==5 插曲：进程 API==

5.1 The fork() System Call
==5.1 fork() 系统调用==

5.2 The wait() System Call
==5.2 wait() 系统调用==

5.3 Finally, The exec() System Call
==5.3 最后，exec() 系统调用==

5.4 Why? Motivating The API
==5.4 为什么？API 的动机==

5.5 Process Control And Users
==5.5 进程控制与用户==

5.6 Useful Tools
==5.6 有用的工具==

5.7 Summary
==5.7 总结==

6 Mechanism: Limited Direct Execution
==6 机制：受限直接执行==

6.1 Basic Technique: Limited Direct Execution
==6.1 基本技术：受限直接执行==

6.2 Problem #1: Restricted Operations
==6.2 问题 #1：受限操作==

6.3 Problem #2: Switching Between Processes
==6.3 问题 #2：在进程间切换==

6.4 Worried About Concurrency?
==6.4 担心并发？==

6.5 Summary
==6.5 总结==

7 Scheduling: Introduction
==7 调度：简介==

7.1 Workload Assumptions
==7.1 工作负载假设==

7.2 Scheduling Metrics
==7.2 调度指标==

7.3 First In, First Out (FIFO)
==7.3 先进先出 (FIFO)==

7.4 Shortest Job First (SJF)
==7.4 最短任务优先 (SJF)==

7.5 Shortest Time-to-Completion First (STCF)
==7.5 最短完成时间优先 (STCF)==

7.6 A New Metric: Response Time
==7.6 一个新指标：响应时间==

7.7 Round Robin
==7.7 轮转调度 (Round Robin)==

7.8 Incorporating I/O
==7.8 结合 I/O==

7.9 No More Oracle
==7.9 不再有预言机==

7.10 Summary
==7.10 总结==

8 Scheduling: The Multi-Level Feedback Queue
==8 调度：多级反馈队列==

8.1 MLFQ: Basic Rules
==8.1 MLFQ：基本规则==

8.2 Attempt #1: How To Change Priority
==8.2 尝试 #1：如何改变优先级==

8.3 Attempt #2: The Priority Boost
==8.3 尝试 #2：优先级提升==

8.4 Attempt #3: Better Accounting
==8.4 尝试 #3：更好的计时==

8.5 Tuning MLFQ And Other Issues
==8.5 调优 MLFQ 及其他问题==

8.6 MLFQ: Summary
==8.6 MLFQ：总结==

9 Scheduling: Proportional Share
==9 调度：比例份额==

9.1 Basic Concept: Tickets Represent Your Share
==9.1 基本概念：彩票代表你的份额==

9.2 Ticket Mechanisms
==9.2 彩票机制==

9.3 Implementation
==9.3 实现==

9.4 An Example
==9.4 一个例子==

9.5 How To Assign Tickets?
==9.5 如何分配彩票？==

9.6 Stride Scheduling
==9.6 步长调度==

9.7 The Linux Completely Fair Scheduler (CFS)
==9.7 Linux 完全公平调度器 (CFS)==

9.8 Summary
==9.8 总结==

10 Multiprocessor Scheduling (Advanced)
==10 多处理器调度（进阶）==

10.1 Background: Multiprocessor Architecture
==10.1 背景：多处理器架构==

10.2 Don't Forget Synchronization
==10.2 别忘了同步==

10.3 One Final Issue: Cache Affinity
==10.3 最后一个问题：缓存亲和性==

10.4 Single-Queue Scheduling
==10.4 单队列调度==

10.5 Multi-Queue Scheduling
==10.5 多队列调度==

10.6 Linux Multiprocessor Schedulers
==10.6 Linux 多处理器调度器==

10.7 Summary
==10.7 总结==

11 Summary Dialogue on CPU Virtualization
==11 关于 CPU 虚拟化的总结对话==

12 A Dialogue on Memory Virtualization
==12 关于内存虚拟化的对话==

13 The Abstraction: Address Spaces
==13 抽象：地址空间==

13.1 Early Systems
==13.1 早期系统==

13.2 Multiprogramming and Time Sharing
==13.2 多道程序设计和分时==

13.3 The Address Space
==13.3 地址空间==

13.4 Goals
==13.4 目标==

13.5 Summary
==13.5 总结==

14 Interlude: Memory API
==14 插曲：内存 API==

14.1 Types of Memory
==14.1 内存类型==

14.2 The malloc() Call
==14.2 malloc() 调用==

14.3 The free() Call
==14.3 free() 调用==

14.4 Common Errors
==14.4 常见错误==

14.5 Underlying OS Support
==14.5 底层操作系统支持==

14.6 Other Calls
==14.6 其他调用==

14.7 Summary
==14.7 总结==

15 Mechanism: Address Translation
==15 机制：地址转换==

15.1 Assumptions
==15.1 假设==

15.2 An Example
==15.2 一个例子==

15.3 Dynamic (Hardware-based) Relocation
==15.3 动态（基于硬件的）重定位==

15.4 Hardware Support: A Summary
==15.4 硬件支持：总结==

15.5 Operating System Issues
==15.5 操作系统问题==

15.6 Summary
==15.6 总结==

16 Segmentation
==16 分段==

16.1 Segmentation: Generalized Base/Bounds
==16.1 分段：广义的基址/界限==

16.2 Which Segment Are We Referring To?
==16.2 我们指的是哪个段？==

16.3 What About The Stack?
==16.3 栈怎么办？==

16.4 Support for Sharing
==16.4 支持共享==

16.5 Fine-grained vs. Coarse-grained Segmentation
==16.5 细粒度与粗粒度分段==

16.6 OS Support
==16.6 操作系统支持==

16.7 Summary
==16.7 总结==

17 Free-Space Management
==17 空闲空间管理==

17.1 Assumptions
==17.1 假设==

17.2 Low-level Mechanisms
==17.2 低级机制==

17.3 Basic Strategies
==17.3 基本策略==

17.4 Other Approaches
==17.4 其他方法==

17.5 Summary
==17.5 总结==

18 Paging: Introduction
==18 分页：简介==

18.1 A Simple Example And Overview
==18.1 简单示例与概述==

18.2 Where Are Page Tables Stored?
==18.2 页表存在哪里？==

18.3 What's Actually In The Page Table?
==18.3 页表里实际上有什么？==

18.4 Paging: Also Too Slow
==18.4 分页：也太慢了==

18.5 A Memory Trace
==18.5 内存追踪==

18.6 Summary
==18.6 总结==

19 Paging: Faster Translations (TLBs)
==19 分页：更快的转换 (TLB)==

19.1 TLB Basic Algorithm
==19.1 TLB 基本算法==

19.2 Example: Accessing An Array
==19.2 示例：访问数组==

19.3 Who Handles The TLB Miss?
==19.3 谁来处理 TLB 未命中？==

19.4 TLB Contents: What's In There?
==19.4 TLB 内容：里面有什么？==

19.5 TLB Issue: Context Switches
==19.5 TLB 问题：上下文切换==

19.6 Issue: Replacement Policy
==19.6 问题：替换策略==

19.7 A Real TLB Entry
==19.7 真实的 TLB 表项==

19.8 Summary
==19.8 总结==

20 Paging: Smaller Tables
==20 分页：更小的表==

20.1 Simple Solution: Bigger Pages
==20.1 简单的解决方案：更大的页面==

20.2 Hybrid Approach: Paging and Segments
==20.2 混合方法：分页和分段==

20.3 Multi-level Page Tables
==20.3 多级页表==

20.4 Inverted Page Tables
==20.4 反向页表==

20.5 Swapping the Page Tables to Disk
==20.5 将页表交换到磁盘==

20.6 Summary
==20.6 总结==

21 Beyond Physical Memory: Mechanisms
==21 超越物理内存：机制==

21.1 Swap Space
==21.1 交换空间==

21.2 The Present Bit
==21.2 存在位 (Present Bit)==

21.3 The Page Fault
==21.3 页面错误 (Page Fault)==

21.4 What If Memory Is Full?
==21.4 如果内存满了怎么办？==

21.5 Page Fault Control Flow
==21.5 页面错误控制流==

21.6 When Replacements Really Occur
==21.6 替换真正发生的时间==

21.7 Summary
==21.7 总结==

22 Beyond Physical Memory: Policies
==22 超越物理内存：策略==

22.1 Cache Management
==22.1 缓存管理==

22.2 The Optimal Replacement Policy
==22.2 最佳替换策略==

22.3 A Simple Policy: FIFO
==22.3 一个简单的策略：先进先出 (FIFO)==

22.4 Another Simple Policy: Random
==22.4 另一个简单的策略：随机==

22.5 Using History: LRU
==22.5 利用历史记录：最近最少使用 (LRU)==

22.6 Workload Examples
==22.6 工作负载示例==

22.7 Implementing Historical Algorithms
==22.7 实现基于历史的算法==

22.8 Approximating LRU
==22.8 近似 LRU==

22.9 Considering Dirty Pages
==22.9 考虑脏页==

22.10 Other VM Policies
==22.10 其他虚拟内存策略==

22.11 Thrashing
==22.11 抖动 (Thrashing)==

22.12 Summary
==22.12 总结==

23 Complete Virtual Memory Systems
==23 完整的虚拟内存系统==

23.1 VAX/VMS Virtual Memory
==23.1 VAX/VMS 虚拟内存==

23.2 The Linux Virtual Memory System
==23.2 Linux 虚拟内存系统==

23.3 Summary
==23.3 总结==

24 Summary Dialogue on Memory Virtualization
==24 关于内存虚拟化的总结对话==

**Part II: Concurrency**
==**第二部分：并发**==

25 A Dialogue on Concurrency
==25 关于并发的对话==

26 Concurrency: An Introduction
==26 并发：简介==

26.1 Why Use Threads?
==26.1 为什么要使用线程？==

26.2 An Example: Thread Creation
==26.2 一个例子：线程创建==

26.3 Why It Gets Worse: Shared Data
==26.3 为什么情况会变糟：共享数据==

26.4 The Heart Of The Problem: Uncontrolled Scheduling
==26.4 问题的核心：不受控制的调度==

26.5 The Wish For Atomicity
==26.5 对原子性的渴望==

26.6 One More Problem: Waiting For Another
==26.6 另一个问题：等待另一个线程==

26.7 Summary: Why in OS Class?
==26.7 总结：为什么要在操作系统课上讲？==

27 Interlude: Thread API
==27 插曲：线程 API==

27.1 Thread Creation
==27.1 线程创建==

27.2 Thread Completion
==27.2 线程完成==

27.3 Locks
==27.3 锁==

27.4 Condition Variables
==27.4 条件变量==

27.5 Compiling and Running
==27.5 编译和运行==

27.6 Summary
==27.6 总结==

28 Locks
==28 锁==

28.1 Locks: The Basic Idea
==28.1 锁：基本思想==

28.2 Pthread Locks
==28.2 Pthread 锁==

28.3 Building A Lock
==28.3 构建锁==

28.4 Evaluating Locks
==28.4 评估锁==

28.5 Controlling Interrupts
==28.5 控制中断==

28.6 A Failed Attempt: Just Using Loads/Stores
==28.6 一次失败的尝试：仅使用加载/存储==

28.7 Building Working Spin Locks with Test-And-Set
==28.7 使用测试并设置 (Test-And-Set) 构建工作的自旋锁==

28.8 Evaluating Spin Locks
==28.8 评估自旋锁==

28.9 Compare-And-Swap
==28.9 比较并交换 (Compare-And-Swap)==

28.10 Load-Linked and Store-Conditional
==28.10 链接加载和条件存储 (Load-Linked and Store-Conditional)==

28.11 Fetch-And-Add
==28.11 获取并增加 (Fetch-And-Add)==

28.12 Too Much Spinning: What Now?
==28.12 自旋过多：现在怎么办？==

28.13 A Simple Approach: Just Yield, Baby
==28.13 一个简单的方法：让出 CPU==

28.14 Using Queues: Sleeping Instead Of Spinning
==28.14 使用队列：休眠代替自旋==

28.15 Different OS, Different Support
==28.15 不同的操作系统，不同的支持==

28.16 Two-Phase Locks
==28.16 两阶段锁==

28.17 Summary
==28.17 总结==

29 Lock-based Concurrent Data Structures
==29 基于锁的并发数据结构==

29.1 Concurrent Counters
==29.1 并发计数器==

29.2 Concurrent Linked Lists
==29.2 并发链表==

29.3 Concurrent Queues
==29.3 并发队列==

29.4 Concurrent Hash Table
==29.4 并发哈希表==

29.5 Summary
==29.5 总结==

30 Condition Variables
==30 条件变量==

30.1 Definition and Routines
==30.1 定义和例程==

30.2 The Producer/Consumer (Bounded Buffer) Problem
==30.2 生产者/消费者（有界缓冲区）问题==

30.3 Covering Conditions
==30.3 覆盖条件==

30.4 Summary
==30.4 总结==

31 Semaphores
==31 信号量==

31.1 Semaphores: A Definition
==31.1 信号量：定义==

31.2 Binary Semaphores (Locks)
==31.2 二值信号量（锁）==

31.3 Semaphores For Ordering
==31.3 用于排序的信号量==

31.4 The Producer/Consumer (Bounded Buffer) Problem
==31.4 生产者/消费者（有界缓冲区）问题==

31.5 Reader-Writer Locks
==31.5 读写锁==

31.6 The Dining Philosophers
==31.6 哲学家就餐问题==

31.7 Thread Throttling
==31.7 线程限流==

31.8 How To Implement Semaphores
==31.8 如何实现信号量==

31.9 Summary
==31.9 总结==

32 Common Concurrency Problems
==32 常见的并发问题==

32.1 What Types Of Bugs Exist?
==32.1 存在哪些类型的 Bug？==

32.2 Non-Deadlock Bugs
==32.2 非死锁 Bug==

32.3 Deadlock Bugs
==32.3 死锁 Bug==

32.4 Summary
==32.4 总结==

33 Event-based Concurrency (Advanced)
==33 基于事件的并发（进阶）==

33.1 The Basic Idea: An Event Loop
==33.1 基本思想：事件循环==

33.2 An Important API: select() (or poll())
==33.2 一个重要的 API：select()（或 poll()）==

33.3 Using select()
==33.3 使用 select()==

33.4 Why Simpler? No Locks Needed
==33.4 为什么更简单？不需要锁==

33.5 A Problem: Blocking System Calls
==33.5 一个问题：阻塞系统调用==

33.6 A Solution: Asynchronous I/O
==33.6 解决方案：异步 I/O==

33.7 Another Problem: State Management
==33.7 另一个问题：状态管理==

33.8 What Is Still Difficult With Events
==33.8 事件驱动还有什么困难==

33.9 Summary
==33.9 总结==

34 Summary Dialogue on Concurrency
==34 关于并发的总结对话==

**Part III: Persistence**
==**第三部分：持久性**==

35 A Dialogue on Persistence
==35 关于持久性的对话==

36 I/O Devices
==36 I/O 设备==

36.1 System Architecture
==36.1 系统架构==

36.2 A Canonical Device
==36.2 典型设备==

36.3 The Canonical Protocol
==36.3 典型协议==

36.4 Lowering CPU Overhead With Interrupts
==36.4 使用中断降低 CPU 开销==

36.5 More Efficient Data Movement With DMA
==36.5 使用 DMA 进行更高效的数据移动==

36.6 Methods Of Device Interaction
==36.6 设备交互方法==

36.7 Fitting Into The OS: The Device Driver
==36.7 融入操作系统：设备驱动程序==

36.8 Case Study: A Simple IDE Disk Driver
==36.8 案例研究：一个简单的 IDE 磁盘驱动程序==

36.9 Historical Notes
==36.9 历史记录==

36.10 Summary
==36.10 总结==

37 Hard Disk Drives
==37 硬盘驱动器==

37.1 The Interface
==37.1 接口==

37.2 Basic Geometry
==37.2 基本几何结构==

37.3 A Simple Disk Drive
==37.3 一个简单的磁盘驱动器==

37.4 I/O Time: Doing The Math
==37.4 I/O 时间：数学计算==

37.5 Disk Scheduling
==37.5 磁盘调度==

37.6 Summary
==37.6 总结==

38 Redundant Arrays of Inexpensive Disks (RAIDs)
==38 廉价磁盘冗余阵列 (RAID)==

38.1 Interface And RAID Internals
==38.1 接口和 RAID 内部结构==

38.2 Fault Model
==38.2 故障模型==

38.3 How To Evaluate A RAID
==38.3 如何评估 RAID==

38.4 RAID Level 0: Striping
==38.4 RAID 0 级：条带化==

38.5 RAID Level 1: Mirroring
==38.5 RAID 1 级：镜像==

38.6 RAID Level 4: Saving Space With Parity
==38.6 RAID 4 级：使用奇偶校验节省空间==

38.7 RAID Level 5: Rotating Parity
==38.7 RAID 5 级：旋转奇偶校验==

38.8 RAID Comparison: A Summary
==38.8 RAID 比较：总结==

38.9 Other Interesting RAID Issues
==38.9 其他有趣的 RAID 问题==

38.10 Summary
==38.10 总结==

**39 Interlude: Files and Directories**
==39 插曲：文件和目录==

39.1 Files And Directories
==39.1 文件和目录==

39.2 The File System Interface
==39.2 文件系统接口==

39.3 Creating Files
==39.3 创建文件==

39.4 Reading And Writing Files
==39.4 读写文件==

39.5 Reading And Writing, But Not Sequentially
==39.5 读写，但非顺序==

39.6 Shared File Table Entries: fork() And dup()
==39.6 共享文件表项：fork() 和 dup()==

39.7 Writing Immediately With fsync()
==39.7 使用 fsync() 立即写入==

39.8 Renaming Files
==39.8 重命名文件==

39.9 Getting Information About Files
==39.9 获取文件信息==

39.10 Removing Files
==39.10 删除文件==

39.11 Making Directories
==39.11 创建目录==

39.12 Reading Directories
==39.12 读取目录==

39.13 Deleting Directories
==39.13 删除目录==

39.14 Hard Links
==39.14 硬链接==

39.15 Symbolic Links
==39.15 符号链接==

39.16 Permission Bits And Access Control Lists
==39.16 权限位和访问控制列表==

39.17 Making And Mounting A File System
==39.17 创建和挂载文件系统==

39.18 Summary
==39.18 总结==

**40 File System Implementation**
==40 文件系统实现==

40.1 The Way To Think
==40.1 思考方式==

40.2 Overall Organization
==40.2 整体组织结构==

40.3 File Organization: The Inode
==40.3 文件组织：索引节点 (Inode)==

40.4 Directory Organization
==40.4 目录组织==

40.5 Free Space Management
==40.5 空闲空间管理==

40.6 Access Paths: Reading and Writing
==40.6 访问路径：读和写==

40.7 Caching and Buffering
==40.7 缓存和缓冲==

40.8 Summary
==40.8 总结==

**41 Locality and The Fast File System**
==41 局部性和快速文件系统 (FFS)==

41.1 The Problem: Poor Performance
==41.1 问题：性能不佳==

41.2 FFS: Disk Awareness Is The Solution
==41.2 FFS：磁盘感知是解决方案==

41.3 Organizing Structure: The Cylinder Group
==41.3 组织结构：柱面组==

41.4 Policies: How To Allocate Files and Directories
==41.4 策略：如何分配文件和目录==

41.5 Measuring File Locality
==41.5 测量文件局部性==

41.6 The Large-File Exception
==41.6 大文件例外==

41.7 A Few Other Things About FFS
==41.7 关于 FFS 的其他几件事==

41.8 Summary
==41.8 总结==

**42 Crash Consistency: FSCK and Journaling**
==42 崩溃一致性：FSCK 和日志记录==

42.1 A Detailed Example
==42.1 一个详细的例子==

42.2 Solution #1: The File System Checker
==42.2 解决方案 #1：文件系统检查器==

42.3 Solution #2: Journaling (or Write-Ahead Logging)
==42.3 解决方案 #2：日志记录（或预写日志）==

42.4 Solution #3: Other Approaches
==42.4 解决方案 #3：其他方法==

42.5 Summary
==42.5 总结==

**43 Log-structured File Systems**
==43 日志结构文件系统 (LFS)==

43.1 Writing To Disk Sequentially
==43.1 顺序写入磁盘==

43.2 Writing Sequentially And Effectively
==43.2 有效地顺序写入==

43.3 How Much To Buffer?
==43.3 缓冲多少？==

43.4 Problem: Finding Inodes
==43.4 问题：查找索引节点==

43.5 Solution Through Indirection: The Inode Map
==43.5 通过间接解决：索引节点映射 (Inode Map)==

43.6 Completing The Solution: The Checkpoint Region
==43.6 完善解决方案：检查点区域 (Checkpoint Region)==

43.7 Reading A File From Disk: A Recap
==43.7 从磁盘读取文件：回顾==

43.8 What About Directories?
==43.8 目录怎么办？==

43.9 A New Problem: Garbage Collection
==43.9 一个新问题：垃圾回收==

43.10 Determining Block Liveness
==43.10 确定块的活跃度==

43.11 A Policy Question: Which Blocks To Clean, And When?
==43.11 一个策略问题：清理哪些块，以及何时清理？==

43.12 Crash Recovery And The Log
==43.12 崩溃恢复和日志==

43.13 Summary
==43.13 总结==

**44 Flash-based SSDs**
==44 基于闪存的 SSD==

44.1 Storing a Single Bit
==44.1 存储单个位==

44.2 From Bits to Banks/Planes
==44.2 从位到 Bank/Plane==

44.3 Basic Flash Operations
==44.3 基本闪存操作==

44.4 Flash Performance And Reliability
==44.4 闪存性能和可靠性==

44.5 From Raw Flash to Flash-Based SSDs
==44.5 从原始闪存到基于闪存的 SSD==

44.6 FTL Organization: A Bad Approach
==44.6 FTL 组织结构：一种糟糕的方法==

44.7 A Log-Structured FTL
==44.7 日志结构 FTL==

44.8 Garbage Collection
==44.8 垃圾回收==

44.9 Mapping Table Size
==44.9 映射表大小==

44.10 Wear Leveling
==44.10 磨损均衡==

44.11 SSD Performance And Cost
==44.11 SSD 性能和成本==

44.12 Summary
==44.12 总结==

**45 Data Integrity and Protection**
==45 数据完整性和保护==

45.1 Disk Failure Modes
==45.1 磁盘故障模式==

45.2 Handling Latent Sector Errors
==45.2 处理潜在扇区错误==

45.3 Detecting Corruption: The Checksum
==45.3 检测损坏：校验和==

45.4 Using Checksums
==45.4 使用校验和==

45.5 A New Problem: Misdirected Writes
==45.5 一个新问题：误导写入 (Misdirected Writes)==

45.6 One Last Problem: Lost Writes
==45.6 最后一个问题：丢失写入 (Lost Writes)==

45.7 Scrubbing
==45.7 清洗 (Scrubbing)==

45.8 Overheads Of Checksumming
==45.8 校验和的开销==

45.9 Summary
==45.9 总结==

46 Summary Dialogue on Persistence
==46 关于持久性的总结对话==

47 A Dialogue on Distribution
==47 关于分布式的对话==

48 Distributed Systems
==48 分布式系统==

48.1 Communication Basics
==48.1 通信基础==

48.2 Unreliable Communication Layers
==48.2 不可靠通信层==

48.3 Reliable Communication Layers
==48.3 可靠通信层==

48.4 Communication Abstractions
==48.4 通信抽象==

48.5 Remote Procedure Call (RPC)
==48.5 远程过程调用 (RPC)==

48.6 Summary
==48.6 总结==

**49 Sun's Network File System (NFS)**
==49 Sun 的网络文件系统 (NFS)==

49.1 A Basic Distributed File System
==49.1 一个基本的分布式文件系统==

49.2 On To NFS
==49.2 进入 NFS==

49.3 Focus: Simple And Fast Server Crash Recovery
==49.3 重点：简单快速的服务器崩溃恢复==

49.4 Key To Fast Crash Recovery: Statelessness
==49.4 快速崩溃恢复的关键：无状态性==

49.5 The NFSv2 Protocol
==49.5 NFSv2 协议==

49.6 From Protocol To Distributed File System
==49.6 从协议到分布式文件系统==

49.7 Handling Server Failure With Idempotent Operations
==49.7 使用幂等操作处理服务器故障==

49.8 Improving Performance: Client-side Caching
==49.8 提高性能：客户端缓存==

49.9 The Cache Consistency Problem
==49.9 缓存一致性问题==

49.10 Assessing NFS Cache Consistency
==49.10 评估 NFS 缓存一致性==

49.11 Implications On Server-Side Write Buffering
==49.11 对服务器端写入缓冲的影响==

49.12 Summary
==49.12 总结==

**50 The Andrew File System (AFS)**
==50 Andrew 文件系统 (AFS)==

50.1 AFS Version 1
==50.1 AFS 版本 1==

50.2 Problems with Version 1
==50.2 版本 1 的问题==

50.3 Improving the Protocol
==50.3 改进协议==

50.4 AFS Version 2
==50.4 AFS 版本 2==

50.5 Cache Consistency
==50.5 缓存一致性==

50.6 Crash Recovery
==50.6 崩溃恢复==

50.7 Scale And Performance Of AFSv2
==50.7 AFSv2 的规模和性能==

50.8 AFS: Other Improvements
==50.8 AFS：其他改进==

50.9 Summary
==50.9 总结==

51 Summary Dialogue on Distribution
==51 关于分布式的总结对话==

General Index
==总索引==

Asides
==旁白==

Tips
==提示==

Cruces
==关键问题==

**1 A Dialogue on the Book**
==1 关于本书的对话==

Professor: Welcome to this book!
==教授：欢迎阅读本书！==

It's called **Operating Systems in Three Easy Pieces**, and I am here to teach you the things you need to know about operating systems.
==它的名字叫《**操作系统导论**》（**Operating Systems in Three Easy Pieces**），我在这里是为了教你关于操作系统需要了解的知识。==

I am called "Professor"; who are you?
==我被称为“教授”；你是谁？==

Student: Hi Professor!
==学生：嗨，教授！==

I am called "Student", as you might have guessed.
==正如你可能猜到的那样，我被称为“学生”。==

And I am here and ready to learn!
==我已经准备好学习了！==

Professor: Sounds good. Any questions?
==教授：听起来不错。有什么问题吗？==

Student: Sure! Why is it called "Three Easy Pieces"?
==学生：当然！为什么叫“三个简单的部分”？==

Professor: That's an easy one.
==教授：这很容易回答。==

Well, you see, there are these great lectures on Physics by Richard Feynman...
==嗯，你看，理查德·费曼有一些关于物理学的精彩讲座……==

Student: Oh! The guy who wrote "Surely You're Joking, Mr. Feynman", right?
==学生：哦！就是写《别闹了，费曼先生》的那个人，对吧？==

Great book!
==很棒的书！==

Is this going to be hilarious like that book was?
==这本书会像那本书一样幽默吗？==

Professor: Um... well, no.
==教授：呃……好吧，不是。==

That book was great, and I'm glad you've read it.
==那本书很棒，我很高兴你读过。==

Hopefully this book is more like his notes on Physics.
==希望这本书更像他的物理学讲义。==

Some of the basics were summed up in a book called "Six Easy Pieces".
==一些基础知识被总结在一本名为《物理之美：费曼物理学讲义入门选》（Six Easy Pieces）的书中。==

He was talking about Physics; we're going to do Three Easy Pieces on the fine topic of Operating Systems.
==他谈论的是物理学；我们将针对操作系统这个精妙的主题进行“三个简单的部分”的讲解。==

This is appropriate, as Operating Systems are about half as hard as Physics.
==这很合适，因为操作系统的难度大约是物理学的一半。==

Student: Well, I liked physics, so that is probably good.
==学生：嗯，我喜欢物理，所以这大概是件好事。==

What are those pieces?
==那些部分是什么？==

Professor: They are the three key ideas we're going to learn about: virtualization, concurrency, and persistence.
==教授：它们是我们将要学习的三个关键思想：虚拟化、并发和持久性。==

In learning about these ideas, we'll learn all about how an operating system works, including how it decides what program to run next on a CPU, how it handles memory overload in a virtual memory system, how virtual machine monitors work, how to manage information on disks, and even a little about how to build a distributed system that works when parts have failed.
==在学习这些思想的过程中，我们将全面了解操作系统是如何工作的，包括它如何决定接下来在 CPU 上运行哪个程序，如何在虚拟内存系统中处理内存过载，虚拟机监视器如何工作，如何管理磁盘上的信息，甚至还会涉及一点关于如何构建在部分组件失效时仍能工作的分布式系统。==

That sort of stuff.
==诸如此类的事情。==

Student: I have no idea what you're talking about, really.
==学生：说实话，我完全不知道你在说什么。==

Professor: Good! That means you are in the right class.
==教授：很好！这意味着你来对课堂了。==

Student: I have another question: what's the best way to learn this stuff?
==学生：我还有一个问题：学习这些东西的最好方法是什么？==

Professor: Excellent query!
==教授：极好的问题！==

Well, each person needs to figure this out on their own, of course, but here is what I would do: go to class, to hear the professor introduce the material.
==当然，每个人都需要自己找到适合的方法，但我会这样做：去上课，听教授介绍材料。==

Then, at the end of every week, read these notes, to help the ideas sink into your head a bit better.
==然后，在每周结束时，阅读这些笔记，以帮助这些思想更好地深入你的脑海。==

Of course, some time later (hint: before the exam!), read the notes again to firm up your knowledge.
==当然，过段时间（提示：在考试前！），再次阅读笔记以巩固你的知识。==

Of course, your professor will no doubt assign some homeworks and projects, so you should do those; in particular, doing projects where you write real code to solve real problems is the best way to put the ideas within these notes into action.
==当然，你的教授毫无疑问会布置一些家庭作业和项目，所以你应该完成它们；特别是，做那些编写真实代码来解决实际问题的项目，是将这些笔记中的思想付诸实践的最佳方式。==

As Confucius said...
==正如孔子所说……==

Student: Oh, I know!
==学生：哦，我知道！==

'I hear and I forget. I see and I remember. I do and I understand.'
==“不闻不若闻之，闻之不若见之，见之不若知之，知之不若行之。”（此处原文引用英文俗语，对应中文典故为荀子《儒效》篇，非孔子）==

Or something like that.
==或者类似的说法。==

Professor: (surprised) How did you know what I was going to say?!
==教授：（惊讶）你怎么知道我要说什么？！==

Student: It seemed to follow.
==学生：这似乎是顺理成章的。==

Also, I am a big fan of Confucius, and an even bigger fan of Xunzi, who actually is a better source for this quote.
==而且，我是孔子的超级粉丝，更是荀子的超级粉丝，他实际上才是这句话更好的出处。==

Professor: (stunned) Well, I think we are going to get along just fine!
==教授：（惊呆了）好吧，我想我们会相处得很好！==

Just fine indeed.
==确实会很好。==

Student: Professor - just one more question, if I may.
==学生：教授——如果可以的话，还有一个问题。==

What are these dialogues for?
==这些对话是用来做什么的？==

I mean, isn't this just supposed to be a book?
==我的意思是，这不应该只是一本书吗？==

Why not present the material directly?
==为什么不直接展示材料呢？==

Professor: Ah, good question, good question!
==教授：啊，好问题，好问题！==

Well, I think it is sometimes useful to pull yourself outside of a narrative and think a bit; these dialogues are those times.
==嗯，我认为有时把自己从叙述中抽离出来思考一下是有用的；这些对话就是这样的时刻。==

So you and I are going to work together to make sense of all of these pretty complex ideas.
==所以你和我将一起努力理清所有这些相当复杂的思想。==

Are you up for it?
==你准备好了吗？==

Student: So we have to think?
==学生：所以我们必须思考？==

Well, I'm up for that.
==好吧，我准备好了。==

I mean, what else do I have to do anyhow?
==我的意思是，反正我还能做什么呢？==

It's not like I have much of a life outside of this book.
==我就像在这本书之外没什么生活一样。==

Professor: Me neither, sadly.
==教授：遗憾的是，我也一样。==

So let's get to work!
==所以让我们开始工作吧！==

**2 Introduction to Operating Systems**
==2 操作系统介绍==

If you are taking an undergraduate operating systems course, you should already have some idea of what a computer program does when it runs.
==如果你正在修读本科操作系统课程，你应该已经对计算机程序运行时做什么有了一些概念。==

If not, this book (and the corresponding course) is going to be difficult so you should probably stop reading this book, or run to the nearest bookstore and quickly consume the necessary background material before continuing (both Patt & Patel and Bryant & O'Hallaron are pretty great books).
==如果没有，这本书（以及相应的课程）将会很难，所以你可能应该停止阅读这本书，或者跑到最近的书店，快速消化必要的背景材料再继续（Patt & Patel 以及 Bryant & O'Hallaron 的书都很棒）。==

So what happens when a program runs?
==那么当程序运行时会发生什么呢？==

Well, a running program does one very simple thing: it executes instructions.
==嗯，一个正在运行的程序做一件非常简单的事情：它执行指令。==

Many millions (and these days, even billions) of times every second, the processor fetches an instruction from memory, decodes it (i.e., figures out which instruction this is), and executes it (i.e., it does the thing that it is supposed to do, like add two numbers together, access memory, check a condition, jump to a function, and so forth).
==每秒数百万次（现在甚至数十亿次），处理器从内存中取出一鸣指令，解码它（即弄清楚这是哪条指令），并执行它（即做它应该做的事情，比如将两个数字相加、访问内存、检查条件、跳转到函数等等）。==

After it is done with this instruction, the processor moves on to the next instruction, and so on, and so on, until the program finally completes.
==在完成这条指令后，处理器继续执行下一条指令，依此类推，直到程序最终完成。==

Thus, we have just described the basics of the Von Neumann model of computing.
==因此，我们要刚刚描述了冯·诺依曼计算模型的基础知识。==

Sounds simple, right?
==听起来很简单，对吧？==

But in this class, we will be learning that while a program runs, a lot of other wild things are going on with the primary goal of making the system easy to use.
==但是在这门课上，我们将了解到，当程序运行时，还会发生很多其他疯狂的事情，其主要目标是使系统易于使用。==

There is a body of software, in fact, that is responsible for making it easy to run programs (even allowing you to seemingly run many at the same time), allowing programs to share memory, enabling programs to interact with devices, and other fun stuff like that.
==事实上，有一套软件负责使运行程序变得容易（甚至允许你看起来同时运行许多程序），允许程序共享内存，使程序能够与设备交互，以及其他类似有趣的事情。==

That body of software is called the **operating system** (OS), as it is in charge of making sure the system operates correctly and efficiently in an easy-to-use manner.
==这套软件被称为**操作系统** (OS)，因为它负责确保系统以易于使用的方式正确高效地运行。==

**The Crux of the Problem: How to Virtualize Resources**
==**问题的关键：如何虚拟化资源**==

One central question we will answer in this book is quite simple: how does the operating system virtualize resources?
==我们将在本书中回答的一个核心问题非常简单：操作系统如何虚拟化资源？==

This is the crux of our problem.
==这是我们问题的关键。==

Why the OS does this is not the main question, as the answer should be obvious: it makes the system easier to use.
==操作系统为什么这样做不是主要问题，因为答案应该是显而易见的：它使系统更易于使用。==

Thus, we focus on the **how**: what mechanisms and policies are implemented by the OS to attain virtualization?
==因此，我们要关注**如何做**：操作系统实施了哪些机制和策略来实现虚拟化？==

How does the OS do so efficiently?
==操作系统如何高效地做到这一点？==

What hardware support is needed?
==需要什么硬件支持？==

We will use the "crux of the problem", in shaded boxes such as this one, as a way to call out specific problems we are trying to solve in building an operating system.
==我们将使用“问题的关键”（如本框所示的阴影框）来指出我们在构建操作系统时试图解决的具体问题。==

Thus, within a note on a particular topic, you may find one or more cruces (yes, this is the proper plural) which highlight the problem.
==因此，在关于特定主题的笔记中，你可能会发现一个或多个 cruces（是的，这是 crux 的正确复数形式），它们突出了问题。==

The details within the chapter, of course, present the solution, or at least the basic parameters of a solution.
==当然，章节中的细节会提供解决方案，或者至少是解决方案的基本参数。==

The primary way the OS does this is through a general technique that we call **virtualization**.
==操作系统执行此操作的主要方式是通过一种我们要称为**虚拟化**的通用技术。==

That is, the OS takes a physical resource (such as the processor, or memory, or a disk) and transforms it into a more general, powerful, and easy-to-use virtual form of itself.
==也就是说，操作系统获取物理资源（如处理器、内存或磁盘）并将其转换为更通用、更强大且更易于使用的虚拟形式。==

Thus, we sometimes refer to the operating system as a **virtual machine**.
==因此，我们要有时将操作系统称为**虚拟机**。==

Of course, in order to allow users to tell the OS what to do and thus make use of the features of the virtual machine (such as running a program, or allocating memory, or accessing a file), the OS also provides some interfaces (APIs) that you can call.
==当然，为了允许用户告诉操作系统要做什么，从而利用虚拟机的特性（如运行程序、分配内存或访问文件），操作系统还提供了一些你可以调用的接口 (API)。==

A typical OS, in fact, exports a few hundred **system calls** that are available to applications.
==事实上，典型的操作系统会向应用程序导出几百个**系统调用**。==

Because the OS provides these calls to run programs, access memory and devices, and other related actions, we also sometimes say that the OS provides a **standard library** to applications.
==因为操作系统提供这些调用来运行程序、访问内存和设备以及其他相关操作，我们要有时也说操作系统为应用程序提供了一个**标准库**。==

Finally, because virtualization allows many programs to run (thus sharing the CPU), and many programs to concurrently access their own instructions and data (thus sharing memory), and many programs to access devices (thus sharing disks and so forth), the OS is sometimes known as a **resource manager**.
==最后，因为虚拟化允许许多程序运行（从而共享 CPU），许多程序并发访问它们自己的指令和数据（从而共享内存），以及许多程序访问设备（从而共享磁盘等），操作系统有时被称为**资源管理器**。==

Each of the CPU, memory, and disk is a **resource** of the system; it is thus the operating system's role to manage those resources, doing so efficiently or fairly or indeed with many other possible goals in mind.
==CPU、内存和磁盘中的每一个都是系统的**资源**；因此，操作系统的角色是管理这些资源，以高效、公平或实际上许多其他可能的目标来做这件事。==

To understand the role of the OS a little bit better, let's take a look at some examples.
==为了更好地理解操作系统的角色，让我们看一些例子。==

**2.1 Virtualizing The CPU**
==2.1 CPU 虚拟化==

Figure 2.1 depicts our first program.
==图 2.1 描绘了我们的第一个程序。==

It doesn't do much.
==它没做什么事。==

In fact, all it does is call `Spin()`, a function that repeatedly checks the time and returns once it has run for a second.
==事实上，它所做的只是调用 `Spin()`，这是一个反复检查时间并在运行一秒钟后返回的函数。==

Then, it prints out the string that the user passed in on the command line, and repeats, forever.
==然后，它打印出用户在命令行上传入的字符串，并永远重复。==

Let's say we save this file as `cpu.c` and decide to compile and run it on a system with a single processor (or CPU as we will sometimes call it).
==假设我们将此文件保存为 `cpu.c`，并决定在具有单个处理器（或者我们有时称之为 CPU）的系统上编译并运行它。==

Here is what we will see:
==这是我们将看到的：==

`prompt> gcc -o cpu cpu.c -Wall`
`prompt> ./cpu "A"`
A
A
A
A
^C
`prompt>`

Not too interesting of a run - the system begins running the program, which repeatedly checks the time until a second has elapsed.
==运行起来不太有趣——系统开始运行程序，程序反复检查时间直到一秒钟过去。==

Once a second has passed, the code prints the input string passed in by the user (in this example, the letter "A"), and continues.
==一旦一秒钟过去，代码就会打印用户传入的输入字符串（在本例中为字母 "A"），然后继续。==

Note the program will run forever; by pressing "Control-c" (which on UNIX-based systems will terminate the program running in the foreground) we can halt the program.
==注意程序将永远运行；通过按 "Control-c"（在基于 UNIX 的系统上将终止在前台运行的程序），我们可以停止程序。==

Now, let's do the same thing, but this time, let's run many different instances of this same program.
==现在，让我们做同样的事情，但这次，我们要运行同一个程序的许多不同实例。==

Figure 2.2 shows the results of this slightly more complicated example.
==图 2.2 显示了这个稍微复杂一点的例子的结果。==

`prompt> ./cpu A & ./cpu B & ./cpu C & ./cpu D &`
`[1] 7353`
`[2] 7354`
`[3] 7355`
`[4] 7356`
A
B
D
C
A
B
D
C
A
...

Well, now things are getting a little more interesting.
==嗯，现在事情变得更有趣了一点。==

Even though we have only one processor, somehow all four of these programs seem to be running at the same time!
==即使我们只有一个处理器，但这四个程序似乎都在同时运行！==

How does this magic happen?
==这种魔法是如何发生的？==

It turns out that the operating system, with some help from the hardware, is in charge of this illusion, i.e., the illusion that the system has a very large number of virtual CPUs.
==原来是操作系统在硬件的帮助下负责这种错觉，即系统拥有大量虚拟 CPU 的错觉。==

Turning a single CPU (or a small set of them) into a seemingly infinite number of CPUs and thus allowing many programs to seemingly run at once is what we call **virtualizing the CPU**, the focus of the first major part of this book.
==将单个 CPU（或一小组 CPU）变成看似无限数量的 CPU，从而允许许多程序看似同时运行，这就是我们要称为**虚拟化 CPU** 的技术，这也是本书第一主要部分的重点。==

Of course, to run programs, and stop them, and otherwise tell the OS which programs to run, there need to be some interfaces (APIs) that you can use to communicate your desires to the OS.
==当然，为了运行程序、停止程序以及以其他方式告诉操作系统运行哪些程序，需要有一些接口 (API) 供你用来与操作系统交流你的意愿。==

We'll talk about these APIs throughout this book; indeed, they are the major way in which most users interact with operating systems.
==我们将在整本书中讨论这些 API；实际上，它们是大多数用户与操作系统交互的主要方式。==

You might also notice that the ability to run multiple programs at once raises all sorts of new questions.
==你可能还会注意到，同时运行多个程序的能力引发了各种新问题。==

For example, if two programs want to run at a particular time, which should run?
==例如，如果两个程序想在特定时间运行，应该运行哪一个？==

This question is answered by a **policy** of the OS; policies are used in many different places within an OS to answer these types of questions, and thus we will study them as we learn about the basic mechanisms that operating systems implement (such as the ability to run multiple programs at once).
==这个问题由操作系统的**策略**回答；策略在操作系统内的许多不同地方用于回答此类问题，因此我们将在学习操作系统实施的基本机制（如同时运行多个程序的能力）时研究它们。==

Hence the role of the OS as a resource manager.
==因此，这也是操作系统作为资源管理器的角色。==

**2.2 Virtualizing Memory**
==2.2 虚拟化内存==

Now let's consider memory.
==现在让我们考虑内存。==

The model of physical memory presented by modern machines is very simple.
==现代机器呈现的物理内存模型非常简单。==

Memory is just an array of bytes; to read memory, one must specify an address to be able to access the data stored there; to write (or update) memory, one must also specify the data to be written to the given address.
==内存只是一个字节数组；要读取内存，必须指定一个地址才能访问存储在那里的数据；要写入（或更新）内存，还必须指定要写入给定地址的数据。==

Memory is accessed all the time when a program is running.
==程序运行时一直都在访问内存。==

A program keeps all of its data structures in memory, and accesses them through various instructions, like loads and stores or other explicit instructions that access memory in doing their work.
==程序将其所有数据结构保存在内存中，并通过各种指令访问它们，例如加载和存储或其他在工作时访问内存的显式指令。==

Don't forget that each instruction of the program is in memory too; thus memory is accessed on each instruction fetch.
==别忘了程序的每一条指令也在内存中；因此，每次获取指令时都会访问内存。==

Let's take a look at a program (in Figure 2.3) that allocates some memory by calling `malloc()`.
==让我们看一个程序（在图 2.3 中），它通过调用 `malloc()` 分配一些内存。==

The output of this program can be found here:
==该程序的输出如下：==

`prompt> ./mem`
`(2134) address pointed to by p: 0x200000`
`(2134) p: 1`
`(2134) p: 2`
`(2134) p: 3`
`(2134) p: 4`
`(2134) p: 5`
^C

The program does a couple of things.
==程序做了几件事。==

First, it allocates some memory (line a1).
==首先，它分配一些内存（a1 行）。==

Then, it prints out the address of the memory (a2), and then puts the number zero into the first slot of the newly allocated memory (a3).
==然后，它打印出内存的地址 (a2)，并将数字零放入新分配内存的第一个槽中 (a3)。==

Finally, it loops, delaying for a second and incrementing the value stored at the address held in p.
==最后，它循环，延迟一秒钟并增加存储在 p 中保存的地址处的值。==

With every print statement, it also prints out what is called the process identifier (the PID) of the running program.
==对于每个打印语句，它还会打印出正在运行的程序的进程标识符 (PID)。==

This PID is unique per running process.
==每个运行进程的 PID 都是唯一的。==

Again, this first result is not too interesting.
==同样，这第一个结果并不是太有趣。==

The newly allocated memory is at address 0x200000.
==新分配的内存位于地址 0x200000。==

As the program runs, it slowly updates the value and prints out the result.
==随着程序的运行，它会缓慢更新该值并打印出结果。==

Now, we again run multiple instances of this same program to see what happens (Figure 2.4).
==现在，我们要再次运行同一个程序的多个实例来看看会发生什么（图 2.4）。==

`prompt> ./mem & ./mem &`
`[1] 24113`
`[2] 24114`
`(24113) address pointed to by p: 0x200000`
`(24114) address pointed to by p: 0x200000`
`(24113) p: 1`
`(24114) p: 1`
`(24114) p: 2`
`(24113) p: 2`
...

We see from the example that each running program has allocated memory at the same address (0x200000), and yet each seems to be updating the value at 0x200000 independently!
==我们要从示例中看到，每个运行的程序都在相同的地址 (0x200000) 分配了内存，但每个程序似乎都在独立地更新 0x200000 处的值！==

It is as if each running program has its own private memory, instead of sharing the same physical memory with other running programs.
==这就好像每个运行的程序都有自己的私有内存，而不是与其他运行的程序共享相同的物理内存。==

Indeed, that is exactly what is happening here as the OS is **virtualizing memory**.
==事实上，这正是这里发生的事情，因为操作系统正在**虚拟化内存**。==

Each process accesses its own private **virtual address space** (sometimes just called its **address space**), which the OS somehow maps onto the physical memory of the machine.
==每个进程访问其自己的私有**虚拟地址空间**（有时简称为**地址空间**），操作系统以某种方式将其映射到机器的物理内存上。==

A memory reference within one running program does not affect the address space of other processes (or the OS itself); as far as the running program is concerned, it has physical memory all to itself.
==一个正在运行的程序中的内存引用不会影响其他进程（或操作系统本身）的地址空间；就正在运行的程序而言，它完全拥有物理内存。==

The reality, however, is that physical memory is a shared resource, managed by the operating system.
==然而，现实情况是物理内存是由操作系统管理的共享资源。==

Exactly how all of this is accomplished is also the subject of the first part of this book, on the topic of virtualization.
==所有这一切究竟是如何完成的也是本书第一部分的主题，即关于虚拟化的主题。==

**2.3 Concurrency**
==2.3 并发==

Another main theme of this book is **concurrency**.
==本书的另一个主要主题是**并发**。==

We use this conceptual term to refer to a host of problems that arise, and must be addressed, when working on many things at once (i.e., concurrently) in the same program.
==我们要使用这个概念性术语来指代在同一个程序中同时（即并发）处理许多事情时出现且必须解决的一系列问题。==

The problems of concurrency arose first within the operating system itself; as you can see in the examples above on virtualization, the OS is juggling many things at once, first running one process, then another, and so forth.
==并发问题首先出现在操作系统本身内部；正如你在上面关于虚拟化的示例中看到的那样，操作系统同时处理许多事情，先运行一个进程，然后运行另一个进程，依此类推。==

As it turns out, doing so leads to some deep and interesting problems.
==事实证明，这样做会导致一些深刻而有趣的问题。==

Unfortunately, the problems of concurrency are no longer limited just to the OS itself.
==不幸的是，并发问题不再仅限于操作系统本身。==

Indeed, modern multi-threaded programs exhibit the same problems.
==事实上，现代多线程程序也表现出同样的问题。==

Let us demonstrate with an example of a multi-threaded program (Figure 2.5).
==让我们用一个多线程程序的例子来演示（图 2.5）。==

Although you might not understand this example fully at the moment (and we'll learn a lot more about it in later chapters, in the section of the book on concurrency), the basic idea is simple.
==虽然你现在可能不完全理解这个例子（我们要将在后面的章节中，在本书关于并发的部分学到更多），但基本思想很简单。==

The main program creates two **threads** using `Pthread_create()`.
==主程序使用 `Pthread_create()` 创建两个**线程**。==

You can think of a thread as a function running within the same memory space as other functions, with more than one of them active at a time.
==你可以将线程视为在与其他函数相同的内存空间中运行的函数，并且一次有多个线程处于活动状态。==

In this example, each thread starts running in a routine called `worker()`, in which it simply increments a counter in a loop for `loops` number of times.
==在此示例中，每个线程都开始在一个名为 `worker()` 的例程中运行，在该例程中，它只是在一个循环中将计数器递增 `loops` 次。==

Below is a transcript of what happens when we run this program with the input value for the variable `loops` set to 1000.
==下面是当我们将变量 `loops` 的输入值设置为 1000 时运行此程序发生的情况的记录。==

The value of `loops` determines how many times each of the two workers will increment the shared counter in a loop.
==`loops` 的值决定了两个工作线程各自将在循环中递增共享计数器多少次。==

When the program is run with the value of `loops` set to 1000, what do you expect the final value of counter to be?
==当程序在 `loops` 值设置为 1000 的情况下运行时，你预计计数器的最终值是多少？==

`prompt> gcc -o threads threads.c -Wall -pthread`
`prompt> ./threads 1000`
`Initial value: 0`
`Final value : 2000`

As you probably guessed, when the two threads are finished, the final value of the counter is 2000, as each thread incremented the counter 1000 times.
==正如你可能猜到的那样，当两个线程完成时，计数器的最终值为 2000，因为每个线程都将计数器递增了 1000 次。==

Indeed, when the input value of `loops` is set to N, we would expect the final output of the program to be 2N.
==实际上，当 `loops` 的输入值设置为 N 时，我们要期望程序的最终输出为 2N。==

But life is not so simple, as it turns out.
==但事实证明，生活并非如此简单。==

Let's run the same program, but with higher values for `loops`, and see what happens:
==让我们运行相同的程序，但使用更大的 `loops` 值，看看会发生什么：==

`prompt> ./threads 100000`
`Initial value: 0`
`Final value : 143012`
`prompt> ./threads 100000`
`Initial value: 0`
`Final value : 137298`
// huh??
// what the??

In this run, when we gave an input value of 100,000, instead of getting a final value of 200,000, we instead first get 143,012.
==在这次运行中，当我们给出 100,000 的输入值时，我要没有得到 200,000 的最终值，而是首先得到了 143,012。==

Then, when we run the program a second time, we not only again get the wrong value, but also a different value than the last time.
==然后，当我们第二次运行该程序时，我们不仅再次得到了错误的值，而且还得到了与上次不同的值。==

In fact, if you run the program over and over with high values of `loops`, you may find that sometimes you even get the right answer!
==事实上，如果你用很大的 `loops` 值反复运行该程序，你可能会发现有时你甚至会得到正确的答案！==

So why is this happening?
==那么为什么会发生这种情况呢？==

As it turns out, the reason for these odd and unusual outcomes relate to how instructions are executed, which is one at a time.
==事实证明，这些奇怪而不寻常的结果的原因与指令的执行方式有关，即一次执行一条指令。==

Unfortunately, a key part of the program above, where the shared counter is incremented, takes three instructions: one to load the value of the counter from memory into a register, one to increment it, and one to store it back into memory.
==不幸的是，上面程序的关键部分（共享计数器递增的地方）需要三条指令：一条将计数器的值从内存加载到寄存器中，一条将其递增，另一条将其存回内存。==

Because these three instructions do not execute **atomically** (all at once), strange things can happen.
==因为这三条指令不是**原子地**（一次性全部）执行的，所以可能会发生奇怪的事情。==

**The Crux of the Problem: How to Build Correct Concurrent Programs**
==**问题的关键：如何构建正确的并发程序**==

When there are many concurrently executing threads within the same memory space, how can we build a correctly working program?
==当同一内存空间内有许多并发执行的线程时，我们如何构建一个正确工作的程序？==

What primitives are needed from the OS?
==需要操作系统提供什么原语？==

What mechanisms should be provided by the hardware?
==硬件应该提供什么机制？==

How can we use them to solve the problems of concurrency?
==我们如何使用它们来解决并发问题？==

It is this problem of concurrency that we will address in great detail in the second part of this book.
==我们将在本书的第二部分详细讨论这个并发问题。==

**2.4 Persistence**
==2.4 持久性==

The third major theme of the course is **persistence**.
==本课程的第三个主要主题是**持久性**。==

In system memory, data can be easily lost, as devices such as DRAM store values in a volatile manner; when power goes away or the system crashes, any data in memory is lost.
==在系统内存中，数据很容易丢失，因为像 DRAM 这样的设备以易失的方式存储值；当断电或系统崩溃时，内存中的任何数据都会丢失。==

Thus, we need hardware and software to be able to store data **persistently**; such storage is thus critical to any system as users care a great deal about their data.
==因此，我们需要硬件和软件能够**持久地**存储数据；这种存储因此对任何系统都至关重要，因为用户非常关心他们的数据。==

The hardware comes in the form of some kind of input/output or I/O device; in modern systems, a hard drive is a common repository for long-lived information, although solid-state drives (SSDs) are making headway in this arena as well.
==硬件以某种输入/输出或 I/O 设备的形式出现；在现代系统中，硬盘驱动器是长期信息的常见存储库，尽管固态驱动器 (SSD) 也在该领域取得进展。==

The software in the operating system that usually manages the disk is called the **file system**; it is thus responsible for storing any files the user creates in a reliable and efficient manner on the disks of the system.
==通常管理磁盘的操作系统软件称为**文件系统**；因此，它负责以可靠和高效的方式将用户创建的任何文件存储在系统的磁盘上。==

Unlike the abstractions provided by the OS for the CPU and memory, the OS does not create a private, virtualized disk for each application.
==与操作系统为 CPU 和内存提供的抽象不同，操作系统不会为每个应用程序创建一个私有的虚拟化磁盘。==

Rather, it is assumed that often times, users will want to share information that is in files.
==相反，人们假设用户通常希望共享文件中的信息。==

For example, when writing a C program, you might first use an editor (e.g., Emacs) to create and edit the C file (`emacs -nw main.c`).
==例如，在编写 C 程序时，你可能首先使用编辑器（如 Emacs）创建和编辑 C 文件 (`emacs -nw main.c`)。==

Once done, you might use the compiler to turn the source code into an executable (e.g., `gcc -o main main.c`).
==完成后，你可能会使用编译器将源代码转换为可执行文件（例如 `gcc -o main main.c`）。==

When you're finished, you might run the new executable (e.g., `./main`).
==完成后，你可能会运行新的可执行文件（例如 `./main`）。==

Thus, you can see how files are shared across different processes.
==因此，你可以看到文件是如何在不同进程之间共享的。==

First, Emacs creates a file that serves as input to the compiler; the compiler uses that input file to create a new executable file (in many steps - take a compiler course for details); finally, the new executable is then run.
==首先，Emacs 创建一个文件作为编译器的输入；编译器使用该输入文件创建一个新的可执行文件（分许多步骤——详情请参加编译器课程）；最后，运行新的可执行文件。==

And thus a new program is born!
==就这样，一个新程序诞生了！==

To understand this better, let's look at some code.
==为了更好地理解这一点，让我们看一些代码。==

Figure 2.6 presents code to create a file (`/tmp/file`) that contains the string "hello world".
==图 2.6 展示了创建一个包含字符串 "hello world" 的文件 (`/tmp/file`) 的代码。==

To accomplish this task, the program makes three calls into the operating system.
==为了完成此任务，程序向操作系统发出了三个调用。==

The first, a call to `open()`, opens the file and creates it; the second, `write()`, writes some data to the file; the third, `close()`, simply closes the file thus indicating the program won't be writing any more data to it.
==第一个调用 `open()` 打开并创建文件；第二个调用 `write()` 将一些数据写入文件；第三个调用 `close()` 只是关闭文件，从而表明程序不会再向其写入任何数据。==

These system calls are routed to the part of the operating system called the file system, which then handles the requests and returns some kind of error code to the user.
==这些系统调用被路由到操作系统中称为文件系统的部分，然后由文件系统处理请求并将某种错误代码返回给用户。==

You might be wondering what the OS does in order to actually write to disk.
==你可能想知道操作系统为了实际写入磁盘做了什么。==

We would show you but you'd have to promise to close your eyes first; it is that unpleasant.
==我们会给你看，但你得先保证闭上眼睛；那场面很不愉快。==

The file system has to do a fair bit of work: first figuring out where on disk this new data will reside, and then keeping track of it in various structures the file system maintains.
==文件系统必须做相当多的工作：首先弄清楚这个新数据将驻留在磁盘上的什么位置，然后在文件系统维护的各种结构中跟踪它。==

Doing so requires issuing I/O requests to the underlying storage device, to either read existing structures or update (write) them.
==这样做需要向底层存储设备发出 I/O 请求，以读取现有结构或更新（写入）它们。==

As anyone who has written a device driver knows, getting a device to do something on your behalf is an intricate and detailed process.
==任何写过设备驱动程序的人都知道，让设备代表你做某事是一个复杂而详细的过程。==

It requires a deep knowledge of the low-level device interface and its exact semantics.
==它需要对低级设备接口及其确切语义有深入的了解。==

Fortunately, the OS provides a standard and simple way to access devices through its system calls.
==幸运的是，操作系统提供了一种通过其系统调用访问设备的标准且简单的方法。==

Thus, the OS is sometimes seen as a standard library.
==因此，操作系统有时被视为标准库。==

Of course, there are many more details in how devices are accessed, and how file systems manage data persistently atop said devices.
==当然，关于如何访问设备以及文件系统如何在所述设备之上持久地管理数据，还有更多细节。==

For performance reasons, most file systems first delay such writes for a while, hoping to batch them into larger groups.
==出于性能原因，大多数文件系统首先将此类写入延迟一段时间，希望能将它们分批处理成更大的组。==

To handle the problems of system crashes during writes, most file systems incorporate some kind of intricate write protocol, such as **journaling** or **copy-on-write**, carefully ordering writes to disk to ensure that if a failure occurs during the write sequence, the system can recover to reasonable state afterwards.
==为了处理写入期间系统崩溃的问题，大多数文件系统都采用了某种复杂的写入协议，例如**日志记录**或**写时复制**，仔细排序对磁盘的写入，以确如果在写入序列期间发生故障，系统之后可以恢复到合理的状态。==

**The Crux of the Problem: How to Store Data Persistently**
==**问题的关键：如何持久地存储数据**==

The file system is the part of the OS in charge of managing persistent data.
==文件系统是操作系统中负责管理持久数据的部分。==

What techniques are needed to do so correctly?
==需要什么技术才能正确地做到这一点？==

What mechanisms and policies are required to do so with high performance?
==需要什么机制和策略才能以高性能做到这一点？==

How is reliability achieved, in the face of failures in hardware and software?
==面对硬件和软件故障，如何实现可靠性？==

To make different common operations efficient, file systems employ many different data structures and access methods, from simple lists to complex b-trees.
==为了使不同的常见操作高效，文件系统采用了许多不同的数据结构和访问方法，从简单的列表到复杂的 B 树。==

If all of this doesn't make sense yet, good!
==如果所有这些还没有意义，很好！==

We'll be talking about all of this quite a bit more in the third part of this book on persistence, where we'll discuss devices and I/O in general, and then disks, RAIDs, and file systems in great detail.
==我们要将在本书关于持久性的第三部分更多地讨论所有这些，我们将讨论一般的设备和 I/O，然后详细讨论磁盘、RAID 和文件系统。==

**2.5 Design Goals**
==2.5 设计目标==

So now you have some idea of what an OS actually does: it takes physical resources, such as a CPU, memory, or disk, and virtualizes them.
==现在你已经对操作系统实际做什么有了一些概念：它获取物理资源，如 CPU、内存或磁盘，并将它们虚拟化。==

It handles tough and tricky issues related to concurrency.
==它处理与并发相关的棘手问题。==

And it stores files persistently, thus making them safe over the long-term.
==它持久地存储文件，从而使它们在长期内安全。==

Given that we want to build such a system, we want to have some goals in mind to help focus our design and implementation and make trade-offs as necessary; finding the right set of trade-offs is a key to building systems.
==鉴于我们想要构建这样一个系统，我们希望心中有一些目标，以帮助我们专注于设计和实现，并在必要时进行权衡；找到正确的权衡组合是构建系统的关键。==

One of the most basic goals is to build up some **abstractions** in order to make the system convenient and easy to use.
==最基本的目标之一是建立一些**抽象**，以使系统方便易用。==

Abstractions are fundamental to everything we do in computer science.
==抽象是我们计算机科学中所做一切的基础。==

Abstraction makes it possible to write a large program by dividing it into small and understandable pieces, to write such a program in a high-level language like C without thinking about assembly, to write code in assembly without thinking about logic gates, and to build a processor out of gates without thinking too much about transistors.
==抽象使得编写大型程序成为可能，方法是将其划分为易于理解的小块；使得用 C 等高级语言编写此类程序成为可能，而无需考虑汇编；使得用汇编编写代码成为可能，而无需考虑逻辑门；使得用门构建处理器成为可能，而无需过多考虑晶体管。==

Abstraction is so fundamental that sometimes we forget its importance, but we won't here; thus, in each section, we'll discuss some of the major abstractions that have developed over time, giving you a way to think about pieces of the OS.
==抽象是如此基础，以至于有时我们会忘记它的重要性，但在这里我们不会忘记；因此，在每一节中，我们要讨论一些随着时间推移而发展起来的主要抽象，为你提供思考操作系统各个部分的方法。==

One goal in designing and implementing an operating system is to provide **high performance**; another way to say this is our goal is to **minimize the overheads** of the OS.
==设计和实现操作系统的一个目标是提供**高性能**；换句话说，我们的目标是**最小化操作系统的开销**。==

Virtualization and making the system easy to use are well worth it, but not at any cost; thus, we must strive to provide virtualization and other OS features without excessive overheads.
==虚拟化和使系统易于使用是非常值得的，但不是不计代价的；因此，我们要努力提供虚拟化和其他操作系统特性，而不会产生过多的开销。==

These overheads arise in a number of forms: extra time (more instructions) and extra space (in memory or on disk).
==这些开销以多种形式出现：额外的时间（更多的指令）和额外的空间（在内存或磁盘上）。==

We'll seek solutions that minimize one or the other or both, if possible.
==如果在可能的情况下，我们要寻求最小化其中之一或两者的解决方案。==

Perfection, however, is not always attainable, something we will learn to notice and (where appropriate) tolerate.
==然而，完美并非总能达到，我们要将学会注意到这一点，并在适当的时候容忍它。==

Another goal will be to provide **protection** between applications, as well as between the OS and applications.
==另一个目标是在应用程序之间以及操作系统和应用程序之间提供**保护**。==

Because we wish to allow many programs to run at the same time, we want to make sure that the malicious or accidental bad behavior of one does not harm others; we certainly don't want an application to be able to harm the OS itself (as that would affect all programs running on the system).
==因为我们希望允许许多程序同时运行，所以我们要确保一个程序的恶意或意外不良行为不会伤害其他程序；我们当然不希望应用程序能够伤害操作系统本身（因为那会影响系统上运行的所有程序）。==

Protection is at the heart of one of the main principles underlying an operating system, which is that of **isolation**; isolating processes from one another is the key to protection and thus underlies much of what an OS must do.
==保护是操作系统主要原则之一的核心，即**隔离**；将进程彼此隔离是保护的关键，因此也是操作系统必须做的大部分工作的基础。==

The operating system must also run non-stop; when it fails, all applications running on the system fail as well.
==操作系统还必须不间断地运行；当它出现故障时，系统上运行的所有应用程序也会失败。==

Because of this dependence, operating systems often strive to provide a high degree of **reliability**.
==由于这种依赖性，操作系统通常努力提供高度的**可靠性**。==

As operating systems grow evermore complex (sometimes containing millions of lines of code), building a reliable operating system is quite a challenge - and indeed, much of the on-going research in the field (including some of our own work) focuses on this exact problem.
==随着操作系统变得越来越复杂（有时包含数百万行代码），构建可靠的操作系统是一项相当大的挑战——事实上，该领域正在进行的许多研究（包括我们要自己的一些工作）都集中在这个确切的问题上。==

Other goals make sense: **energy-efficiency** is important in our increasingly green world; **security** (an extension of protection, really) against malicious applications is critical, especially in these highly-networked times; **mobility** is increasingly important as OSes are run on smaller and smaller devices.
==其他目标也是有意义的：在我们日益绿色的世界中，**能源效率**很重要；针对恶意应用程序的**安全性**（实际上是保护的延伸）至关重要，特别是在这个高度网络化的时代；随着操作系统在越来越小的设备上运行，**移动性**越来越重要。==

Depending on how the system is used, the OS will have different goals and thus likely be implemented in at least slightly different ways.
==根据系统的使用方式，操作系统将有不同的目标，因此可能会以至少略有不同的方式实现。==

However, as we will see, many of the principles we will present on how to build an OS are useful on a range of different devices.
==然而，正如我们将看到的，我们将介绍的关于如何构建操作系统的许多原则在一系列不同的设备上都是有用的。==

**2.6 Some History**
==2.6 一些历史==

Before closing this introduction, let us present a brief history of how operating systems developed.
==在结束本介绍之前，让我们简要介绍一下操作系统是如何发展的。==

Like any system built by humans, good ideas accumulated in operating systems over time, as engineers learned what was important in their design.
==像任何人类构建的系统一样，随着工程师们了解到设计中什么是重要的，好点子在操作系统中随时间积累。==

Here, we discuss a few major developments.
==在这里，我们要讨论几个主要的发展。==

For a richer treatment, see Brinch Hansen's excellent history of operating systems.
==要获得更丰富的内容，请参阅 Brinch Hansen 优秀的操作系统历史。==

**Early Operating Systems: Just Libraries**
==**早期操作系统：仅仅是库**==

In the beginning, the operating system didn't do too much.
==一开始，操作系统并没有做太多事情。==

Basically, it was just a set of libraries of commonly-used functions; for example, instead of having each programmer of the system write low-level I/O handling code, the "OS" would provide such APIs, and thus make life easier for the developer.
==基本上，它只是一组常用函数的库；例如，与其让系统的每个程序员编写低级 I/O 处理代码，“操作系统”会提供此类 API，从而使开发人员的生活更轻松。==

Usually, on these old mainframe systems, one program ran at a time, as controlled by a human operator.
==通常，在这些旧的大型机系统上，一次运行一个程序，由人工操作员控制。==

Much of what you think a modern OS would do (e.g., deciding what order to run jobs in) was performed by this operator.
==你认为现代操作系统会做的大部分工作（例如，决定以什么顺序运行作业）都是由这位操作员完成的。==

If you were a smart developer, you would be nice to this operator, so that they might move your job to the front of the queue.
==如果你是一个聪明的开发人员，你会对这位操作员很好，这样他们可能会把你的作业移到队列的前面。==

This mode of computing was known as **batch** processing, as a number of jobs were set up and then run in a "batch" by the operator.
==这种计算模式被称为**批处理**，因为许多作业被设置好，然后由操作员“批量”运行。==

Computers, as of that point, were not used in an interactive manner, because of cost: it was simply too expensive to let a user sit in front of the computer and use it, as most of the time it would just sit idle then, costing the facility hundreds of thousands of dollars per hour.
==在那时，计算机并不是以交互方式使用的，因为成本太高：让用户坐在计算机前使用它实在是太贵了，因为大多数时候它只是闲置着，每小时花费设施数十万美元。==

**Beyond Libraries: Protection**
==**超越库：保护**==

In moving beyond being a simple library of commonly-used services, operating systems took on a more central role in managing machines.
==在超越常用服务的简单库的过程中，操作系统在管理机器方面承担了更核心的角色。==

One important aspect of this was the realization that code run on behalf of the OS was special; it had control of devices and thus should be treated differently than normal application code.
==其中一个重要方面是意识到代表操作系统运行的代码是特殊的；它控制着设备，因此应该与普通应用程序代码区别对待。==

Why is this?
==为什么会这样？==

Well, imagine if you allowed any application to read from anywhere on the disk; the notion of privacy goes out the window, as any program could read any file.
==嗯，想象一下，如果你允许任何应用程序从磁盘上的任何位置读取；隐私的概念就会消失，因为任何程序都可以读取任何文件。==

Thus, implementing a file system (to manage your files) as a library makes little sense.
==因此，将文件系统（管理你的文件）作为库来实现是没有意义的。==

Instead, something else was needed.
==相反，需要其他东西。==

Thus, the idea of a **system call** was invented, pioneered by the Atlas computing system.
==因此，**系统调用**的想法被发明出来，由 Atlas 计算系统首创。==

Instead of providing OS routines as a library (where you just make a procedure call to access them), the idea here was to add a special pair of hardware instructions and hardware state to make the transition into the OS a more formal, controlled process.
==与其将操作系统例程作为库提供（只需进行过程调用即可访问它们），这里的想法是添加一对特殊的硬件指令和硬件状态，使进入操作系统的转换成为一个更正式、受控的过程。==

The key difference between a system call and a procedure call is that a system call transfers control (i.e., jumps) into the OS while simultaneously raising the hardware privilege level.
==系统调用和过程调用之间的关键区别在于，系统调用将控制权转移（即跳转）到操作系统，同时提高硬件特权级别。==

User applications run in what is referred to as **user mode** which means the hardware restricts what applications can do; for example, an application running in user mode can't typically initiate an I/O request to the disk, access any physical memory page, or send a packet on the network.
==用户应用程序在所谓的**用户模式**下运行，这意味着硬件限制了应用程序可以做什么；例如，在用户模式下运行的应用程序通常无法向磁盘发起 I/O 请求、访问任何物理内存页面或在网络上发送数据包。==

When a system call is initiated (usually through a special hardware instruction called a **trap**), the hardware transfers control to a pre-specified **trap handler** (that the OS set up previously) and simultaneously raises the privilege level to **kernel mode**.
==当发起系统调用时（通常通过称为**陷阱**的特殊硬件指令），硬件将控制权转移到预先指定的**陷阱处理程序**（操作系统之前设置的），同时将特权级别提高到**内核模式**。==

In kernel mode, the OS has full access to the hardware of the system and thus can do things like initiate an I/O request or make more memory available to a program.
==在内核模式下，操作系统可以完全访问系统的硬件，因此可以做诸如发起 I/O 请求或为程序提供更多内存之类的事情。==

When the OS is done servicing the request, it passes control back to the user via a special **return-from-trap** instruction, which reverts to user mode while simultaneously passing control back to where the application left off.
==当操作系统完成服务请求时，它通过特殊的**从陷阱返回**指令将控制权交还给用户，该指令恢复到用户模式，同时将控制权交还给应用程序中断的地方。==

**The Era of Multiprogramming**
==**多道程序设计时代**==

Where operating systems really took off was in the era of computing beyond the mainframe, that of the **minicomputer**.
==操作系统真正腾飞是在超越大型机的计算时代，即**小型机**时代。==

Classic machines like the PDP family from Digital Equipment made computers hugely more affordable; thus, instead of having one mainframe per large organization, now a smaller collection of people within an organization could likely have their own computer.
==像 Digital Equipment 的 PDP 系列这样的经典机器使计算机更加实惠；因此，与其每个大型组织拥有一台大型机，现在组织内的一小群人可能拥有自己的计算机。==

Not surprisingly, one of the major impacts of this drop in cost was an increase in developer activity; more smart people got their hands on computers and thus made computer systems do more interesting and beautiful things.
==毫不奇怪，成本下降的主要影响之一是开发人员活动的增加；更多的聪明人接触到了计算机，从而使计算机系统做了更多有趣和美妙的事情。==

In particular, **multiprogramming** became commonplace due to the desire to make better use of machine resources.
==特别是，由于希望更好地利用机器资源，**多道程序设计**变得普遍起来。==

Instead of just running one job at a time, the OS would load a number of jobs into memory and switch rapidly between them, thus improving CPU utilization.
==与其一次只运行一个作业，操作系统会将许多作业加载到内存中并在它们之间快速切换，从而提高 CPU 利用率。==

This switching was particularly important because I/O devices were slow; having a program wait on the CPU while its I/O was being serviced was a waste of CPU time.
==这种切换特别重要，因为 I/O 设备很慢；让程序在等待 I/O 服务时占用 CPU 是对 CPU 时间的浪费。==

Instead, why not switch to another job and run it for a while?
==相反，为什么不切换到另一个作业并运行一段时间呢？==

The desire to support multiprogramming and overlap in the presence of I/O and interrupts forced innovation in the conceptual development of operating systems along a number of directions.
==支持多道程序设计以及在 I/O 和中断存在的情况下进行重叠的愿望迫使操作系统在概念发展方面沿着多个方向进行创新。==

Issues such as memory protection became important; we wouldn't want one program to be able to access the memory of another program.
==诸如内存保护之类的问题变得重要起来；我们要不希望一个程序能够访问另一个程序的内存。==

Understanding how to deal with the concurrency issues introduced by multiprogramming was also critical; making sure the OS was behaving correctly despite the presence of interrupts is a great challenge.
==理解如何处理由多道程序设计引入的并发问题也至关重要；确保操作系统在存在中断的情况下仍能正确运行是一项巨大的挑战。==

We will study these issues and related topics later in the book.
==我们将在本书后面研究这些问题和相关主题。==

One of the major practical advances of the time was the introduction of the **UNIX** operating system, primarily thanks to Ken Thompson (and Dennis Ritchie) at Bell Labs (yes, the phone company).
==当时主要的实践进步之一是 **UNIX** 操作系统的引入，这主要归功于贝尔实验室（是的，电话公司）的 Ken Thompson（和 Dennis Ritchie）。==

UNIX took many good ideas from different operating systems (particularly from Multics, and some from systems like TENEX and the Berkeley Time-Sharing System), but made them simpler and easier to use.
==UNIX 从不同的操作系统（特别是 Multics，以及 TENEX 和伯克利分时系统等系统）中吸取了许多好点子，但使它们更简单、更易于使用。==

Soon this team was shipping tapes containing UNIX source code to people around the world, many of whom then got involved and added to the system themselves; see the Aside (next page) for more detail.
==很快，这个团队就开始向世界各地的人们运送包含 UNIX 源代码的磁带，其中许多人随后参与进来并亲自为系统添砖加瓦；有关更多详细信息，请参阅旁白（下一页）。==

**The Modern Era**
==**现代**==

Beyond the minicomputer came a new type of machine, cheaper, faster, and for the masses: the **personal computer**, or **PC** as we call it today.
==继小型机之后，出现了一种新型机器，更便宜、更快、面向大众：**个人计算机**，也就是我们要今天所说的 **PC**。==

Led by Apple's early machines (e.g., the Apple II) and the IBM PC, this new breed of machine would soon become the dominant force in computing.
==在苹果早期机器（例如 Apple II）和 IBM PC 的带领下，这种新型机器很快将成为计算领域的主导力量。==

Aside: The Importance of UNIX

==旁白：UNIX 的重要性==

It is difficult to overstate the importance of UNIX in the history of operating systems.

==很难夸大 UNIX 在操作系统历史上的重要性 1。==

Influenced by earlier systems (in particular, the famous Multics system from MIT), UNIX brought together many great ideas and made a system that was both simple and powerful.

==受早期系统（特别是麻省理工学院著名的 Multics 系统）的影响，UNIX 汇集了许多伟大的思想，制造了一个既简单又强大的系统 2。==

Underlying the original "Bell Labs" UNIX was the unifying principle of building small powerful programs that could be connected together to form larger workflows.

==最初的“贝尔实验室”UNIX 的基础是构建小型强大程序的统一原则，这些程序可以连接在一起形成更大的工作流 3。==

The shell, where you type commands, provided primitives such as pipes to enable such meta-level programming, and thus it became easy to string together programs to accomplish a bigger task.

==你输入命令的 Shell 提供了诸如管道之类的原语来实现这种元级编程，因此很容易将程序串联起来完成更大的任务 45。6==

For example, to find lines of a text file that have the word "foo" in them, and then to count how many such lines exist, you would type: grep foo file.txt | wc -l, thus using the grep a7nd wc (word count) programs to achieve your task.

==例如，要查找文本文件中包含单词 "foo" 的行，然后计算有多少这样的行，你可以输入：grep foo file.txt | wc -l，从而使用 grep 和 wc（字数统计）程序来完成你的任务 8。==

The UNIX environment was friendly for programmers and developers alike, also providing a compiler for the new C programming language.

==UNIX 环境对程序员和开发人员都很友好，还为新的 C 编程语言提供了编译器 9。==

Making it easy for programmers to write their own programs, as well as share them, made UNIX enormously popular.

==使程序员能够轻松编写自己的程序以及分享它们，这使得 UNIX 非常流行 10。==

And it probably helped a lot that the authors gave out copies for free to anyone who asked, an early form of open-source software.

==作者向任何索要副本的人免费提供副本（一种早期的开源软件形式），这可能也有很大的帮助 11。==

Also of critical importance was the accessibility and readability of the code.

==代码的可访问性和可读性也至关重要 12。==

Having a beautiful, small kernel written in C invited others to play with the kernel, adding new and cool features.

==拥有一个用 C 语言编写的漂亮、小巧的内核，邀请其他人来玩转内核，添加新的酷炫功能 13。==

For example, an enterprising group at Berkeley, led by Bill Joy, made a wonderful distribution (the Berkeley Systems Distribution, or BSD) which had some advanced virtual memory, file system, and networking subsystems.

==例如，由 Bill Joy 领导的伯克利一个进取的小组制作了一个精彩的发行版（伯克利系统发行版，或 BSD），其中包含一些先进的虚拟内存、文件系统和网络子系统 14。==

Joy later co-founded Sun Microsystems.

==Joy 后来联合创立了 Sun Microsystems 15。==

Unfortunately, the spread of UNIX was slowed a bit as companies tried to assert ownership and profit from it, an unfortunate (but common) result of lawyers getting involved.

==不幸的是，由于公司试图宣示所有权并从中获利，UNIX 的传播速度有所放缓，这是律师介入的一个不幸（但常见）的结果 16。==

Many companies had their own variants: SunOS from Sun Microsystems, AIX from IBM, HPUX (a.k.a. "H-Pucks") from HP, and IRIX from SGI.17

==许多公司都有自己的变体：Sun Microsystems 的 SunOS，IBM 的 AIX，HP 的 HPUX（又名 "H-Pucks"），以及 SGI 的 IRIX 181920。2122==

The legal wrangling among AT&T/Bell L23abs and these other players cast a dark cloud over UNIX, and many wondered if it would survive, e24specially as Windows was introduced and took over much of the PC market... as their low-cost enabled one machine per desktop instead of a shared minicomputer per workgroup.

==AT&T/贝尔实验室与其他参与者之间的法律纠纷给 UNIX 蒙上了一层阴影，许多人怀疑它能否生存下去，尤其是在 Windows 推出并占据了大部分 PC 市场的时候……因为它们的低成本使得每张桌子上都有一台机器，而不是每个工作组共享一台小型机 25。==

Unfortunately, for operating systems, the PC at first represented a great leap backwards, as early systems forgot (or never knew of) the lessons learned in the era of minicomputers.

==不幸的是，对于操作系统来说，PC 最初代表了一个巨大的倒退，因为早期的系统忘记了（或从未知道）在小型机时代学到的教训 26。==

For example, early operating systems such as DOS (the Disk Operating System, from Microsoft) didn't think memory protection was important; thus, a malicious (or perhaps just a poorly-programmed) application could scribble all over memory.

==例如，早期的操作系统如 DOS（微软的磁盘操作系统）认为内存保护并不重要；因此，恶意（或者是编写糟糕）的应用程序可以在内存上乱写乱画 27。==

Aside: And Then Came Linux

==旁白：然后 Linux 来了==

Fortunately for UNIX, a young Finnish hacker named Linus Torvalds decided to write his own version of UNIX which borrowed heavily on the principles and ideas behind the original system, but not from the code base, thus avoiding issues of legality.

==对 UNIX 来说幸运的是，一位名叫 Linus Torvalds 的年轻芬兰黑客决定编写他自己的 UNIX 版本，该版本大量借鉴了原始系统背后的原则和思想，但没有借鉴代码库，从而避免了法律问题 28。==

He enlisted help from many others around the world, took advantage of the sophisticated GNU tools that already existed, and soon Linux was born (as well as the modern open-source software movement).29

==他寻求了世界各地许多其他人的帮助，利用了已经存在的复杂 GNU 工具，很快 Linux 就诞生了（以及现代开源软件运30动） 31。==

As the internet era came into place, most companies (such as Google, Amazon, Facebook, and others) chose to run Linux, as it was free and could be readily modified to suit their needs; indeed, it is hard to imagine the success of these new companies had such a system not existed.

==随着互联网时代的到来，大多数公司（如 Google、Amazon、Facebook 等）选择运行 Linux，因为它是免费的，并且可以随时修改以满足他们的需求；事实上，如果不存在这样的系统，很难想象这些新公司的成功 32。==

As smart phones became a dominant user-facing platform, Linux found a stronghold there too (via Android), for many of the same reasons.

==随着智能手机成为面向用户的主导平台，Linux 出于许多相同的原因也在那里找到了据点（通过 Android） 33。==

And Steve Jobs took his UNIX-based NeXTStep operating environment with him to Apple, thus making UNIX popular on desktops (though many users of Apple technology are probably not even aware of this fact).

==史蒂夫·乔布斯带着他基于 UNIX 的 NeXTStep 操作环境回到了 Apple，从而使 UNIX 在桌面上流行起来（尽管许多 Apple 技术用户可能甚至没有意识到这一事实） 34。==

Thus UNIX lives on, more important today than ever before.

==因此，UNIX 继续存在，今天比以往任何时候都更重要 35。==

The computing gods, if you believe in them, should be thanked for this wonderful outcome.

==如果你相信计算之神，应该感谢他们带来了这个美妙的结果 36。==

The first generations of the Mac OS (v9 and earlier) took a cooperative approach to job scheduling; thus, a thread that accidentally got stuck in an infinite loop could take over the entire system, forcing a reboot.

==第一代 Mac OS（v9 及更早版本）采用了协作式作业调度方法；因此，意外陷入无限循环的线程可能会接管整个系统，迫使重启 37。==

The painful list of OS features missing in this generation of systems is long, too long for a full discussion here.

==这一代系统中缺失的操作系统特性列表很长，长到无法在这里全面讨论 38。==

Fortunately, after some years of suffering, the old features of minicomputer operating systems started to find their way onto the desktop.

==幸运的是，经过几年的痛苦，小型机操作系统的旧特性开始进入桌面 39。==

For example, Mac OS X/macOS has UNIX at its core, including all of the features one would expect from such a mature system.

==例如，Mac OS X/macOS 以 UNIX 为核心，包括人们对此类成熟系统所期望的所有功能 40。==

Windows has similarly adopted many of the great ideas in computing history, starting in particular with Windows NT, a great leap forward in Microsoft OS technology.

==Windows 同样采用了计算历史上的许多伟大思想，特别是从 Windows NT 开始，这是微软操作系统技术的巨大飞跃 41。==

Even today's cell phones run operating systems (such as Linux) that are much more like what a minicomputer ran in the 1970s than what a PC ran in the 1980s (thank goodness); it is good to see that the good ideas developed in the heyday of OS development have found their way into the modern world.

==即使是今天的手机运行的操作系统（如 Linux）也更像 20 世纪 70 年代小型机运行的系统，而不是 80 年代 PC 运行的系统（谢天谢地）；很高兴看到在操作系统开发鼎盛时期产生的好点子已经进入现代世界 42。==

Even better is that these ideas continue to develop, providing more features and making modern systems even better for users and applications.

==更好的是，这些想法还在继续发展，提供更多功能，使现代系统对用户和应用程序来说更加完善 43。==

2.7 Summary

==2.7 总结==

Thus, we have an introduction to the OS.

==至此，我们要已经介绍了操作系统 44。==

Today's operating systems make systems relatively easy to use, and virtually all operating systems you use today have been influenced by the developments we will discuss throughout the book.

==今天的操作系统使系统相对易于使用，实际上你今天使用的所有操作系统都受到了我们将在本书中讨论的发展的影响 45。==

Unfortunately, due to time constraints, there are a number of parts of the OS we won't cover in the book.

==遗憾的是，由于时间限制，我们在本书中不会涵盖操作系统的许多部分 46。==

For example, there is a lot of networking code in the operating system; we leave it to you to take the networking class to learn more about that.4748

==例如，操作系统中有大量的网络代码；我们留给你去上网络课程来了解更多相关信息 495051。5253==

Similarly, graphics devices ar54e particularly 55important; take the graphics course to expand your knowledge in that direction.

==同样，图形设备特别重要；参加图形学课程以扩展你在该方向的知识 56。==

Finally, some operating system books talk a great deal about security; we will do so in the sense that the OS must provide protection between running programs and give users the ability to protect their files, but we won't delve into deeper security issues that one might find in a security course.

==最后，一些操作系统书籍大量谈论安全性；我们会在这方面进行讨论，即操作系统必须在运行的程序之间提供保护，并赋予用户保护其文件的能力，但我们不会深入探讨在安全课程中可能会发现的更深层次的安全问题 57。==

However, there are many important topics that we will cover, including the basics of virtualization of the CPU and memory, concurrency, and persistence via devices and file systems.

==然而，我们要将涵盖许多重要主题，包括 CPU 和内存虚拟化的基础知识、并发性以及通过设备和文件系统实现的持久性 58。==

Don't worry!

==别担心！ 59==

While there is a lot of ground to cover, most of it is quite cool, and at the end of the road, you'll have a new appreciation for how computer systems really work.

==虽然有很多内容要涵盖，但其中大部分都很酷，在最后，你将对计算机系统是如何真正工作的有一个新的认识 60。==

Now get to work!

==现在开始工作吧！ 61==

References

==参考文献==

[BS+09] "Tolerating File-System Mistakes with EnvyFS" by L. Bairavasundaram, S. Sundararaman, A. Arpaci-Dusseau, R. Arpaci-Dusseau.

==[BS+09] 《使用 EnvyFS 容忍文件系统错误》，作者：L. Bairavasundaram, S. Sundararaman, A. Arpaci-Dusseau, R. Arpaci-Dusseau 62。==

USENIX '09, San Diego, CA, June 2009. A fun paper about using multiple file systems at once to tolerate a mistake in any one of them.

==USENIX '09，加利福尼亚州圣地亚哥，2009 年 6 月。一篇关于同时使用多个文件系统以容忍其中任何一个错误的有趣论文 63。==

[BH00] "The Evolution of Operating Systems" by P. Brinch Hansen. In 'Classic Operating Systems: From Batch Processing to Distributed Systems.'

==[BH00] 《操作系统的演变》，作者：P. Brinch Hansen。收录于《经典操作系统：从批处理到分布式系统》 64。==

Springer-Verlag, New York, 2000. This essay provides an intro to a wonderful collection of papers about historically significant systems.

==Springer-Verlag，纽约，2000 年。这篇文章介绍了一系列关于具有历史意义系统的精彩论文 65。==

[B+72] "TENEX, A Paged Time Sharing System for the PDP-10" by D. Bobrow, J. Burchfiel, D. Murphy, R. Tomlinson.

==[B+72] 《TENEX，用于 PDP-10 的分页分时系统》，作者：D. Bobrow, J. Burchfiel, D. Murphy, R. Tomlinson 66。==

CACM, Volume 15, Number 3, March 1972. TENEX has much of the machinery found in modern operating systems; read more about it to see how much innovation was already in place in the early 1970's.

==CACM，第 15 卷，第 3 期，1972 年 3 月。TENEX 拥有现代操作系统中的许多机制；阅读更多关于它的内容，看看在 20 世纪 70 年代初已经实施了多少创新 67。==

[B75] "The Mythical Man-Month" by F. Brooks. Addison-Wesley, 1975. A classic text on software engineering; well worth the read.

==[B75] 《人月神话》，作者：F. Brooks。Addison-Wesley，1975 年。关于软件工程的经典著作；非常值得一读 68。==

[BOH10] "Computer Systems: A Programmer's Perspective" by R. Bryant and D. O'Hallaron.

==[BOH10] 《深入理解计算机系统》，作者：R. Bryant 和 D. O'Hallaron 69。==

Addison-Wesley, 2010. Another great intro to how computer systems work.

==Addison-Wesley，2010 年。关于计算机系统如何工作的另一本很棒的入门书 70。==

Has a little bit of overlap with this book so if you'd like, you can skip the last few chapters of that book, or simply read them to get a different perspective on some of the same material.

==与本书有一点重叠，所以如果你愿意，你可以跳过那本书的最后几章，或者干脆阅读它们以从不同的角度了解一些相同的材料 71。==

After all, one good way to build up your own knowledge is to hear as many other perspectives as possible, and then develop your own opinion and thoughts on the matter.

==毕竟，建立自己知识的一个好方法是尽可能多地听取其他观点，然后就此事形成自己的观点和想法 72。==

You know, by thinking!

==你知道的，通过思考！ 73==

[G85] "The GNU Manifesto" by R. Stallman. 1985.

==[G85] 《GNU 宣言》，作者：R. Stallman。1985 年 74。==

A huge part of Linux's success was no doubt the presence of an excellent compiler, gcc, and other relevant pieces of open software, thanks to the GNU effort headed by Stallman.

==Linux 成功的很大一部分毫无疑问是优秀编译器 gcc 以及其他相关开源软件的存在，这要归功于 Stallman 领导的 GNU 工作 75。==

Stallman is a visionary when it comes to open source, and this manifesto lays out his thoughts as to why.

==Stallman 在开源方面富有远见，这份宣言阐述了他关于为什么要开源的想法 76。==

[K+61] "One-Level Storage System" by T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner.

==[K+61] 《一级存储系统》，作者：T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner 77。==

IRE Transactions on Electronic Computers, April 1962. The Atlas pioneered much of what you see in modern systems.

==IRE 电子计算机学报，1962 年 4 月。Atlas 开创了你在现代系统中看到的许多东西 78。==

However, this paper is not the best read.

==不过，这篇论文并不是最好的读物 79。==

If you were to only read one, you might try the historical perspective below.

==如果你只读一篇，你可以尝试下面的历史视角文章 80。==

[L78] "The Manchester Mark I and Atlas: A Historical Perspective" by S. H. Lavington.

==[L78] 《曼彻斯特马克一号和 Atlas：历史视角》，作者：S. H. Lavington 81。==

Communications of the ACM, Volume 21:1, January 1978. A nice piece of history on the early development of computer systems and the pioneering efforts of the Atlas.

==ACM 通讯，第 21 卷第 1 期，1978 年 1 月。关于计算机系统早期发展和 Atlas 开创性努力的一篇很好的历史文章 82。==

Of course, one could go back and read the Atlas papers themselves, but this paper provides a great overview and adds some historical perspective.

==当然，人们可以回去阅读 Atlas 论文本身，但这篇论文提供了一个很好的概述并增加了一些历史视角 83。==

[O72] "The Multics System: An Examination of its Structure" by Elliott Organick. MIT Press, 1972. A great overview of Multics.

==[O72] 《Multics 系统：对其结构的考察》，作者：Elliott Organick。麻省理工学院出版社，1972 年。Multics 的精彩概述 84。==

So many good ideas, and yet it was an over-designed system, shooting for too much, and thus never really worked.

==这么多好点子，但它是一个过度设计的系统，目标太高，因此从未真正工作过 85。==

A classic example of what Fred Brooks would call the "second-system effect".

==Fred Brooks 所谓的“第二系统效应”的经典例子 86。==

[PP03] "Introduction to Computing Systems: From Bits and Gates to C and Beyond" by Yale N. Patt, Sanjay J. Patel.

==[PP03] 《计算机系统概论：从位和门到 C 及其他》，作者：Yale N. Patt, Sanjay J. Patel 87。==

McGraw-Hill, 2003. One of our favorite intro to computing systems books.

==McGraw-Hill，2003 年。我们最喜欢的计算机系统入门书籍之一 88。==

Starts at transistors and gets you all the way up to C; the early material is particularly great.

==从晶体管开始，一直讲到 C 语言；早期的材料特别棒 89。==

[RT74] "The UNIX Time-Sharing System" by Dennis M. Ritchie, Ken Thompson.

==[RT74] 《UNIX 分时系统》，作者：Dennis M. Ritchie, Ken Thompson 90。==

CACM, Volume 17: 7, July 1974. A great summary of UNIX written as it was taking over the world of computing, by the people who wrote it.

==CACM，第 17 卷：7，1974 年 7 月。由编写它的人撰写的关于 UNIX 的精彩总结，当时它正在接管计算世界 91。==

[S68] "SDS 940 Time-Sharing System" by Scientific Data Systems. TECHNICAL MANUAL, SDS 90 11 168, August 1968.

==[S68] 《SDS 940 分时系统》，作者：Scientific Data Systems。技术手册，SDS 90 11 168，1968 年 8 月 92。==

Yes, a technical manual was the best we could find.

==是的，我们要能找到的最好的就是一本技术手册 93。==

But it is fascinating to read these old system documents, and see how much was already in place in the late 1960's.

==但是阅读这些旧的系统文档并看看在 20 世纪 60 年代末已经有多少东西到位是很有趣的 94。==

One of the minds behind the Berkeley Time-Sharing System (which eventually became the SDS system) was Butler Lampson, who later won a Turing award for his contributions in systems.

==伯克利分时系统（最终成为 SDS 系统）背后的思想家之一是 Butler Lampson，他后来因在系统方面的贡献而获得图灵奖 95。==

[SS+10] "Membrane: Operating System Support for Restartable File Systems" by S. Sundararaman, S. Subramanian, A. Rajimwale, A. Arpaci-Dusseau, R. Arpaci-Dusseau, M. Swift.

==[SS+10] 《Membrane：操作系统对可重启文件系统的支持》，作者：S. Sundararaman, S. Subramanian, A. Rajimwale, A. Arpaci-Dusseau, R. Arpaci-Dusseau, M. Swift 96。==

FAST '10, San Jose, CA, February 2010. The great thing about writing your own class notes: you can advertise your own research.

==FAST '10，加利福尼亚州圣何塞，2010 年 2 月。编写自己的课堂讲义的好处：你可以宣传自己的研究 97。==

But this paper is actually pretty neat when a file system hits a bug and crashes, Membrane auto-magically restarts it, all without applications or the rest of the system being affected.98

==但这篇论文实际上相当不错，当文件系统遇到错误并崩溃时，Membrane 会自动神奇地重启它，而不会影响应用程序或系统的其余部分 99100。101==

Homework102

==家庭作业103==

Mos104t (and eventually, all) chapters of this book have homework sections at the end.

==本书的大多数（最终是所有）章节末尾都有家庭作业部分 105。==

Doing these homeworks is important, as each lets you, the reader, gain more experience with the concepts presented within the chapter.

==做这些作业很重要，因为每一个作业都能让你，也就是读者，获得更多关于本章所介绍概念的经验 106。==

There are two types of homeworks.

==有两种类型的家庭作业 107。==

The first is based on simulation.

==第一种是基于模拟的 108。==

A simulation of a computer system is just a simple program that pretends to do some of the interesting parts of what a real system does, and then report some output metrics to show how the system behaves.

==计算机系统的模拟只是一个简单的程序，假装做真实系统所做的一些有趣部分，然后报告一些输出指标来展示系统的行为 109。==

For example, a hard drive simulator might take a series of requests, simulate how long they would take to get serviced by a hard drive with certain performance characteristics, and then report the average latency of the requests.

==例如，硬盘驱动器模拟器可能会接受一系列请求，模拟具有特定性能特征的硬盘驱动器为它们提供服务需要多长时间，然后报告请求的平均延迟 110。==

The cool thing about simulations is they let you easily explore how systems behave without the difficulty of running a real system.

==模拟的酷炫之处在于，它们让你能够轻松探索系统的行为，而无需运行真实系统的困难 111。==

Indeed, they even let you create systems that cannot exist in the real world (for example, a hard drive with unimaginably fast performance), and thus see the potential impact of future technologies.

==事实上，它们甚至允许你创建现实世界中不存在的系统（例如，具有难以想象的快速性能的硬盘驱动器），从而看到未来技术的潜在影响 112。==

Of course, simulations are not without their downsides.113

==当然，模拟并非没有缺点 114115。116==

By their very nature, simulations are just approximations of how a real system behaves.117

==就其本质而言，模拟只是真实系统行为的近似 118119。120==

I121f an important aspect of real-world behavior is omitted, the simulation will report bad results.

==如果忽略了现实世界行为的一个重要方面，模拟将报告错误的结果 122。==

Thus, results from a simulation should always be treated with some suspicion.

==因此，模拟结果应始终受到一定的怀疑 123。==

In the end, how a system behaves in the real world is what matters.

==最终，系统在现实世界中的表现才是最重要的 124。==

The second type of homework requires interaction with real-world code.

==第二种类型的家庭作业需要与现实世界的代码进行交互 125。==

Some of these homeworks are measurement focused, whereas others just require some small-scale development and experimentation.

==其中一些作业侧重于测量，而另一些则只需要一些小规模的开发和实验 126。==

Both are just small forays into the larger world you should be getting into, which is how to write systems code in C on UNIX-based systems.

==两者都只是对你应该进入的更大世界的小小尝试，即如何在基于 UNIX 的系统上用 C 编写系统代码 127。==

Indeed, larger-scale projects, which go beyond these homeworks, are needed to push you in this direction; thus, beyond just doing homeworks, we strongly recommend you do projects to solidify your systems skills.

==事实上，需要超越这些家庭作业的更大规模的项目来推动你朝这个方向发展；因此，除了做家庭作业之外，我们要强烈建议你做项目来巩固你的系统技能 128。==

See this page (https://github.com/remzi-arpacidusseau/ostep-projects) for some projects.

==请参阅此页面 (https://github.com/remzi-arpacidusseau/ostep-projects) 获取一些项目 129。==

To do these homeworks, you likely have to be on a UNIX-based machine, running either Linux, macOS, or some similar system.

==要做这些家庭作业，你可能需要在基于 UNIX 的机器上，运行 Linux、macOS 或类似的系统 130。==

It should also have a C compiler installed (e.g., gcc) as well as Python.

==它还应该安装了 C 编译器（例如 gcc）以及 Python 131。==

You should also know how to edit code in a real code editor of some kind.

==你也应该知道如何在某种真正的代码编辑器中编辑代码 132。==

Part I: Virtualization

==第一部分：虚拟化==

3 A Dialogue on Virtualization

==3 关于虚拟化的对话==

Professor: And thus we reach the first of our three pieces on operating systems: virtualization.

==教授：这样我们就到了关于操作系统的三个部分中的第一个：虚拟化 133。==

Student: But what is virtualization, oh noble professor?

==学生：但是什么是虚拟化呢，哦，高尚的教授？ 134==

Professor: Imagine we have a peach.

==教授：想象我们有一个桃子 135。==

Student: A peach? (incredulous)

==学生：一个桃子？（怀疑地） 136==

Professor: Yes, a peach. Let us call that the physical peach.

==教授：是的，一个桃子。让我们称之为物理桃子 137。==

But we have many eaters who would like to eat this peach.

==但是我们有许多食客想要吃这个桃子 138。==

What we would like to present to each eater is their own peach, so that they can be happy.

==我们要希望呈现给每个食客的是他们自己的桃子，这样他们就会很高兴 139。==

We call the peach we give eaters virtual peaches; we somehow create many of these virtual peaches out of the one physical peach.

==我们要称给食客的桃子为虚拟桃子；我们不知何故用一个物理桃子创造了许多这样的虚拟桃子 140。==

And the important thing: in this illusion, it looks to each eater like they have a physical peach, but in reality they don't.

==重要的事情是：在这个幻觉中，每个食客看起来都拥有一个物理桃子，但实际上他们没有 141。==

Student: So you are sharing the peach, but you don't even know it?

==学生：所以你们在分享桃子，但你们甚至不知道？ 142==

Professor: Right! Exactly.

==教授：对！没错 143。==

Student: But there's only one peach.

==学生：但是只有一个桃子 144。==

Professor: Yes. And...?

==教授：是的。然后呢……？ 145==

Student: Well, if I was sharing a peach with somebody else, I think I would notice.

==学生：嗯，如果我和别人分享一个桃子，我想我会注意到的 146。==

Professor: Ah yes! Good point.

==教授：啊是的！说得好 147。==

But that is the thing with many eaters; most of the time they are napping or doing something else, and thus, you can snatch that peach away and give it to someone else for a while.

==但这就是许多食客的情况；大多数时候他们在打盹或做其他事情，因此，你可以把那个桃子抢走，给别人一会儿 148。==

And thus we create the illusion of many virtual peaches, one peach for each person!

==这样我们就创造了许多虚拟桃子的幻觉，每个人一个桃子！ 149==

Student: Sounds like a bad campaign slogan.

==学生：听起来像是一个糟糕的竞选口号 150。==

You are talking about computers, right Professor?

==你在谈论计算机，对吧教授？ 151==

Professor: Ah, young grasshopper, you wish to have a more concrete example.

==教授：啊，年轻的蚱蜢，你希望有一个更具体的例子 152。==

Good idea!

==好主意！ 153==

Let us take the most basic of resources, the CPU.

==让我们以最基本的资源 CPU 为例 154。==

Assume there is one physical CPU in a system (though now there are often two or four or more).

==假设系统中有一个物理 CPU（尽管现在通常有两个或四个或更多） 155。==

What virtualization does is take that single CPU and make it look like many virtual CPUs to the applications running on the system.

==虚拟化所做的是获取那个单一的 CPU，并使其在系统上运行的应用程序看来像是有许多虚拟 CPU 156。==

Thus, while each application thinks it has its own CPU to use, there is really only one.157

==因此，虽然每个应用程序都认为它有自己的 CPU 可以使用，但实际上只有一个 158159。160==

And thus the OS has created a beautiful illusion: it has virtua161lized the CPU.

==这样操作系统就创造了一个美丽的幻觉：它已经虚拟化了 CPU 162。==

Student: Wow! That sounds like magic.

==学生：哇！那听起来像魔法 163。==

Tell me more!

==告诉我更多！ 164==

How does that work?

==那是如何工作的？ 165==

Professor: In time, young student, in good time.

==教授：到时候，年轻的学生，到时候 166。==

Sounds like you are ready to begin.

==听起来你已经准备好开始了 167。==

Student: I am! Well, sort of.

==学生：我是！嗯，算是吧 168。==

I must admit, I'm a little worried you are going to start talking about peaches again.

==我必须承认，我有点担心你会再次开始谈论桃子 169。==

Professor: Don't worry too much; I don't even like peaches.

==教授：别太担心；我甚至不喜欢桃子 170。==

And thus we begin...

==我们就这样开始…… 171==

4 The Abstraction: The Process

==4 抽象：进程==

In this chapter, we discuss one of the most fundamental abstractions that the OS provides to users: the process.

==在本章中，我们要讨论操作系统提供给用户的最基本的抽象之一：进程 172。==

The definition of a process, informally, is quite simple: it is a running program.

==非正式地讲，进程的定义非常简单：它是一个正在运行的程序 173。==

The program itself is a lifeless thing: it just sits there on the disk, a bunch of instructions (and maybe some static data), waiting to spring into action.

==程序本身是一个无生命的东西：它只是静静地待在磁盘上，一堆指令（也许还有一些静态数据），等待着付诸行动 174。==

It is the operating system that takes these bytes and gets them running, transforming the program into something useful.

==正是操作系统获取这些字节并让它们运行起来，将程序转化为有用的东西 175。==

It turns out that one often wants to run more than one program at once; for example, consider your desktop or laptop where you might like to run a web browser, mail program, a game, a music player, and so forth.

==事实证明，人们经常希望同时运行多个程序；例如，考虑你的台式机或笔记本电脑，你可能想要运行网络浏览器、邮件程序、游戏、音乐播放器等等 176。177==

In fact, a typical system may be seemingly running tens or even hundreds of processes at the same time.178

==事实上，一个典型的系统可能看似同时运行着数十甚至数百个进程 179180。181==

Doing so makes the system easy to use, as182 one never need be concerned with whether a CPU is available; one simply runs programs.

==这样做使系统易于使用，因为人们永远不需要关心 CPU 是否可用；只需运行程序即可 183。==

Hence our challenge:

==因此我们的挑战是： 184==

The Crux of the Problem: How to Provide the Illusion of Many CPUs?

==问题的关键：如何提供许多 CPU 的幻觉？==

Although there are only a few physical CPUs available, how can the OS provide the illusion of a nearly-endless supply of said CPUs?

==虽然只有少数物理 CPU 可用，但操作系统如何提供看似无限供应的 CPU 的幻觉？ 185==

The OS creates this illusion by virtualizing the CPU.

==操作系统通过虚拟化 CPU 来创造这种幻觉 186。==

By running one process, then stopping it and running another, and so forth, the OS can promote the illusion that many virtual CPUs exist when in fact there is only one physical CPU (or a few).

==通过运行一个进程，然后停止它并运行另一个进程，依此类推，操作系统可以制造出存在许多虚拟 CPU 的假象，而实际上只有一个物理 CPU（或几个） 187。==

This basic technique, known as time sharing of the CPU, allows users to run as many concurrent processes as they would like; the potential cost is performance, as each will run more slowly if the CPU(s) must be shared.

==这种基本技术称为 CPU 分时，允许用户运行任意数量的并发进程；潜在的代价是性能，如果必须共享 CPU，每个进程的运行速度都会变慢 188。==

To implement virtualization of the CPU, and to implement it well, the OS will need both some low-level machinery and some high-level intelligence.

==为了实现 CPU 虚拟化，并且实现得好，操作系统既需要一些低级机制，也需要一些高级智能 189。==

We call the low-level machinery mechanisms; mechanisms are low-level methods or protocols that implement a needed piece of functionality.190191

==我们要称低级机器为机制；机制是实现所需功能的低级方法或协议 192193194。195196==

For example, we'll learn later how to implement a context switc197h, which gives the OS t198he ability to stop running one program and start running another on a given CPU; this time-sharing mechanism is employed by all modern OSes.

==例如，我们要稍后将学习如何实现上下文切换，这使操作系统能够停止运行一个程序并在给定的 CPU 上开始运行另一个程序；所有现代操作系统都采用了这种分时机制 199199199199。==

Tip: Use Time Sharing (and Space Sharing)

==提示：使用分时（和空间共享）200==

Time sharing is a basic technique used by an OS to share a resource.201

==分时是操作系统用来共享资源的一种基本技术 202203。204==

By allowing the resource to be used for a little while by one entity, and then a little whi205le by another, and so forth, the resource in question (e.g., the CPU, or a network link) can be shared by many.

==通过允许资源被一个实体使用一会儿，然后被另一个实体使用一会儿，依此类推，所讨论的资源（例如，CPU 或网络链接）可以被许多人共享 206。==

The counterpart of time sharing is space sharing, where a resource is divided (in space) among those who wish to use it.

==分时的对应物是空间共享，其中资源在希望使用它的人之间（在空间上）进行划分 207。==

For example, disk space is naturally a space-shared resource; once a block is assigned to a file, it is normally not assigned to another file until the user deletes the original file.

==例如，磁盘空间自然是一种空间共享资源；一旦一个块被分配给一个文件，它通常不会被分配给另一个文件，直到用户删除原始文件 208。==

On top of these mechanisms resides some of the intelligence in the OS, in the form of policies.

==在这些机制之上，存在着操作系统中的一些智能，以策略的形式出现 209。==

Policies are algorithms for making some kind of decision within the OS.

==策略是在操作系统内做出某种决定的算法 210。==

For example, given a number of possible programs to run on a CPU, which program should the OS run?

==例如，给定要在 CPU 上运行的多个可能的程序，操作系统应该运行哪个程序？ 211==

A scheduling policy in the OS will make this decision, likely using historical information (e.g., which program has run more over the last minute?), workload knowledge (e.g., what types of programs are run), and performance metrics (e.g., is the system optimizing for interactive performance, or throughput?) to make its decision.

==操作系统中的调度策略将做出此决定，可能会使用历史信息（例如，哪个程序在最后一分钟运行得更多？）、工作负载知识（例如，运行什么类型的程序）和性能指标（例如，系统是否针对交互性能或吞吐量进行优化？）来做出决定 212。==

4.1 The Abstraction: A Process

==4.1 抽象：一个进程==

The abstraction provided by the OS of a running program is something we will call a process.

==操作系统提供的运行程序的抽象就是我们要所说的进程 213。==

As we said above, a process is simply a running program; at any instant in time, we can summarize a process by taking an inventory of the different pieces of the system it accesses or affects during the course of its execution.

==正如我们上面所说，进程只是一个正在运行的程序；在任何时刻，我们可以通过盘点它在执行过程中访问或影响的系统的不同部分来总结一个进程 214。==

To understand what constitutes a process, we thus have to understand its machine state: what a program can read or update when it is running.

==为了理解什么构成了进程，我们要必须理解它的机器状态：程序在运行时可以读取或更新什么 215。==

At any given time, what parts of the machine are important to the execution of this program?

==在任何给定时间，机器的哪些部分对该程序的执行很重要？ 216==

One obvious component of machine state that comprises a process is its memory.

==构成进程的机器状态的一个明显组成部分是其内存 217。==

Instructions lie in memory; the data that the running program reads and writes sits in memory as well.

==指令位于内存中；正在运行的程序读取和写入的数据也位于内存中 218。==

Thus the memory that the process can address (called its address space) is part of the process.

==因此，进程可以寻址的内存（称为其地址空间）是进程的一部分 219。==

Also part of the process's machine state are registers; many instructions explicitly read or update registers and thus clearly they are important to the execution of the process.

==进程机器状态的一部分还有寄存器；许多指令显式读取或更新寄存器，因此显然它们对进程的执行很重要 220221。222==

Note that there are some particularly special registers that form part of this machine state.223

==请注意，有一些特别特殊的寄存器构成了此机器状态的一部分 224225。226==

For example, the program counter (PC) (sometimes 227called the instruction pointer or IP) tells us which instruction of the program will execute next; similarly a stack pointer and associated frame pointer are used to manage the stack for function parameters, local variables, and return addresses.228229

==例如，程序计数器 (PC)（有时称为指令指针或 IP）告诉我们将执行程序的哪一条指令；类似地，栈指针和相关的帧指针230用于管理函数参数、局部变量和返回地址的栈 231232232232232。233==

Finally, programs often access persisten234t storage devices too.

==最后，程序通常也会访问持久存储设备 235。==

Such I/O information might include a list of the files the process currently has open.

==此类 I/O 信息可能包括进程当前已打开的文件列表 236。==

Tip: Separate Policy and Mechanism

==提示：分离策略和机制==

In many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms.237238

==在许多操作系统中，一种常见的设计范式是将高级策略与其低级机制分离开来 239240241。242243==

You can think of the mechanism as providing the answer to a how question about a system; for example, how does an operating system perform244 a context switch?245

==你可以将机制视为提供关于系统的如何做问题的答案；例如，操作系统如何执行上下文切换？ 246==

The policy provides the answer to a which question; for example, which process should the operating system run right now?

==策略提供哪一个问题的答案；例如，操作系统现在应该运行哪个进程？ 247==

Separating the two allows one easily to change policies without having to rethink the mechanism and is thus a form of modularity, a general software design principle.

==分离这两者允许人们轻松更改策略，而无需重新考虑机制，因此这是一种模块化形式，一种通用的软件设计原则 248。==

4.2 Process API

==4.2 进程 API==

Though we defer discussion of a real process API until a subsequent chapter, here we first give some idea of what must be included in any interface of an operating system.

==虽然我们要将对真实进程 API 的讨论推迟到后续章节，但在这里我们首先给出一些关于操作系统任何接口中必须包含内容的想法 249。==

These APIs, in some form, are available on any modern operating system.

==这些 API 以某种形式存在于任何现代操作系统中 250。==

- Create: An operating system must include some method to create new processes. When you type a command into the shell, or double-click on an application icon, the OS is invoked to create a new process to run the program you have indicated.
    
==**创建**：操作系统必须包含某种创建新进程的方法。当你在 Shell 中输入命令或双击应用程序图标时，操作系统会被调用来创建一个新进程以运行你指定的程序 251。==
    
- Destroy: As there is an interface for process creation, systems also provide an interface to destroy processes forcefully. Of course, many processes will run and just exit by themselves when complete; when they don't, however, the user may wish to kill them, and thus an interface to halt a runaway process is quite useful.
    
==**销毁**：既然有进程创建的接口，系统也提供强制销毁进程的接口。当然，许多进程会运行并在完成后自行退出；然而，当它们不退出时，用户可能希望杀死它们，因此停止失控进程的接口非常有用 252253。254==
    
- Wait: Sometimes it is useful to wait for a process to stop running; thus some kind of waiting interface is often provided.255
    
==**等待**：有时等待进程停止运行是有用的；因此通常提供某种等待接口 256257。==
    
- Miscellaneous Control: Other than killing or waiting for a process, there are sometimes other controls that are possible. For example, most operating systems provide some kind of method to suspend a process (stop it from running for a while) and then resume it (continue it running).
    
==**其他控制**：除了杀死或等待进程之外，有时还可以进行其他控制。例如，大多数操作系统提供某种暂停进程（使其停止运行一段时间）然后恢复它（使其继续运行）的方法 258。==
    
- Status: There are usually interfaces to get some status information about a process as well, such as how long it has run for, or what state it is in.
    
==**状态**：通常还有获取有关进程状态信息的接口，例如它运行了多长时间，或者它处于什么状态 259。==
    

4.3 Process Creation: A Little More Detail

==4.3 进程创建：更多细节==

One mystery that we should unmask a bit is how programs are transformed into processes.

==我们要应该揭开的一个谜团是程序如何转化为进程 260。==

Specifically, how does the OS get a program up and running?

==具体来说，操作系统如何让程序启动并运行？ 261==

How does process creation actually work?

==进程创建实际上是如何工作的？ 262==

The first thing that the OS must do to run a program is to load its code and any static data (e.g., initialized variables) into memory, into the address space of the process.

==操作系统运行程序必须做的第一件事是将其代码和任何静态数据（例如，初始化的变量）加载到内存中，加载到进程的地址空间中 263。==

Programs initially reside on disk (or, in some modern systems, flash-based SSDs) in some kind of executable format; thus, the process of loading a program and static data into memory requires the OS to read those bytes from disk and place them in memory somewhere (as shown in Figure 4.1).

==程序最初以某种可执行格式驻留在磁盘（或在一些现代系统中，基于闪存的 SSD）上；因此，将程序和静态数据加载到内存的过程需要操作系统从磁盘读取这些字节并将它们放置在内存中的某个位置（如图 4.1 所示） 264。==

In early (or simple) operating systems, the loading process is done eagerly, i.e., all at once before running the program; modern OSes perform the process lazily, i.e., by loading pieces of code or data only as they are needed during program execution.

==在早期（或简单）的操作系统中，加载过程是急切地完成的，即在运行程序之前一次性完成；现代操作系统懒惰地执行该过程，即仅在程序执行期间需要时才加载代码或数据片段 265。==

To truly understand how lazy loading of pieces of code and data works, you'll have to understand more about the machinery of paging and swapping, topics we'll cover in the future when we discuss the virtualization of memory.

==要真正理解代码和数据片段的延迟加载是如何工作的，你要必须了解更多关于分页和交换机制的知识，我们要将在讨论内存虚拟化时涵盖这些主题 266266266266。==

For now, just remember that before running anything, the OS clearly must do some work to get the important program bits from disk into memory.

==现在，只需记住，在运行任何东西之前，操作系统显然必须做一些工作，将重要的程序位从磁盘获取到内存中 267。==

Once the code and static data are loaded into memory, there are a few other things the OS needs to do before running the process.

==一旦代码和静态数据被加载到内存中，操作系统在运行进程之前还需要做其他几件事 268。==

Some memory must be allocated for the program's run-time stack (or just stack).

==必须为程序的运行时栈（或简称栈）分配一些内存 269。==

As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the OS allocates this memory and gives it to the process.

==正如你应该已经知道的那样，C 程序使用栈来存储局部变量、函数参数和返回地址；操作系统分配这块内存并将其交给进程 270。==

The OS will also likely initialize the stack with arguments; specifically, it will fill in the parameters to the main() function, i.e., argc and the argv array.

==操作系统也可能会用参数初始化栈；具体来说，它将填充 main() 函数的参数，即 argc 和 argv 数组 271。==

The OS may also allocate some memory for the program's heap.

==操作系统也可能为程序的堆分配一些内存 272。==

In C programs, the heap is used for explicitly requested dynamically-allocated data; programs request such space by calling malloc() and free it explicitly by calling free().

==在 C 程序中，堆用于显式请求的动态分配数据；程序通过调用 malloc() 请求此类空间，并通过调用 free() 显式释放它 273。==

The heap is needed for data structures such as linked lists, hash tables, trees, and other interesting data structures.

==堆用于链表、哈希表、树和其他有趣的数据结构等数据结构 274。==

The heap will be small at first; as the program runs, and requests more memory via the malloc() library API, the OS may get involved and allocate more memory to the process to help satisfy such calls.275276

==堆起初会很小；随着程序的运行，并通过 malloc() 库 API 请求更多内存，操作系统可能会介入并为进程分配更多内存以帮助满足此类调用 277278279。280281==

The OS will also do some other initialization tasks, particularly as related to input/output (I/O282).283

==操作系统还会做一些其他的初始化任务，284特别是与输入/输出 (I/O) 相关的任务 285。==

For example, in UNIX systems, each process by default has three open file descriptors, for standard input, output, and error; these descriptors let programs easily read input from the terminal and print output to the screen.

==例如，在 UNIX 系统中，每个进程默认有三个打开的文件描述符，用于标准输入、输出和错误；这些描述符让程序可以轻松地从终端读取输入并将输出打印到屏幕上 286。==

We'll learn more about I/O, file descriptors, and the like in the third part of the book on persistence.

==我们要将在关于持久性的本书第三部分中了解更多关于 I/O、文件描述符等内容 287。==

By loading the code and static data into memory, by creating and initializing a stack, and by doing other work as related to I/O setup, the OS has now (finally) set the stage for program execution.288

==通过将代码和静态数据加载到内存中，通过创建和初始化栈，以及通过做与 I/O 设置相关的其他工作，操作系统现在（终于）为程序执行做好了准备 289290。291==

It thus has one last task: to start the program running at the entry point, namely main().

==292因此，它还有最后一个任务：在入口点（即 main()）开始运行程序 293。==

By jumping to the main() routine (through a specialized mechanism that we will discuss next chapter), the OS transfers control of the CPU to the newly-created process, and thus the program begins its execution.

==通过跳转到 main() 例程（通过我们将在下一章讨论的专门机制），操作系统将 CPU 的控制权转移给新创建的进程，从而程序开始执行 294。==

4.4 Process States

==4.4 进程状态==

Now that we have some idea of what a process is (though we will continue to refine this notion), and (roughly) how it is created, let us talk about the different states a process can be in at a given time.

==既然我们已经对什么是进程（尽管我们将继续完善这个概念）以及（大致）它是如何创建的有了一些概念，让我们来谈谈进程在给定时间可能处于的不同状态 295。==

The notion that a process can be in one of these states arose in early computer systems.

==进程可能处于这些状态之一的概念出现在早期的计算机系统中 296。==

In a simplified view, a process can be in one of three states:

==在一个简化的视图中，进程可以处于以下三种状态之一 297：==

- Running: In the running state, a process is running on a processor. This means it is executing instructions.
    
==**运行**：在运行状态下，进程正在处理器上运行。这意味着它正在执行指令 298。==
    
- Ready: In the ready state, a process is ready to run but for some reason the OS has chosen not to run it at this given moment.
    
==**就绪**：在就绪状态下，进程已准备好运行，但由于某种原因，操作系统选择在此时刻不运行它 299。==
    
- Blocked: In the blocked state, a process has performed some kind of operation that makes it not ready to run until some other event takes place. A common example: when a process initiates an I/O request to a disk, it becomes blocked and thus some other process can use the processor.
    
==**阻塞**：在阻塞状态下，进程执行了某种操作，使其在发生其他事件之前尚未准备好运行。一个常见的例子：当进程向磁盘发起 I/O 请求时，它会变为阻塞状态，因此其他进程可以使用处理器 300。==
    

If we were to map these states to a graph, we would arrive at the diagram in Figure 4.2.

==如果我们把这些状态映射到一个图表中，我们就会得到图 4.2 中的图表 301。==

As you can see in the diagram, a process can be moved between the ready and running states at the discretion of the OS.

==正如你在图表中看到的，操作系统可以自行决定将进程在就绪和运行状态之间移动 302。==

Being moved from ready to running means the process has been scheduled; being moved from running to ready means the process has been descheduled.

==从就绪移动到运行意味着进程已被调度；从运行移动到就绪意味着进程已被取消调度 303。==

Once a process has become blocked (e.g., by initiating an I/O operation), the OS will keep it as such until some event occurs (e.g., I/O completion); at that point, the process moves to the ready state again (and potentially immediately to running again, if the OS so decides).

==一旦进程变为阻塞状态（例如，通过发起 I/O 操作），操作系统将保持这种状态，直到发生某些事件（例如，I/O 完成）；在那一点上，进程再次移动到就绪状态（如果操作系统这样决定，也可能立即再次移动到运行状态） 304。==

Let's look at an example of how two processes might transition through some of these states.

==让我们看一个例子，看看两个进程如何通过其中一些状态进行转换 305。==

First, imagine two processes running, each of which only use the CPU (they do no I/O).

==首先，想象两个进程正在运行，每个进程只使用 CPU（它们不进行 I/O） 306。==

In this case, a trace of the state of each process might look like this (Figure 4.3).

==在这种情况下，每个进程的状态跟踪可能如图 4.3 所示 307。==

In this next example, the first process issues an I/O after running for some time.

==在下一个示例中，第一个进程在运行一段时间后发出 I/O 308。==

At that point, the process is blocked, giving the other process a chance to run.

==在那一点上，进程被阻塞，给另一个进程运行的机会 309。==

Figure 4.4 shows a trace of this scenario.

==图 4.4 显示了此场景的跟踪 310。==

More specifically, Process 0 initiates an I/O and becomes blocked waiting for it to complete; processes become blocked, for example, when reading from a disk or waiting for a packet from a network.311

==更具体地说，进程 0 发起 I/O 并变为阻塞状态等待其完成；例如，当从磁盘读取或等待来自网络的数据包时，进程会变为阻塞状态 312313。314==

The OS recognizes Process 0 is not using the CPU and starts running Process 1.

==操作系统识别出进程 0 未使用 CPU 并开始运行进程 1 315。==

While Process 1 is running, the I/O completes, moving Process 0 back to ready.

==当进程 1 运行时，I/O 完成，将进程 0 移回就绪状态 316。==

Finally, Process 1 finishes, and Process 0 runs and then is done.

==最后，进程 1 完成，进程 0 运行，然后完成 317。==

Note that there are many decisions the OS must make, even in this simple example.

==请注意，即使在这个简单的例子中，操作系统也必须做出许多决定 318。==

First, the system had to decide to run Process 1 while Process 0 issued an I/O; doing so improves resource utilization by keeping the CPU busy.

==首先，系统必须决定在进程 0 发出 I/O 时运行进程 1；这样做通过保持 CPU 忙碌来提高资源利用率 319。==

Second, the system decided not to switch back to Process 0 when its I/O completed; it is not clear if this is a good decision or not.320

==其次，系统决定在进程 0 的 I/O 完成时不切回进程 0；目前尚不清楚这是否是一个好的决定 321322。323==

What do you think?324

==你怎么看？ 325326==

These types of decisions are made by the OS scheduler, a topic we will discuss a few chapters in the futu327re.

==这类决定由操作系统调度程序做出，我们要将在未来几章讨论这个主题 328。==

4.5 Data Structures

==4.5 数据结构==

The OS is a program, and like any program, it has some key data structures that track various relevant pieces of information.329

==操作系统是一个程序，像任何程序一样，它有一些关键的数据结构来跟踪各种相关信息 330331。332==

To track the state of each process, for example, the O333S likely will keep some kind of process list for all processes that are ready and some additional information to track which process is currently running.

==为了跟踪每个进程的状态，例如，操作系统可能会为所有就绪的进程保留某种进程列表，并保留一些额外信息来跟踪当前正在运行哪个进程 334。==

The OS must also track, in some way, blocked processes; when an I/O event completes, the OS should make sure to wake the correct process and ready it to run again.

==操作系统还必须以某种方式跟踪阻塞的进程；当 I/O 事件完成时，操作系统应确保唤醒正确的进程并使其准备好再次运行 335。==

Figure 4.5 shows what type of information an OS needs to track about each process in the xv6 kernel.

==图 4.5 显示了操作系统需要在 xv6 内核中跟踪关于每个进程的哪种类型的信息 336。==

Similar process structures exist in "real" operating systems such as Linux, Mac OS X, or Windows; look them up and see how much more complex they are.337

==类似的进程结构存在于“真实”操作系统中，如 Linux、Mac OS X 或 Windows；查阅它们，看看它们要复杂多少 338339。340==

From the figure, you can see a couple of important pieces o341f information the OS tracks about a process.

==从图中，你可以看到操作系统跟踪的关于进程的几个重要信息 342。==

The register context will hold, for a stopped process, the contents of its registers.

==**寄存器上下文**将保存已停止进程的寄存器内容 343343343343。==

When a process is stopped, its registers will be saved to this memory location; by restoring these registers (i.e., placing their values back into the actual physical registers), the OS can resume running the process.

==当进程停止时，其寄存器将保存到此内存位置；通过恢复这些寄存器（即将其值放回实际物理寄存器中），操作系统可以恢复运行进程 344。==

We'll learn more about this technique known as a context switch in future chapters.

==我们要将在以后的章节中了解更多关于这种称为上下文切换的技术 345。==

You can also see from the figure that there are some other states a process can be in, beyond running, ready, and blocked.

==你还可以从图中看到，除了运行、就绪和阻塞之外，进程还可以处于其他一些状态 346347348。349350==

Sometimes a system will have an initial state that the process is in when it is being created.351352

==有时系统会有一个初始状态，即进程在创建时所处的状态 353354355。356357==

Also, a process could be placed in a final state where it has exite358d but has not yet been cleaned up (in UN359IX-based systems, this is called the zombie state).

==此外，进程可能会处于最终状态，即它已退出但尚未被清理（在基于 UNIX 的系统中，这称为僵尸状态） 360360360360。==

This final state can be useful as it allows other processes (usually the parent that created the process) to examine the return code of the process and see if the just-finished process executed successfully (usually, programs return zero in UNIX-based systems when they have accomplished a task successfully, and non-zero otherwise).

==这个最终状态很有用，因为它允许其他进程（通常是创建该进程的父进程）检查该进程的返回代码，并查看刚完成的进程是否成功执行（通常，在基于 UNIX 的系统中，程序成功完成任务时返回零，否则返回非零值） 361。==

When finished, the parent will make one final call (e.g., wait()) to wait for the completion of the child, and to also indicate to the OS that it can clean up any relevant data structures that referred to the now-extinct process.362

==完成后，父进程将进行最后一次调用（例如，wait()）以等待子进程完成，并向操作系统指示它可以清理引用该现已灭绝进程的任何相关数据结构 363364。365==

Aside: Data Structure - The Process List366

==旁367白：数据结构——进程列表==

Operating systems are replete with various important data structures that we will discuss in these notes.

==操作系统充满了我们将在这些笔记中讨论的各种重要数据结构 368。==

The process list (also called the task list) is the first such structure.

==**进程列表**（也称为**任务列表**）是第一个这样的结构 369。==

It is one of the simpler ones, but certainly any OS that has the ability to run multiple programs at once will have something akin to this structure in order to keep track of all the running programs in the system.

==它是较简单的结构之一，但任何有能力同时运行多个程序的操作系统肯定会有类似于此结构的东西，以便跟踪系统中所有正在运行的程序 370。==

Sometimes people refer to the individual structure that stores information about a process as a Process Control Block (PCB), a fancy way of talking about a C structure that contains information about each process (also sometimes called a process descriptor).

==有时人们将存储有关进程信息的单个结构称为进程控制块 (PCB)，这是一种谈论包含有关每个进程信息的 C 结构的花哨方式（有时也称为进程描述符） 371。==

4.6 Summary

==4.6 总结==

We have introduced the most basic abstraction of the OS: the process.

==我们要已经介绍了操作系统最基本的抽象：进程 372。==

It is quite simply viewed as a running program.

==它被简单地视为一个正在运行的程序 373。==

With this conceptual view in mind, we will now move on to the nitty-gritty: the low-level mechanisms needed to implement processes, and the higher-level policies required to schedule them in an intelligent way.

==有了这个概念性的观点，我们要现在将通过具体细节：实现进程所需的低级机制，以及以智能方式调度它们所需的高级策略 374。==

By combining mechanisms and policies, we will build up our understanding of how an operating system virtualizes the CPU.

==通过结合机制和策略，我们要将建立对操作系统如何虚拟化 CPU 的理解 375。==

Aside: Key Process Terms

==旁白：关键进程术语==

- The process is the major OS abstraction of a running program. At any point in time, the process can be described by its state: the contents of memory in its address space, the contents of CPU registers (including the program counter and stack pointer, among others), and information about I/O (such as open files which can be read or written).
    
==**进程**是正在运行的程序的主要操作系统抽象。在任何时间点，进程都可以通过其状态来描述：其地址空间中的内存内容、CPU 寄存器的内容（包括程序计数器和栈指针等）以及有关 I/O 的信息（例如可以读取或写入的打开文件） 376。==
    
- The process API consists of calls programs can make related to processes. Typically, this includes creation, destruction, and other useful calls.
    
==**进程 API** 由程序可以进行的与进程相关的调用组成。通常，这包括创建、销毁和其他有用的调用 377。==
    
- Processes exist in one of many different process states, including running, ready to run, and blocked. Different events (e.g., getting scheduled or descheduled, or waiting for an I/O to complete) transition a process from one of these states to the other.
    
==**进程**存在于许多不同的**进程状态**之一中，包括运行、准备运行和阻塞。不同的事件（例如，被调度或取消调度，或等待 I/O 完成）将进程从这些状态之一转换为另一种状态 378。==
    
- A process list contains information about all processes in the system. Each entry is found in what is sometimes called a process control block (PCB), which is really just a structure that contains information about a specific process.
    
==**进程列表**包含有关系统中所有进程的信息。每个条目都在有时称为**进程控制块** (PCB) 的结构中找到，这实际上只是一个包含有关特定进程信息的结构 379。==

Homework (Simulation)
==家庭作业（模拟）==

THE ABSTRACTION: THE PROCESS
==抽象：进程==

This program, process-run.py, allows you to see how process states change as programs run and either use the CPU (e.g., perform an add instruction) or do I/O (e.g., send a request to a disk and wait for it to complete).
==这个程序 process-run.py 允许你查看进程状态在程序运行时如何变化，包括使用 CPU（例如，执行加法指令）或进行 I/O（例如，向磁盘发送请求并等待完成）。==

See the README for details.
==详情请参阅 README 文件。==

Questions
==问题==

1. Run process-run.py with the following flags: -l 5:100,5:100.
==2. 使用以下标志运行 process-run.py：-l 5:100,5:100。==

What should the CPU utilization be (e.g., the percent of time the CPU is in use?)
==CPU 利用率应该是多少（例如，CPU 处于使用状态的时间百分比）？==

Why do you know this?
==你是怎么知道的？==

Use the -c and -p flags to see if you were right.
==使用 -c 和 -p 标志来查看你是否正确。==

2. Now run with these flags: ./process-run.py -l 4:100,1:0.
==3. 现在使用这些标志运行：./process-run.py -l 4:100,1:0。==

These flags specify one process with 4 instructions (all to use the CPU), and one that simply issues an I/O and waits for it to be done.
==这些标志指定了一个包含 4 条指令的进程（全部使用 CPU），以及一个仅仅发出 I/O 请求并等待其完成的进程。==

How long does it take to complete both processes?
==完成这两个进程需要多长时间？==

Use -c and -p to find out if you were right.
==使用 -c 和 -p 来确定你是否正确。==

3. Switch the order of the processes: -l 1:0,4:100.
==4. 交换进程的顺序：-l 1:0,4:100。==

What happens now?
==现在会发生什么？==

Does switching the order matter?
==交换顺序有影响吗？==

Why?
==为什么？==

(As always, use -c and -p to see if you were right)
==（像往常一样，使用 -c 和 -p 来查看你是否正确）==

4. We'll now explore some of the other flags.
==5. 我们现在将探索其他一些标志。==

One important flag is -S, which determines how the system reacts when a process issues an I/O.
==一个重要的标志是 -S，它决定了系统在进程发出 I/O 请求时的反应。==

With the flag set to SWITCH_ON_END, the system will NOT switch to another process while one is doing I/O, instead waiting until the process is completely finished.
==当标志设置为 SWITCH_ON_END 时，系统在一个进程进行 I/O 时不会切换到另一个进程，而是等待该进程完全结束。==

What happens when you run the following two processes (-l 1:0,4:100 -c -S SWITCH_ON_END), one doing I/O and the other doing CPU work?
==当你运行以下两个进程（-l 1:0,4:100 -c -S SWITCH_ON_END），一个进行 I/O 而另一个进行 CPU 工作时，会发生什么？==

5. Now, run the same processes, but with the switching behavior set to switch to another process whenever one is WAITING for I/O (-l 1:0,4:100 -c -S SWITCH_ON_IO).
==6. 现在，运行相同的进程，但将切换行为设置为每当有进程等待 I/O 时就切换到另一个进程（-l 1:0,4:100 -c -S SWITCH_ON_IO）。==

What happens now?
==现在会发生什么？==

Use -c and -p to confirm that you are right.
==使用 -c 和 -p 来确认你是否正确。==

6. One other important behavior is what to do when an I/O completes.
==7. 另一个重要的行为是当 I/O 完成时该做什么。==

With -I IO_RUN_LATER, when an I/O completes, the process that issued it is not necessarily run right away; rather, whatever was running at the time keeps running.
==使用 -I IO_RUN_LATER，当 I/O 完成时，发出该请求的进程不一定立即运行；相反，当时正在运行的进程会继续运行。==

What happens when you run this combination of processes? (./process-run.py -l 3:0,5:100,5:100,5:100 -S SWITCH_ON_IO -c -p -I IO_RUN_LATER)
==当你运行这种进程组合时会发生什么？（./process-run.py -l 3:0,5:100,5:100,5:100 -S SWITCH_ON_IO -c -p -I IO_RUN_LATER）==

Are system resources being effectively utilized?
==系统资源是否得到了有效利用？==

7. Now run the same processes, but with -I IO_RUN_IMMEDIATE set, which immediately runs the process that issued the I/O.
==8. 现在运行相同的进程，但设置 -I IO_RUN_IMMEDIATE，这会立即运行发出 I/O 请求的进程。==

How does this behavior differ?
==这种行为有何不同？==

Why might running a process that just completed an I/O again be a good idea?
==为什么重新运行刚刚完成 I/O 的进程可能是一个好主意？==

8. Now run with some randomly generated processes using flags -s 1 -l 3:50,3:50 or -s 2 -l 3:50,3:50 or -s 3 -l 3:50,3:50.
==9. 现在使用标志 -s 1 -l 3:50,3:50 或 -s 2 -l 3:50,3:50 或 -s 3 -l 3:50,3:50 运行一些随机生成的进程。==

See if you can predict how the trace will turn out.
==看看你是否能预测追踪结果会是怎样。==

What happens when you use the flag -I IO_RUN_IMMEDIATE versus that flag -I IO_RUN_LATER?
==当你使用 -I IO_RUN_IMMEDIATE 标志与 -I IO_RUN_LATER 标志时，会发生什么？==

What happens when you use the flag -S SWITCH_ON_IO versus -S SWITCH_ON_END?
==当你使用 -S SWITCH_ON_IO 标志与 -S SWITCH_ON_END 标志时，会发生什么？==

Interlude: Process API
==插曲：进程 API==

ASIDE: INTERLUDES
==旁白：关于插曲==

Interludes will cover more practical aspects of systems, including a particular focus on operating system APIs and how to use them.
==插曲章节将涵盖系统更实际的方面，特别是侧重于操作系统 API 及其使用方法。==

If you don't like practical things, you could skip these interludes.
==如果你不喜欢实用的东西，你可以跳过这些插曲。==

But you should like practical things, because, well, they are generally useful in real life; companies, for example, don't usually hire you for your non-practical skills.
==但你应该喜欢实用的东西，因为，好吧，它们在现实生活中通常很有用；例如，公司通常不会因为你的非实用技能而雇佣你。==

In this interlude, we discuss process creation in UNIX systems.
==在这个插曲中，我们将讨论 UNIX 系统中的进程创建。==

UNIX presents one of the most intriguing ways to create a new process with a pair of system calls: fork() and exec().
==UNIX 提供了一种最有趣的方式来通过一对系统调用创建新进程：fork() 和 exec()。==

A third routine, wait(), can be used by a process wishing to wait for a process it has created to complete.
==第三个例程 wait() 可供希望等待其创建的进程完成的进程使用。==

We now present these interfaces in more detail, with a few simple examples to motivate us.
==我们现在将更详细地介绍这些接口，并辅以几个简单的例子来激发我们的兴趣。==

And thus, our problem:
==因此，我们的问题是：==

CRUX: HOW TO CREATE AND CONTROL PROCESSES
==关键问题：如何创建和控制进程==

What interfaces should the OS present for process creation and control?
==操作系统应该为进程创建和控制提供什么接口？==

How should these interfaces be designed to enable powerful functionality, ease of use, and high performance?
==应该如何设计这些接口以实现强大的功能、易用性和高性能？==

5.1 The fork() System Call
==5.1 fork() 系统调用==

The fork() system call is used to create a new process.
==fork() 系统调用用于创建一个新进程。==

However, be forewarned: it is certainly the strangest routine you will ever call.
==然而，预先警告：这肯定是你调用过的最奇怪的例程。==

More specifically, you have a running program whose code looks like what you see in Figure 5.1; examine the code, or better yet, type it in and run it yourself!
==具体来说，你有一个正在运行的程序，其代码如图 5.1 所示；检查代码，或者更好的是，把它输入进去并亲自运行它！==

Well, OK, we admit that we don't know that for sure; who knows what routines you call when no one is looking?
==好吧，确实，我们要承认我们要对此并不十分确定；谁知道你在没人看的时候会调用什么例程呢？==

But fork() is pretty odd, no matter how unusual your routine-calling patterns are.
==但是，无论你的例程调用习惯多么不寻常，fork() 都是相当奇怪的。==

Figure 5.1: Calling fork() (p1.c)
==图 5.1：调用 fork() (p1.c)==

When you run this program (called p1.c), you'll see the following:
==当你运行这个程序（名为 p1.c）时，你会看到以下内容：==

Let us understand what happened in more detail in p1.c.
==让我们更详细地了解 p1.c 中发生了什么。==

When it first started running, the process prints out a hello message; included in that message is its process identifier, also known as a PID.
==当它第一次开始运行时，进程打印出一条 hello 消息；该消息中包含其进程标识符，也称为 PID。==

The process has a PID of 29146; in UNIX systems, the PID is used to name the process if one wants to do something with the process, such as (for example) stop it from running.
==该进程的 PID 为 29146；在 UNIX 系统中，如果想要对进程进行某种操作，例如（举个例子）停止其运行，则使用 PID 来命名该进程。==

So far, so good.
==到目前为止，一切顺利。==

Now the interesting part begins.
==现在有趣的部分开始了。==

The process calls the fork() system call, which the OS provides as a way to create a new process.
==该进程调用 fork() 系统调用，这是操作系统提供的一种创建新进程的方式。==

The odd part: the process that is created is an (almost) exact copy of the calling process.
==奇怪的部分是：被创建的进程是调用进程的一个（几乎）完全相同的副本。==

That means that to the OS, it now looks like there are two copies of the program p1 running, and both are about to return from the fork() system call.
==这意味着对于操作系统来说，现在看起来好像有两个 p1 程序的副本在运行，并且它们都即将从 fork() 系统调用返回。==

The newly-created process (called the child, in contrast to the creating parent) doesn't start running at main(), like you might expect (note, the "hello" message only got printed out once); rather, it just comes into life as if it had called fork() itself.
==新创建的进程（称为子进程，与创建它的父进程相对）并不像你预期的那样从 main() 开始运行（注意，"hello" 消息只打印了一次）；相反，它就像自己调用了 fork() 一样开始运行。==

Figure 5.2: Calling fork() And wait() (p2.c)
==图 5.2：调用 fork() 和 wait() (p2.c)==

You might have noticed: the child isn't an exact copy.
==你可能已经注意到了：子进程并非完全相同的副本。==

Specifically, although it now has its own copy of the address space (i.e., its own private memory), its own registers, its own PC, and so forth, the value it returns to the caller of fork() is different.
==具体来说，虽然它现在拥有自己的地址空间副本（即它自己的私有内存）、自己的寄存器、自己的 PC 等等，但它返回给 fork() 调用者的值是不同的。==

Specifically, while the parent receives the PID of the newly-created child, the child receives a return code of zero.
==具体而言，父进程接收到新创建子进程的 PID，而子进程接收到的返回码为零。==

This differentiation is useful, because it is simple then to write the code that handles the two different cases (as above).
==这种区分很有用，因为这样编写处理两种不同情况的代码就很简单了（如上所示）。==

You might also have noticed: the output (of p1.c) is not deterministic.
==你可能还注意到了：(p1.c 的) 输出不是确定性的。==

When the child process is created, there are now two active processes in the system that we care about: the parent and the child.
==当子进程被创建时，系统中现在有两个我们关心的活动进程：父进程和子进程。==

Assuming we are running on a system with a single CPU (for simplicity), then either the child or the parent might run at that point.
==假设我们在单 CPU 系统上运行（为了简单起见），那么此时子进程或父进程都可能运行。==

In our example (above), the parent did and thus printed out its message first.
==在我们的例子（如上）中，父进程运行了，因此先打印了它的消息。==

In other cases, the opposite might happen.
==在其他情况下，可能会发生相反的情况。==

The CPU scheduler, a topic we'll discuss in great detail soon, determines which process runs at a given moment in time; because the scheduler is complex, we cannot usually make strong assumptions about what it will choose to do, and hence which process will run first.
==CPU 调度程序（我们很快将详细讨论的一个主题）决定了在给定时刻运行哪个进程；由于调度程序很复杂，我们通常无法对其选择做什么以及哪个进程将先运行做出强有力的假设。==

This non-determinism, as it turns out, leads to some interesting problems, particularly in multi-threaded programs; hence, we'll see a lot more non-determinism when we study concurrency in the second part of the book.
==事实证明，这种不确定性会导致一些有趣的问题，特别是在多线程程序中；因此，我们在本书第二部分学习并发时，将会看到更多的不确定性。==

5.2 The wait() System Call
==5.2 wait() 系统调用==

So far, we haven't done much: just created a child that prints out a message and exits.
==到目前为止，我们做的事情并不多：只是创建了一个打印消息并退出的子进程。==

Sometimes, as it turns out, it is quite useful for a parent to wait for a child process to finish what it has been doing.
==事实证明，有时父进程等待子进程完成其工作是非常有用的。==

This task is accomplished with the wait() system call (or its more complete sibling waitpid()); see Figure 5.2 for details.
==这个任务是通过 wait() 系统调用（或其更完整的兄弟 waitpid()）完成的；详情请见图 5.2。==

In this example (p2.c), the parent process calls wait() to delay its execution until the child finishes executing.
==在这个例子 (p2.c) 中，父进程调用 wait() 来延迟其执行，直到子进程执行完毕。==

When the child is done, wait() returns to the parent.
==当子进程完成后，wait() 返回给父进程。==

Adding a wait() call to the code above makes the output deterministic.
==在上面的代码中添加 wait() 调用使得输出变得具有确定性。==

Can you see why?
==你能明白为什么吗？==

Go ahead, think about it.
==来吧，思考一下。==

(waiting for you to think .... and done)
==（等待你思考……思考完毕）==

Now that you have thought a bit, here is the output:
==既然你已经思考了一会儿，这里是输出结果：==

With this code, we now know that the child will always print first.
==有了这段代码，我们现在知道子进程总是会先打印。==

Why do we know that?
==为什么我们知道这一点？==

Well, it might simply run first, as before, and thus print before the parent.
==好吧，它可能像以前一样简单地先运行，从而在父进程之前打印。==

However, if the parent does happen to run first, it will immediately call wait(); this system call won't return until the child has run and exited.
==然而，如果父进程碰巧先运行，它将立即调用 wait()；在这个系统调用直到子进程运行并退出之前都不会返回。==

Thus, even when the parent runs first, it politely waits for the child to finish running, then wait() returns, and then the parent prints its message.
==因此，即使父进程先运行，它也会礼貌地等待子进程运行结束，然后 wait() 返回，接着父进程打印它的消息。==

5.3 Finally, The exec() System Call
==5.3 最后，exec() 系统调用==

A final and important piece of the process creation API is the exec() system call.
==进程创建 API 的最后也是重要的一部分是 exec() 系统调用。==

This system call is useful when you want to run a program that is different from the calling program.
==当你想要运行一个与调用程序不同的程序时，这个系统调用非常有用。==

For example, calling fork() in p2.c is only useful if you want to keep running copies of the same program.
==例如，在 p2.c 中调用 fork() 只有在你想继续运行同一个程序的副本时才有用。==

However, often you want to run a different program; exec() does just that (Figure 5.3).
==然而，你通常想要运行一个不同的程序；exec() 就是用来做这个的（图 5.3）。==

Figure 5.3: Calling fork(), wait(), And exec() (p3.c)
==图 5.3：调用 fork()、wait() 和 exec() (p3.c)==

In this example, the child process calls execvp() in order to run the program wc, which is the word counting program.
==在这个例子中，子进程调用 execvp() 以运行 wc 程序，即字数统计程序。==

In fact, it runs wc on the source file p3.c, thus telling us how many lines, words, and bytes are found in the file.
==实际上，它在源文件 p3.c 上运行 wc，从而告诉我们文件中包含多少行、单词和字节。==

The fork() system call is strange; its partner in crime, exec(), is not so normal either.
==fork() 系统调用很奇怪；它的同伙 exec() 也没那么正常。==

What it does: given the name of an executable (e.g., wc), and some arguments (e.g., p3.c), it loads code (and static data) from that executable and overwrites its current code segment (and current static data) with it; the heap and stack and other parts of the memory space of the program are re-initialized.
==它的作用是：给定一个可执行文件的名称（例如 wc）和一些参数（例如 p3.c），它从该可执行文件加载代码（和静态数据），并用它覆盖当前的代码段（和当前的静态数据）；程序的堆、栈和内存空间的其他部分将被重新初始化。==

Then the OS simply runs that program, passing in any arguments as the argv of that process.
==然后操作系统只是简单地运行该程序，将任何参数作为该进程的 argv 传入。==

Thus, it does not create a new process; rather, it transforms the currently running program (formerly p3) into a different running program (wc).
==因此，它不会创建一个新进程；相反，它将当前正在运行的程序（以前是 p3）转变为另一个正在运行的程序（wc）。==

After the exec() in the child, it is almost as if p3.c never ran; a successful call to exec() never returns.
==在子进程中执行 exec() 之后，就好像 p3.c 从未运行过一样；对 exec() 的成功调用永远不会返回。==

TIP: GETTING IT RIGHT (LAMPSON'S LAW)
==提示：做正确的事（兰普森定律）==

As Lampson states in his well-regarded "Hints for Computer Systems Design", "Get it right. Neither abstraction nor simplicity is a substitute for getting it right."
==正如 Lampson 在其备受推崇的“计算机系统设计提示”中所述，“把事情做对。无论是抽象还是简单都无法替代正确性。”==

Sometimes, you just have to do the right thing, and when you do, it is way better than the alternatives.
==有时候，你只需要做正确的事，当你这样做时，它比其他选择要好得多。==

There are lots of ways to design APIs for process creation; however, the combination of fork() and exec() are simple and immensely powerful.
==设计用于进程创建的 API 有很多种方法；然而，fork() 和 exec() 的组合既简单又非常强大。==

Here, the UNIX designers simply got it right.
==在这里，UNIX 设计师们确实做对了。==

And because Lampson so often "got it right", we name the law in his honor.
==因为 Lampson 经常“做对”，我们以他的名字命名了这条定律。==

5.4 Why? Motivating The API
==5.4 为什么？API 的设计初衷==

Of course, one big question you might have: why would we build such an odd interface to what should be the simple act of creating a new process?
==当然，你可能会有一个大问题：为什么我们要为创建新进程这一本该简单的行为构建如此奇怪的接口？==

Well, as it turns out, the separation of fork() and exec() is essential in building a UNIX shell, because it lets the shell run code after the call to fork() but before the call to exec(); this code can alter the environment of the about-to-be-run program, and thus enables a variety of interesting features to be readily built.
==事实证明，将 fork() 和 exec() 分离对于构建 UNIX shell 至关重要，因为它允许 shell 在调用 fork() 之后但在调用 exec() 之前运行代码；这些代码可以改变即将运行的程序的环境，从而可以轻松构建各种有趣的功能。==

The shell is just a user program.
==Shell 只是一个用户程序。==

It shows you a prompt and then waits for you to type something into it.
==它显示一个提示符，然后等待你输入内容。==

You then type a command (i.e., the name of an executable program, plus any arguments) into it; in most cases, the shell then figures out where in the file system the executable resides, calls fork() to create a new child process to run the command, calls some variant of exec() to run the command, and then waits for the command to complete by calling wait().
==然后你输入一个命令（即一个可执行程序的名称，加上任何参数）；在大多数情况下，shell 随后会找出可执行文件在文件系统中的位置，调用 fork() 创建一个新的子进程来运行该命令，调用 exec() 的某种变体来运行该命令，然后通过调用 wait() 等待命令完成。==

When the child completes, the shell returns from wait() and prints out a prompt again, ready for your next command.
==当子进程完成后，shell 从 wait() 返回并再次打印提示符，准备接受你的下一个命令。==

The separation of fork() and exec() allows the shell to do a whole bunch of useful things rather easily.
==fork() 和 exec() 的分离使得 shell 可以相当容易地做很多有用的事情。==

For example: prompt> wc p3.c > newfile.txt
==例如：prompt> wc p3.c > newfile.txt==

In the example above, the output of the program wc is redirected into the output file newfile.txt (the greater-than sign is how said redirection is indicated).
==在上面的例子中，程序 wc 的输出被重定向到输出文件 newfile.txt 中（大于号是表示这种重定向的方式）。==

The way the shell accomplishes this task is quite simple: when the child is created, before calling exec(), the shell (specifically, the code executed in the child process) closes standard output and opens the file newfile.txt.
==Shell 完成此任务的方式非常简单：当子进程被创建时，在调用 exec() 之前，shell（具体来说，是在子进程中执行的代码）关闭标准输出并打开文件 newfile.txt。==

By doing so, any output from the soon-to-be-running program wc is sent to the file instead of the screen (open file descriptors are kept open across the exec() call, thus enabling this behavior).
==这样做之后，即将运行的程序 wc 的任何输出都会发送到文件而不是屏幕（打开的文件描述符在 exec() 调用期间保持打开状态，从而实现了这种行为）。==

Figure 5.4: All Of The Above With Redirection (p4.c)
==图 5.4：包含上述所有内容的重定向示例 (p4.c)==

Figure 5.4 (page 8) shows a program that does exactly this.
==图 5.4（第 8 页）展示了一个完全执行此操作的程序。==

The reason this redirection works is due to an assumption about how the operating system manages file descriptors.
==这种重定向之所以有效，是因为基于操作系统如何管理文件描述符的一个假设。==

Specifically, UNIX systems start looking for free file descriptors at zero.
==具体来说，UNIX 系统从零开始查找空闲的文件描述符。==

In this case, STDOUT_FILENO will be the first available one and thus get assigned when open() is called.
==在这种情况下，STDOUT_FILENO 将是第一个可用的描述符，因此会在调用 open() 时被分配。==

Subsequent writes by the child process to the standard output file descriptor, for example by routines such as printf(), will then be routed transparently to the newly-opened file instead of the screen.
==随后子进程对标准输出文件描述符的写入，例如通过 printf() 等例程，将透明地路由到新打开的文件而不是屏幕。==

You'll notice (at least) two interesting tidbits about this output.
==你会注意到关于这个输出的（至少）两个有趣的细节。==

First, when p4 is run, it looks as if nothing has happened; the shell just prints the command prompt and is immediately ready for your next command.
==首先，当运行 p4 时，看起来好像什么也没发生；shell 只是打印命令提示符并立即准备好接受你的下一个命令。==

However, that is not the case; the program p4 did indeed call fork() to create a new child, and then run the wc program via a call to execvp().
==然而，情况并非如此；程序 p4 确实调用了 fork() 来创建一个新的子进程，然后通过调用 execvp() 运行了 wc 程序。==

You don't see any output printed to the screen because it has been redirected to the file p4.output.
==你看不到任何输出打印到屏幕上，因为它已被重定向到文件 p4.output。==

Second, you can see that when we cat the output file, all the expected output from running wc is found.
==其次，你可以看到当我们 cat 输出文件时，可以找到运行 wc 的所有预期输出。==

Cool, right?
==很酷，对吧？==

UNIX pipes are implemented in a similar way, but with the pipe() system call.
==UNIX 管道的实现方式类似，但使用的是 pipe() 系统调用。==

In this case, the output of one process is connected to an in-kernel pipe (i.e., queue), and the input of another process is connected to that same pipe; thus, the output of one process seamlessly is used as input to the next, and long and useful chains of commands can be strung together.
==在这种情况下，一个进程的输出连接到一个内核管道（即队列），另一个进程的输入连接到同一个管道；因此，一个进程的输出无缝地用作下一个进程的输入，并且可以将长而有用的命令链串联在一起。==

As a simple example, consider looking for a word in a file, and then counting how many times said word occurs; with pipes and the utilities grep and wc, it is easy; just type grep -o foo file | wc -l into the command prompt and marvel at the result.
==作为一个简单的例子，考虑在文件中查找一个单词，然后统计该单词出现的次数；使用管道以及工具 grep 和 wc，这很容易；只需在命令提示符中输入 grep -o foo file | wc -l 并惊叹于结果。==

Finally, while we just have sketched out the process API at a high level, there is a lot more detail about these calls out there to be learned and digested; we'll learn more, for example, about file descriptors when we talk about file systems in the third part of the book.
==最后，虽然我们只是在高层次上勾勒了进程 API，但关于这些调用还有很多细节需要学习和消化；例如，当我们在本书第三部分讨论文件系统时，我们将学习更多关于文件描述符的知识。==

For now, suffice it to say that the fork()/exec() combination is a powerful way to create and manipulate processes.
==目前，只需说 fork()/exec() 组合是创建和操作进程的一种强大方式就足够了。==

5.5 Process Control And Users
==5.5 进程控制和用户==

Beyond fork(), exec(), and wait(), there are a lot of other interfaces for interacting with processes in UNIX systems.
==除了 fork()、exec() 和 wait() 之外，UNIX 系统中还有许多其他用于与进程交互的接口。==

For example, the kill() system call is used to send signals to a process, including directives to pause, die, and other useful imperatives.
==例如，kill() 系统调用用于向进程发送信号，包括暂停、终止和其他有用命令的指令。==

For convenience, in most UNIX shells, certain keystroke combinations are configured to deliver a specific signal to the currently running process; for example, control-c sends a SIGINT (interrupt) to the process (normally terminating it) and control-z sends a SIGTSTP (stop) signal thus pausing the process in mid-execution (you can resume it later with a command, e.g., the fg built-in command found in many shells).
==为了方便起见，在大多数 UNIX shell 中，某些组合键被配置为向当前运行的进程发送特定信号；例如，control-c 向进程发送 SIGINT（中断）（通常会终止它），而 control-z 发送 SIGTSTP（停止）信号，从而在执行中途暂停进程（你可以稍后使用命令恢复它，例如许多 shell 中的 fg 内置命令）。==

The entire signals subsystem provides a rich infrastructure to deliver external events to processes, including ways to receive and process those signals within individual processes, and ways to send signals to individual processes as well as entire process groups.
==整个信号子系统提供了一个丰富的基础设施，用于向进程传递外部事件，包括在单个进程内接收和处理这些信号的方法，以及向单个进程和整个进程组发送信号的方法。==

ASIDE: RTFM-READ THE MAN PAGES
==旁白：RTFM——阅读手册页==

Many times in this book, when referring to a particular system call or library call, we'll tell you to read the manual pages, or man pages for short.
==在本书中，很多时候当提到特定的系统调用或库调用时，我们会告诉你去阅读手册页，简称 man pages。==

Man pages are the original form of documentation that exist on UNIX systems; realize that they were created before the thing called the web existed.
==手册页是 UNIX 系统上存在的原始文档形式；要意识到它们是在所谓的万维网出现之前创建的。==

Spending some time reading man pages is a key step in the growth of a systems programmer; there are tons of useful tidbits hidden in those pages.
==花时间阅读手册页是系统程序员成长的关键一步；这些页面中隐藏着大量有用的趣闻。==

Some particularly useful pages to read are the man pages for whichever shell you are using (e.g., tcsh, or bash), and certainly for any system calls your program makes (in order to see what return values and error conditions exist).
==一些特别值得阅读的页面包括你正在使用的任何 shell（例如 tcsh 或 bash）的手册页，当然还有你的程序进行的任何系统调用的手册页（以便查看存在哪些返回值和错误条件）。==

Finally, reading the man pages can save you some embarrassment.
==最后，阅读手册页可以为你避免一些尴尬。==

When you ask colleagues about some intricacy of fork(), they may simply reply: "RTFM."
==当你向同事询问 fork() 的某些复杂之处时，他们可能只是简单地回答：“RTFM。”==

This is your colleagues' way of gently urging you to Read The Man pages.
==这是你的同事温和地敦促你阅读手册页的方式。==

The F in RTFM just adds a little color to the phrase...
==RTFM 中的 F 只是为这个短语增添了一点色彩……==

To use this form of communication, a process should use the signal() system call to "catch" various signals; doing so ensures that when a particular signal is delivered to a process, it will suspend its normal execution and run a particular piece of code in response to the signal.
==要使用这种通信形式，进程应该使用 signal() 系统调用来“捕捉”各种信号；这样做可以确保当特定信号传递给进程时，它将暂停其正常执行并运行特定的代码片段来响应该信号。==

Read elsewhere to learn more about signals and their many intricacies.
==请阅读其他资料以了解有关信号及其许多复杂之处的更多信息。==

This naturally raises the question: who can send a signal to a process, and who cannot?
==这自然引出了一个问题：谁可以向进程发送信号，谁不可以？==

Generally, the systems we use can have multiple people using them at the same time; if one of these people can arbitrarily send signals such as SIGINT (to interrupt a process, likely terminating it), the usability and security of the system will be compromised.
==通常，我们要使用的系统可能有多人同时使用；如果其中一个人可以任意发送诸如 SIGINT 之类的信号（以中断进程，可能会终止它），系统的可用性和安全性将受到损害。==

As a result, modern systems include a strong conception of the notion of a user.
==因此，现代系统包含了一个强烈的“用户”概念。==

The user, after entering a password to establish credentials, logs in to gain access to system resources.
==用户在输入密码建立凭据后，登录以获得对系统资源的访问权限。==

The user may then launch one or many processes, and exercise full control over them (pause them, kill them, etc.).
==然后用户可以启动一个或多个进程，并对它们行使完全控制权（暂停它们、杀死它们等）。==

Users generally can only control their own processes; it is the job of the operating system to parcel out resources (such as CPU, memory, and disk) to each user (and their processes) to meet overall system goals.
==用户通常只能控制自己的进程；操作系统的职责是将资源（如 CPU、内存和磁盘）分配给每个用户（及其进程），以实现整体系统目标。==

5.6 Useful Tools
==5.6 有用的工具==

There are many command-line tools that are useful as well.
==还有许多命令行工具也很有用。==

For example, using the ps command allows you to see which processes are running; read the man pages for some useful flags to pass to ps.
==例如，使用 ps 命令可以让你查看哪些进程正在运行；阅读手册页以了解一些传递给 ps 的有用标志。==

The tool top is also quite helpful, as it displays the processes of the system and how much CPU and other resources they are eating up.
==工具 top 也很有帮助，因为它显示系统中的进程以及它们消耗了多少 CPU 和其他资源。==

Humorously, many times when you run it, top claims it is the top resource hog; perhaps it is a bit of an egomaniac.
==幽默的是，很多时候当你运行它时，top 声称它是最大的资源占用者；也许它有点自大狂。==

The command kill can be used to send arbitrary signals to processes, as can the slightly more user friendly killall.
==命令 kill 可用于向进程发送任意信号，稍微更用户友好的 killall 也可以。==

Be sure to use these carefully; if you accidentally kill your window manager, the computer you are sitting in front of may become quite difficult to use.
==请务必小心使用这些命令；如果你不小心杀死了你的窗口管理器，你面前的计算机可能会变得非常难以使用。==

ASIDE: THE SUPERUSER (ROOT)
==旁白：超级用户 (ROOT)==

A system generally needs a user who can administer the system, and is not limited in the way most users are.
==系统通常需要一个能够管理系统的用户，并且不受大多数用户的限制。==

Such a user should be able to kill an arbitrary process (e.g., if it is abusing the system in some way), even though that process was not started by this user.
==这样的用户应该能够杀死任意进程（例如，如果它以某种方式滥用系统），即使该进程不是由该用户启动的。==

Such a user should also be able to run powerful commands such as shutdown (which, unsurprisingly, shuts down the system).
==这样的用户还应该能够运行强大的命令，例如 shutdown（毫不奇怪，它会关闭系统）。==

In UNIX-based systems, these special abilities are given to the superuser (sometimes called root).
==在基于 UNIX 的系统中，这些特殊能力被赋予超级用户（有时称为 root）。==

While most users can't kill other users' processes, the superuser can.
==虽然大多数用户不能杀死其他用户的进程，但超级用户可以。==

Being root is much like being Spider-Man: with great power comes great responsibility.
==成为 root 很像成为蜘蛛侠：能力越大，责任越大。==

Thus, to increase security (and avoid costly mistakes), it's usually better to be a regular user; if you do need to be root, tread carefully, as all of the destructive powers of the computing world are now at your fingertips.
==因此，为了提高安全性（并避免代价高昂的错误），通常最好做一个普通用户；如果你确实需要成为 root，请小心行事，因为计算世界的所有破坏力现在都在你的指尖。==

Finally, there are many different kinds of CPU meters you can use to get a quick glance understanding of the load on your system; for example, we always keep MenuMeters (from Raging Menace software) running on our Macintosh toolbars, so we can see how much CPU is being utilized at any moment in time.
==最后，你可以使用许多不同种类的 CPU 仪表来快速了解系统的负载；例如，我们总是在 Macintosh 工具栏上运行 MenuMeters（来自 Raging Menace 软件），这样我们就可以随时看到 CPU 的利用率。==

In general, the more information about what is going on, the better.
==总的来说，关于正在发生的事情的信息越多越好。==

5.7 Summary
==5.7 总结==

We have introduced some of the APIs dealing with UNIX process creation: fork(), exec(), and wait().
==我们已经介绍了一些处理 UNIX 进程创建的 API：fork()、exec() 和 wait()。==

However, we have just skimmed the surface.
==然而，我们要只是略知皮毛。==

For more detail, read Stevens and Rago, of course, particularly the chapters on Process Control, Process Relationships, and Signals; there is much to extract from the wisdom therein.
==要了解更多细节，当然要阅读 Stevens 和 Rago 的书，特别是关于进程控制、进程关系和信号的章节；那里有很多智慧可以汲取。==

While our passion for the UNIX process API remains strong, we should also note that such positivity is not uniform.
==虽然我们对 UNIX 进程 API 的热情依然强烈，但也应注意到这种积极态度并非是一致的。==

For example, a recent paper by systems researchers from Microsoft, Boston University, and ETH in Switzerland details some problems with fork(), and advocates for other, simpler process creation APIs such as spawn().
==例如，来自微软、波士顿大学和瑞士 ETH 的系统研究人员最近发表的一篇论文详细介绍了 fork() 的一些问题，并提倡其他更简单的进程创建 API，如 spawn()。==

Read it, and the related work it refers to, to understand this different vantage point.
==阅读它以及它引用的相关工作，以理解这个不同的观点。==

While it's generally good to trust this book, remember too that the authors have opinions; those opinions may not (always) be as widely shared as you might think.
==虽然相信这本书通常是好的，但也请记住作者有自己的观点；这些观点可能并不（总是）像你想象的那样被广泛认同。==

ASIDE: KEY PROCESS API TERMS
==旁白：关键进程 API 术语==

Each process has a name; in most systems, that name is a number known as a process ID (PID).
==每个进程都有一个名字；在大多数系统中，该名字是一个称为进程 ID (PID) 的数字。==

The fork() system call is used in UNIX systems to create a new process.
==fork() 系统调用在 UNIX 系统中用于创建新进程。==

The creator is called the parent; the newly created process is called the child.
==创建者称为父进程；新创建的进程称为子进程。==

As sometimes occurs in real life, the child process is a nearly identical copy of the parent.
==就像现实生活中通过发生的那样，子进程是父进程几乎完全相同的副本。==

The wait() system call allows a parent to wait for its child to complete execution.
==wait() 系统调用允许父进程等待其子进程完成执行。==

The exec() family of system calls allows a child to break free from its similarity to its parent and execute an entirely new program.
==exec() 系列系统调用允许子进程摆脱与其父进程的相似性，并执行一个全新的程序。==

A UNIX shell commonly uses fork(), wait(), and exec() to launch user commands; the separation of fork and exec enables features like input/output redirection, pipes, and other cool features, all without changing anything about the programs being run.
==UNIX shell 通常使用 fork()、wait() 和 exec() 来启动用户命令；fork 和 exec 的分离实现了输入/输出重定向、管道和其他很酷的功能，而无需更改正在运行的程序的任何内容。==

Process control is available in the form of signals, which can cause jobs to stop, continue, or even terminate.
==进程控制以信号的形式提供，这可能导致作业停止、继续甚至终止。==

Which processes can be controlled by a particular person is encapsulated in the notion of a user; the operating system allows multiple users onto the system, and ensures users can only control their own processes.
==特定人员可以控制哪些进程被封装在用户的概念中；操作系统允许多个用户进入系统，并确保用户只能控制自己的进程。==

A superuser can control all processes (and indeed do many other things); this role should be assumed infrequently and with caution for security reasons.
==超级用户可以控制所有进程（实际上还可以做很多其他事情）；出于安全原因，应谨慎且不频繁地担任此角色。==

Homework (Simulation)
==家庭作业（模拟）==

This simulation homework focuses on fork.py, a simple process creation simulator that shows how processes are related in a single "familial" tree.
==这个模拟家庭作业专注于 fork.py，这是一个简单的进程创建模拟器，展示了进程如何在单个“家族”树中相互关联。==

Read the relevant README for details about how to run the simulator.
==阅读相关的 README 以了解有关如何运行模拟器的详细信息。==

Questions
==问题==

1. Run ./fork.py -s 10 and see which actions are taken.
==2. 运行 ./fork.py -s 10 并查看采取了哪些操作。==

Can you predict what the process tree looks like at each step?
==你能预测每一步的进程树是什么样的吗？==

Use the -c flag to check your answers.
==使用 -c 标志检查你的答案。==

Try some different random seeds (-s) or add more actions (-a) to get the hang of it.
==尝试一些不同的随机种子 (-s) 或添加更多操作 (-a) 以掌握它。==

2. One control the simulator gives you is the fork percentage, controlled by the -f flag.
==3. 模拟器提供的一个控制是 fork 百分比，由 -f 标志控制。==

The higher it is, the more likely the next action is a fork; the lower it is, the more likely the action is an exit.
==它越高，下一个动作是 fork 的可能性就越大；它越低，动作是退出的可能性就越大。==

Run the simulator with a large number of actions (e.g., -a 100) and vary the fork percentage from 0.1 to 0.9.
==运行带有大量操作的模拟器（例如 -a 100），并将 fork 百分比从 0.1 变化到 0.9。==

What do you think the resulting final process trees will look like as the percentage changes?
==随着百分比的变化，你认为最终的进程树会是什么样子？==

Check your answer with -c.
==用 -c 检查你的答案。==

3. Now, switch the output by using the -t flag (e.g., run ./fork.py -t).
==4. 现在，使用 -t 标志切换输出（例如，运行 ./fork.py -t）。==

Given a set of process trees, can you tell which actions were taken?
==给定一组进程树，你能分辨出采取了哪些操作吗？==

4. One interesting thing to note is what happens when a child exits; what happens to its children in the process tree?
==5. 值得注意的一件有趣的事情是当子进程退出时会发生什么；在进程树中它的子进程会怎样？==

To study this, let's create a specific example: ./fork.py -A a+b,b+c,c+d,c+e,c-.
==为了研究这个问题，让我们创建一个特定的例子：./fork.py -A a+b,b+c,c+d,c+e,c-。==

This example has process 'a' create 'b', which in turn creates 'c', which then creates 'd' and 'e'.
==在这个例子中，进程 'a' 创建 'b'，'b' 依次创建 'c'，然后 'c' 创建 'd' 和 'e'。==

However, then, 'c' exits.
==然而，紧接着 'c' 退出了。==

What do you think the process tree should like after the exit?
==你认为退出后进程树应该是什么样子？==

What if you use the -R flag?
==如果你使用 -R 标志会怎样？==

Learn more about what happens to orphaned processes on your own to add more context.
==自行了解更多关于孤儿进程发生的情况，以增加更多背景知识。==

5. One last flag to explore is the -F flag, which skips intermediate steps and only asks to fill in the final process tree.
==6. 最后一个要探索的标志是 -F 标志，它跳过中间步骤，只要求填写最终的进程树。==

Run ./fork.py -F and see if you can write down the final tree by looking at the series of actions generated.
==运行 ./fork.py -F，看看你是否可以通过观察生成的一系列动作来写下最终的树。==

Use different random seeds to try this a few times.
==使用不同的随机种子尝试几次。==

6. Finally, use both -t and -F together.
==7. 最后，同时使用 -t 和 -F。==

This shows the final process tree, but then asks you to fill in the actions that took place.
==这会显示最终的进程树，但随后要求你填写发生的操作。==

By looking at the tree, can you determine the exact actions that took place?
==通过观察树，你能确定发生了哪些确切的操作吗？==

In which cases can you tell?
==在哪些情况下你能分辨出来？==

In which can't you tell?
==在哪些情况下你分辨不出来？==

Try some different random seeds to delve into this question.
==尝试一些不同的随机种子来深入研究这个问题。==

ASIDE: CODING HOMEWORKS
==旁白：编码家庭作业==

Coding homeworks are small exercises where you write code to run on a real machine to get some experience with some basic operating system APIs.
==编码家庭作业是一些小型练习，你需要编写代码在真实机器上运行，以获得一些基本操作系统 API 的经验。==

After all, you are (probably) a computer scientist, and therefore should like to code, right?
==毕竟，你（可能）是一名计算机科学家，因此应该喜欢编码，对吧？==

If you don't, there is always CS theory, but that's pretty hard.
==如果你不喜欢，总还有计算机科学理论，但那相当难。==

Of course, to truly become an expert, you have to spend more than a little time hacking away at the machine; indeed, find every excuse you can to write some code and see how it works.
==当然，要真正成为一名专家，你必须花大量时间在机器上进行黑客攻击般的钻研；确实，要找尽一切借口编写一些代码并看看它是如何工作的。==

Spend the time, and become the wise master you know you can be.
==花点时间，成为你知道自己能成为的明智大师。==

Homework (Code)
==家庭作业（代码）==

In this homework, you are to gain some familiarity with the process management APIs about which you just read.
==在这个家庭作业中，你要熟悉刚才读到的进程管理 API。==

Don't worry it's even more fun than it sounds!
==别担心，这比听起来更有趣！==

You'll in general be much better off if you find as much time as you can to write some code, so why not start now?
==如果你能找尽可能多的时间来编写代码，通常你会受益匪浅，那么为什么不现在就开始呢？==

Questions
==问题==

1. Write a program that calls fork().
==2. 编写一个调用 fork() 的程序。==

Before calling fork(), have the main process access a variable (e.g., x) and set its value to something (e.g., 100).
==在调用 fork() 之前，让主进程访问一个变量（例如 x）并将其值设置为某个值（例如 100）。==

What value is the variable in the child process?
==子进程中的变量值是多少？==

What happens to the variable when both the child and parent change the value of x?
==当子进程和父进程都改变 x 的值时，变量会发生什么？==

2. Write a program that opens a file (with the open() system call) and then calls fork() to create a new process.
==3. 编写一个程序，打开一个文件（使用 open() 系统调用），然后调用 fork() 创建一个新进程。==

Can both the child and parent access the file descriptor returned by open()?
==子进程和父进程都可以访问 open() 返回的文件描述符吗？==

What happens when they are writing to the file concurrently, i.e., at the same time?
==当它们并发地（即同时）写入文件时会发生什么？==

3. Write another program using fork().
==4. 使用 fork() 编写另一个程序。==

The child process should print "hello"; the parent process should print "goodbye".
==子进程应该打印 "hello"；父进程应该打印 "goodbye"。==

You should try to ensure that the child process always prints first; can you do this without calling wait() in the parent?
==你应该尝试确保子进程总是先打印；你能不在父进程中调用 wait() 而做到这一点吗？==

4. Write a program that calls fork() and then calls some form of exec() to run the program /bin/ls.
==5. 编写一个调用 fork() 的程序，然后调用某种形式的 exec() 来运行程序 /bin/ls。==

See if you can try all of the variants of exec(), including (on Linux) execl(), execle(), execlp(), execv(), execvp(), and execvpe().
==看看你是否可以尝试 exec() 的所有变体，包括（在 Linux 上）execl()、execle()、execlp()、execv()、execvp() 和 execvpe()。==

Why do you think there are so many variants of the same basic call?
==你认为为什么同一个基本调用有这么多变体？==

5. Now write a program that uses wait() to wait for the child process to finish in the parent.
==6. 现在编写一个程序，在父进程中使用 wait() 等待子进程完成。==

What does wait() return?
==wait() 返回什么？==

What happens if you use wait() in the child?
==如果你在子进程中使用 wait() 会发生什么？==

6. Write a slight modification of the previous program, this time using waitpid() instead of wait().
==7. 对前一个程序稍作修改，这次使用 waitpid() 代替 wait()。==

When would waitpid() be useful?
==waitpid() 什么时候有用？==

7. Write a program that creates a child process, and then in the child closes standard output (STDOUT_FILENO).
==8. 编写一个程序，创建一个子进程，然后在子进程中关闭标准输出 (STDOUT_FILENO)。==

What happens if the child calls printf() to print some output after closing the descriptor?
==如果在关闭描述符后子进程调用 printf() 打印一些输出，会发生什么？==

8. Write a program that creates two children, and connects the standard output of one to the standard input of the other, using the pipe() system call.
==9. 编写一个程序，创建两个子进程，并使用 pipe() 系统调用将一个子进程的标准输出连接到另一个子进程的标准输入。==

Mechanism: Limited Direct Execution
==机制：受限直接执行==

In order to virtualize the CPU, the operating system needs to somehow share the physical CPU among many jobs running seemingly at the same time.
==为了虚拟化 CPU，操作系统需要以某种方式在许多看似同时运行的作业之间共享物理 CPU。==

The basic idea is simple: run one process for a little while, then run another one, and so forth.
==基本思想很简单：运行一个进程一小会儿，然后运行另一个，依此类推。==

By time sharing the CPU in this manner, virtualization is achieved.
==通过这种方式分时复用 CPU，就实现了虚拟化。==

There are a few challenges, however, in building such virtualization machinery.
==然而，在构建这种虚拟化机制时存在一些挑战。==

The first is performance: how can we implement virtualization without adding excessive overhead to the system?
==首先是性能：我们如何在不给系统增加过多开销的情况下实现虚拟化？==

The second is control: how can we run processes efficiently while retaining control over the CPU?
==其次是控制：我们如何在保持对 CPU 控制的同时高效地运行进程？==

Control is particularly important to the OS, as it is in charge of resources; without control, a process could simply run forever and take over the machine, or access information that it should not be allowed to access.
==控制对操作系统尤为重要，因为它负责资源管理；如果没有控制，进程可能会永远运行并接管机器，或者访问它不应被允许访问的信息。==

Obtaining high performance while maintaining control is thus one of the central challenges in building an operating system.
==因此，在保持控制的同时获得高性能是构建操作系统的核心挑战之一。==

THE CRUX: HOW TO EFFICIENTLY VIRTUALIZE THE CPU WITH CONTROL
==关键问题：如何在受控的情况下高效地虚拟化 CPU==

The OS must virtualize the CPU in an efficient manner while retaining control over the system.
==操作系统必须在保持对系统控制的同时，以高效的方式虚拟化 CPU。==

To do so, both hardware and operating-system support will be required.
==为此，需要硬件和操作系统的支持。==

The OS will often use a judicious bit of hardware support in order to accomplish its work effectively.
==操作系统通常会明智地利用一些硬件支持来有效地完成其工作。==

6.1 Basic Technique: Limited Direct Execution
==6.1 基本技术：受限直接执行==

To make a program run as fast as one might expect, not surprisingly OS developers came up with a technique, which we call limited direct execution.
==为了使程序运行得像预期的那样快，操作系统开发人员毫不奇怪地想出了一种技术，我们称之为受限直接执行。==

The "direct execution" part of the idea is simple: just run the program directly on the CPU.
==这个想法中“直接执行”的部分很简单：直接在 CPU 上运行程序。==

Thus, when the OS wishes to start a program running, it creates a process entry for it in a process list, allocates some memory for it, loads the program code into memory (from disk), locates its entry point (i.e., the main() routine or something similar), jumps to it, and starts running the user's code.
==因此，当操作系统希望开始运行一个程序时，它会在进程列表中为其创建一个进程条目，为其分配一些内存，将程序代码加载到内存中（从磁盘），找到其入口点（即 main() 例程或类似的东西），跳转到那里，并开始运行用户的代码。==

Figure 6.1 shows this basic direct execution protocol (without any limits, yet), using a normal call and return to jump to the program's main() and later back into the kernel.
==图 6.1 显示了这个基本的直接执行协议（目前还没有任何限制），使用普通的调用和返回跳转到程序的 main()，稍后再返回内核。==

Sounds simple, no?
==听起来很简单，不是吗？==

But this approach gives rise to a few problems in our quest to virtualize the CPU.
==但是，这种方法在我们虚拟化 CPU 的过程中引发了一些问题。==

The first is simple: if we just run a program, how can the OS make sure the program doesn't do anything that we don't want it to do, while still running it efficiently?
==第一个问题很简单：如果我们只是运行一个程序，操作系统如何确保程序不做任何我们不希望它做的事情，同时仍然高效地运行它？==

The second: when we are running a process, how does the operating system stop it from running and switch to another process, thus implementing the time sharing we require to virtualize the CPU?
==第二个问题：当我们运行一个进程时，操作系统如何停止它的运行并切换到另一个进程，从而实现我们需要用来虚拟化 CPU 的分时复用？==

In answering these questions below, we'll get a much better sense of what is needed to virtualize the CPU.
==在回答下面的这些问题时，我们将更清楚地了解虚拟化 CPU 需要什么。==

In developing these techniques, we'll also see where the "limited" part of the name arises from; without limits on running programs, the OS wouldn't be in control of anything and thus would be "just a library" - a very sad state of affairs for an aspiring operating system!
==在开发这些技术的过程中，我们还将看到名称中“受限”部分的由来；如果不对运行的程序加以限制，操作系统将无法控制任何东西，因此将“只是一个库”——对于一个有抱负的操作系统来说，这真是一个悲惨的境地！==

6.2 Problem #1: Restricted Operations
==6.2 问题 #1：受限操作==

Direct execution has the obvious advantage of being fast; the program runs natively on the hardware CPU and thus executes as quickly as one would expect.
==直接执行具有速度快的明显优势；程序在硬件 CPU 上原生运行，因此执行速度正如预期的那样快。==

But running on the CPU introduces a problem: what if the process wishes to perform some kind of restricted operation, such as issuing an I/O request to a disk, or gaining access to more system resources such as CPU or memory?
==但是在 CPU 上运行引入了一个问题：如果进程希望执行某种受限操作，例如向磁盘发出 I/O 请求，或者获取更多系统资源（如 CPU 或内存）的访问权限，该怎么办？==

THE CRUX: HOW TO PERFORM RESTRICTED OPERATIONS
==关键问题：如何执行受限操作==

A process must be able to perform I/O and some other restricted operations, but without giving the process complete control over the system.
==进程必须能够执行 I/O 和一些其他受限操作，但不能赋予进程对系统的完全控制权。==

How can the OS and hardware work together to do so?
==操作系统和硬件如何协同工作来实现这一点？==

ASIDE: WHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS
==旁白：为什么系统调用看起来像过程调用==

You may wonder why a call to a system call, such as open() or read(), looks exactly like a typical procedure call in C; that is, if it looks just like a procedure call, how does the system know it's a system call, and do all the right stuff?
==你可能会想，为什么对系统调用（如 open() 或 read()）的调用看起来与 C 中的典型过程调用完全一样；也就是说，如果它看起来就像一个过程调用，系统怎么知道它是一个系统调用并执行所有正确的操作呢？==

The simple reason: it is a procedure call, but hidden inside that procedure call is the famous trap instruction.
==原因很简单：它确实是一个过程调用，但隐藏在该过程调用内部的是著名的 trap 指令。==

More specifically, when you call open() (for example), you are executing a procedure call into the C library.
==更具体地说，当你调用 open()（例如）时，你正在执行进入 C 库的过程调用。==

Therein, whether for open() or any of the other system calls provided, the library uses an agreed-upon calling convention with the kernel to put the arguments to open() in well-known locations (e.g., on the stack, or in specific registers), puts the system-call number into a well-known location as well (again, onto the stack or a register), and then executes the aforementioned trap instruction.
==在那里，无论是 open() 还是提供的任何其他系统调用，库都使用与内核商定的调用约定将 open() 的参数放入众所周知的位置（例如，在堆栈上或在特定寄存器中），将系统调用号也放入一个众所周知的位置（同样，在堆栈上或寄存器中），然后执行上述的 trap 指令。==

The code in the library after the trap unpacks return values and returns control to the program that issued the system call.
==trap 之后的库代码解包返回值，并将控制权返回给发出系统调用的程序。==

Thus, the parts of the C library that make system calls are hand-coded in assembly, as they need to carefully follow convention in order to process arguments and return values correctly, as well as execute the hardware-specific trap instruction.
==因此，C 库中进行系统调用的部分是用汇编语言手工编写的，因为它们需要仔细遵循约定才能正确处理参数和返回值，以及执行特定于硬件的 trap 指令。==

And now you know why you personally don't have to write assembly code to trap into an OS; somebody has already written that assembly for you.
==现在你知道为什么你个人不必编写汇编代码来陷入 (trap) 操作系统了；已经有人为你编写了那段汇编代码。==

One approach would simply be to let any process do whatever it wants in terms of I/O and other related operations.
==一种方法是简单地让任何进程在 I/O 和其他相关操作方面随心所欲。==

However, doing so would prevent the construction of many kinds of systems that are desirable.
==然而，这样做将阻碍许多理想系统的构建。==

For example, if we wish to build a file system that checks permissions before granting access to a file, we can't simply let any user process issue I/Os to the disk; if we did, a process could simply read or write the entire disk and thus all protections would be lost.
==例如，如果我们希望构建一个在授予文件访问权限之前检查权限的文件系统，我们不能简单地让任何用户进程向磁盘发出 I/O；如果我们这样做，一个进程可以简单地读取或写入整个磁盘，从而失去所有保护。==

Thus, the approach we take is to introduce a new processor mode, known as user mode; code that runs in user mode is restricted in what it can do.
==因此，我们采取的方法是引入一种新的处理器模式，称为用户模式；在用户模式下运行的代码在功能上受到限制。==

For example, when running in user mode, a process can't issue I/O requests; doing so would result in the processor raising an exception; the OS would then likely kill the process.
==例如，在用户模式下运行时，进程不能发出 I/O 请求；这样做会导致处理器引发异常；操作系统随后可能会杀死该进程。==

In contrast to user mode is kernel mode, which the operating system (or kernel) runs in.
==与用户模式相对的是内核模式，操作系统（或内核）在该模式下运行。==

In this mode, code that runs can do what it likes, including privileged operations such as issuing I/O requests and executing all types of restricted instructions.
==在这种模式下，运行的代码可以做它喜欢做的事情，包括特权操作，如发出 I/O 请求和执行所有类型的受限指令。==

We are still left with a challenge, however: what should a user process do when it wishes to perform some kind of privileged operation, such as reading from disk?
==然而，我们仍然面临一个挑战：当用户进程希望执行某种特权操作（如从磁盘读取）时，它应该怎么做？==

To enable this, virtually all modern hardware provides the ability for user programs to perform a system call.
==为了实现这一点，实际上所有现代硬件都提供了让用户程序执行系统调用的能力。==

Pioneered on ancient machines such as the Atlas, system calls allow the kernel to carefully expose certain key pieces of functionality to user programs, such as accessing the file system, creating and destroying processes, communicating with other processes, and allocating more memory.
==系统调用在 Atlas 等古老机器上首创，允许内核谨慎地向用户程序公开某些关键功能，例如访问文件系统、创建和销毁进程、与其他进程通信以及分配更多内存。==


==根据您提供的 PDF 内容，我整理并翻译了关于“受限直接执行（Limited Direct Execution）”机制和“进程调度（Scheduling）”的核心章节。以下是严格按照您的要求格式化的翻译：==

MECHANISM: LIMITED DIRECT EXECUTION

==机制：受限直接执行==

TIP: USE PROTECTED CONTROL TRANSFER

==提示：使用受保护的控制转移==

The hardware assists the OS by providing different modes of execution.

==硬件通过提供不同的执行模式来协助操作系统。==

In user mode, applications do not have full access to hardware resources.

==在用户模式下，应用程序无法完全访问硬件资源。==

In kernel mode, the OS has access to the full resources of the machine.

==在内核模式下，操作系统可以访问机器的全部资源。==

Special instructions to trap into the kernel and return-from-trap back to user-mode programs are also provided.

==此外，还提供了陷入（trap）内核和从陷阱返回（return-from-trap）到用户模式程序的特殊指令。==

Instructions that allow the OS to tell the hardware where the trap table resides in memory are also provided.

==还提供了允许操作系统告诉硬件陷阱表（trap table）在内存中驻留位置的指令。==

To execute a system call, a program must execute a special trap instruction.

==要执行系统调用，程序必须执行一条特殊的陷阱指令。==

This instruction simultaneously jumps into the kernel and raises the privilege level to kernel mode.

==该指令同时跳入内核并将特权级别提升为内核模式。==

Once in the kernel, the system can now perform whatever privileged operations are needed (if allowed), and thus do the required work for the calling process.

==一旦进入内核，系统就可以执行所需的任何特权操作（如果允许），从而为调用进程完成所需的工作。==

When finished, the OS calls a special return-from-trap instruction, which returns into the calling user program while simultaneously reducing the privilege level back to user mode.

==完成后，操作系统调用一条特殊的从陷阱返回指令，该指令返回到调用的用户程序，同时将特权级别降低回用户模式。==

The hardware needs to be a bit careful when executing a trap, in that it must make sure to save enough of the caller's registers in order to be able to return correctly.

==硬件在执行陷阱时需要小心一点，因为它必须确保保存足够多的调用者寄存器，以便能够正确返回。==

On x86, for example, the processor will push the program counter, flags, and a few other registers onto a per-process kernel stack.

==例如，在 x86 上，处理器会将程序计数器、标志寄存器和其他几个寄存器压入每个进程的内核栈中。==

The return-from-trap will pop these values off the stack and resume execution of the user-mode program.

==从陷阱返回指令将从栈中弹出这些值，并恢复用户模式程序的执行。==

There is one important detail left out of this discussion: how does the trap know which code to run inside the OS?

==讨论中遗漏了一个重要细节：陷阱如何知道在操作系统内部运行哪段代码？==

Clearly, the calling process can't specify an address to jump to.

==显然，调用进程不能指定要跳转到的地址。==

Doing so would allow programs to jump anywhere into the kernel which clearly is a Very Bad Idea.

==这样做将允许程序跳转到内核中的任何位置，这显然是一个非常糟糕的主意。==

Thus the kernel must carefully control what code executes upon a trap.

==因此，内核必须仔细控制在发生陷阱时执行的代码。==

The kernel does so by setting up a trap table at boot time.

==内核通过在引导时设置陷阱表来实现这一点。==

When the machine boots up, it does so in privileged (kernel) mode, and thus is free to configure machine hardware as need be.

==当机器启动时，它处于特权（内核）模式，因此可以根据需要自由配置机器硬件。==

One of the first things the OS thus does is to tell the hardware what code to run when certain exceptional events occur.

==因此，操作系统做的第一件事就是告诉硬件当某些异常事件发生时运行什么代码。==

For example, what code should run when a hard-disk interrupt takes place, when a keyboard interrupt occurs, or when a program makes a system call?

==例如，当发生硬盘中断、键盘中断或程序进行系统调用时，应该运行什么代码？==

The OS informs the hardware of the locations of these trap handlers, usually with some kind of special instruction.

==操作系统通常使用某种特殊指令将这些陷阱处理程序的位置告知硬件。==

Once the hardware is informed, it remembers the location of these handlers until the machine is next rebooted.

==一旦硬件被告知，它就会记住这些处理程序的位置，直到机器下次重新启动。==

TIP: BE WARY OF USER INPUTS IN SECURE SYSTEMS

==提示：在安全系统中要警惕用户输入==

In general, a secure system must treat user inputs with great suspicion.

==总的来说，安全系统必须以极大的怀疑态度对待用户输入。==

To specify the exact system call, a system-call number is usually assigned to each system call.

==为了指定确切的系统调用，通常会为每个系统调用分配一个系统调用号。==

The user code is thus responsible for placing the desired system-call number in a register or at a specified location on the stack.

==因此，用户代码负责将所需的系统调用号放入寄存器或栈上的指定位置。==

The OS, when handling the system call inside the trap handler, examines this number, ensures it is valid, and, if it is, executes the corresponding code.

==操作系统在陷阱处理程序内处理系统调用时，会检查该号码，确保其有效，如果有效，则执行相应的代码。==

This level of indirection serves as a form of protection.

==这种间接层作为一种保护形式。==

User code cannot specify an exact address to jump to, but rather must request a particular service via number.

==用户代码不能指定要跳转的确切地址，而必须通过号码请求特定的服务。==

Point to ponder: what horrible things could you do to a system if you could install your own trap table?

==值得思考的一点：如果你能安装自己的陷阱表，你会对系统做什么可怕的事情？==

6.3 Problem #2: Switching Between Processes

==6.3 问题 #2：在进程之间切换==

The next problem with direct execution is achieving a switch between processes.

==直接执行的下一个问题是实现进程之间的切换。==

Switching between processes should be simple, right?

==进程之间的切换应该很简单，对吧？==

But it actually is a little bit tricky: specifically, if a process is running on the CPU, this by definition means the OS is not running.

==但这实际上有点棘手：具体来说，如果一个进程正在 CPU 上运行，根据定义，这意味着操作系统没有在运行。==

If the OS is not running, how can it do anything at all?

==如果操作系统没有运行，它怎么能做任何事情呢？==

THE CRUX: HOW TO REGAIN CONTROL OF THE CPU

==关键问题：如何重新获得 CPU 的控制权==

How can the operating system regain control of the CPU so that it can switch between processes?

==操作系统如何重新获得 CPU 的控制权，以便在进程之间进行切换？==

A Cooperative Approach: Wait For System Calls

==协作方式：等待系统调用==

In this style, the OS trusts the processes of the system to behave reasonably.

==在这种风格下，操作系统信任系统的进程会表现得合情合理。==

Processes that run for too long are assumed to periodically give up the CPU so that the OS can decide to run some other task.

==运行时间过长的进程被假定会周期性地放弃 CPU，以便操作系统可以决定运行其他任务。==

Most processes transfer control of the CPU to the OS quite frequently by making system calls.

==大多数进程通过进行系统调用相当频繁地将 CPU 的控制权转移给操作系统。==

Systems like this often include an explicit yield system call, which does nothing except to transfer control to the OS so it can run other processes.

==像这样的系统通常包含一个显式的 yield 系统调用，它除了将控制权转移给操作系统以便其运行其他进程外，什么也不做。==

Applications also transfer control to the OS when they do something illegal.

==当应用程序执行非法操作时，也会将控制权转移给操作系统。==

For example, if an application divides by zero, or tries to access memory that it shouldn't be able to access, it will generate a trap to the OS.

==例如，如果应用程序除以零，或试图访问它不应该访问的内存，它将向操作系统生成一个陷阱。==

A Non-Cooperative Approach: The OS Takes Control

==非协作方式：操作系统获取控制权==

What happens, for example, if a process ends up in an infinite loop, and never makes a system call?

==例如，如果一个进程陷入无限循环，并且从不进行系统调用，会发生什么？==

THE CRUX: HOW TO GAIN CONTROL WITHOUT COOPERATION

==关键问题：如何在不协作的情况下获得控制权==

How can the OS gain control of the CPU even if processes are not being cooperative?

==即使进程不配合，操作系统如何获得 CPU 的控制权？==

The answer turns out to be simple and was discovered by a number of people building computer systems many years ago: a timer interrupt.

==答案很简单，多年前由许多构建计算机系统的人发现：时钟中断（timer interrupt）。==

A timer device can be programmed to raise an interrupt every so many milliseconds.

==时钟设备可以被编程为每隔多少毫秒产生一次中断。==

When the interrupt is raised, the currently running process is halted, and a pre-configured interrupt handler in the OS runs.

==当中断发生时，当前运行的进程被暂停，操作系统中预先配置的中断处理程序开始运行。==

At this point, the OS has regained control of the CPU, and thus can do what it pleases: stop the current process, and start a different one.

==此时，操作系统重新获得了 CPU 的控制权，因此可以做它想做的事：停止当前进程，并启动另一个进程。==

At boot time, the OS must inform the hardware of which code to run when the timer interrupt occurs.

==在引导时，操作系统必须通知硬件当时钟中断发生时运行哪段代码。==

Also during the boot sequence, the OS must start the timer, which is of course a privileged operation.

==同样在引导序列期间，操作系统必须启动计时器，这当然是一个特权操作。==

Saving and Restoring Context

==保存和恢复上下文==

Now that the OS has regained control, a decision has to be made: whether to continue running the currently-running process, or switch to a different one.

==既然操作系统已经重新获得了控制权，就必须做出决定：是继续运行当前运行的进程，还是切换到另一个进程。==

This decision is made by a part of the operating system known as the scheduler.

==这个决定是由操作系统中称为调度程序（scheduler）的部分做出的。==

If the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a context switch.

==如果决定进行切换，操作系统就会执行一段底层代码，我们称之为上下文切换（context switch）。==

A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process and restore a few for the soon-to-be-executing process.

==上下文切换在概念上很简单：操作系统所要做的就是为当前正在执行的进程保存一些寄存器值，并为即将执行的进程恢复一些寄存器值。==

By doing so, the OS thus ensures that when the return-from-trap instruction is finally executed, the system resumes execution of another process.

==通过这样做，操作系统确保了当最终执行从陷阱返回指令时，系统恢复另一个进程的执行。==

Scheduling: Introduction

==调度：简介==

THE CRUX: HOW TO DEVELOP SCHEDULING POLICY

==关键问题：如何制定调度策略==

How should we develop a basic framework for thinking about scheduling policies?

==我们应该如何开发一个基本的框架来思考调度策略？==

What are the key assumptions?

==关键假设是什么？==

What metrics are important?

==哪些指标是重要的？==

7.1 Workload Assumptions

==7.1 工作负载假设==

We will make the following assumptions about the processes, sometimes called jobs, that are running in the system:

==我们将对系统中运行的进程（有时称为作业）做出以下假设：==

1. Each job runs for the same amount of time.
    
==2. 每个作业运行相同的时间。==
    
3. All jobs arrive at the same time.
    
==4. 所有作业同时到达。==
    
5. Once started, each job runs to completion.
    
==6. 一旦开始，每个作业都会运行直到完成。==
    
7. All jobs only use the CPU (i.e., they perform no I/O).
    
==8. 所有作业只使用 CPU（即它们不执行 I/O）。==
    
9. The run-time of each job is known.
    
==10. 每个作业的运行时间是已知的。==
    

7.2 Scheduling Metrics

==7.2 调度指标==

Beyond making workload assumptions, we also need one more thing to enable us to compare different scheduling policies: a scheduling metric.

==除了做出工作负载假设外，我们还需要一样东西来比较不同的调度策略：调度指标。==

For now, however, let us also simplify our life by simply having a single metric: turnaround time.

==然而，为了简化现在的讨论，我们只使用一个指标：周转时间（turnaround time）。==

The turnaround time of a job is defined as the time at which the job completes minus the time at which the job arrived in the system.

==作业的周转时间定义为作业完成的时间减去作业到达系统的时间。==

More formally, the turnaround time $T_{turnaround}$ is: $T_{turnaround} = T_{completion} - T_{arrival}$.

==更正式地说，周转时间 $T_{turnaround}$ 是：$T_{turnaround} = T_{completion} - T_{arrival}$。==

7.3 First In, First Out (FIFO)

==7.3 先进先出 (FIFO)==

The most basic algorithm we can implement is known as First In, First Out (FIFO) scheduling or sometimes First Come, First Served (FCFS).

==我们能实现的最基本的算法被称为先进先出（FIFO）调度，有时也称为先到先服务（FCFS）。==

FIFO has a number of positive properties: it is clearly simple and thus easy to implement.

==FIFO 有许多积极的特性：它显然很简单，因此很容易实现。==

Let's do a quick example together.

==让我们一起做一个简单的例子。==

Imagine three jobs arrive in the system, A, B, and C, at roughly the same time ($T_{arrival}=0$).

==想象一下，三个作业 A、B 和 C 几乎同时到达系统（$T_{arrival}=0$）。==

Assume also that each job runs for 10 seconds.

==还要假设每个作业运行 10 秒。==

The average turnaround time for the three jobs is simply $\frac{10+20+30}{3} = 20$.

==这就三个作业的平均周转时间仅仅是 $\frac{10+20+30}{3} = 20$。==

Now let's relax one of our assumptions.

==现在让我们放宽其中一个假设。==

In particular, let's relax assumption 1, and thus no longer assume that each job runs for the same amount of time.

==具体来说，让我们放宽假设 1，因此不再假设每个作业运行相同的时间。==

In particular, let's again assume three jobs (A, B, and C), but this time A runs for 100 seconds while B and C run for 10 each.

==具体来说，让我们再次假设有三个作业（A、B 和 C），但这次 A 运行 100 秒，而 B 和 C 各运行 10 秒。==

Job A runs first for the full 100 seconds before B or C even get a chance to run.

==作业 A 首先运行整整 100 秒，然后 B 或 C 才有机会运行。==

Thus, the average turnaround time for the system is high: a painful 110 seconds $(\frac{100+110+120}{3}=110)$.

==因此，系统的平均周转时间很高：令人痛苦的 110 秒 $(\frac{100+110+120}{3}=110)$。==

This problem is generally referred to as the convoy effect.

==这个问题通常被称为护航效应（convoy effect）。==

7.4 Shortest Job First (SJF)

==7.4 最短作业优先 (SJF)==

It turns out that a very simple approach solves this problem.

==事实证明，一个非常简单的方法可以解决这个问题。==

This new scheduling discipline is known as Shortest Job First (SJF).

==这种新的调度规则被称为最短作业优先（SJF）。==

It runs the shortest job first, then the next shortest, and so on.

==它首先运行最短的作业，然后运行下一个最短的作业，依此类推。==

Simply by running B and C before A, SJF reduces average turnaround from 110 seconds to 50 $(\frac{10+20+120}{3}=50)$, more than a factor of two improvement.

==仅仅通过在 A 之前运行 B 和 C，SJF 就将平均周转时间从 110 秒减少到 50 秒 $(\frac{10+20+120}{3}=50)$，性能提高了一倍以上。==

Let's relax another.

==让我们再放宽一个假设。==

In particular, we can target assumption 2, and now assume that jobs can arrive at any time instead of all at once.

==具体来说，我们可以针对假设 2，现在假设作业可以在任何时间到达，而不是全部同时到达。==

This time, assume A arrives at $t=0$ and needs to run for 100 seconds, whereas B and C arrive at $t=10$ and each need to run for 10 seconds.

==这一次，假设 A 在 $t=0$ 到达并需要运行 100 秒，而 B 和 C 在 $t=10$ 到达并且各需要运行 10 秒。==

Even though B and C arrived shortly after A, they still are forced to wait until A has completed, and thus suffer the same convoy problem.

==即使 B 和 C 在 A 之后不久到达，它们仍然被迫等待直到 A 完成，因此遭受同样的护航问题。==

7.5 Shortest Time-to-Completion First (STCF)

==7.5 最短完成时间优先 (STCF)==

To address this concern, we need to relax assumption 3 (that jobs must run to completion), so let's do that.

==为了解决这个问题，我们需要放宽假设 3（作业必须运行直到完成），所以让我们这样做。==

The scheduler can certainly do something else when B and C arrive: it can preempt job A and decide to run another job, perhaps continuing A later.

==当 B 和 C 到达时，调度程序当然可以做其他事情：它可以抢占（preempt）作业 A 并决定运行另一个作业，稍后或许再继续运行 A。==

SJF by our definition is a non-preemptive scheduler.

==根据我们的定义，SJF 是一个非抢占式调度程序。==

Fortunately, there is a scheduler which does exactly that: add preemption to SJF, known as the Shortest Time-to-Completion First (STCF) or Preemptive Shortest Job First (PSJF) scheduler.

==幸运的是，有一个调度程序正是这样做的：将抢占添加到 SJF，称为最短完成时间优先（STCF）或抢占式最短作业优先（PSJF）调度程序。==

Any time a new job enters the system, the STCF scheduler determines which of the remaining jobs (including the new job) has the least time left, and schedules that one.

==每当新作业进入系统时，STCF 调度程序就会确定剩余作业（包括新作业）中哪个剩余时间最少，并调度该作业。==

The result is a much-improved average turnaround time.

==结果是平均周转时间大大改善。==

7.6 A New Metric: Response Time

==7.6 一个新指标：响应时间==

Thus, if we knew job lengths, and that jobs only used the CPU, and our only metric was turnaround time, STCF would be a great policy.

==因此，私如果我们知道作业长度，且作业只使用 CPU，而我们唯一的指标是周转时间，那么 STCF 将是一个很好的策略。==

However, the introduction of time-shared machines changed all that.

==然而，分时机器的引入改变了一切。==

Now users would sit at a terminal and demand interactive performance from the system as well.

==现在，用户坐在终端前，也要求系统提供交互式性能。==

And thus, a new metric was born: response time.

==因此，一个新的指标诞生了：响应时间。==

We define response time as the time from when the job arrives in a system to the first time it is scheduled.

==我们将响应时间定义为从作业到达系统到它第一次被调度的时间。==

STCF and related disciplines are not particularly good for response time.

==STCF 和相关的规则在响应时间方面并不是特别好。==

If three jobs arrive at the same time, for example, the third job has to wait for the previous two jobs to run in their entirety before being scheduled just once.

==例如，如果三个作业同时到达，第三个作业必须等待前两个作业完全运行完毕，才能被调度一次。==

While great for turnaround time, this approach is quite bad for response time and interactivity.

==虽然这种方法对周转时间很有利，但对响应时间和交互性却很糟糕。==

7.7 Round Robin

==7.7 轮转调度==

To solve this problem, we will introduce a new scheduling algorithm, classically referred to as Round-Robin (RR) scheduling.

==为了解决这个问题，我们将引入一种新的调度算法，经典地称为轮转（Round-Robin, RR）调度。==

The basic idea is simple: instead of running jobs to completion, RR runs a job for a time slice (sometimes called a scheduling quantum) and then switches to the next job in the run queue.

==基本思想很简单：RR 不是运行作业直到完成，而是运行作业一个时间片（time slice，有时称为调度量子），然后切换到运行队列中的下一个作业。==

It repeatedly does so until the jobs are finished.

==它重复这样做，直到作业完成。==

For this reason, RR is sometimes called time-slicing.

==因此，RR 有时也被称为时间切片（time-slicing）。==

SCHEDULING: INTRODUCTION

==调度：导论==

TIP: AMORTIZATION CAN REDUCE COSTS

==提示：摊销可以降低成本==

The general technique of amortization is commonly used in systems when there is a fixed cost to some operation.

==摊销这一通用技术通常用于当某些操作存在固定成本时的系统中。==

By incurring that cost less often (i.e., by performing the operation fewer times), the total cost to the system is reduced.

==通过减少该成本发生的频率（即减少操作执行的次数），系统的总成本得以降低。==

For example, if the time slice is set to 10 ms, and the context-switch cost is 1 ms, roughly 10% of time is spent context switching and is thus wasted.

==例如，如果时间片设置为 10 ms，而上下文切换成本为 1 ms，那么大约 10% 的时间花费在上下文切换上，因此被浪费了。==

If we want to amortize this cost, we can increase the time slice, e.g., to 100 ms.

==如果我们想要摊销这个成本，我们可以增加时间片，例如增加到 100 ms。==

In this case, less than 1% of time is spent context switching, and thus the cost of time-slicing has been amortized.

==在这种情况下，花费在上下文切换上的时间不到 1%，因此时间片的成本就被摊销了。==

they each wish to run for 5 seconds.

==它们每个都希望运行 5 秒钟。==

An SJF scheduler runs each job to completion before running another.

==SJF（最短任务优先）调度程序会在运行另一个任务之前将每个任务运行至完成。==

In contrast, RR with a time-slice of 1 second would cycle through the jobs quickly.

==相比之下，时间片为 1 秒的 RR（轮转）调度程序会快速地在任务之间循环。==

The average response time of RR is: $\frac{0+1+2}{3}=1;$ for SJF, average response time is: $\frac{0+5+10}{3}=5$.

==RR 的平均响应时间是：$\frac{0+1+2}{3}=1$；而对于 SJF，平均响应时间是：$\frac{0+5+10}{3}=5$。==

As you can see, the length of the time slice is critical for RR.

==如你所见，时间片的长度对 RR 至关重要。==

The shorter it is, the better the performance of RR under the response-time metric.

==它越短，RR 在响应时间指标下的表现就越好。==

However, making the time slice too short is problematic: suddenly the cost of context switching will dominate overall performance.

==然而，将时间片设置得太短是有问题的：上下文切换的成本会突然主导整体性能。==

Thus, deciding on the length of the time slice presents a trade-off to a system designer, making it long enough to amortize the cost of switching without making it so long that the system is no longer responsive.

==因此，确定时间片的长度给系统设计者带来了一个权衡问题，既要使其足够长以摊销切换成本，又不能长到使系统不再响应。==

Note that the cost of context switching does not arise solely from the OS actions of saving and restoring a few registers.

==请注意，上下文切换的成本不仅仅来自操作系统保存和恢复少量寄存器的操作。==

When programs run, they build up a great deal of state in CPU caches, TLBs, branch predictors, and other on-chip hardware.

==当程序运行时，它们会在 CPU 缓存、TLB、分支预测器和其他片上硬件中建立大量的状态。==

Switching to another job causes this state to be flushed and new state relevant to the currently-running job to be brought in, which may exact a noticeable performance cost.

==切换到另一个任务会导致这些状态被刷新，并引入与当前运行任务相关的新状态，这可能会产生显著的性能成本。==

RR, with a reasonable time slice, is thus an excellent scheduler if response time is our only metric.

==因此，如果响应时间是我们唯一的指标，那么拥有合理时间片的 RR 是一个极好的调度程序。==

But what about our old friend turnaround time?

==但是我们那个老朋友——周转时间呢？==

Let's look at our example above again.

==让我们再次看看上面的例子。==

A, B, and C, each with running times of 5 seconds, arrive at the same time, and RR is the scheduler with a (long) 1-second time slice.

==A、B 和 C 每个运行时间为 5 秒，它们同时到达，RR 是调度程序，时间片为（较长的）1 秒。==

We can see from the picture above that A finishes at 13, B at 14, and C at 15, for an average of 14.

==我们可以从上图看到，A 在 13 结束，B 在 14 结束，C 在 15 结束，平均值为 14。==

Pretty awful!

==相当糟糕！==

It is not surprising, then, that RR is indeed one of the worst policies if turnaround time is our metric.

==不足为奇的是，如果周转时间是我们的指标，RR 确实是最差的策略之一。==

Intuitively, this should make sense: what RR is doing is stretching out each job as long as it can, by only running each job for a short bit before moving to the next.

==直观上这也是讲得通的：RR 所做的就是尽可能地拉长每个任务，它只运行每个任务一小会儿就移动到下一个。==

Because turnaround time only cares about when jobs finish, RR is nearly pessimal, even worse than simple FIFO in many cases.

==因为周转时间只关心任务何时完成，RR 几乎是最差的，在许多情况下甚至比简单的 FIFO 还要糟糕。==

More generally, any policy (such as RR) that is fair, i.e., that evenly divides the CPU among active processes on a small time scale, will perform poorly on metrics such as turnaround time.

==更一般地说，任何公平的策略（如 RR），即在小的时间尺度上平均分配 CPU 给活动进程的策略，在周转时间等指标上都会表现不佳。==

Indeed, this is an inherent trade-off: if you are willing to be unfair, you can run shorter jobs to completion, but at the cost of response time;

==确实，这是一个固有的权衡：如果你愿意不公平，你可以运行较短的任务至完成，但代价是响应时间；==

if you instead value fairness, response time is lowered, but at the cost of turnaround time.

==如果你反而重视公平性，响应时间会降低，但代价是周转时间。==

This type of trade-off is common in systems;

==这种类型的权衡在系统中很常见；==

you can't have your cake and eat it too.

==你不能既拥有蛋糕又吃掉它（鱼和熊掌不可兼得）。==

We have developed two types of schedulers.

==我们已经开发了两种类型的调度程序。==

The first type (SJF, STCF) optimizes turnaround time, but is bad for response time.

==第一种类型（SJF, STCF）优化了周转时间，但对响应时间不利。==

The second type (RR) optimizes response time but is bad for turnaround.

==第二种类型（RR）优化了响应时间，但对周转时间不利。==

And we still have two assumptions which need to be relaxed: assumption 4 (that jobs do no I/O), and assumption 5 (that the run-time of each job is known).

==我们要还有两个假设需要放宽：假设 4（任务不进行 I/O）和假设 5（每个任务的运行时间是已知的）。==

Let's tackle those assumptions next.

==让我们接下来解决这些假设。==

7.8 Incorporating I/O

==7.8 纳入 I/O==

First we will relax assumption 4: of course all programs perform I/O.

==首先我们将放宽假设 4：当然所有程序都会执行 I/O。==

Imagine a program that didn't take any input: it would produce the same output each time.

==想象一个不接受任何输入的程序：它每次都会产生相同的输出。==

Imagine one without output: it is the proverbial tree falling in the forest, with no one to see it;

==想象一个没有输出的程序：它就像谚语中森林里倒下的树，没有人看到它；==

it doesn't matter that it ran.

==它是否运行过并不重要。==

A scheduler clearly has a decision to make when a job initiates an I/O request, because the currently-running job won't be using the CPU during the I/O;

==当一个任务发起 I/O 请求时，调度程序显然需要做出决定，因为当前运行的任务在 I/O 期间不会使用 CPU；==

it is blocked waiting for I/O completion.

==它被阻塞以等待 I/O 完成。==

If the I/O is sent to a hard disk drive, the process might be blocked for a few milliseconds or longer, depending on the current I/O load of the drive.

==如果 I/O 被发送到硬盘驱动器，该进程可能会被阻塞几毫秒或更长时间，具体取决于驱动器当前的 I/O 负载。==

Thus, the scheduler should probably schedule another job on the CPU at that time.

==因此，调度程序大概应该在那时在 CPU 上调度另一个任务。==

The scheduler also has to make a decision when the I/O completes.

==当 I/O 完成时，调度程序也必须做出决定。==

When that occurs, an interrupt is raised, and the OS runs and moves the process that issued the I/O from blocked back to the ready state.

==当这种情况发生时，会产生一个中断，操作系统运行并将发出 I/O 的进程从阻塞状态移回就绪状态。==

Of course, it could even decide to run the job at that point.

==当然，它甚至可以决定在那个时间点运行该任务。==

How should the OS treat each job?

==操作系统应该如何对待每个任务？==

To understand this issue better, let us assume we have two jobs, A and B, which each need 50 ms of CPU time.

==为了更好地理解这个问题，让我们假设有两个任务 A 和 B，它们各需要 50 ms 的 CPU 时间。==

However, there is one obvious difference: A runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10 ms), whereas B simply uses the CPU for 50 ms and performs no I/O.

==然而，有一个明显的区别：A 运行 10 ms 然后发起一个 I/O 请求（假设这里的 I/O 每次花费 10 ms），而 B 仅仅使用 CPU 50 ms 且不执行 I/O。==

The scheduler runs A first, then B after.

==调度程序先运行 A，然后运行 B。==

Assume we are trying to build a STCF scheduler.

==假设我们正试图构建一个 STCF 调度程序。==

How should such a scheduler account for the fact that A is broken up into 5 10-ms sub-jobs, whereas B is just a single 50-ms CPU demand?

==这样的调度程序该如何考虑到 A 被分解成 5 个 10 ms 的子任务，而 B 只是一个单一的 50 ms CPU 需求这一事实？==

Clearly, just running one job and then the other without considering how to take I/O into account makes little sense.

==显然，不考虑如何将 I/O 纳入考量而只是运行一个任务然后再运行另一个任务是没有意义的。==

TIP: OVERLAP ENABLES HIGHER UTILIZATION

==提示：重叠可以实现更高的利用率==

When possible, overlap operations to maximize the utilization of systems.

==如果可能，重叠操作以最大化系统的利用率。==

Overlap is useful in many different domains, including when performing disk I/O or sending messages to remote machines;

==重叠在许多不同的领域都很有用，包括执行磁盘 I/O 或向远程机器发送消息时；==

in either case, starting the operation and then switching to other work is a good idea, and improves the overall utilization and efficiency of the system.

==在任何一种情况下，启动操作然后切换到其他工作都是一个好主意，并且可以提高系统的整体利用率和效率。==

A common approach is to treat each 10-ms sub-job of A as an independent job.

==一种常见的方法是将 A 的每个 10 ms 子任务视为一个独立的任务。==

Thus, when the system starts, its choice is whether to schedule a 10-ms A or a 50-ms B.

==因此，当系统启动时，它的选择是调度一个 10 ms 的 A 还是一个 50 ms 的 B。==

With STCF, the choice is clear: choose the shorter one, in this case A.

==对于 STCF，选择很明确：选择较短的那个，也就是 A。==

Then, when the first sub-job of A has completed, only B is left, and it begins running.

==然后，当 A 的第一个子任务完成后，只剩下 B，它开始运行。==

Then a new sub-job of A is submitted, and it preempts B and runs for 10 ms.

==然后 A 的一个新的子任务被提交，它抢占 B 并运行 10 ms。==

Doing so allows for overlap, with the CPU being used by one process while waiting for the I/O of another process to complete;

==这样做允许重叠，即在一个进程等待 I/O 完成的同时，CPU 被另一个进程使用；==

the system is thus better utilized.

==系统因此得到了更好的利用。==

And thus we see how a scheduler might incorporate I/O.

==因此我们可以看到调度程序如何纳入 I/O。==

By treating each CPU burst as a job, the scheduler makes sure processes that are "interactive" get run frequently.

==通过将每个 CPU 爆发（CPU burst）视为一个任务，调度程序确保“交互式”进程得到频繁运行。==

While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor.

==当这些交互式任务正在执行 I/O 时，其他 CPU 密集型任务运行，从而更好地利用处理器。==

7.9 No More Oracle

==7.9 不再有预言机==

With a basic approach to I/O in place, we come to our final assumption: that the scheduler knows the length of each job.

==有了处理 I/O 的基本方法后，我们来到最后一个假设：调度程序知道每个任务的长度。==

As we said before, this is likely the worst assumption we could make.

==正如我们之前所说，这可能是我们能做出的最糟糕的假设。==

In fact, in a general-purpose OS (like the ones we care about), the OS usually knows very little about the length of each job.

==事实上，在一个通用操作系统（像我们关心的那些）中，操作系统通常对每个任务的长度知之甚少。==

Thus, how can we build an approach that behaves like SJF/STCF without such a priori knowledge?

==因此，我们如何才能在没有这种先验知识的情况下，构建一种行为类似 SJF/STCF 的方法？==

Further, how can we incorporate some of the ideas we have seen with the RR scheduler so that response time is also quite good?

==此外，我们如何才能结合我们看到的 RR 调度程序的一些思想，以便响应时间也相当不错？==

7.10 Summary

==7.10 总结==

We have introduced the basic ideas behind scheduling and developed two families of approaches.

==我们已经介绍了调度背后的基本思想，并开发了两类方法。==

The first runs the shortest job remaining and thus optimizes turnaround time;

==第一类运行剩余时间最短的任务，从而优化周转时间；==

the second alternates between all jobs and thus optimizes response time.

==第二类在所有任务之间交替运行，从而优化响应时间。==

Both are bad where the other is good, alas, an inherent trade-off common in systems.

==唉，两者都在对方擅长的地方表现糟糕，这是系统中常见的一种固有权衡。==

We have also seen how we might incorporate I/O into the picture, but have still not solved the problem of the fundamental inability of the OS to see into the future.

==我们也看到了如何将 I/O 纳入考量，但仍未解决操作系统无法预见未来的根本性问题。==

Shortly, we will see how to overcome this problem, by building a scheduler that uses the recent past to predict the future.

==很快，我们将看到如何通过构建一个利用最近的过去来预测未来的调度程序来克服这个问题。==

This scheduler is known as the multi-level feedback queue, and it is the topic of the next chapter.

==这个调度程序被称为多级反馈队列，它是下一章的主题。==

Scheduling: The Multi-Level Feedback Queue

==调度：多级反馈队列==

In this chapter, we'll tackle the problem of developing one of the most well-known approaches to scheduling, known as the Multi-level Feedback Queue (MLFQ).

==在本章中，我们将解决开发一种最著名的调度方法的问题，即多级反馈队列（MLFQ）。==

The Multi-level Feedback Queue (MLFQ) scheduler was first described by Corbato et al. in 1962 in a system known as the Compatible Time-Sharing System (CTSS), and this work, along with later work on Multics, led the ACM to award Corbato its highest honor, the Turing Award.

==多级反馈队列（MLFQ）调度程序最早由 Corbato 等人在 1962 年的一个名为兼容分时系统（CTSS）的系统中描述，这项工作以及后来关于 Multics 的工作，使得 ACM 授予 Corbato 其最高荣誉——图灵奖。==

The scheduler has subsequently been refined throughout the years to the implementations you will encounter in some modern systems.

==该调度程序在随后的岁月中不断改进，演变成了你在一些现代系统中会遇到的实现。==

The fundamental problem MLFQ tries to address is two-fold.

==MLFQ 试图解决的根本问题有两个方面。==

First, it would like to optimize turnaround time, which, as we saw in the previous note, is done by running shorter jobs first;

==首先，它希望优化周转时间，正如我们在之前的笔记中看到的，这通过优先运行较短的任务来实现；==

unfortunately, the OS doesn't generally know how long a job will run for, exactly the knowledge that algorithms like SJF (or STCF) require.

==不幸的是，操作系统通常不知道一个任务会运行多久，而这正是 SJF（或 STCF）等算法所需的知识。==

Second, MLFQ would like to make a system feel responsive to interactive users (i.e., users sitting and staring at the screen, waiting for a process to finish), and thus minimize response time;

==其次，MLFQ 希望让系统对交互式用户（即坐在屏幕前等待进程完成的用户）感觉响应灵敏，从而最小化响应时间；==

unfortunately, algorithms like Round Robin reduce response time but are terrible for turnaround time.

==不幸的是，像轮转（Round Robin）这样的算法虽然减少了响应时间，但周转时间却很糟糕。==

Thus, our problem: given that we in general do not know anything about a process, how can we build a scheduler to achieve these goals?

==因此，我们的问题是：鉴于我们通常对进程一无所知，我们如何构建一个调度程序来实现这些目标？==

How can the scheduler learn, as the system runs, the characteristics of the jobs it is running, and thus make better scheduling decisions?

==调度程序如何在系统运行时了解它正在运行的任务的特征，从而做出更好的调度决策？==

THE CRUX: HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?

==关键问题：如何在没有完美知识的情况下进行调度？==

How can we design a scheduler that both minimizes response time for interactive jobs while also minimizing turnaround time without a priori knowledge of job length?

==我们如何设计一个调度程序，既能最小化交互式任务的响应时间，又能在没有任务长度先验知识的情况下最小化周转时间？==

TIP: LEARN FROM HISTORY

==提示：从历史中学习==

The multi-level feedback queue is an excellent example of a system that learns from the past to predict the future.

==多级反馈队列是一个利用过去来预测未来的系统的绝佳例子。==

Such approaches are common in operating systems (and many other places in Computer Science, including hardware branch predictors and caching algorithms).

==这种方法在操作系统（以及计算机科学的许多其他领域，包括硬件分支预测器和缓存算法）中很常见。==

Such approaches work when jobs have phases of behavior and are thus predictable;

==当任务具有行为阶段并因此具有可预测性时，这种方法是有效的；==

of course, one must be careful with such techniques, as they can easily be wrong and drive a system to make worse decisions than they would have with no knowledge at all.

==当然，必须谨慎使用此类技术，因为它们很容易出错，并导致系统做出的决策比毫无知识时更糟糕。==

8.1 MLFQ: Basic Rules

==8.1 MLFQ：基本规则==

To build such a scheduler, in this chapter we will describe the basic algorithms behind a multi-level feedback queue;

==为了构建这样的调度程序，本章我们将描述多级反馈队列背后的基本算法；==

although the specifics of many implemented MLFQs differ, most approaches are similar.

==尽管许多已实现的 MLFQ 在细节上有所不同，但大多数方法是相似的。==

In our treatment, the MLFQ has a number of distinct queues, each assigned a different priority level.

==在我们的讨论中，MLFQ 拥有许多不同的队列，每个队列被分配了不同的优先级。==

At any given time, a job that is ready to run is on a single queue.

==在任何给定时间，一个准备运行的任务位于单个队列中。==

MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run.

==MLFQ 使用优先级来决定在给定时间应运行哪个任务：具有较高优先级的任务（即位于较高队列中的任务）被选中运行。==

Of course, more than one job may be on a given queue, and thus have the same priority.

==当然，可能有多个任务位于同一个队列中，因此具有相同的优先级。==

In this case, we will just use round-robin scheduling among those jobs.

==在这种情况下，我们将仅仅在这些任务之间使用轮转调度。==

Thus, we arrive at the first two basic rules for MLFQ:

==因此，我们得出了 MLFQ 的前两条基本规则：==

• Rule 1: If Priority(A) > Priority(B), A runs (B doesn't).

==• 规则 1：如果 优先级(A) > 优先级(B)，则 A 运行（B 不运行）。==

• Rule 2: If Priority(A) = Priority(B), A & B run in RR.

==• 规则 2：如果 优先级(A) = 优先级(B)，则 A 和 B 进行轮转调度（RR）。==

The key to MLFQ scheduling therefore lies in how the scheduler sets priorities.

==因此，MLFQ 调度的关键在于调度程序如何设置优先级。==

Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior.

==MLFQ 不是给每个任务一个固定的优先级，而是根据观察到的行为来改变任务的优先级。==

If, for example, a job repeatedly relinquishes the CPU while waiting for input from the keyboard, MLFQ will keep its priority high, as this is how an interactive process might behave.

==例如，如果一个任务在等待键盘输入时反复放弃 CPU，MLFQ 将保持其高优先级，因为这是交互式进程可能的行为方式。==

If, instead, a job uses the CPU intensively for long periods of time, MLFQ will reduce its priority.

==相反，如果一个任务长时间密集使用 CPU，MLFQ 将降低其优先级。==

In this way, MLFQ will try to learn about processes as they run, and thus use the history of the job to predict its future behavior.

==通过这种方式，MLFQ 将尝试在进程运行时了解它们，从而利用任务的历史来预测其未来的行为。==

If we were to put forth a picture of what the queues might look like at a given instant, we might see something like the following (Figure 8.1).

==如果我们展示某一瞬间队列可能的样子，我们可能会看到如下图所示的情况（图 8.1）。==

In the figure, two jobs (A and B) are at the highest priority level, while job C is in the middle and Job D is at the lowest priority.

==在图中，两个任务（A 和 B）处于最高优先级，而任务 C 处于中间，任务 D 处于最低优先级。==

Given our current knowledge of how MLFQ works, the scheduler would just alternate time slices between A and B because they are the highest priority jobs in the system;

==鉴于我们目前对 MLFQ 工作原理的了解，调度程序将只在 A 和 B 之间交替时间片，因为它们是系统中优先级最高的任务；==

poor jobs C and D would never even get to run — an outrage!

==可怜的任务 C 和 D 甚至永远得不到运行——简直是暴行！==

Of course, just showing a static snapshot of some queues does not really give you an idea of how MLFQ works.

==当然，仅仅展示一些队列的静态快照并不能真正让你了解 MLFQ 是如何工作的。==

What we need is to understand how job priority changes over time.

==我们需要的是了解任务优先级随时间如何变化。==

And that, in a surprise only to those who are reading a chapter from this book for the first time, is exactly what we will do next.

==这正是我们接下来要做的，只有第一次阅读本书这一章的人才会感到惊讶。==

8.2 Attempt #1: How To Change Priority

==8.2 尝试 #1：如何改变优先级==

We now must decide how MLFQ is going to change the priority level of a job (and thus which queue it is on) over the lifetime of a job.

==我们现在必须决定 MLFQ 将如何在任务的生命周期内改变其优先级（也就是它所在的队列）。==

To do this, we must keep in mind our workload: a mix of interactive jobs that are short-running (and may frequently relinquish the CPU), and some longer-running "CPU-bound" jobs that need a lot of CPU time but where response time isn't important.

==为此，我们必须牢记我们的工作负载：混合了运行时间短（且可能频繁放弃 CPU）的交互式任务，以及一些需要大量 CPU 时间但响应时间并不重要的长运行“CPU 密集型”任务。==

For this, we need a new concept, which we will call the job's allotment.

==为此，我们需要一个新概念，我们称之为任务的配额（allotment）。==

The allotment is the amount of time a job can spend at a given priority level before the scheduler reduces its priority.

==配额是一个任务在调度程序降低其优先级之前，可以在给定优先级上花费的时间量。==

For simplicity, at first, we will assume the allotment is equal to a single time slice.

==为了简单起见，起初我们假设配额等于单个时间片。==

Here is our first attempt at a priority-adjustment algorithm:

==这是我们对优先级调整算法的第一次尝试：==

• Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).

==• 规则 3：当一个任务进入系统时，它被放在最高优先级（最顶层的队列）。==

• Rule 4a: If a job uses up its allotment while running, its priority is reduced (i.e., it moves down one queue).

==• 规则 4a：如果一个任务在运行时用完了它的配额，它的优先级会被降低（即，它向下移动一个队列）。==

• Rule 4b: If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the same priority level (i.e., its allotment is reset).

==• 规则 4b：如果一个任务在配额用完之前放弃 CPU（例如，通过执行 I/O 操作），它保持在相同的优先级（即，它的配额被重置）。==

Example 1: A Single Long-Running Job

==例子 1：单个长运行任务==

Let's look at some examples.

==让我们看一些例子。==

First, we'll look at what happens when there has been a long running job in the system, with a time slice of 10 ms (and with the allotment set equal to the time slice).

==首先，我们来看看当系统中有一个长运行任务时会发生什么，时间片为 10 ms（且配额设为等于时间片）。==

Figure 8.2 shows what happens to this job over time in a three-queue scheduler.

==图 8.2 展示了在三队列调度程序中，该任务随时间推移发生的情况。==

As you can see in the example, the job enters at the highest priority (Q2).

==正如你在例子中所见，任务以最高优先级（Q2）进入。==

After a single time slice of 10 ms, the scheduler reduces the job's priority by one, and thus the job is on Q1.

==在单个 10 ms 时间片后，调度程序将任务的优先级降低一级，因此任务位于 Q1。==

After running at Q1 for a time slice, the job is finally lowered to the lowest priority in the system (Q0), where it remains.

==在 Q1 运行一个时间片后，任务最终被降低到系统的最低优先级（Q0），并留在那里。==

Pretty simple, no?

==很简单，不是吗？==

Example 2: Along Came A Short Job

==例子 2：来了一个短任务==

Now let's look at a more complicated example, and hopefully see how MLFQ tries to approximate SJF.

==现在让我们看一个更复杂的例子，希望能看到 MLFQ 如何尝试近似 SJF。==

In this example, there are two jobs: A, which is a long-running CPU-intensive job, and B, which is a short-running interactive job.

==在这个例子中，有两个任务：A，它是一个长运行的 CPU 密集型任务；B，它是一个短运行的交互式任务。==

Assume A has been running for some time, and then B arrives.

==假设 A 已经运行了一段时间，然后 B 到达。==

What will happen?

==会发生什么？==

Will MLFQ approximate SJF for B?

==MLFQ 会对 B 近似 SJF 吗？==

Figure 8.3 (left) plots the results of this scenario.

==图 8.3（左）绘制了这个场景的结果。==

Job A (shown in black) is running along in the lowest-priority queue (as would any long-running CPU-intensive jobs);

==任务 A（黑色显示）在最低优先级队列中运行（就像任何长运行的 CPU 密集型任务一样）；==

B (shown in gray) arrives at time $T=100$, and thus is inserted into the highest queue;

==B（灰色显示）在时间 $T=100$ 到达，因此被插入到最高队列；==

as its run-time is short (only 20 ms), B completes before reaching the bottom queue, in two time slices;

==由于它的运行时间很短（只有 20 ms），B 在到达底部队列之前就完成了，用了两个时间片；==

then A resumes running (at low priority).

==然后 A 恢复运行（在低优先级）。==

From this example, you can hopefully understand one of the major goals of the algorithm: because it doesn't know whether a job will be a short job or a long-running job, it first assumes it might be a short job, thus giving the job high priority.

==从这个例子中，你有望理解该算法的主要目标之一：因为它不知道一个任务是短任务还是长运行任务，它首先假设它可能是一个短任务，从而赋予该任务高优先级。==

If it actually is a short job, it will run quickly and complete;

==如果它确实是一个短任务，它将快速运行并完成；==

if it is not a short job, it will slowly move down the queues, and thus soon prove itself to be a long-running more batch-like process.

==如果它不是一个短任务，它将慢慢向下移动队列，从而很快证明自己是一个长运行的、更像批处理的进程。==

In this manner, MLFQ approximates SJF.

==通过这种方式，MLFQ 近似了 SJF。==

Example 3: What About I/O?

==例子 3：I/O 呢？==

Let's now look at an example with some I/O.

==现在让我们看一个带有一些 I/O 的例子。==

As Rule 4b states above, if a process gives up the processor before using up its allotment, we keep it at the same priority level.

==正如上面的规则 4b 所述，如果一个进程在用完其配额之前放弃处理器，我们将其保持在相同的优先级。==

The intent of this rule is simple: if an interactive job, for example, is doing a lot of I/O (say by waiting for user input from the keyboard or mouse), it will relinquish the CPU before its allotment is complete;

==这条规则的意图很简单：例如，如果一个交互式任务正在进行大量 I/O（比如等待来自键盘或鼠标的用户输入），它会在配额完成之前放弃 CPU；==

in such case, we don't wish to penalize the job and thus simply keep it at the same level.

==在这种情况下，我们不希望惩罚该任务，因此只需将其保持在同一级别。==

Figure 8.3 (right) shows an example of how this works, with an interactive job B (shown in gray) that needs the CPU only for 1 ms before performing an I/O competing for the CPU with a long-running batch job A (shown in black).

==图 8.3（右）展示了这个机制如何工作的例子，其中一个交互式任务 B（灰色显示）在执行 I/O 之前只需要 1 ms 的 CPU 时间，它与一个长运行的批处理任务 A（黑色显示）争夺 CPU。==

The MLFQ approach keeps B at the highest priority because B keeps releasing the CPU;

==MLFQ 方法将 B 保持在最高优先级，因为 B 不断释放 CPU；==

if B is an interactive job, MLFQ further achieves its goal of running interactive jobs quickly.

==如果 B 是一个交互式任务，MLFQ 进一步实现了其快速运行交互式任务的目标。==

Problems With Our Current MLFQ

==当前 MLFQ 的问题==

We thus have a basic MLFQ.

==因此我们有了一个基本的 MLFQ。==

It seems to do a fairly good job, sharing the CPU fairly between long-running jobs, and letting short or I/O-intensive interactive jobs run quickly.

==它似乎做得相当不错，在长运行任务之间公平地共享 CPU，并让短的或 I/O 密集的交互式任务快速运行。==

Unfortunately, the approach we have developed thus far contains serious flaws.

==不幸的是，我们迄今为止开发的方法包含严重的缺陷。==

Can you think of any?

==你能想到什么吗？==

(This is where you pause and think as deviously as you can)

==（这里是你停下来尽可能狡猾地思考的地方）==

First, there is the problem of starvation: if there are "too many" interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time (they starve).

==首先，存在饥饿问题：如果系统中有“太多”交互式任务，它们将联合消耗所有 CPU 时间，因此长运行任务将永远得不到任何 CPU 时间（它们饿死了）。==

We'd like to make some progress on these jobs even in this scenario.

==即使在这种情况下，我们也希望这些任务能取得一些进展。==

Second, a smart user could rewrite their program to game the scheduler.

==其次，聪明的用户可以重写他们的程序来欺骗调度程序。==

Gaming the scheduler generally refers to the idea of doing something sneaky to trick the scheduler into giving you more than your fair share of the resource.

==欺骗调度程序通常是指做一些偷偷摸摸的事情来诱使调度程序给你超过你公平份额的资源。==

TIP: SCHEDULING MUST BE SECURE FROM ATTACK

==提示：调度必须安全以防攻击==

You might think that a scheduling policy, whether inside the OS itself (as discussed herein), or in a broader context (e.g., in a distributed storage system's I/O request handling), is not a security concern, but in increasingly many cases, it is exactly that.

==你可能认为调度策略，无论是在操作系统内部（如本文所述），还是在更广泛的背景下（例如，在分布式存储系统的 I/O 请求处理中），都不是安全问题，但在越来越多的情况下，它正是安全问题。==

Consider the modern datacenter, in which users from around the world share CPUs, memories, networks, and storage systems;

==考虑现代数据中心，来自世界各地的用户共享 CPU、内存、网络和存储系统；==

without care in policy design and enforcement, a single user may be able to adversely harm others and gain advantage for itself.

==如果在策略设计和执行方面不加小心，单个用户可能会恶意伤害他人并为自己获取优势。==

Thus, scheduling policy forms an important part of the security of a system, and should be carefully constructed.

==因此，调度策略构成了系统安全的重要组成部分，应谨慎构建。==

The algorithm we have described is susceptible to the following attack: before the allotment is used, issue an I/O operation (e.g., to a file) and thus relinquish the CPU;

==我们描述的算法容易受到以下攻击：在配额用完之前，发出一个 I/O 操作（例如，对文件），从而放弃 CPU；==

doing so allows you to remain in the same queue, and thus gain a higher percentage of CPU time.

==这样做允许你留在同一个队列中，从而获得更高比例的 CPU 时间。==

When done right (e.g., by running for 99% of the allotment before relinquishing the CPU), a job could nearly monopolize the CPU.

==如果做得对（例如，在放弃 CPU 之前运行配额的 99%），一个任务几乎可以独占 CPU。==

Finally, a program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity.

==最后，程序可能会随时间改变其行为；原本是 CPU 密集型的任务可能会过渡到交互阶段。==

With our current approach, such a job would be out of luck and not be treated like the other interactive jobs in the system.

==用我们目前的方法，这样的任务将运气不佳，不会像系统中的其他交互式任务那样被对待。==

8.3 Attempt #2: The Priority Boost

==8.3 尝试 #2：优先级提升==

Let's try to change the rules and see if we can avoid the problem of starvation.

==让我们尝试改变规则，看看是否可以避免饥饿问题。==

What could we do in order to guarantee that CPU-bound jobs will make some progress (even if it is not much?).

==为了保证 CPU 密集型任务能取得一些进展（即使不多），我们可以做什么？==

The simple idea here is to periodically boost the priority of all the jobs in the system.

==这里简单的想法是定期提升系统中所有任务的优先级。==

There are many ways to achieve this, but let's just do something simple: throw them all in the topmost queue;

==有很多方法可以实现这一点，但让我们做些简单的：把它们都扔进最高优先级的队列；==

hence, a new rule:

==因此，产生了一条新规则：==

• Rule 5: After some time period S, move all the jobs in the system to the topmost queue.

==• 规则 5：经过一段时间 S 后，将系统中的所有任务移动到最高优先级队列。==

Our new rule solves two problems at once.

==我们的新规则一次解决了两个问题。==

First, processes are guaranteed not to starve: by sitting in the top queue, a job will share the CPU with other high-priority jobs in a round-robin fashion, and thus eventually receive service.

==首先，保证进程不会挨饿：通过位于顶部队列，任务将以轮转方式与其他高优先级任务共享 CPU，从而最终获得服务。==

Second, if a CPU-bound job has become interactive, the scheduler treats it properly once it has received the priority boost.

==其次，如果一个 CPU 密集型任务变得交互式，一旦它获得了优先级提升，调度程序就会正确对待它。==

Let's see an example.

==让我们看一个例子。==

In this scenario, we just show the behavior of a long-running job when competing for the CPU with two short-running interactive jobs.

==在这个场景中，我们只展示一个长运行任务在与两个短运行交互式任务争夺 CPU 时的行为。==

Two graphs are shown in Figure 8.4.

==图 8.4 中显示了两个图表。==

On the left, there is no priority boost, and thus the long-running job gets starved once the two short jobs arrive;

==在左边，没有优先级提升，因此长运行任务一旦两个短任务到达就会挨饿；==

on the right, there is a priority boost every 100 ms (which is likely too small of a value, but used here for the example), and thus we at least guarantee that the long-running job will make some progress, getting boosted to the highest priority every 100 ms and thus getting to run periodically.

==在右边，每 100 ms 有一次优先级提升（这可能是一个太小的值，但这里用于示例），因此我们至少保证了长运行任务将取得一些进展，每 100 ms 被提升到最高优先级，从而得到周期性运行。==

Of course, the addition of the time period S leads to the obvious question: what should S be set to?

==当然，增加时间段 S 引出了一个显而易见的问题：S 应该设为多少？==

John Ousterhout, a well-regarded systems researcher, used to call such values in systems voo-doo constants, because they seemed to require some form of black magic to set them correctly.

==John Ousterhout，一位备受推崇的系统研究员，过去常将系统中的此类值称为“巫毒常量”，因为它们似乎需要某种形式的黑魔法才能正确设置。==

Unfortunately, S has that flavor.

==不幸的是，S 就有这种味道。==

If it is set too high, long-running jobs could starve;

==如果设置得太高，长运行任务可能会挨饿；==

too low, and interactive jobs may not get a proper share of the CPU.

==太低，交互式任务可能无法获得适当的 CPU 份额。==

As such, it is often left to the system administrator to find the right value or in the modern world, increasingly, to automatic methods based on machine learning.

==因此，通常留给系统管理员来找到合适的值，或者在现代世界中，越来越多地留给基于机器学习的自动方法。==

8.4 Attempt #3: Better Accounting

==8.4 尝试 #3：更好的计时==

We now have one more problem to solve: how to prevent gaming of our scheduler?

==我们现在还有一个问题要解决：如何防止对我们的调度程序的欺骗？==

The real culprit here, as you might have guessed, are Rules 4a and 4b, which let a job retain its priority by relinquishing the CPU before its allotment expires.

==真正的罪魁祸首，正如你可能已经猜到的，是规则 4a 和 4b，它们允许任务通过在配额到期前放弃 CPU 来保持其优先级。==

So what should we do?

==那么我们该怎么办？==

TIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT'S LAW)

==提示：避免巫毒常量（OUSTERHOUT 定律）==

Avoiding voo-doo constants is a good idea whenever possible.

==尽可能避免巫毒常量是一个好主意。==

Unfortunately, as in the example above, it is often difficult.

==不幸的是，如上例所示，这通常很难。==

One could try to make the system learn a good value, but that too is not straightforward.

==人们可以尝试让系统学习一个好的值，但这也不是直截了当的。==

The frequent result: a configuration file filled with default parameter values that a seasoned administrator can tweak when something isn't quite working correctly.

==常见的结果是：一个充满默认参数值的配置文件，经验丰富的管理员可以在某些东西工作不正常时对其进行调整。==

As you can imagine, these are often left unmodified, and thus we are left to hope that the defaults work well in the field.

==可以想象，这些通常保持未修改状态，因此我们只能希望默认值在实际应用中工作良好。==

This tip brought to you by our old OS professor, John Ousterhout, and hence we call it Ousterhout's Law.

==这个提示是由我们的老操作系统教授 John Ousterhout 带来的，因此我们称之为 Ousterhout 定律。==

The solution here is to perform better accounting of CPU time at each level of the MLFQ.

==这里的解决方案是在 MLFQ 的每个级别执行更好的 CPU 时间计时。==

Instead of forgetting how much of its allotment a process used at a given level when it performs I/O, the scheduler should keep track;

==调度程序不应在进程执行 I/O 时忘记它在给定级别使用了多少配额，而应进行跟踪；==

once a process has used its allotment, it is demoted to the next priority queue.

==一旦一个进程用完了它的配额，它就会被降级到下一个优先级队列。==

Whether it uses its allotment in one long burst or many small ones should not matter.

==无论它是在一次长爆发中还是多次小爆发中用完其配额都不重要。==

We thus rewrite Rules 4a and 4b to the following single rule:

==因此，我们将规则 4a 和 4b 重写为以下单一规则：==

• Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).

==• 规则 4：一旦一个任务用完了它在给定级别的时间配额（无论它放弃了多少次 CPU），它的优先级就会降低（即，它向下移动一个队列）。==

Let's look at an example.

==让我们看一个例子。==

Figure 8.5 shows what happens when a workload tries to game the scheduler with the old Rules 4a and 4b (on the left) as well the new anti-gaming Rule 4.

==图 8.5 展示了当工作负载试图利用旧规则 4a 和 4b（左侧）以及新的反欺骗规则 4 欺骗调度程序时会发生什么。==

Without any protection from gaming, a process can issue an I/O before its allotment ends, thus staying at the same priority level, and dominating CPU time.

==没有任何防止欺骗的保护，进程可以在配额结束前发出 I/O，从而停留在同一优先级水平，并主导 CPU 时间。==

With better accounting in place (right), regardless of the I/O behavior of the process, it slowly moves down the queues, and thus cannot gain an unfair share of the CPU.

==有了更好的计时（右侧），无论进程的 I/O 行为如何，它都会慢慢向下移动队列，因此无法获得不公平的 CPU 份额。==

8.5 Tuning MLFQ And Other Issues

==8.5 调优 MLFQ 及其他问题==

A few other issues arise with MLFQ scheduling.

==MLFQ 调度还出现了一些其他问题。==

One big question is how to parameterize such a scheduler.

==一个大问题是如何参数化这样的调度程序。==

For example, how many queues should there be?

==例如，应该有多少个队列？==

How big should the time slice be per queue?

==每个队列的时间片应该多大？==

The allotment?

==配额呢？==

How often should priority be boosted in order to avoid starvation and account for changes in behavior?

==为了避免饥饿并顾及行为的变化，优先级应该多久提升一次？==

There are no easy answers to these questions, and thus only some experience with workloads and subsequent tuning of the scheduler will lead to a satisfactory balance.

==这些问题没有简单的答案，因此只有对工作负载的一些经验以及随后的调度程序调优才能带来令人满意的平衡。==

For example, most MLFQ variants allow for varying time-slice length across different queues.

==例如，大多数 MLFQ 变体允许不同队列具有不同的时间片长度。==

The high-priority queues are usually given short time slices;

==高优先级队列通常被给予短时间片；==

they are comprised of interactive jobs, after all, and thus quickly alternating between them makes sense (e.g., 10 or fewer milliseconds).

==毕竟它们由交互式任务组成，因此在它们之间快速交替是有意义的（例如，10 毫秒或更少）。==

The low-priority queues, in contrast, contain long-running jobs that are CPU-bound;

==相比之下，低优先级队列包含 CPU 密集型的长运行任务；==

hence, longer time slices work well (e.g., 100s of ms).

==因此，较长的时间片效果很好（例如，数百毫秒）。==

Figure 8.6 shows an example in which two jobs run for 20 ms at the highest queue (with a 10-ms time slice), 40 ms in the middle (20-ms time slice), and with a 40-ms time slice at the lowest.

==图 8.6 展示了一个例子，其中两个任务在最高队列运行 20 ms（时间片为 10 ms），在中间运行 40 ms（时间片为 20 ms），在最低处时间片为 40 ms。==

The Solaris MLFQ implementation — the Time-Sharing scheduling class, or TS — is particularly easy to configure;

==Solaris 的 MLFQ 实现——分时调度类，或称 TS——特别容易配置；==

it provides a set of tables that determine exactly how the priority of a process is altered throughout its lifetime, how long each time slice is, and how often to boost the priority of a job;

==它提供了一组表，精确决定了进程在其生命周期中优先级如何改变，每个时间片多长，以及多久提升一次任务的优先级；==

an administrator can muck with this table in order to make the scheduler behave in different ways.

==管理员可以修改这个表，以使调度程序表现出不同的行为方式。==

Default values for the table are 60 queues, with slowly increasing time-slice lengths from 20 milliseconds (highest priority) to a few hundred milliseconds (lowest), and priorities boosted around every 1 second or so.

==该表的默认值是 60 个队列，时间片长度从 20 毫秒（最高优先级）缓慢增加到几百毫秒（最低优先级），优先级大约每 1 秒左右提升一次。==

Other MLFQ schedulers don't use a table or the exact rules described in this chapter;

==其他 MLFQ 调度程序不使用表或本章描述的确切规则；==

rather they adjust priorities using mathematical formulae.

==而是使用数学公式调整优先级。==

For example, the FreeBSD scheduler (version 4.3) uses a formula to calculate the current priority level of a job, basing it on how much CPU the process has used;

==例如，FreeBSD 调度程序（版本 4.3）使用一个公式来计算任务的当前优先级，基于进程已使用了多少 CPU；==

in addition, usage is decayed over time, providing the desired priority boost in a different manner than described herein.

==此外，使用量随时间衰减，以不同于本文所述的方式提供所需的优先级提升。==

See Epema's paper for an excellent overview of such decay-usage algorithms and their properties.

==请参阅 Epema 的论文，了解此类衰减使用算法及其属性的精彩概述。==

Finally, many schedulers have a few other features that you might encounter.

==最后，许多调度程序还有一些你可能会遇到的其他功能。==

For example, some schedulers reserve the highest priority levels for operating system work;

==例如，一些调度程序为操作系统工作保留最高优先级；==

thus typical user jobs can never obtain the highest levels of priority in the system.

==因此，典型的用户任务永远无法获得系统中的最高优先级。==

Some systems also allow some user advice to help set priorities;

==一些系统还允许一些用户建议来帮助设置优先级；==

for example, by using the command-line utility nice you can increase or decrease the priority of a job (somewhat) and thus increase or decrease its chances of running at any given time.

==例如，通过使用命令行实用程序 nice，你可以（稍微）增加或减少任务的优先级，从而增加或减少其在任何给定时间运行的机会。==

See the man page for more.

==更多信息请参阅 man 页面。==

TIP: USE ADVICE WHERE POSSIBLE

==提示：尽可能使用建议==

As the operating system rarely knows what is best for each and every process of the system, it is often useful to provide interfaces to allow users or administrators to provide some hints to the OS.

==由于操作系统很少知道什么对系统的每个进程都是最好的，因此提供接口允许用户或管理员向操作系统提供一些提示通常很有用。==

We often call such hints advice, as the OS need not necessarily pay attention to it, but rather might take the advice into account in order to make a better decision.

==我们通常称此类提示为建议，因为操作系统不一定非要注意它，而是可以将建议纳入考虑，以便做出更好的决定。==

Such hints are useful in many parts of the OS, including the scheduler (e.g., with nice), memory manager (e.g., madvise), and file system (e.g., informed prefetching and caching).

==此类提示在操作系统的许多部分都很有用，包括调度程序（例如用 nice）、内存管理器（例如 madvise）和文件系统（例如知情预取和缓存）。==

8.6 MLFQ: Summary

==8.6 MLFQ：总结==

We have described a scheduling approach known as the Multi-Level Feedback Queue (MLFQ).

==我们已经描述了一种称为多级反馈队列（MLFQ）的调度方法。==

Hopefully you can now see why it is called that: it has multiple levels of queues, and uses feedback to determine the priority of a given job.

==希望你现在能明白为什么它叫这个名字：它有多级队列，并使用反馈来确定给定任务的优先级。==

History is its guide: pay attention to how jobs behave over time and treat them accordingly.

==历史是它的向导：关注任务随时间的行为表现，并据此对待它们。==

The refined set of MLFQ rules, spread throughout the chapter, are reproduced here for your viewing pleasure:

==本章中详述的经过改进的 MLFQ 规则集在此重现，以供您查阅：==

• Rule 1: If Priority(A) > Priority(B), A runs (B doesn't).

==• 规则 1：如果 优先级(A) > 优先级(B)，则 A 运行（B 不运行）。==

• Rule 2: If Priority(A) = Priority(B), A & B run in round-robin fashion using the time slice (quantum length) of the given queue.

==• 规则 2：如果 优先级(A) = 优先级(B)，则 A 和 B 使用给定队列的时间片（量子长度）以轮转方式运行。==

• Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).

==• 规则 3：当一个任务进入系统时，它被放在最高优先级（最顶层的队列）。==

• Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).

==• 规则 4：一旦一个任务用完了它在给定级别的时间配额（无论它放弃了多少次 CPU），它的优先级就会降低（即，它向下移动一个队列）。==

• Rule 5: After some time period S, move all the jobs in the system to the topmost queue.

==• 规则 5：经过一段时间 S 后，将系统中的所有任务移动到最高优先级队列。==

MLFQ is interesting for the following reason: instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly.

==MLFQ 之所以有趣，原因如下：它不要求对任务的性质有先验知识，而是观察任务的执行并相应地进行优先级排序。==

In this way, it manages to achieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and makes progress for long-running CPU-intensive workloads.

==通过这种方式，它设法实现了两全其美：它可以为短运行的交互式任务提供出色的整体性能（类似于 SJF/STCF），并且对于长运行的 CPU 密集型工作负载也是公平的并能取得进展。==

For this reason, many systems, including BSD UNIX derivatives, Solaris, and Windows NT and subsequent Windows operating systems use a form of MLFQ as their base scheduler.

==因此，许多系统，包括 BSD UNIX 衍生系统、Solaris 以及 Windows NT 和随后的 Windows 操作系统，都使用某种形式的 MLFQ 作为其基础调度程序。==

Scheduling: Proportional Share

==调度：比例份额==

In this chapter, we'll examine a different type of scheduler known as a proportional-share scheduler, also sometimes referred to as a fair-share scheduler.

==在本章中，我们将研究一种不同类型的调度程序，称为比例份额调度程序，有时也称为公平份额调度程序。==

Proportional-share is based around a simple concept: instead of optimizing for turnaround or response time, a scheduler might instead try to guarantee that each job obtain a certain percentage of CPU time.

==比例份额基于一个简单的概念：调度程序不是优化周转时间或响应时间，而是试图保证每个任务获得一定比例的 CPU 时间。==

An excellent early example of proportional-share scheduling is found in research by Waldspurger and Weihl, and is known as lottery scheduling;

==比例份额调度的一个极好的早期例子可以在 Waldspurger 和 Weihl 的研究中找到，被称为彩票调度；==

however, the idea is certainly older.

==然而，这个想法肯定更古老。==

The basic idea is quite simple: every so often, hold a lottery to determine which process should get to run next;

==基本思想很简单：每隔一段时间，举行一次彩票抽奖，以确定哪个进程应该下一个运行；==

processes that should run more often should be given more chances to win the lottery.

==应该运行得更频繁的进程应该被给予更多的机会赢得彩票。==

Easy, no?

==很简单，不是吗？==

Now, onto the details!

==现在，进入细节！==

But not before our crux:

==但在那之前是我们的关键问题：==

CRUX: HOW TO SHARE THE CPU PROPORTIONALLY

==关键问题：如何按比例共享 CPU==

How can we design a scheduler to share the CPU in a proportional manner?

==我们如何设计一个调度程序来按比例共享 CPU？==

What are the key mechanisms for doing so?

==这样做的关键机制是什么？==

How effective are they?

==它们有多有效？==

9.1 Basic Concept: Tickets Represent Your Share

==9.1 基本概念：彩票代表你的份额==

Underlying lottery scheduling is one very basic concept: tickets, which are used to represent the share of a resource that a process (or user or whatever) should receive.

==彩票调度的基础是一个非常基本的概念：彩票，它用于代表一个进程（或用户或其他）应该获得的资源份额。==

The percent of tickets that a process has represents its share of the system resource in question.

==一个进程拥有的彩票百分比代表了它在相关系统资源中的份额。==

Let's look at an example.

==让我们看一个例子。==

Imagine two processes, A and B, and further that A has 75 tickets while B has only 25.

==想象两个进程 A 和 B，并且 A 有 75 张彩票，而 B 只有 25 张。==

Thus, what we would like is for A to receive 75% of the CPU and B the remaining 25%.

==因此，我们要的是 A 获得 75% 的 CPU，而 B 获得剩余的 25%。==

Lottery scheduling achieves this probabilistically (but not deterministically) by holding a lottery every so often (say, every time slice).

==彩票调度通过每隔一段时间（比如，每个时间片）举行一次彩票抽奖，以概率性地（但不是确定性地）实现这一目标。==

Holding a lottery is straightforward: the scheduler must know how many total tickets there are (in our example, there are 100).

==举行彩票抽奖很简单：调度程序必须知道总共有多少张彩票（在我们的例子中，有 100 张）。==

The scheduler then picks a winning ticket, which is a number from 0 to 99.

==然后调度程序挑选一张中奖彩票，这是一个从 0 到 99 的数字。==

Assuming A holds tickets 0 through 74 and B 75 through 99, the winning ticket simply determines whether A or B runs.

==假设 A 持有彩票 0 到 74，B 持有 75 到 99，中奖彩票仅仅决定是 A 还是 B 运行。==

The scheduler then loads the state of that winning process and runs it.

==然后调度程序加载该获胜进程的状态并运行它。==

TIP: USE RANDOMNESS

==提示：使用随机性==

One of the most beautiful aspects of lottery scheduling is its use of randomness.

==彩票调度最美妙的方面之一是它对随机性的使用。==

When you have to make a decision, using such a randomized approach is often a robust and simple way of doing so.

==当你必须做出决定时，使用这种随机化方法通常是一种稳健且简单的做法。==

Random approaches have at least three advantages over more traditional decisions.

==与更传统的决策相比，随机方法至少有三个优点。==

First, random often avoids strange corner-case behaviors that a more traditional algorithm may have trouble handling.

==首先，随机通常避免了更传统的算法可能难以处理的奇怪的边缘情况行为。==

For example, consider the LRU replacement policy (studied in more detail in a future chapter on virtual memory);

==例如，考虑 LRU 替换策略（在未来关于虚拟内存的一章中会有更详细的研究）；==

while often a good replacement algorithm, LRU attains worst-case performance for some cyclic-sequential workloads.

==虽然通常是一个很好的替换算法，但 LRU 在某些循环顺序工作负载下会达到最差性能。==

Random, on the other hand, has no such worst case.

==另一方面，随机没有这种最差情况。==

Second, random also is lightweight, requiring little state to track alternatives.

==其次，随机也是轻量级的，几乎不需要状态来跟踪备选方案。==

In a traditional fair-share scheduling algorithm, tracking how much CPU each process has received requires per-process accounting, which must be updated after running each process.

==在传统的公平份额调度算法中，跟踪每个进程获得了多少 CPU 需要按进程进行计时，这必须在运行每个进程后更新。==

Doing so randomly necessitates only the most minimal of per-process state (e.g., the number of tickets each has).

==随机地这样做只需要最少的每进程状态（例如，每个进程拥有的彩票数量）。==

Finally, random can be quite fast.

==最后，随机可以相当快。==

As long as generating a random number is quick, making the decision is also, and thus random can be used in a number of places where speed is required.

==只要生成随机数很快，做出决定也很快，因此随机可以用于许多需要速度的地方。==

Of course, the faster the need, the more random tends towards pseudo-random.

==当然，需求越快，随机就越倾向于伪随机。==

SCHEDULING: PROPORTIONAL SHARE

==调度：比例份额==

  

TIP: USE TICKETS TO REPRESENT SHARES

==提示：使用彩票表示份额==

  

One of the most powerful (and basic) mechanisms in the design of lottery (and stride) scheduling is that of the ticket.

==彩票（lottery）调度（以及步长调度）设计中最强大（也是最基础）的机制之一就是彩票。==

  

The ticket is used to represent a process's share of the CPU in these examples, but can be applied much more broadly.

==在这些例子中，彩票用于表示进程占有的 CPU 份额，但它的应用范围其实可以更广。==

  

For example, in more recent work on virtual memory management for hypervisors, Waldspurger shows how tickets can be used to represent a guest operating system's share of memory [W02].

==例如，在关于虚拟机监视器虚拟内存管理的较新研究中，Waldspurger 展示了如何利用彩票来表示客户操作系统（Guest OS）占有的内存份额 [W02]。==

  

Thus, if you are ever in need of a mechanism to represent a proportion of ownership, this concept just might be... (wait for it) ... the ticket.

==因此，如果你需要一种机制来表示所有权的比例，这个概念可能正是……（等着瞧）……彩票。==

  

9.2 Ticket Mechanisms

==9.2 彩票机制==

  

Lottery scheduling also provides a number of mechanisms to manipulate tickets in different and sometimes useful ways.

==彩票调度还提供了一些以不同且有用的方式来操纵彩票的机制。==

  

One way is with the concept of ticket currency.

==其中一种方式是利用彩票货币（ticket currency）的概念。==

  

Currency allows a user with a set of tickets to allocate tickets among their own jobs in whatever currency they would like;

==货币允许拥有一组彩票的用户以其喜欢的任何货币在自己的作业之间分配彩票；==

  

the system then automatically converts said currency into the correct global value.

==然后系统会自动将该货币兑换为正确的全局价值。==

  

For example, assume users A and B have each been given 100 tickets.

==例如，假设用户 A 和用户 B 各自分到了 100 张彩票。==

  

User A is running two jobs, A1 and A2, and gives them each 500 tickets (out of 1000 total) in A's currency.

==用户 A 运行两个作业 A1 和 A2，并用 A 的货币给每个作业分配了 500 张彩票（总共 1000 张）。==

  

User B is running only 1 job and gives it 10 tickets (out of 10 total).

==用户 B 只运行 1 个作业，并给它分配了 10 张彩票（总共 10 张）。==

  

The system converts A1's and A2's allocation from 500 each in A's currency to 50 each in the global currency;

==系统将 A1 和 A2 的分配额从 A 货币下的各 500 张转换为全局货币下的各 50 张；==

  

similarly, B1's 10 tickets is converted to 100 tickets.

==同样地，B1 的 10 张彩票被转换为 100 张彩票。==

  

The lottery is then held over the global ticket currency (200 total) to determine which job runs.

==然后，基于全局彩票货币（总共 200 张）进行开奖，以决定运行哪个作业。==

  

Another useful mechanism is ticket transfer.

==另一个有用的机制是彩票转让（ticket transfer）。==

  

With transfers, a process can temporarily hand off its tickets to another process.

==通过转让，一个进程可以暂时将其彩票交给另一个进程。==

  

This ability is especially useful in a client/server setting, where a client process sends a message to a server asking it to do some work on the client's behalf.

==这种能力在客户端/服务器环境中特别有用，此时客户端进程向服务器发送消息，请求服务器代表其完成某些工作。==

  

To speed up the work, the client can pass the tickets to the server and thus try to maximize the performance of the server while the server is handling the client's request.

==为了加速工作，客户端可以将彩票传递给服务器，从而试图在服务器处理客户端请求时最大化服务器的性能。==

  

When finished, the server then transfers the tickets back to the client and all is as before.

==当完成后，服务器再将彩票转回给客户端，一切恢复如初。==

  

Finally, ticket inflation can sometimes be a useful technique.

==最后，彩票通胀（ticket inflation）有时也是一种有用的技术。==

  

With inflation, a process can temporarily raise or lower the number of tickets it owns.

==通过通胀，进程可以暂时增加或减少其拥有的彩票数量。==

  

Of course, in a competitive scenario with processes that do not trust one another, this makes little sense;

==当然，在进程互不信任的竞争场景中，这没有什么意义；==

  

one greedy process could give itself a vast number of tickets and take over the machine.

==一个贪婪的进程可能会给自己分配大量的彩票从而霸占机器。==

  

Rather, inflation can be applied in an environment where a group of processes trust one another;

==相反，通胀可以应用于一组进程相互信任的环境中；==

  

in such a case, if any one process knows it needs more CPU time, it can boost its ticket value as a way to reflect that need to the system, all without communicating with any other processes.

==在这种情况下，如果任何一个进程知道它需要更多的 CPU 时间，它可以增加自己的彩票价值，以此向系统反映这种需求，而无需与其他任何进程通信。==

  

9.3 Implementation

==9.3 实现==

  

Probably the most amazing thing about lottery scheduling is the simplicity of its implementation.

==彩票调度最令人惊奇的地方可能在于其实现的简单性。==

  

All you need is a good random number generator to pick the winning ticket, a data structure to track the processes of the system (e.g., a list), and the total number of tickets.

==你只需要一个好的随机数生成器来抽取中奖彩票，一个用于跟踪系统进程的数据结构（例如列表），以及彩票的总数。==

  

Let's assume we keep the processes in a list.

==让我们假设我们将进程保存在一个列表中。==

  

Here is an example comprised of three processes, A, B, and C, each with some number of tickets.

==这是一个由三个进程 A、B 和 C 组成的示例，每个进程都有一定数量的彩票。==

  

To make a scheduling decision, we first have to pick a random number (the winner) from the total number of tickets (400).

==为了做出调度决策，我们首先必须从彩票总数（400）中选取一个随机数（中奖者）。==

  

Let's say we pick the number 300.

==假设我们要选取的数字是 300。==

  

Then, we simply traverse the list, with a simple counter used to help us find the winner (Figure 9.1).

==然后，我们只需遍历列表，并使用一个简单的计数器来帮助我们找到赢家（图 9.1）。==

  

The code walks the process list, adding each ticket value to counter until the value exceeds winner.

==代码遍历进程列表，将每个彩票值加到计数器上，直到该值超过中奖号码。==

  

Once that is the case, the current list element is the winner.

==一旦出现这种情况，当前的列表元素就是赢家。==

  

With our example of the winning ticket being 300, the following takes place.

==以中奖彩票为 300 为例，过程如下。==

  

First, counter is incremented to 100 to account for A's tickets;

==首先，计数器增加到 100 以计入 A 的彩票；==

  

because 100 is less than 300, the loop continues.

==因为 100 小于 300，循环继续。==

  

Then counter would be updated to 150 (B's tickets), still less than 300 and thus again we continue.

==然后计数器将更新为 150（加上 B 的彩票），仍然小于 300，因此我们再次继续。==

  

Finally, counter is updated to 400 (clearly greater than 300), and thus we break out of the loop with current pointing at C (the winner).

==最后，计数器更新为 400（显然大于 300），因此我们跳出循环，此时 current 指向 C（赢家）。==

  

To make this process most efficient, it might generally be best to organize the list in sorted order, from the highest number of tickets to the lowest.

==为了使这个过程最高效，通常最好按排序顺序组织列表，从彩票数量最多到最少。==

  

The ordering does not affect the correctness of the algorithm;

==排序不会影响算法的正确性；==

  

however, it does ensure in general that the fewest number of list iterations are taken, especially if there are a few processes that possess most of the tickets.

==然而，这通常能确保进行最少的列表迭代，特别是当有少数进程拥有大部分彩票时。==

  

9.4 An Example

==9.4 一个例子==

  

To make the dynamics of lottery scheduling more understandable, we now perform a brief study of the completion time of two jobs competing against one another, each with the same number of tickets (100) and same run time (R, which we will vary).

==为了让彩票调度的动态过程更易于理解，我们现在简要研究两个相互竞争的作业的完成时间，它们拥有相同数量的彩票（100）和相同的运行时间（R，我们会改变这个值）。==

  

In this scenario, we'd like for each job to finish at roughly the same time, but due to the randomness of lottery scheduling, sometimes one job finishes before the other.

==在这种情况下，我们希望每个作业都在大致相同的时间完成，但由于彩票调度的随机性，有时一个作业会先于另一个完成。==

  

To quantify this difference, we define a simple fairness metric, F which is simply the time the first job completes divided by the time that the second job completes.

==为了量化这种差异，我们定义了一个简单的公平性指标 F，即第一个作业完成的时间除以第二个作业完成的时间。==

  

When both jobs finish at nearly the same time, F will be quite close to 1.

==当两个作业几乎同时完成时，F 将非常接近 1。==

  

In this scenario, that is our goal: a perfectly fair scheduler would achieve F = 1.

==在这种情况下，这是我们的目标：一个完全公平的调度器将实现 F = 1。==

  

As you can see from the graph, when the job length is not very long, average fairness can be quite low.

==从图中可以看出，当作业长度不是很长时，平均公平性可能会很低。==

  

Only as the jobs run for a significant number of time slices does the lottery scheduler approach the desired fair outcome.

==只有当作业运行了相当数量的时间片后，彩票调度器才会接近预期的公平结果。==

  

9.5 How To Assign Tickets?

==9.5 如何分配彩票？==

  

One problem we have not addressed with lottery scheduling is: how to assign tickets to jobs?

==我们在彩票调度中尚未解决的一个问题是：如何给作业分配彩票？==

  

This problem is a tough one, because of course how the system behaves is strongly dependent on how tickets are allocated.

==这是一个棘手的问题，因为系统的行为当然在很大程度上取决于彩票是如何分配的。==

  

One approach is to assume that the users know best;

==一种方法是假设用户最清楚；==

  

in such a case, each user is handed some number of tickets, and a user can allocate tickets to any jobs they run as desired.

==在这种情况下，每个用户被分发一定数量的彩票，用户可以根据需要将彩票分配给他们运行的任何作业。==

  

However, this solution is a non-solution: it really doesn't tell you what to do.

==然而，这个解决方案等于没有解决问题：它实际上并没有告诉你该怎么做。==

  

Thus, given a set of jobs, the "ticket-assignment problem" remains open.

==因此，给定一组作业，“彩票分配问题”仍然悬而未决。==

  

9.6 Stride Scheduling

==9.6 步长调度==

  

You might also be wondering: why use randomness at all?

==你可能还会想：为什么要使用随机性呢？==

  

As we saw above, while randomness gets us a simple (and approximately correct) scheduler, it occasionally will not deliver the exact right proportions, especially over short time scales.

==正如我们在上面看到的，虽然随机性为我们要来了一个简单（且近似正确）的调度器，但它偶尔无法提供完全正确的比例，特别是在短时间范围内。==

  

For this reason, Waldspurger invented stride scheduling, a deterministic fair-share scheduler [W95].

==出于这个原因，Waldspurger 发明了步长调度（stride scheduling），这是一种确定性的公平份额调度器 [W95]。==

  

Stride scheduling is also straightforward.

==步长调度也很直观。==

  

Each job in the system has a stride, which is inverse in proportion to the number of tickets it has.

==系统中的每个作业都有一个步长（stride），其与它拥有的彩票数量成反比。==

  

In our example above, with jobs A, B, and C, with 100, 50, and 250 tickets, respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned.

==在上面的例子中，作业 A、B 和 C 分别拥有 100、50 和 250 张彩票，我们可以通过用一个大数除以每个进程被分配的彩票数量来计算每个进程的步长。==

  

For example, if we divide 10,000 by each of those ticket values, we obtain the following stride values for A, B, and C: 100, 200, and 40.

==例如，如果我们用 10,000 除以这些彩票值，我们会得到 A、B 和 C 的步长值分别为：100、200 和 40。==

  

We call this value the stride of each process;

==我们将此值称为每个进程的步长；==

  

every time a process runs, we will increment a counter for it (called its pass value) by its stride to track its global progress.

==每次进程运行时，我们会将其计数器（称为行程值，pass value）增加其步长，以跟踪其全局进度。==

  

The scheduler then uses the stride and pass to determine which process should run next.

==然后调度器使用步长和行程值来确定接下来应该运行哪个进程。==

  

The basic idea is simple: at any given time, pick the process to run that has the lowest pass value so far;

==基本思想很简单：在任何给定时间，选择目前行程值最低的进程运行；==

  

when you run a process, increment its pass counter by its stride.

==当你运行一个进程时，将其行程计数器增加其步长。==

  

Lottery scheduling achieves the proportions probabilistically over time;

==彩票调度随时间推移以概率方式实现比例；==

  

stride scheduling gets them exactly right at the end of each scheduling cycle.

==步长调度在每个调度周期结束时都能完全正确地实现比例。==

  

So you might be wondering: given the precision of stride scheduling, why use lottery scheduling at all?

==所以你可能会想：既然步长调度如此精确，为什么还要使用彩票调度呢？==

  

Well, lottery scheduling has one nice property that stride scheduling does not: no global state.

==嗯，彩票调度有一个步长调度所没有的良好属性：没有全局状态。==

  

Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be?

==想象一下，在上面的步长调度示例中途加入了一个新作业；它的行程值应该是多少？==

  

Should it be set to 0? If so, it will monopolize the CPU.

==应该设为 0 吗？如果是这样，它将独占 CPU。==

  

With lottery scheduling, there is no global state per process;

==使用彩票调度，每个进程没有全局状态；==

  

we simply add a new process with whatever tickets it has, update the single global variable to track how many total tickets we have, and go from there.

==我们只需添加一个带有其彩票的新进程，更新跟踪总彩票数量的单个全局变量，然后继续即可。==

  

In this way, lottery makes it much easier to incorporate new processes in a sensible manner.

==通过这种方式，彩票调度使得以合理的方式纳入新进程变得容易得多。==

  

9.7 The Linux Completely Fair Scheduler (CFS)

==9.7 Linux 完全公平调度器（CFS）==

  

Despite these earlier works in fair-share scheduling, the current Linux approach achieves similar goals in an alternate manner.

==尽管有这些早期的公平份额调度研究，当前的 Linux 方法以另一种方式实现了类似的目标。==

  

The scheduler, entitled the Completely Fair Scheduler (or CFS) [J09], implements fair-share scheduling, but does so in a highly efficient and scalable manner.

==该调度器名为完全公平调度器（或 CFS）[J09]，它实现了公平份额调度，但以一种高效且可扩展的方式进行。==

  

To achieve its efficiency goals, CFS aims to spend very little time making scheduling decisions, through both its inherent design and its clever use of data structures well-suited to the task.

==为了实现其效率目标，CFS 旨在通过其内在设计和巧妙使用适合该任务的数据结构，在制定调度决策上花费极少的时间。==

  

Recent studies have shown that scheduler efficiency is surprisingly important;

==最近的研究表明，调度器效率惊人地重要；==

  

specifically, in a study of Google datacenters, Kanev et al. show that even after aggressive optimization, scheduling uses about 5% of overall datacenter CPU time [K+15].

==具体来说，在对 Google 数据中心的一项研究中，Kanev 等人表明，即使经过积极优化，调度仍占用了整个数据中心约 5% 的 CPU 时间 [K+15]。==

  

Reducing that overhead as much as possible is thus a key goal in modern scheduler architecture.

==因此，尽可能减少这种开销是现代调度器架构的一个关键目标。==

  

Basic Operation

==基本操作==

  

Whereas most schedulers are based around the concept of a fixed time slice, CFS operates a bit differently.

==虽然大多数调度器都基于固定时间片的概念，但 CFS 的运作方式略有不同。==

  

Its goal is simple: to fairly divide a CPU evenly among all competing processes.

==它的目标很简单：在所有竞争进程之间公平地平均分配 CPU。==

  

It does so through a simple counting-based technique known as virtual runtime (vruntime).

==它通过一种简单的基于计数的被称为虚拟运行时间（vruntime）的技术来实现这一点。==

  

As each process runs, it accumulates vruntime.

==随着每个进程的运行，它会积累 vruntime。==

  

In the most basic case, each process's vruntime increases at the same rate, in proportion with physical (real) time.

==在最基本的情况下，每个进程的 vruntime 以相同的速率增加，与物理（实际）时间成正比。==

  

When a scheduling decision occurs, CFS will pick the process with the lowest vruntime to run next.

==当进行调度决策时，CFS 将选择具有最低 vruntime 的进程在接下来运行。==

  

This raises a question: how does the scheduler know when to stop the currently running process, and run the next one?

==这提出了一个问题：调度器如何知道何时停止当前运行的进程并运行下一个进程？==

  

The tension here is clear: if CFS switches too often, fairness is increased, as CFS will ensure that each process receives its share of CPU even over miniscule time windows, but at the cost of performance (too much context switching);

==这里的矛盾显而易见：如果 CFS 切换得太频繁，公平性会增加，因为 CFS 甚至在微小的时间窗口内也能确保每个进程获得其 CPU 份额，但这会以性能为代价（过多的上下文切换）；==

  

if CFS switches less often, performance is increased (reduced context switching), but at the cost of near-term fairness.

==如果 CFS 切换得不那么频繁，性能会提高（减少了上下文切换），但这会以短期公平性为代价。==

  

CFS manages this tension through various control parameters. The first is sched_latency.

==CFS 通过各种控制参数来管理这种矛盾。第一个是 sched_latency（调度延迟）。==

  

CFS uses this value to determine how long one process should run before considering a switch (effectively determining its time slice but in a dynamic fashion).

==CFS 使用此值来确定一个进程在考虑切换之前应运行多长时间（实际上是以动态方式确定其时间片）。==

  

A typical sched_latency value is 48 (milliseconds);

==典型的 sched_latency 值为 48（毫秒）；==

  

CFS divides this value by the number (n) of processes running on the CPU to determine the time slice for a process, and thus ensures that over this period of time, CFS will be completely fair.

==CFS 将此值除以 CPU 上运行的进程数 (n)，以确定进程的时间片，从而确保在这段时间内，CFS 将是完全公平的。==

  

For example, if there are n=4 processes running, CFS divides the value of sched_latency by n to arrive at a per-process time slice of 12 ms.

==例如，如果有 n=4 个进程在运行，CFS 将 sched_latency 的值除以 n，得出每个进程的时间片为 12 毫秒。==

  

CFS then schedules the first job and runs it until it has used 12 ms of (virtual) runtime, and then checks to see if there is a job with lower vruntime to run instead.

==然后 CFS 调度第一个作业并运行它，直到它使用了 12 毫秒的（虚拟）运行时间，然后检查是否有 vruntime 更低的作业可以运行。==

  

But what if there are "too many" processes running? Wouldn't that lead to too small of a time slice, and thus too many context switches?

==但是如果有“太多”进程在运行怎么办？那不会导致时间片太小，从而导致太多的上下文切换吗？==

  

Good question! And the answer is yes.

==好问题！答案是肯定的。==

  

To address this issue, CFS adds another parameter, min_granularity, which is usually set to a value like 6 ms.

==为了解决这个问题，CFS 添加了另一个参数 min_granularity（最小粒度），通常设置为 6 毫秒之类的值。==

  

CFS will never set the time slice of a process to less than this value, ensuring that not too much time is spent in scheduling overhead.

==CFS 绝不会将进程的时间片设置为小于此值，以确保不会在调度开销上花费太多时间。==

  

Weighting (Niceness)

==加权（Niceness）==

  

CFS also enables controls over process priority, enabling users or administrators to give some processes a higher share of the CPU.

==CFS 还启用了对进程优先级的控制，允许用户或管理员给予某些进程更高的 CPU 份额。==

  

It does this not with tickets, but through a classic UNIX mechanism known as the nice level of a process.

==它不是通过彩票，而是通过一种被称为进程 nice 值的经典 UNIX 机制来实现这一点的。==

  

The nice parameter can be set anywhere from -20 to +19 for a process, with a default of 0.

==进程的 nice 参数可以设置为 -20 到 +19 之间的任何值，默认为 0。==

  

Positive nice values imply lower priority and negative values imply higher priority;

==正的 nice 值意味着较低的优先级，负值意味着较高的优先级；==

  

when you're too nice, you just don't get as much (scheduling) attention, alas.

==当你太“nice”（友善/优先级低）时，你就得不到那么多（调度）关注，唉。==

  

Using Red-Black Trees

==使用红黑树==

  

One major focus of CFS is efficiency, as stated above.

==如前所述，CFS 的一个主要关注点是效率。==

  

For a scheduler, there are many facets of efficiency, but one of them is as simple as this: when the scheduler has to find the next job to run, it should do so as quickly as possible.

==对于调度器来说，效率有很多方面，但其中之一非常简单：当调度器必须找到下一个要运行的作业时，它应该尽可能快地完成。==

  

Simple data structures like lists don't scale: modern systems sometimes are comprised of 1000s of processes, and thus searching through a long-list every so many milliseconds is wasteful.

==简单的列表等数据结构无法扩展：现代系统有时由数千个进程组成，因此每隔几毫秒搜索一次长列表是浪费的。==

  

CFS addresses this by keeping processes in a red-black tree [B72].

==CFS 通过将进程保存在红黑树 [B72] 中来解决这个问题。==

  

A red-black tree is one of many types of balanced trees;

==红黑树是众多平衡树类型中的一种；==

  

in contrast to a simple binary tree (which can degenerate to list-like performance under worst-case insertion patterns), balanced trees do a little extra work to maintain low depths, and thus ensure that operations are logarithmic (and not linear) in time.

==与简单的二叉树（在最坏的插入模式下性能会退化为链表）相比，平衡树做了一些额外的工作来保持较低的深度，从而确保操作时间是对数的（而不是线性的）。==

  

CFS does not keep all processes in this structure; rather, only running (or runnable) processes are kept therein.

==CFS 并不将所有进程都保存在这个结构中；相反，其中只保留正在运行（或可运行）的进程。==

  

If a process goes to sleep (say, waiting on an I/O to complete, or for a network packet to arrive), it is removed from the tree and kept track of elsewhere.

==如果一个进程进入睡眠状态（例如，等待 I/O 完成，或等待网络数据包到达），它就会从树中移除并被跟踪在其他地方。==

  

Dealing With I/O And Sleeping Processes

==处理 I/O 和睡眠进程==

  

One problem with picking the lowest vruntime to run next arises with jobs that have gone to sleep for a long period of time.

==选择最低 vruntime 的作业运行会导致一个问题，这出现在长时间睡眠的作业上。==

  

Imagine two processes, A and B, one of which (A) runs continuously, and the other (B) which has gone to sleep for a long period of time (say, 10 seconds).

==想象两个进程 A 和 B，其中一个（A）连续运行，另一个（B）已经睡眠了很长一段时间（比如 10 秒）。==

  

When B wakes up, its vruntime will be 10 seconds behind A's, and thus (if we're not careful), B will now monopolize the CPU for the next 10 seconds while it catches up, effectively starving A.

==当 B 醒来时，它的 vruntime 将比 A 落后 10 秒，因此（如果我们不小心的话），B 将在接下来的 10 秒内独占 CPU 以追赶进度，实际上导致 A 饥饿。==

  

CFS handles this case by altering the vruntime of a job when it wakes up.

==CFS 通过在作业醒来时更改其 vruntime 来处理这种情况。==

  

Specifically, CFS sets the vruntime of that job to the minimum value found in the tree (remember, the tree only contains running jobs) [B+18].

==具体来说，CFS 将该作业的 vruntime 设置为树中找到的最小值（记住，树只包含正在运行的作业）[B+18]。==

  

In this way, CFS avoids starvation, but not without a cost: jobs that sleep for short periods of time frequently do not ever get their fair share of the CPU [AC97].

==通过这种方式，CFS 避免了饥饿，但并非没有代价：频繁短暂睡眠的作业永远无法获得其公平的 CPU 份额 [AC97]。==

  

9.8 Summary

==9.8 总结==

  

We have introduced the concept of proportional-share scheduling and briefly discussed three approaches: lottery scheduling, stride scheduling, and the Completely Fair Scheduler (CFS) of Linux.

==我们介绍了比例份额调度的概念，并简要讨论了三种方法：彩票调度、步长调度和 Linux 的完全公平调度器（CFS）。==

  

Lottery uses randomness in a clever way to achieve proportional share; stride does so deterministically.

==彩票调度巧妙地利用随机性来实现比例份额；步长调度则以确定性的方式实现。==

  

CFS, the only "real" scheduler discussed in this chapter, is a bit like weighted round-robin with dynamic time slices, but built to scale and perform well under load;

==CFS 是本章讨论的唯一“真实”调度器，它有点像带有动态时间片的加权轮转调度，但它是为在负载下扩展和良好运行而构建的；==

  

to our knowledge, it is the most widely used fair-share scheduler in existence today.

==据我们要知，它是当今存在的使用最广泛的公平份额调度器。==

  

No scheduler is a panacea, and fair-share schedulers have their fair share of problems.

==没有哪个调度器是万能药，公平份额调度器也有其自身的问题。==

  

Multiprocessor Scheduling (Advanced)

==多处理器调度（进阶）==

  

CRUX: HOW TO SCHEDULE JOBS ON MULTIPLE CPUS

==关键问题：如何在多个 CPU 上调度作业==

  

How should the OS schedule jobs on multiple CPUs?

==操作系统应该如何在多个 CPU 上调度作业？==

  

What new problems arise? Do the same old techniques work, or are new ideas required?

==会出现什么新问题？相同的旧技术还管用吗，还是需要新思路？==

  

10.1 Background: Multiprocessor Architecture

==10.1 背景：多处理器架构==

  

To understand the new issues surrounding multiprocessor scheduling, we have to understand a new and fundamental difference between single-CPU hardware and multi-CPU hardware.

==要理解围绕多处理器调度的新问题，我们必须理解单 CPU 硬件和多 CPU 硬件之间的一个新的根本差异。==

  

This difference centers around the use of hardware caches (e.g., Figure 10.1), and exactly how data is shared across multiple processors.

==这种差异集中在硬件缓存的使用（例如图 10.1），以及数据究竟如何在多个处理器之间共享。==

  

In a system with a single CPU, there are a hierarchy of hardware caches that in general help the processor run programs faster.

==在单 CPU 系统中，存在硬件缓存层次结构，通常有助于处理器更快地运行程序。==

  

Caches are small, fast memories that (in general) hold copies of popular data that is found in the main memory of the system.

==缓存是小而快的存储器，（通常）保存着系统主内存中热门数据的副本。==

  

Main memory, in contrast, holds all of the data, but access to this larger memory is slower.

==相比之下，主内存保存所有数据，但访问这个更大的内存速度较慢。==

  

By keeping frequently accessed data in a cache, the system can make the large, slow memory appear to be a fast one.

==通过将频繁访问的数据保存在缓存中，系统可以使大而慢的内存看起来像快内存一样。==

  

Now for the tricky part: what happens when you have multiple processors in a single system, with a single shared main memory, as we see in Figure 10.2?

==现在的棘手部分是：当你如我们在图 10.2 中看到的那样，在一个系统中拥有多个处理器，并共享一个主内存时，会发生什么？==

  

As it turns out, caching with multiple CPUs is much more complicated.

==事实证明，多 CPU 缓存要复杂得多。==

  

Imagine, for example, that a program running on CPU 1 reads a data item (with value D) at address A;

==例如，想象一个在 CPU 1 上运行的程序读取地址 A 处的数据项（值为 D）；==

  

because the data is not in the cache on CPU 1, the system fetches it from main memory, and gets the value D.

==因为数据不在 CPU 1 的缓存中，系统从主内存中获取它，并得到值 D。==

  

The program then modifies the value at address A, just updating its cache with the new value D';

==程序然后修改地址 A 处的值，仅用新值 D' 更新其缓存；==

  

writing the data through all the way to main memory is slow, so the system will (usually) do that later.

==将数据一直写回主内存很慢，所以系统（通常）会在稍后进行。==

  

Then assume the OS decides to stop running the program and move it to CPU 2.

==然后假设操作系统决定停止运行该程序并将其移动到 CPU 2。==

  

The program then re-reads the value at address A;

==程序随后重新读取地址 A 处的值；==

  

there is no such data in CPU 2's cache, and thus the system fetches the value from main memory, and gets the old value D instead of the correct value D'.

==CPU 2 的缓存中没有此类数据，因此系统从主内存中获取该值，得到的是旧值 D 而不是正确的值 D'。==

  

Oops!

==哎呀！==

  

This general problem is called the problem of cache coherence, and there is a vast research literature that describes many different subtleties involved with solving the problem [SHW11].

==这个普遍存在的问题被称为缓存一致性（cache coherence）问题，有大量的研究文献描述了解决该问题所涉及的许多不同细节 [SHW11]。==

  

The basic solution is provided by the hardware: by monitoring memory accesses, hardware can ensure that basically the "right thing" happens and that the view of a single shared memory is preserved.

==基本解决方案由硬件提供：通过监控内存访问，硬件可以确保基本发生“正确的事情”，并保留单个共享内存的视图。==

  

One way to do this on a bus-based system (as described above) is to use an old technique known as bus snooping [G83];

==在基于总线的系统（如上所述）中，做到这一点的一种方法是使用一种称为总线窥探（bus snooping）的旧技术 [G83]；==

  

each cache pays attention to memory updates by observing the bus that connects them to main memory.

==每个缓存通过观察连接它们到主内存的总线来关注内存更新。==

  

When a CPU then sees an update for a data item it holds in its cache, it will notice the change and either invalidate its copy (i.e., remove it from its own cache) or update it (i.e., put the new value into its cache too).

==当 CPU 看到它在其缓存中持有的数据项有更新时，它会注意到这种变化，要么使其副本无效（即，从其自己的缓存中移除它），要么更新它（即，将新值也放入其缓存中）。==

  

10.2 Don't Forget Synchronization

==10.2 别忘了同步==

  

Given that the caches do all of this work to provide coherence, do programs (or the OS itself) have to worry about anything when they access shared data?

==既然缓存做了所有这些工作来提供一致性，程序（或操作系统本身）在访问共享数据时还需要担心什么吗？==

  

The answer, unfortunately, is yes, and is documented in great detail in the second piece of this book on the topic of concurrency.

==不幸的是，答案是肯定的，这在本书关于并发主题的第二部分中有非常详细的记录。==

  

When accessing (and in particular, updating) shared data items or structures across CPUs, mutual exclusion primitives (such as locks) should likely be used to guarantee correctness.

==当跨 CPU 访问（特别是更新）共享数据项或结构时，可能应该使用互斥原语（如锁）来保证正确性。==

  

Without locks, adding or removing elements from the queue concurrently will not work as expected, even with the underlying coherence protocols;

==如果没有锁，即使有底层的相关性协议，并发地从队列中添加或删除元素也无法按预期工作；==

  

one needs locks to atomically update the data structure to its new state.

==需要锁来原子地将数据结构更新到其新状态。==

  

10.3 One Final Issue: Cache Affinity

==10.3 最后一个问题：缓存亲和性==

  

One final issue arises in building a multiprocessor cache scheduler, known as cache affinity [TTG95].

==在构建多处理器缓存调度器时出现了最后一个问题，称为缓存亲和性（cache affinity）[TTG95]。==

  

This notion is simple: a process, when run on a particular CPU, builds up a fair bit of state in the caches (and TLBs) of the CPU.

==这个概念很简单：一个进程在特定 CPU 上运行时，会在该 CPU 的缓存（和 TLB）中建立相当多的状态。==

  

The next time the process runs, it is often advantageous to run it on the same CPU, as it will run faster if some of its state is already present in the caches on that CPU.

==下次该进程运行时，在同一个 CPU 上运行通常是有利的，因为如果它的某些状态已经存在于该 CPU 的缓存中，它将运行得更快。==

  

Thus, a multiprocessor scheduler should consider cache affinity when making its scheduling decisions, perhaps preferring to keep a process on the same CPU if at all possible.

==因此，多处理器调度器在做出调度决策时应考虑缓存亲和性，如果可能的话，也许倾向于将进程保持在同一个 CPU 上。==

  

10.4 Single-Queue Scheduling

==10.4 单队列调度==

  

The most basic approach is to simply reuse the basic framework for single processor scheduling, by putting all jobs that need to be scheduled into a single queue;

==最基本的方法是简单地重用单处理器调度的基本框架，将所有需要调度的作业放入一个队列中；==

  

we call this single-queue multiprocessor scheduling or SQMS for short.

==我们称之为单队列多处理器调度，简称 SQMS。==

  

However, SQMS has obvious shortcomings. The first problem is a lack of scalability.

==然而，SQMS 有明显的缺点。第一个问题是缺乏可扩展性。==

  

To ensure the scheduler works correctly on multiple CPUs, the developers will have inserted some form of locking into the code, as described above.

==为了确保调度器在多个 CPU 上正确工作，开发人员将在代码中插入某种形式的锁，如上所述。==

  

Locks ensure that when SQMS code accesses the single queue (say, to find the next job to run), the proper outcome arises.

==锁确保当 SQMS 代码访问单队列（例如，查找下一个要运行的作业）时，会产生正确的结果。==

  

Locks, unfortunately, can greatly reduce performance, particularly as the number of CPUs in the systems grows [A90].

==不幸的是，锁会大大降低性能，特别是随着系统中 CPU 数量的增加 [A90]。==

  

The second main problem with SQMS is cache affinity.

==SQMS 的第二个主要问题是缓存亲和性。==

  

Because each CPU simply picks the next job to run from the globally-shared queue, each job ends up bouncing around from CPU to CPU, thus doing exactly the opposite of what would make sense from the standpoint of cache affinity.

==因为每个 CPU 只是从全局共享队列中选取下一个作业来运行，每个作业最终会在 CPU 之间跳来跳去，这恰恰与从缓存亲和性角度来看合理的做法背道而驰。==

  

To handle this problem, most SQMS schedulers include some kind of affinity mechanism to try to make it more likely that process will continue to run on the same CPU if possible.

==为了处理这个问题，大多数 SQMS 调度器都包含某种亲和性机制，试图使进程尽可能继续在同一个 CPU 上运行。==

  

10.5 Multi-Queue Scheduling

==10.5 多队列调度==

  

Because of the problems caused in single-queue schedulers, some systems opt for multiple queues, e.g., one per CPU.

==由于单队列调度器引起的问题，一些系统选择多队列，例如每个 CPU 一个队列。==

  

We call this approach multi-queue multiprocessor scheduling (or MQMS).

==我们将这种方法称为多队列多处理器调度（或 MQMS）。==

  

In MQMS, our basic scheduling framework consists of multiple scheduling queues.

==在 MQMS 中，我们的基本调度框架由多个调度队列组成。==

  

Each queue will likely follow a particular scheduling discipline, such as round robin, though of course any algorithm can be used.

==每个队列可能会遵循特定的调度规则，例如轮转调度，当然也可以使用任何算法。==

  

When a job enters the system, it is placed on exactly one scheduling queue, according to some heuristic (e.g., random, or picking one with fewer jobs than others).

==当一个作业进入系统时，根据某种启发式算法（例如随机，或选择作业较少的队列），它被放置在且仅放置在一个调度队列中。==

  

Then it is scheduled essentially independently, thus avoiding the problems of information sharing and synchronization found in the single-queue approach.

==然后它基本上是独立调度的，从而避免了单队列方法中发现的信息共享和同步问题。==

  

MQMS has a distinct advantage of SQMS in that it should be inherently more scalable.

==MQMS 相比 SQMS 有一个明显的优势，即它本质上应该更具可扩展性。==

  

As the number of CPUs grows, so too does the number of queues, and thus lock and cache contention should not become a central problem.

==随着 CPU 数量的增加，队列数量也会增加，因此锁和缓存争用不应成为核心问题。==

  

In addition, MQMS intrinsically provides cache affinity;

==此外，MQMS 本质上提供了缓存亲和性；==

  

jobs stay on the same CPU and thus reap the advantage of reusing cached contents therein.

==作业停留在同一个 CPU 上，从而获得重用其中缓存内容的好处。==

  

But, if you've been paying attention, you might see that we have a new problem, which is fundamental in the multi-queue based approach: load imbalance.

==但是，如果你一直在关注，你可能会发现我们有一个新问题，这是多队列方法中的根本问题：负载不平衡。==

  

If we then run our round-robin policy on each queue of the system, we will see this resulting schedule:

==如果我们随后在系统的每个队列上运行我们的轮转策略，我们将看到这样的结果调度：==

  

As you can see from this diagram, A gets twice as much CPU as B and D, which is not the desired outcome.

==从图中可以看出，A 获得的 CPU 资源是 B 和 D 的两倍，这不是预期的结果。==

  

How terrible - CPU 0 is idle! (insert dramatic and sinister music here)

==太糟糕了——CPU 0 是空闲的！（此处插入戏剧性和阴险的音乐）==

MULTIPROCESSOR SCHEDULING (ADVANCED)
==多处理器调度（高级）==

So what should a poor multi-queue multiprocessor scheduler do?
==那么，一个可怜的多队列多处理器调度程序应该怎么做呢？==

How can we overcome the insidious problem of load imbalance and defeat the evil forces of ... the Decepticons?
==我们如何才能克服负载不均这个潜在的问题，并击败……霸天虎的邪恶势力呢？==

How do we stop asking questions that are hardly relevant to this otherwise wonderful book?
==我们怎样才能停止问那些与这本精彩书籍毫无关系的无关紧要的问题呢？==

CRUX: HOW TO DEAL WITH LOAD IMBALANCE
==关键问题：如何处理负载不均==

How should a multi-queue multiprocessor scheduler handle load imbalance, so as to better achieve its desired scheduling goals?
==多队列多处理器调度程序应该如何处理负载不均，以便更好地实现其预期的调度目标？==

The obvious answer to this query is to move jobs around, a technique which we (once again) refer to as migration.
==这个问题的显而易见答案是移动工作，我们（再次）将这种技术称为迁移。==

By migrating a job from one CPU to another, true load balance can be achieved.
==通过将工作从一个 CPU 迁移到另一个 CPU，可以实现真正的负载均衡。==

Let's look at a couple of examples to add some clarity.
==让我们看几个例子来增加清晰度。==

Once again, we have a situation where one CPU is idle and the other has some jobs.
==再一次，我们面临的情况是：一个 CPU 空闲，而另一个 CPU 有一些工作。==

In this case, the desired migration is easy to understand: the OS should simply move one of B or D to CPU 0.
==在这种情况下，所需的迁移很容易理解：操作系统应该简单地将 B 或 D 中的一个移动到 CPU 0。==

The result of this single job migration is evenly balanced load and everyone is happy.
==这次单一工作迁移的结果是负载均匀平衡，皆大欢喜。==

A more tricky case arises in our earlier example, where A was left alone on CPU 0 and B and D were alternating on CPU 1.
==在我们之前的例子中出现了一个更棘手的情况，其中 A 独自留在 CPU 0 上，而 B 和 D 在 CPU 1 上交替运行。==

In this case, a single migration does not solve the problem.
==在这种情况下，单一的迁移无法解决问题。==

What would you do in this case?
==在这种情况下你会怎么做？==

The answer, alas, is continuous migration of one or more jobs.
==遗憾的是，答案是一个或多个工作的持续迁移。==

One possible solution is to keep switching jobs, as we see in the following timeline.
==一种可能的解决方案是不断切换工作，正如我们在以下时间线中看到的那样。==

In the figure, first A is alone on CPU 0, and B and D alternate on CPU 1.
==在图中，首先 A 独自在 CPU 0 上，B 和 D 在 CPU 1 上交替。==

After a few time slices, B is moved to compete with A on CPU 0, while D enjoys a few time slices alone on CPU 1.
==经过几个时间片后，B 被移至 CPU 0 与 A 竞争，而 D 则在 CPU 1 上独自享受几个时间片。==

And thus load is balanced.
==因此，负载得到了平衡。==

Of course, many other possible migration patterns exist.
==当然，还存在许多其他可能的迁移模式。==

But now for the tricky part: how should the system decide to enact such a migration?
==但现在的棘手部分是：系统应该如何决定实施这种迁移？==

One basic approach is to use a technique known as work stealing.
==一种基本的方法是使用一种称为工作窃取的技术。==

With a work-stealing approach, a (source) queue that is low on jobs will occasionally peek at another (target) queue, to see how full it is.
==采用工作窃取方法时，作业较少的（源）队列会偶尔查看另一个（目标）队列，看看它有多满。==

If the target queue is (notably) more full than the source queue, the source will "steal" one or more jobs from the target to help balance load.
==如果目标队列（明显）比源队列更满，源队列将从目标队列“窃取”一个或多个作业以帮助平衡负载。==

Of course, there is a natural tension in such an approach.
==当然，这种方法存在一种天然的张力。==

If you look around at other queues too often, you will suffer from high overhead and have trouble scaling, which was the entire purpose of implementing the multiple queue scheduling in the first place!
==如果你太频繁地查看其他队列，你将遭受高开销并难以扩展，而扩展恰恰是最初实施多队列调度的全部目的！==

If, on the other hand, you don't look at other queues very often, you are in danger of suffering from severe load imbalances.
==另一方面，如果你不常查看其他队列，你就有可能遭受严重的负载不均。==

Finding the right threshold remains, as is common in system policy design, a black art.
==找到合适的阈值仍然是一门“黑魔法”，这在系统策略设计中很常见。==

Linux Multiprocessor Schedulers
==Linux 多处理器调度程序==

Interestingly, in the Linux community, no common solution has emerged to building a multiprocessor scheduler.
==有趣的是，在 Linux 社区中，对于构建多处理器调度程序并没有出现通用的解决方案。==

Over time, three different schedulers arose: the  scheduler, the Completely Fair Scheduler (CFS), and the BF Scheduler .
==随着时间的推移，出现了三种不同的调度程序： 调度程序、完全公平调度程序 (CFS) 和 BF 调度程序 。==

See Meehean's dissertation for an excellent overview of the strengths and weaknesses of said schedulers; here we just summarize a few of the basics.
==请参阅 Meehean 的论文，了解上述调度程序优缺点的精彩概述；在这里我们只总结一些基础知识。==

Both  and CFS use multiple queues, whereas BFS uses a single queue, showing that both approaches can be successful.
== 和 CFS 都使用多队列，而 BFS 使用单队列，这表明这两种方法都可以成功。==

Of course, there are many other details which separate these schedulers.
==当然，还有许多其他细节区分了这些调度程序。==

For example, the  scheduler is a priority-based scheduler (similar to the MLFQ discussed before), changing a process's priority over time and then scheduling those with highest priority in order to meet various scheduling objectives.
==例如， 调度程序是一种基于优先级的调度程序（类似于之前讨论的 MLFQ），它随时间改变进程的优先级，然后调度优先级最高的进程，以满足各种调度目标。==

Interactivity is a particular focus.
==交互性是一个特别的关注点。==

CFS, in contrast, is a deterministic proportional-share approach (more like Stride scheduling, as discussed earlier).
==相比之下，CFS 是一种确定性的比例共享方法（更像之前讨论的 Stride 调度）。==

BFS, the only single-queue approach among the three, is also proportional-share, but based on a more complicated scheme known as Earliest Eligible Virtual Deadline First (EEVDF).
==BFS 是这三者中唯一的单队列方法，也是比例共享的，但基于一种更复杂的方案，称为最早合格虚拟截止时间优先 (EEVDF)。==

Read more about these modern algorithms on your own; you should be able to understand how they work now!
==请自行阅读更多关于这些现代算法的内容；你现在应该能够理解它们是如何工作的了！==

Summary
==总结==

We have seen various approaches to multiprocessor scheduling.
==我们要看到了多处理器调度的各种方法。==

The single-queue approach (SQMS) is rather straightforward to build and balances load well but inherently has difficulty with scaling to many processors and cache affinity.
==单队列方法 (SQMS) 构建起来相当简单，并且能很好地平衡负载，但在扩展到许多处理器和缓存亲和度方面天生存在困难。==

The multiple-queue approach (MQMS) scales better and handles cache affinity well, but has trouble with load imbalance and is more complicated.
==多队列方法 (MQMS) 扩展性更好，并且能很好地处理缓存亲和度，但在负载不均方面存在问题，且更加复杂。==

Whichever approach you take, there is no simple answer: building a general purpose scheduler remains a daunting task, as small code changes can lead to large behavioral differences.
==无论你采取哪种方法，都没有简单的答案：构建通用调度程序仍然是一项艰巨的任务，因为微小的代码更改可能导致巨大的行为差异。==

Only undertake such an exercise if you know exactly what you are doing, or, at least, are getting paid a large amount of money to do so.
==只有在你确切知道自己在做什么，或者至少为此获得大量报酬时，才进行这样的练习。==

Homework (Simulation)
==作业（模拟）==

In this homework, we'll use multi.py to simulate a multi-processor CPU scheduler, and learn about some of its details.
==在这个作业中，我们将使用 multi.py 来模拟多处理器 CPU 调度程序，并了解其一些细节。==

Read the related README for more information about the simulator and its options.
==请阅读相关的 README 以获取有关模拟器及其选项的更多信息。==

Questions
==问题==

1. To start things off, let's learn how to use the simulator to study how to build an effective multi-processor scheduler.
==2. 首先，让我们学习如何使用模拟器来研究如何构建有效的多处理器调度程序。==

The first simulation will run just one job, which has a run-time of 30, and a working-set size of 200.
==第一个模拟将只运行一个作业，其运行时间为 30，工作集大小为 200。==

Run this job (called job 'a' here) on one simulated CPU as follows: ./multi.py -n 1 La:30:200.
==在一个模拟 CPU 上运行此作业（此处称为作业 'a'），如下所示：./multi.py -n 1 La:30:200。==

How long will it take to complete?
==完成需要多长时间？==

Turn on the -c flag to see a final answer, and the -t flag to see a tick-by-tick trace of the job and how it is scheduled.
==打开 -c 标志以查看最终答案，打开 -t 标志以查看作业的逐个时钟周期的跟踪及其调度方式。==

2. Now increase the cache size so as to make the job's working set  fit into the cache (which, by default, is size=100); for example, run /multi.py -n 1 La:30:200 M 300.
==3. 现在增加缓存大小，使作业的工作集  适合缓存（默认大小为 100）；例如，运行 /multi.py -n 1 La:30:200 M 300。==

Can you predict how fast the job will run once it fits in cache?
==你能预测一旦作业放入缓存后运行速度有多快吗？==

(hint: remember the key parameter of the warm rate, which is set by the -r flag) Check your answer by running with the solve flag (-c) enabled.
==（提示：记住热速率的关键参数，由 -r 标志设置）通过启用解决标志 (-c) 来检查你的答案。==

3. One cool thing about multi.py is that you can see more detail about what is going on with different tracing flags.
==4. 关于 multi.py 的一个很酷的事情是，你可以通过不同的跟踪标志看到更多关于正在发生的事情的细节。==

Run the same simulation as above, but this time with time left tracing enabled (-T).
==运行与上面相同的模拟，但这次启用剩余时间跟踪 (-T)。==

This flag shows both the job that was scheduled on a CPU at each time step, as well as how much run-time that job has left after each tick has run.
==此标志不仅显示每个时间步长在 CPU 上调度的作业，还显示每个时钟周期运行后该作业剩余的运行时间。==

What do you notice about how that second column decreases?
==你注意到第二列是如何减少的吗？==

4. Now add one more bit of tracing, to show the status of each CPU cache for each job, with the C flag.
==5. 现在添加更多一点的跟踪，使用 C 标志显示每个作业的每个 CPU 缓存的状态。==

For each job, each cache will either show a blank space (if the cache is cold for that job) or a 'w' (if the cache is warm for that job).
==对于每个作业，每个缓存将显示空白（如果该作业的缓存是冷的）或 'w'（如果该作业的缓存是热的）。==

At what point does the cache become warm for job 'a' in this simple example?
==在这个简单的例子中，作业 'a' 的缓存在什么时间点变热？==

What happens as you change the warmup time parameter (-w) to lower or higher values than the default?
==当你将预热时间参数 (-w) 更改为低于或高于默认值时，会发生什么？==

5. At this point, you should have a good idea of how the simulator works for a single job running on a single CPU.
==6. 至此，你应该对模拟器如何在单个 CPU 上运行单个作业有了一个很好的了解。==

But hey, isn't this a multi-processor CPU scheduling chapter?
==但是，嘿，这不是多处理器 CPU 调度章节吗？==

Oh yeah! So let's start working with multiple jobs.
==哦，是的！所以让我们开始处理多个作业。==

Specifically, let's run the following three jobs on a two-CPU system (i.e., type /multi.py -n 2 -La:100:100, 100:100,b:100:50,c:100:50).
==具体来说，让我们在双 CPU 系统上运行以下三个作业（即输入 /multi.py -n 2 -La:100:100, 100:100,b:100:50,c:100:50）。==

Can you predict how long this will take, given a round-robin centralized scheduler?
==给定一个轮询集中式调度程序，你能预测这需要多长时间吗？==

Use -c to see if you were right, and then dive down into details with -t to see a step-by-step and then -C to see whether caches got warmed effectively for these jobs.
==使用 -c 查看你是否正确，然后使用 -t 深入了解细节以查看逐步过程，再使用 -C 查看这些作业的缓存是否得到了有效预热。==

What do you notice?
==你注意到了什么？==

6. Now we'll apply some explicit controls to study cache affinity, as described in the chapter.
==7. 现在我们将应用一些显式控制来研究缓存亲和度，正如本章所述。==

To do this, you'll need the A flag.
==为此，你需要 A 标志。==

This flag can be used to limit which CPUs the scheduler can place a particular job upon.
==此标志可用于限制调度程序可以将特定作业放置在哪些 CPU 上。==

In this case, let's use it to place jobs 'b' and 'c' on CPU 1, while restricting 'a' to CPU 0.
==在这种情况下，让我们使用它将作业 'b' 和 'c' 放置在 CPU 1 上，同时将 'a' 限制在 CPU 0 上。==

This magic is accomplished by typing this: ./multi.py -n 2 La:100:100,b:100:50, c:100:50 -A a:0,b:1,c:1;
==这个魔法是通过输入以下内容实现的：./multi.py -n 2 La:100:100,b:100:50, c:100:50 -A a:0,b:1,c:1;==

Don't forget to turn on various tracing options to see what is really happening!
==不要忘记打开各种跟踪选项以查看实际发生的情况！==

Can you predict how fast this version will run?
==你能预测这个版本的运行速度有多快吗？==

Why does it do better?
==为什么它表现得更好？==

Will other combinations of 'a', 'b', and 'c' onto the two processors run faster or slower?
==将 'a'、'b' 和 'c' 组合到两个处理器上的其他方式会运行得更快还是更慢？==

7. One interesting aspect of caching multiprocessors is the opportunity for better-than-expected speed up of jobs when using multiple CPUs (and their caches) as compared to running jobs on a single processor.
==8. 缓存多处理器的一个有趣方面是，与在单个处理器上运行作业相比，使用多个 CPU（及其缓存）时，作业加速可能会好于预期。==

Specifically, when you run on N CPUs, sometimes you can speed up by more than a factor of N, a situation entitled super-linear speedup.
==具体来说，当你在 N 个 CPU 上运行时，有时加速甚至可能超过 N 倍，这种情况被称为超线性加速。==

To experiment with this, use the job description here (-La:100:100,b:100:100,c: 100:100) with a small cache (-M 50) to create three jobs.
==为了对此进行实验，请使用此处的工作描述 (-La:100:100,b:100:100,c: 100:100) 和一个小缓存 (-M 50) 来创建三个作业。==

Run this on systems with 1, 2, and 3 CPUs (-n 1, n 2, n 3).
==在具有 1、2 和 3 个 CPU 的系统上运行此程序（-n 1, n 2, n 3）。==

Now, do the same, but with a larger per-CPU cache of size 100.
==现在，做同样的事情，但使用更大的每个 CPU 缓存，大小为 100。==

What do you notice about performance as the number of CPUs scales?
==随着 CPU 数量的扩展，你注意到性能有什么变化？==

Use c to confirm your guesses, and other tracing flags to dive even deeper.
==使用 c 确认你的猜测，并使用其他跟踪标志进行更深入的研究。==

8. One other aspect of the simulator worth studying is the per-CPU scheduling option, the -p flag.
==9. 模拟器值得研究的另一个方面是每个 CPU 调度选项，即 -p 标志。==

Run with two CPUs again, and this three job configuration (-La: 100:100,b:100:50,c:100:50).
==再次使用两个 CPU 和这个三作业配置 (-La: 100:100,b:100:50,c:100:50) 运行。==

How does this option do, as opposed to the hand-controlled affinity limits you put in place above?
==与你在上面设置的手动控制亲和度限制相比，这个选项表现如何？==

How does performance change as you alter the 'peek interval' (-P) to lower or higher values?
==当你将“窥视间隔”(-P) 更改为更低或更高的值时，性能会发生什么变化？==

How does this per-CPU approach work as the number of CPUs scales?
==随着 CPU 数量的扩展，这种每个 CPU 的方法如何工作？==

9. Finally, feel free to just generate random workloads and see if you can predict their performance on different numbers of processors, cache sizes, and scheduling options.
==10. 最后，随意生成随机工作负载，看看你是否能预测它们在不同数量的处理器、缓存大小和调度选项下的性能。==

If you do this, you'll soon be a multi-processor scheduling master, which is a pretty awesome thing to be.
==如果你这样做，你很快就会成为多处理器调度大师，这可是一件非常了不起的事情。==

Good luck!
==祝你好运！==

Summary Dialogue on CPU Virtualization
==关于 CPU 虚拟化的总结对话==

Professor: So, Student, did you learn anything?
==教授：那么，学生，你学到了什么吗？==

Student: Well, Professor, that seems like a loaded question.
==学生：嗯，教授，这似乎是一个话里有话的问题。==

I think you only want me to say "yes."
==我觉得你只想让我说“是”。==

Professor: That's true. But it's also still an honest question.
==教授：确实如此。但这依然是一个诚实的问题。==

Come on, give a professor a break, will you?
==拜托，给教授留点面子，好吗？==

Student: OK, OK. I think I did learn a few things.
==学生：好的，好的。我想我确实学到了一些东西。==

First, I learned a little about how the OS virtualizes the CPU.
==首先，我学到了一点关于操作系统如何虚拟化 CPU 的知识。==

There are a bunch of important mechanisms that I had to understand to make sense of this: traps and trap handlers, timer interrupts, and how the OS and the hardware have to carefully save and restore state when switching between processes.
==为了理解这一点，我必须理解一堆重要的机制：陷阱和陷阱处理程序、时钟中断，以及操作系统和硬件在进程切换时如何小心地保存和恢复状态。==

Professor: Good, good!
==教授：很好，很好！==

Student: All those interactions do seem a little complicated though; how can I learn more?
==学生：不过所有这些交互看起来确实有点复杂；我怎样才能学到更多呢？==

Professor: Well, that's a good question. I think there is no substitute for doing; just reading about these things doesn't quite give you the proper sense.
==教授：嗯，这是一个好问题。我认为没有什么可以替代实践；仅仅阅读这些东西并不能完全让你获得正确的感觉。==

Do the class projects and I bet by the end it will all kind of make sense.
==做课程项目，我打赌到最后一切都会变得豁然开朗。==

Student: Sounds good. What else can I tell you?
==学生：听起来不错。我还能告诉你什么？==

Professor: Well, did you get some sense of the philosophy of the OS in your quest to understand its basic machinery?
==教授：嗯，在你探索操作系统基本机制的过程中，你是否对操作系统的哲学有了一些感悟？==

Student: Hmm... I think so. It seems like the OS is fairly paranoid.
==学生：嗯……我想是的。看起来操作系统相当偏执。==

It wants to make sure it stays in charge of the machine.
==它想确保自己始终掌管机器。==

While it wants a program to run as efficiently as possible (and hence the whole reasoning behind limited direct execution), the OS also wants to be able to say "Ah! Not so fast my friend" in case of an errant or malicious process.
==虽然它希望程序尽可能高效地运行（这就是受限直接执行背后的全部理由），但在遇到错误或恶意进程时，操作系统也希望能够说“啊！别那么快，朋友”。==

Paranoia rules the day, and certainly keeps the OS in charge of the machine.
==偏执统治着一切，当然也让操作系统掌控着机器。==

Perhaps that is why we think of the OS as a resource manager.
==也许这就是为什么我们将操作系统视为资源管理器的原因。==

Professor: Yes indeed - sounds like you are starting to put it together! Nice.
==教授：确实如此——听起来你已经开始融会贯通了！不错。==

Professor: And what about the policies on top of those mechanisms - any interesting lessons there?
==教授：那么在这些机制之上的策略呢——有什么有趣的教训吗？==

Student: Some lessons to be learned there for sure. Perhaps a little obvious, but obvious can be good.
==学生：肯定有一些教训。也许有点显而易见，但显而易见有时是件好事。==

Like the notion of bumping short jobs to the front of the queue - I knew that was a good idea ever since the one time I was buying some gum at the store, and the guy in front of me had a credit card that wouldn't work.
==比如把短作业排到队列前面的概念——自从有一次我在商店买口香糖，而排在我前面的那个人信用卡刷不过去时，我就知道这是个好主意。==

He was no short job, let me tell you.
==告诉你吧，他可不是个短作业。==

Professor: That sounds oddly rude to that poor fellow. What else?
==教授：这对那个可怜的家伙来说听起来有点粗鲁。还有什么？==

Student: Well, that you can build a smart scheduler that tries to be like SJF and RR all at once - that MLFQ was pretty neat.
==学生：嗯，你可以构建一个智能调度程序，试图同时像 SJF 和 RR 一样——那个 MLFQ 相当巧妙。==

Building up a real scheduler seems difficult.
==构建一个真正的调度程序似乎很难。==

Professor: Indeed it is. That's why there is still controversy to this day over which scheduler to use; see the Linux battles between CFS, BFS, and the  scheduler, for example.
==教授：确实如此。这就是为什么直到今天关于使用哪种调度程序仍有争议；例如，看看 Linux 中 CFS、BFS 和  调度程序之间的争斗。==

And no, I will not spell out the full name of BFS.
==而且不，我不会拼出 BFS 的全名。==

Student: And I won't ask you to!
==学生：我也不会让你拼的！==

These policy battles seem like they could rage forever; is there really a right answer?
==这些策略之争似乎会永远持续下去；真的有正确答案吗？==

Professor: Probably not.
==教授：可能没有。==

After all, even our own metrics are at odds: if your scheduler is good at turnaround time, it's bad at response time, and vice versa.
==毕竟，即使是我们自己的指标也是相互矛盾的：如果你的调度程序擅长周转时间，它在响应时间上就会很差，反之亦然。==

As Lampson said, perhaps the goal isn't to find the best solution, but rather to avoid disaster.
==正如 Lampson 所说，也许目标不是找到最好的解决方案，而是避免灾难。==

Student: That's a little depressing.
==学生：这有点令人沮丧。==

Professor: Good engineering can be that way.
==教授：优秀的工程学可能就是这样。==

And it can also be uplifting!
==它也可以是令人振奋的！==

It's just your perspective on it, really.
==这真的只是你的看法问题。==

I personally think being pragmatic is a good thing, and pragmatists realize that not all problems have clean and easy solutions.
==我个人认为务实是一件好事，而务实主义者意识到并非所有问题都有干净简单的解决方案。==

Anything else that caught your fancy?
==还有什么吸引你的吗？==

Student: I really liked the notion of gaming the scheduler; it seems like that might be something to look into when I'm next running a job on Amazon's EC2 service.
==学生：我真的很喜欢“玩弄”调度程序的概念；下次我在亚马逊的 EC2 服务上运行作业时，这似乎值得研究一下。==

Maybe I can steal some cycles from some other unsuspecting (and more importantly, OS-ignorant) customer!
==也许我可以从其他毫无戒心（更重要的是，对操作系统一无所知）的客户那里窃取一些周期！==

Professor: It looks like I might have created a monster!
==教授：看起来我可能创造了一个怪物！==

Professor Frankenstein is not what I'd like to be called, you know.
==你知道，我不想被称为弗兰肯斯坦教授。==

Student: But isn't that the idea? To get us excited about something, so much so that we look into it on our own?
==学生：但这不就是目的吗？让我们对某事感到兴奋，以至于我们会自己去研究它？==

Lighting fires and all that?
==点燃火种之类的？==

Professor: I guess so. But I didn't think it would work!
==教授：我想是的。但我没想到这真的会奏效！==

A Dialogue on Memory Virtualization
==关于内存虚拟化的对话==

Student: So, are we done with virtualization?
==学生：那么，我们结束关于虚拟化的内容了吗？==

Professor: No!
==教授：不！==

Student: Hey, no reason to get so excited; I was just asking a question.
==学生：嘿，没必要这么激动；我只是问个问题。==

Students are supposed to do that, right?
==学生应该这样做，对吧？==

Professor: Well, professors do always say that, but really they mean this: ask questions, if they are good questions, and you have actually put a little thought into them.
==教授：嗯，教授们总是那么说，但他们的真正意思是：问问题，如果它们是好问题，并且你实际上已经对此进行了一些思考。==

Student: Well, that sure takes the wind out of my sails.
==学生：哎，这真让我泄气。==

Professor: Mission accomplished.
==教授：任务完成。==

In any case, we are not nearly done with virtualization!
==无论如何，我们关于虚拟化的内容还远未结束！==

Rather, you have just seen how to virtualize the CPU, but really there is a big monster waiting in the closet: memory.
==相反，你刚刚看到了如何虚拟化 CPU，但实际上衣橱里还等着一个大怪物：内存。==

Virtualizing memory is complicated and requires us to understand many more intricate details about how the hardware and OS interact.
==虚拟化内存很复杂，需要我们理解更多关于硬件和操作系统如何交互的复杂细节。==

Student: That sounds cool. Why is it so hard?
==学生：听起来很酷。为什么这么难？==

Professor: Well, there are a lot of details, and you have to keep them straight in your head to really develop a mental model of what is going on.
==教授：嗯，有很多细节，你必须在脑海中理清它们，才能真正建立起正在发生的事情的心智模型。==

We'll start simple, with very basic techniques like base/bounds, and slowly add complexity to tackle new challenges, including fun topics like TLBs and multi-level page tables.
==我们将从简单的开始，比如基址/界限这样的非常基本的技术，然后慢慢增加复杂性来应对新的挑战，包括像 TLB 和多级页表这样的有趣话题。==

Eventually, we'll be able to describe the workings of a fully-functional modern virtual memory manager.
==最终，我们将能够描述一个功能齐全的现代虚拟内存管理器的运作方式。==

Student: Neat! Any tips for the poor student, inundated with all of this information and generally sleep-deprived?
==学生：太棒了！对于这个被所有这些信息淹没且普遍睡眠不足的可怜学生，有什么建议吗？==

Professor: For the sleep deprivation, that's easy: sleep more (and party less).
==教授：关于睡眠不足，这很简单：多睡觉（少聚会）。==

For understanding virtual memory, start with this: every address generated by a user program is a virtual address.
==为了理解虚拟内存，从这一点开始：用户程序生成的每一个地址都是虚拟地址。==

The OS is just providing an illusion to each process, specifically that it has its own large and private memory; with some hardware help, the OS will turn these pretend virtual addresses into real physical addresses, and thus be able to locate the desired information.
==操作系统只是为每个进程提供一种幻觉，具体来说就是它拥有自己巨大的私有内存；在一些硬件的帮助下，操作系统会将这些假装的虚拟地址转换为真实的物理地址，从而能够定位所需的信息。==

Student: OK, I think I can remember that... (to self) every address from a user program is virtual, every address from a user program is virtual, every ...
==学生：好的，我想我能记住这一点……（自言自语）用户程序的每个地址都是虚拟的，用户程序的每个地址都是虚拟的，每个……==

Professor: What are you mumbling about?
==教授：你在嘀咕什么？==

Student: Oh nothing.... (awkward pause) ... Anyway, why does the OS want to provide this illusion again?
==学生：哦，没什么……（尴尬的停顿）……话说回来，为什么操作系统要再次提供这种幻觉？==

Professor: Mostly ease of use: the OS will give each program the view that it has a large contiguous address space to put its code and data into; thus, as a programmer, you never have to worry about things like "where should I store this variable?" because the virtual address space of the program is large and has lots of room for that sort of thing.
==教授：主要是为了易用性：操作系统会给每个程序一种视图，即它有一个大的连续地址空间来放置其代码和数据；因此，作为程序员，你永远不必担心诸如“我应该把这个变量存在哪里？”之类的问题，因为程序的虚拟地址空间很大，有足够的空间来处理这类事情。==

Life, for a programmer, becomes much more tricky if you have to worry about fitting all of your code data into a small, crowded memory.
==如果你必须担心将所有代码数据塞进一个狭小拥挤的内存中，程序员的生活就会变得更加棘手。==

Student: Why else?
==学生：还有其他原因吗？==

Professor: Well, isolation and protection are big deals, too.
==教授：嗯，隔离和保护也是重要因素。==

We don't want one errant program to be able to read, or worse, overwrite, some other program's memory, do we?
==我们不想让一个错误的程序能够读取，或者更糟糕的是，覆盖其他程序的内存，对吧？==

Student: Probably not. Unless it's a program written by someone you don't like.
==学生：大概不想。除非那是你讨厌的人写的程序。==

Professor: Hmmm.... I think we might need to add a class on morals and ethics to your schedule for next semester.
==教授：嗯……我想我们可能需要在你下学期的课程表中增加一门道德与伦理课。==

Perhaps OS class isn't getting the right message across.
==也许操作系统课程没有传达正确的信息。==

Student: Maybe we should. But remember, it's not me who taught us that the proper OS response to errant process behavior is to kill the offending process!
==学生：也许我们应该。但请记住，不是我教大家操作系统对错误进程行为的正确反应是杀死违规进程的！==

The Abstraction: Address Spaces
==抽象：地址空间==

In the early days, building computer systems was easy.
==在早期，构建计算机系统很容易。==

Why, you ask? Because users didn't expect much.
==你问为什么？因为用户的期望值不高。==

It is those darned users with their expectations of "ease of use", "high performance", "reliability", etc., that really have led to all these headaches.
==正是那些该死的对“易用性”、“高性能”、“可靠性”等抱有期望的用户，才真正导致了所有这些令人头疼的问题。==

Next time you meet one of those computer users, thank them for all the problems they have caused.
==下次你遇到那些计算机用户时，感谢他们制造的所有这些问题。==

Early Systems
==早期系统==

From the perspective of memory, early machines didn't provide much of an abstraction to users.
==从内存的角度来看，早期的机器并没有为用户提供太多的抽象。==

Basically, the physical memory of the machine looked something like what you see in Figure 13.1 (page 2).
==基本上，机器的物理内存看起来就像你在图 13.1（第 2 页）中看到的那样。==

The OS was a set of routines (a library, really) that sat in memory (starting at physical address 0 in this example), and there would be one running program (a process) that currently sat in physical memory (starting at physical address 64k in this example) and used the rest of memory.
==操作系统是一组驻留在内存中的例程（实际上是一个库）（在本例中从物理地址 0 开始），并且会有一个正在运行的程序（一个进程）当前驻留在物理内存中（在本例中从物理地址 64k 开始）并使用其余的内存。==

There were few illusions here, and the user didn't expect much from the OS.
==这里几乎没有幻觉，用户对操作系统也没什么期望。==

Life was sure easy for OS developers in those days, wasn't it?
==那时的操作系统开发人员的日子肯定过得很轻松，不是吗？==

Multiprogramming and Time Sharing
==多道程序设计与分时==

After a time, because machines were expensive, people began to share machines more effectively.
==过了一段时间，由于机器昂贵，人们开始更有效地共享机器。==

Thus the era of multiprogramming was born [DV66], in which multiple processes were ready to run at a given time, and the OS would switch between them, for example when one decided to perform an .
==因此，多道程序设计的时代诞生了 [DV66]，在这个时代，多个进程准备在给定时间运行，并且操作系统会在它们之间切换，例如当其中一个决定执行  时。==

Doing so increased the effective utilization of the CPU.
==这样做提高了 CPU 的有效利用率。==

Such increases in efficiency were particularly important in those days where each machine cost hundreds of thousands or even millions of dollars (and you thought your Mac was expensive!).
==效率的这种提高在那些每台机器花费数十万甚至数百万美元的日子里尤为重要（你还以为你的 Mac 很贵呢！）。==

Soon enough, however, people began demanding more of machines, and the era of time sharing was born [S59, L60, M62, M83].
==然而很快，人们开始对机器提出更多要求，分时时代诞生了 [S59, L60, M62, M83]。==

Specifically, many realized the limitations of batch computing, particularly on programmers themselves [CV65], who were tired of long (and hence ineffective) program-debug cycles.
==具体来说，许多人意识到了批处理计算的局限性，尤其是对程序员本身而言 [CV65]，他们厌倦了漫长（因此效率低下）的程序调试周期。==

The notion of interactivity became important, as many users might be concurrently using a machine, each waiting for (or hoping for) a timely response from their currently-executing tasks.
==交互性的概念变得重要起来，因为许多用户可能同时使用一台机器，每个人都等待着（或希望）他们当前执行的任务能及时响应。==

One way to implement time sharing would be to run one process for a short while, giving it full access to all memory (Figure 13.1), then stop it, save all of its state to some kind of disk (including all of physical memory), load some other process's state, run it for a while, and thus implement some kind of crude sharing of the machine [M+63].
==实现分时的一种方法是运行一个进程一小段时间，让它完全访问所有内存（图 13.1），然后停止它，将其所有状态保存到某种磁盘上（包括所有物理内存），加载其他进程的状态，运行它一小段时间，从而实现某种粗略的机器共享 [M+63]。==

Unfortunately, this approach has a big problem: it is way too slow, particularly as memory grows.
==不幸的是，这种方法有一个大问题：它太慢了，尤其是随着内存的增长。==

While saving and restoring register-level state (the PC, general-purpose registers, etc.) is relatively fast, saving the entire contents of memory to disk is brutally non-performant.
==虽然保存和恢复寄存器级状态（PC、通用寄存器等）相对较快，但将整个内存内容保存到磁盘是极其低效的。==

Thus, what we'd rather do is leave processes in memory while switching between them, allowing the OS to implement time sharing efficiently (as shown in Figure 13.2, page 3).
==因此，我们更愿意在进程之间切换时将它们保留在内存中，从而允许操作系统高效地实现分时（如图 13.2 所示，第 3 页）。==

In the diagram, there are three processes (A, B, and C) and each of them have a small part of the 512KB physical memory carved out for them.
==在图中，有三个进程（A、B 和 C），它们每一个都在 512KB 的物理内存中分得了一小部分。==

Assuming a single CPU, the OS chooses to run one of the processes (say A), while the others (B and C) sit in the ready queue waiting to run.
==假设是单 CPU，操作系统选择运行其中一个进程（比如 A），而其他进程（B 和 C）则在就绪队列中等待运行。==

As time sharing became more popular, you can probably guess that new demands were placed on the operating system.
==随着分时变得越来越流行，你可能猜到了操作系统面临着新的要求。==

In particular, allowing multiple programs to reside concurrently in memory makes protection an important issue; you don't want a process to be able to read, or worse, write some other process's memory.
==特别是，允许多个程序同时驻留在内存中使得保护成为一个重要问题；你不想让一个进程能够读取，或者更糟糕的是，写入其他进程的内存。==

The Address Space
==地址空间==

However, we have to keep those pesky users in mind, and doing so requires the OS to create an easy to use abstraction of physical memory.
==然而，我们必须记住那些麻烦的用户，这样做需要操作系统创建一种易于使用的物理内存抽象。==

We call this abstraction the address space, and it is the running program's view of memory in the system.
==我们将这种抽象称为地址空间，它是正在运行的程序对系统中内存的视图。==

Understanding this fundamental OS abstraction of memory is key to understanding how memory is virtualized.
==理解这个基本的操作系统内存抽象是理解内存如何被虚拟化的关键。==

The address space of a process contains all of the memory state of the running program.
==一个进程的地址空间包含运行程序的所有内存状态。==

For example, the code of the program (the instructions) have to live in memory somewhere, and thus they are in the address space.
==例如，程序的代码（指令）必须驻留在内存的某个地方，因此它们在地址空间中。==

The program, while it is running, uses a stack to keep track of where it is in the function call chain as well as to allocate local variables and pass parameters and return values to and from routines.
==程序在运行时使用栈来跟踪其在函数调用链中的位置，以及分配局部变量、传递参数和在例程之间传递返回值。==

Finally, the heap is used for dynamically-allocated, user-managed memory, such as that you might receive from a call to malloc() in C or new in an object-oriented language such as C++ or Java.
==最后，堆用于动态分配的、用户管理的内存，例如你可能通过在 C 中调用 malloc() 或在 C++ 或 Java 等面向对象语言中调用 new 获得的内存。==

Of course, there are other things in there too (e.g., statically-initialized variables), but for now let us just assume those three components: code, stack, and heap.
==当然，里面还有其他东西（例如，静态初始化的变量），但现在让我们只假设这三个部分：代码、栈和堆。==

In the example in Figure 13.3 (page 4), we have a tiny address space (only ).
==在图 13.3（第 4 页）的示例中，我们有一个微小的地址空间（只有 ）。==

The program code lives at the top of the address space (starting at 0 in this example, and is packed into the first 1K of the address space).
==程序代码位于地址空间的顶部（在本例中从 0 开始，并被压缩在地址空间的前 1K 中）。==

Code is static (and thus easy to place in memory), so we can place it at the top of the address space and know that it won't need any more space as the program runs.
==代码是静态的（因此很容易放置在内存中），所以我们可以将它放在地址空间的顶部，并且知道随着程序的运行它不需要更多的空间。==

Next, we have the two regions of the address space that may grow (and shrink) while the program runs.
==接下来，我们有两个地址空间区域，它们可能会随着程序的运行而增长（和收缩）。==

Those are the heap (at the top) and the stack (at the bottom).
==它们是堆（在顶部）和栈（在底部）。==

We place them like this because each wishes to be able to grow, and by putting them at opposite ends of the address space, we can allow such growth: they just have to grow in opposite directions.
==我们这样放置它们是因为它们都希望能增长，通过将它们放在地址空间的两端，我们可以允许这种增长：它们只需要向相反的方向增长。==

The heap thus starts just after the code (at 1KB) and grows downward (say when a user requests more memory via malloc()); the stack starts at 16KB and grows upward (say when a user makes a procedure call).
==因此，堆紧接在代码之后开始（在 1KB 处）并向下增长（例如当用户通过 malloc() 请求更多内存时）；栈从 16KB 开始并向上增长（例如当用户进行过程调用时）。==

However, this placement of stack and heap is just a convention; you could arrange the address space in a different way if you'd like (as we'll see later, when multiple threads co-exist in an address space, no nice way to divide the address space like this works anymore, alas).
==然而，这种栈和堆的放置只是一个约定；如果你愿意，你可以以不同的方式安排地址空间（正如我们稍后将看到的，当多个线程共存于一个地址空间时，遗憾的是，像这样划分地址空间的这种好方法就不再适用了）。==

Of course, when we describe the address space, what we are describing is the abstraction that the OS is providing to the running program.
==当然，当我们描述地址空间时，我们描述的是操作系统提供给运行程序的抽象。==

The program really isn't in memory at physical addresses 0 through 16KB; rather it is loaded at some arbitrary physical address(es).
==程序实际上并不在物理地址 0 到 16KB 的内存中；相反，它被加载到某个任意的物理地址处。==

Examine processes A, B, and C in Figure 13.2; there you can see how each process is loaded into memory at a different address.
==检查图 13.2 中的进程 A、B 和 C；你可以看到每个进程是如何加载到不同地址的内存中的。==

And hence the problem:
==因此问题来了：==

THE CRUX: HOW TO VIRTUALIZE MEMORY
==关键问题：如何虚拟化内存==

How can the OS build this abstraction of a private, potentially large address space for multiple running processes (all sharing memory) on top of a single, physical memory?
==操作系统如何在单个物理内存之上，为多个运行进程（都共享内存）构建这种私有的、潜在巨大的地址空间抽象呢？==

When the OS does this, we say the OS is virtualizing memory, because the running program thinks it is loaded into memory at a particular address (say 0) and has a potentially very large address space (say 32-bits or 64-bits); the reality is quite different.
==当操作系统这样做时，我们说操作系统正在虚拟化内存，因为正在运行的程序认为它被加载到特定地址（例如 0）的内存中，并且具有潜在的非常大的地址空间（例如 32 位或 64 位）；现实却大不相同。==

When, for example, process A in Figure 13.2 tries to perform a load at address 0 (which we will call a virtual address), somehow the OS, in tandem with some hardware support, will have to make sure the load doesn't actually go to physical address 0 but rather to physical address 320KB (where A is loaded into memory).
==例如，当图 13.2 中的进程 A 试图在地址 0 执行加载操作时（我们称之为虚拟地址），操作系统必须与某种硬件支持相配合，以确保加载操作实际上不会去往物理地址 0，而是去往物理地址 320KB（A 被加载到内存的地方）。==

This is the key to virtualization of memory, which underlies every modern computer system in the world.
==这就是内存虚拟化的关键，它是世界上每个现代计算机系统的基础。==

Goals
==目标==

Thus we arrive at the job of the OS in this set of notes: to virtualize memory.
==因此，我们得出了这组笔记中操作系统的工作：虚拟化内存。==

The OS will not only virtualize memory, though; it will do so with style.
==不过，操作系统不仅要虚拟化内存；它还要做得有格调。==

To make sure the OS does so, we need some goals to guide us.
==为了确保操作系统这样做，我们需要一些目标来指导我们。==

We have seen these goals before (think of the Introduction), and we'll see them again, but they are certainly worth repeating.
==我们之前见过这些目标（想想引言部分），我们将再次看到它们，但它们肯定值得重复一遍。==

One major goal of a virtual memory (VM) system is transparency.
==虚拟内存 (VM) 系统的一个主要目标是透明度。==

The OS should implement virtual memory in a way that is invisible to the running program.
==操作系统应该以对运行程序不可见的方式实现虚拟内存。==

Thus, the program shouldn't be aware of the fact that memory is virtualized; rather, the program behaves as if it has its own private physical memory.
==因此，程序不应该意识到内存被虚拟化的事实；相反，程序的行为就像它拥有自己的私有物理内存一样。==

Behind the scenes, the OS (and hardware) does all the work to multiplex memory among many different jobs, and hence implements the illusion.
==在幕后，操作系统（和硬件）做了所有的工作，在许多不同的作业之间复用内存，从而实现了这种幻觉。==

Another goal of VM is efficiency.
==VM 的另一个目标是效率。==

The OS should strive to make the virtualization as efficient as possible, both in terms of time (i.e., not making programs run much more slowly) and space (i.e., not using too much memory for structures needed to support virtualization).
==操作系统应该努力使虚拟化尽可能高效，无论是在时间方面（即不让程序运行得太慢）还是在空间方面（即不使用太多内存来存储支持虚拟化所需的结构）。==

In implementing time-efficient virtualization, the OS will have to rely on hardware support, including hardware features such as TLBs (which we will learn about in due course).
==在实现时间高效的虚拟化时，操作系统将不得不依赖硬件支持，包括诸如 TLB（我们将在适当的时候了解到）之类的硬件特性。==

Finally, a third VM goal is protection.
==最后，VM 的第三个目标是保护。==

The OS should make sure to protect processes from one another as well as the OS itself from processes.
==操作系统应确保保护进程免受彼此侵害，并保护操作系统本身免受进程侵害。==

TIP: THE PRINCIPLE OF ISOLATION
==提示：隔离原则==

Isolation is a key principle in building reliable systems.
==隔离是构建可靠系统的关键原则。==

If two entities are properly isolated from one another, this implies that one can fail without affecting the other.
==如果两个实体彼此适当隔离，这意味着一个实体的失败不会影响另一个实体。==

Operating systems strive to isolate processes from each other and in this way prevent one from harming the other.
==操作系统努力将进程彼此隔离，并以此防止一个进程伤害另一个进程。==

By using memory isolation, the OS further ensures that running programs cannot affect the operation of the underlying OS.
==通过使用内存隔离，操作系统进一步确保运行程序无法影响底层操作系统的运行。==

Some modern OS's take isolation even further, by walling off pieces of the OS from other pieces of the OS.
==一些现代操作系统甚至进一步隔离，将操作系统的部分与其他部分隔离开来。==

Such microkernels [BH70, R+89, S+03] thus may provide greater reliability than typical monolithic kernel designs.
==这种微内核 [BH70, R+89, S+03] 因此可能提供比典型宏内核设计更高的可靠性。==

When one process performs a load, a store, or an instruction fetch, it should not be able to access or affect in any way the memory contents of any other process or the OS itself (that is, anything outside its address space).
==当一个进程执行加载、存储或指令获取时，它不应该能够访问或以任何方式影响任何其他进程或操作系统本身的内存内容（即其地址空间之外的任何内容）。==

Protection thus enables us to deliver the property of isolation among processes; each process should be running in its own isolated cocoon, safe from the ravages of other faulty or even malicious processes.
==因此，保护使我们能够在进程之间提供隔离属性；每个进程都应该运行在自己隔离的茧中，免受其他错误甚至恶意进程的破坏。==

In the next chapters, we'll focus our exploration on the basic mechanisms needed to virtualize memory, including hardware and operating systems support.
==在接下来的章节中，我们将重点探讨虚拟化内存所需的基本机制，包括硬件和操作系统的支持。==

We'll also investigate some of the more relevant policies that you'll encounter in operating systems, including how to manage free space and which pages to kick out of memory when you run low on space.
==我们还将调查你在操作系统中会遇到的一些更相关的策略，包括如何管理空闲空间以及当空间不足时将哪些页面踢出内存。==

In doing so, we'll build up your understanding of how a modern virtual memory system really works.
==通过这样做，我们将建立你对现代虚拟内存系统实际工作原理的理解。==

Summary
==总结==

We have seen the introduction of a major OS subsystem: virtual memory.
==我们要看到了一个主要操作系统子系统的介绍：虚拟内存。==

The VM system is responsible for providing the illusion of a large, sparse, private address space to each running program; each virtual address space contains all of a program's instructions and data, which can be referenced by the program via virtual addresses.
==VM 系统负责为每个运行程序提供一个大的、稀疏的、私有的地址空间的幻觉；每个虚拟地址空间包含程序的所有指令和数据，程序可以通过虚拟地址引用这些指令和数据。==

The OS, with some serious hardware help, will take each of these virtual memory references and turn them into physical addresses, which can be presented to the physical memory in order to fetch or update the desired information.
==操作系统在一些重要的硬件帮助下，将获取每一个虚拟内存引用并将它们转换为物理地址，这些地址可以呈现给物理内存以获取或更新所需的信息。==

The OS will provide this service for many processes at once, making sure to protect programs from one another, as well as protect the OS.
==操作系统将同时为许多进程提供此服务，确保保护程序免受彼此侵害，以及保护操作系统。==

The entire approach requires a great deal of mechanism (i.e., lots of low-level machinery) as well as some critical policies to work; we'll start from the bottom up, describing the critical mechanisms first.
==整个方法需要大量的机制（即大量的低级机器）以及一些关键策略才能工作；我们将从下往上开始，首先描述关键机制。==

And thus we proceed!
==我们就这样继续！==

ASIDE: EVERY ADDRESS YOU SEE IS VIRTUAL
==旁白：你看到的每一个地址都是虚拟的==

Ever write a C program that prints out a pointer?
==曾经写过一个打印指针的 C 程序吗？==

The value you see (some large number, often printed in hexadecimal), is a virtual address.
==你看到的值（某个大数字，通常以十六进制打印）是一个虚拟地址。==

Ever wonder where the code of your program is found?
==有没有想过你的程序代码在哪里？==

You can print that out too, and yes, if you can print it, it also is a virtual address.
==你也可以把它打印出来，是的，如果你能打印它，它也是一个虚拟地址。==

In fact, any address you can see as a programmer of a user-level program is a virtual address.
==事实上，作为用户级程序的程序员，你能看到的任何地址都是虚拟地址。==

It's only the OS, through its tricky techniques of virtualizing memory, that knows where in the physical memory of the machine these instructions and data values lie.
==只有操作系统，通过其虚拟化内存的巧妙技术，才知道这些指令和数据值位于机器物理内存的何处。==

So never forget: if you print out an address in a program, it's a virtual one, an illusion of how things are laid out in memory; only the OS (and the hardware) knows the real truth.
==所以永远不要忘记：如果你在程序中打印出一个地址，它就是一个虚拟地址，是内存中布局的一种幻觉；只有操作系统（和硬件）知道真相。==

Here's a little program (va.c) that prints out the locations of the main () routine (where code lives), the value of a heap-allocated value returned from malloc(), and the location of an integer on the stack:
==这是一个小程序 (va.c)，它打印出 main() 例程（代码所在位置）的位置、从 malloc() 返回的堆分配值的值以及栈上整数的位置：==

[Code omitted]
==[代码省略]==

When run on a 64-bit Mac, we get the following output:
==在 64 位 Mac 上运行时，我们得到以下输出：==

location of code: 
==代码位置：==

location of heap: 
==堆位置：==

location of stack: 
==栈位置：==

From this, you can see that code comes first in the address space, then the heap, and the stack is all the way at the other end of this large virtual space.
==由此可见，代码在地址空间中排在最前面，然后是堆，栈则位于这个巨大虚拟空间的另一端。==

All of these addresses are virtual, and will be translated by the OS and hardware in order to fetch values from their true physical locations.
==所有这些地址都是虚拟的，并将由操作系统和硬件进行转换，以便从其真实的物理位置获取值。==

Homework (Code)
==作业（代码）==

In this homework, we'll just learn about a few useful tools to examine virtual memory usage on Linux-based systems.
==在这个作业中，我们将学习一些有用的工具来检查基于 Linux 的系统上的虚拟内存使用情况。==

This will only be a brief hint at what is possible; you'll have to dive deeper on your own to truly become an expert (as always!).
==这只是对可能性的一个简短提示；你必须自己深入研究才能真正成为专家（一如既往！）。==

Questions
==问题==

1. The first Linux tool you should check out is the very simple tool free.
==2. 你应该查看的第一个 Linux 工具是非常简单的工具 free。==

First, type man free and read its entire manual page; it's short, don't worry!
==首先，输入 man free 并阅读其完整的手册页；它很短，别担心！==

2. Now, run free, perhaps using some of the arguments that might be useful (e.g., -m, to display memory totals in megabytes).
==3. 现在，运行 free，也许使用一些可能有用的参数（例如，-m，以兆字节显示内存总量）。==

How much memory is in your system?
==你的系统中有多少内存？==

How much is free?
==有多少是空闲的？==

Do these numbers match your intuition?
==这些数字符合你的直觉吗？==

3. Next, create a little program that uses a certain amount of memory, called memory-user.c.
==4. 接下来，创建一个使用一定量内存的小程序，名为 memory-user.c。==

This program should take one command-line argument: the number of megabytes of memory it will use.
==该程序应该接受一个命令行参数：它将使用的内存兆字节数。==

When run, it should allocate an array, and constantly stream through the array, touching each entry.
==运行时，它应该分配一个数组，并不断流经该数组，触碰每个条目。==

The program should do this indefinitely, or, perhaps, for a certain amount of time also specified at the command line.
==程序应该无限期地执行此操作，或者，也许在命令行指定的一段时间内执行。==

4. Now, while running your memory-user program, also (in a different terminal window, but on the same machine) run the free tool.
==5. 现在，在运行你的 memory-user 程序的同时，也（在不同的终端窗口中，但在同一台机器上）运行 free 工具。==

How do the memory usage totals change when your program is running?
==当你的程序运行时，内存使用总量如何变化？==

How about when you kill the memory-user program?
==当你杀死 memory-user 程序时呢？==

Do the numbers match your expectations?
==这些数字符合你的预期吗？==

Try this for different amounts of memory usage.
==尝试不同的内存使用量。==

What happens when you use really large amounts of memory?
==当你使用非常大量的内存时会发生什么？==

5. Let's try one more tool, known as pmap.
==6. 让我们尝试另一个工具，称为 pmap。==

Spend some time, and read the pmap manual page in detail.
==花点时间，详细阅读 pmap 手册页。==

6. To use pmap, you have to know the process ID of the process you're interested in.
==7. 要使用 pmap，你必须知道你感兴趣的进程的进程 ID。==

Thus, first run ps auxw to see a list of all processes; then, pick an interesting one, such as a browser.
==因此，首先运行 ps auxw 查看所有进程的列表；然后，选择一个有趣的进程，例如浏览器。==

You can also use your memory-user program in this case (indeed, you can even have that program call getpid() and print out its PID for your convenience).
==在这种情况下，你也可以使用你的 memory-user 程序（实际上，你甚至可以让该程序调用 getpid() 并打印出其 PID 以方便你）。==

7. Now run pmap on some of these processes, using various flags (like -X) to reveal many details about the process.
==8. 现在在其中一些进程上运行 pmap，使用各种标志（如 -X）来揭示有关进程的许多细节。==

What do you see?
==你看到了什么？==

How many different entities make up a modern address space, as opposed to our simple conception of code/stack/heap?
==与我们对代码/栈/堆的简单概念相比，有多少不同的实体构成了一个现代地址空间？==

8. Finally, let's run pmap on your memory-user program, with different amounts of used memory.
==9. 最后，让我们在你的 memory-user 程序上运行 pmap，使用不同数量的已用内存。==

What do you see here?
==你在这里看到了什么？==

Does the output from pmap match your expectations?
==pmap 的输出符合你的预期吗？==

Interlude: Memory API
==插曲：内存 API==

In this interlude, we discuss the memory allocation interfaces in UNIX systems.
==在这个插曲中，我们将讨论 UNIX 系统中的内存分配接口。==

The interfaces provided are quite simple, and hence the chapter is short and to the point.
==提供的接口非常简单，因此这一章简短而切中要害。==

The main problem we address is this:
==我们要解决的主要问题是：==

CRUX: HOW TO ALLOCATE AND MANAGE MEMORY
==关键问题：如何分配和管理内存==

In UNIX/C programs, understanding how to allocate and manage memory is critical in building robust and reliable software.
==在 UNIX/C 程序中，了解如何分配和管理内存对于构建健壮和可靠的软件至关重要。==

What interfaces are commonly used?
==通常使用哪些接口？==

What mistakes should be avoided?
==应该避免哪些错误？==

Types of Memory
==内存类型==

In running a C program, there are two types of memory that are allocated.
==在运行 C 程序时，会分配两种类型的内存。==

The first is called stack memory, and allocations and deallocations of it are managed implicitly by the compiler for you, the programmer; for this reason it is sometimes called automatic memory.
==第一种称为栈内存，其分配和释放由编译器为你（程序员）隐式管理；因此它有时被称为自动内存。==

Declaring memory on the stack in C is easy.
==在 C 中在栈上声明内存很容易。==

For example, let's say you need some space in a function func() for an integer, called x.
==例如，假设你在函数 func() 中需要一些空间来存放一个名为 x 的整数。==

To declare such a piece of memory, you just do something like this:
==要声明这样一块内存，你只需要这样做：==

void func() {
int x; // declares an integer on the stack
}

void func() {
==int x; // 在栈上声明一个整数==
}

The compiler does the rest, making sure to make space on the stack when you call into func().
==编译器完成其余工作，确保当你调用 func() 时在栈上腾出空间。==

When you return from the function, the compiler deallocates the memory for you; thus, if you want some information to live beyond the call invocation, you had better not leave that information on the stack.
==当你从函数返回时，编译器会为你释放内存；因此，如果你希望某些信息在调用结束后仍然存在，最好不要将该信息留在栈上。==

It is this need for long-lived memory that gets us to the second type of memory, called heap memory, where all allocations and deallocations are explicitly handled by you, the programmer.
==正是这种对长寿命内存的需求引出了第二种类型的内存，称为堆内存，其中所有的分配和释放都由你（程序员）显式处理。==

A heavy responsibility, no doubt!
==毫无疑问，这是一项重大的责任！==

And certainly the cause of many bugs.
==当然也是许多错误的根源。==

But if you are careful and pay attention, you will use such interfaces correctly and without too much trouble.
==但如果你小心谨慎并加以注意，你就能正确使用这些接口，而不会遇到太多麻烦。==

Here is an example of how one might allocate an integer on the heap:
==这是一个如何在堆上分配整数的例子：==

void func() {
int *x = (int*) malloc(sizeof(int));
}

void func() {
int *x = (int*) malloc(sizeof(int));
}

A couple of notes about this small code snippet.
==关于这个小代码片段的几点说明。==

First, you might notice that both stack and heap allocation occur on this line: first the compiler knows to make room for a pointer to an integer when it sees your declaration of said pointer (int *x);
==首先，你可能会注意到栈和堆分配都发生在这行代码中：首先，当编译器看到你声明该指针 (int *x) 时，它知道要为指向整数的指针腾出空间；==

Subsequently, when the program calls malloc(), it requests space for an integer on the heap; the routine returns the address of such an integer (upon success, or NULL on failure), which is then stored on the stack for use by the program.
==随后，当程序调用 malloc() 时，它请求在堆上分配一个整数的空间；该例程返回该整数的地址（成功时，失败时返回 NULL），然后将其存储在栈上供程序使用。==

Because of its explicit nature, and because of its more varied usage, heap memory presents more challenges to both users and systems.
==由于其显式的性质，以及其更多样化的用法，堆内存给用户和系统都带来了更多挑战。==

Thus, it is the focus of the remainder of our discussion.
==因此，它是我们后续讨论的重点。==

The malloc() Call
==malloc() 调用==

The malloc() call is quite simple: you pass it a size asking for some room on the heap, and it either succeeds and gives you back a pointer to the newly-allocated space, or fails and returns NULL.
==malloc() 调用非常简单：你传递给它一个大小，请求堆上的一些空间，它要么成功并返回指向新分配空间的指针，要么失败并返回 NULL。==

The manual page shows what you need to do to use malloc; type man malloc at the command line and you will see:
==手册页显示了使用 malloc 需要做什么；在命令行输入 man malloc，你会看到：==

#include <stdlib.h>
void *malloc(size_t size);

#include <stdlib.h>
void *malloc(size_t size);

From this information, you can see that all you need to do is include the header file stdlib.h to use malloc.
==从这些信息中，你可以看到使用 malloc 所需要做的就是包含头文件 stdlib.h。==

In fact, you don't really need to even do this, as the C library, which all C programs link with by default, has the code for malloc() inside of it; adding the header just lets the compiler check whether you are calling malloc() correctly (e.g., passing the right number of arguments to it, of the right type).
==事实上，你甚至不需要这样做，因为所有 C 程序默认链接的 C 库里面都有 malloc() 的代码；添加头文件只是为了让编译器检查你是否正确调用了 malloc()（例如，传递给它的参数数量是否正确，类型是否正确）。==

The single parameter malloc() takes is of type size_t which simply describes how many bytes you need.
==malloc() 接受的单个参数类型为 size_t，它只是描述你需要多少字节。==

However, most programmers do not type in a number here directly (such as 10); indeed, it would be...
==然而，大多数程序员不会直接在这里输入数字（例如 10）；确实，这将是……==


INTERLUDE: MEMORY API

==插叙：内存 API==

  

TIP: WHEN IN DOUBT, TRY IT OUT

==提示：有疑问时，亲自试一试==

  

If you aren't sure how some routine or operator you are using behaves, there is no substitute for simply trying it out and making sure it behaves as you expect.

==如果您不确定正在使用的某个例程或运算符的具体行为，没有什么比亲自尝试并确保其行为符合您的预期更好的方法了。==

  

While reading the manual pages or other documentation is useful, how it works in practice is what matters.

==虽然阅读手册页或其他文档很有用，但在实践中它是如何工作的才是最重要的。==

  

Write some code and test it!

==写一些代码并进行测试！==

  

That is no doubt the best way to make sure your code behaves as you desire.

==毫无疑问，这是确保您的代码按照您的意愿运行的最佳方式。==

  

Indeed, that is what we did to double-check the things we were saying about sizeof() were actually true!

==事实上，为了再次核实我们关于 sizeof() 的说法是否属实，我们就是这么做的！==

  

It is considered poor form to do so.

==这样做被认为是糟糕的编程风格。==

  

Instead, various routines and macros are utilized.

==取而代之的是，人们使用各种例程和宏。==

  

For example, to allocate space for a double-precision floating point value, you simply do this:

==例如，要为一个双精度浮点值分配空间，您只需这样做：==

  

double *d = (double *) malloc(sizeof(double));

double *d = (double *) malloc(sizeof(double));

  

Wow, that's a lot of double-ing!

==哇，这里的 double 可真多！==

  

This invocation of malloc() uses the sizeof() operator to request the right amount of space.

==这次 malloc() 的调用使用了 sizeof() 运算符来请求正确大小的空间。==

  

In C, this is generally thought of as a compile-time operator, meaning that the actual size is known at compile time and thus a number (in this case, 8, for a double) is substituted as the argument to malloc().

==在 C 语言中，这通常被认为是一个编译时运算符，意味着实际大小在编译时就是已知的，因此一个数字（在本例中，double 为 8）会被替换为 malloc() 的参数。==

  

For this reason, sizeof() is correctly thought of as an operator and not a function call (a function call would take place at run time).

==因此，将 sizeof() 视为运算符而不是函数调用是正确的（函数调用会发生在运行时）。==

  

You can also pass in the name of a variable (and not just a type) to sizeof(), but in some cases you may not get the desired results, so be careful.

==您也可以将变量名（而不仅仅是类型）传递给 sizeof()，但在某些情况下您可能无法得到想要的结果，所以要小心。==

  

For example, let's look at the following code snippet:

==例如，让我们看看下面的代码片段：==

  

int *x = malloc(10 * sizeof(int));

int *x = malloc(10 * sizeof(int));

  

printf("%d\n", sizeof(x));

printf("%d\n", sizeof(x));

  

In the first line, we've declared space for an array of 10 integers, which is fine and dandy.

==在第一行，我们声明了 10 个整数数组的空间，这很好。==

  

However, when we use sizeof() in the next line, it returns a small value, such as 4 (on 32-bit machines) or 8 (on 64-bit machines).

==然而，当我们在下一行使用 sizeof() 时，它返回一个很小的值，例如 4（在 32 位机器上）或 8（在 64 位机器上）。==

  

The reason is that in this case, sizeof() thinks we are simply asking how big a pointer to an integer is, not how much memory we have dynamically allocated.

==原因是，在这种情况下，sizeof() 认为我们只是在询问指向整数的指针有多大，而不是我们动态分配了多少内存。==

  

However, sometimes sizeof() does work as you might expect:

==但是，有时 sizeof() 确实会按您的预期工作：==

  

int x[10];

int x[10];

  

printf("%d\n", sizeof(x));

printf("%d\n", sizeof(x));

  

In this case, there is enough static information for the compiler to know that 40 bytes have been allocated.

==在这种情况下，编译器有足够的静态信息知道已经分配了 40 个字节。==

  

Another place to be careful is with strings.

==另一个需要小心的地方是字符串。==

  

When declaring space for a string, use the following idiom: malloc(strlen(s) + 1), which gets the length of the string using the function strlen(), and adds 1 to it in order to make room for the end-of-string character.

==当为字符串声明空间时，请使用以下习语：malloc(strlen(s) + 1)，它使用函数 strlen() 获取字符串的长度，并加 1 以便为字符串结束符腾出空间。==

  

Using sizeof() may lead to trouble here.

==在这里使用 sizeof() 可能会导致麻烦。==

  

You might also notice that malloc() returns a pointer to type void.

==您可能还注意到 malloc() 返回一个 void 类型的指针。==

  

Doing so is just the way in C to pass back an address and let the programmer decide what to do with it.

==这样做只是 C 语言传回地址并让程序员决定如何处理它的一种方式。==

  

The programmer further helps out by using what is called a cast.

==程序员通过使用所谓的强制类型转换（cast）来进一步提供帮助。==

  

In our example above, the programmer casts the return type of malloc() to a pointer to a double.

==在我们上面的例子中，程序员将 malloc() 的返回类型转换为指向 double 的指针。==

  

Casting doesn't really accomplish anything, other than tell the compiler and other programmers who might be reading your code: "yeah, I know what I'm doing."

==强制类型转换实际上并没有完成任何事情，除了告诉编译器和其他可能阅读您代码的程序员：“是的，我知道我在做什么。”==

  

By casting the result of malloc(), the programmer is just giving some reassurance; the cast is not needed for the correctness.

==通过转换 malloc() 的结果，程序员只是给出了一些保证；这种转换对于正确性来说并不是必需的。==

  

14.3 The free() Call

==14.3 free() 调用==

  

As it turns out, allocating memory is the easy part of the equation; knowing when, how, and even if to free memory is the hard part.

==事实证明，分配内存是等式中容易的部分；知道何时、如何甚至是否释放内存才是困难的部分。==

  

To free heap memory that is no longer in use, programmers simply call free():

==要释放不再使用的堆内存，程序员只需调用 free()：==

  

int *x = malloc(10 * sizeof(int));

int *x = malloc(10 * sizeof(int));

  

free(x);

free(x);

  

The routine takes one argument, a pointer returned by malloc().

==该例程接受一个参数，即 malloc() 返回的指针。==

  

Thus, you might notice, the size of the allocated region is not passed in by the user, and must be tracked by the memory-allocation library itself.

==因此，您可能会注意到，分配区域的大小不是由用户传入的，而必须由内存分配库本身来跟踪。==

  

14.4 Common Errors

==14.4 常见错误==

  

There are a number of common errors that arise in the use of malloc() and free().

==在使用 malloc() 和 free() 时会出现许多常见错误。==

  

Here are some we've seen over and over again in teaching the undergraduate operating systems course.

==以下是我们在教授本科操作系统课程时反复看到的一些错误。==

  

All of these examples compile and run with nary a peep from the compiler.

==所有这些示例都能编译并运行，编译器甚至不会发出一点声音。==

  

While compiling a C program is necessary to build a correct C program, it is far from sufficient, as you will learn (often in the hard way).

==虽然编译 C 程序是构建正确 C 程序的必要条件，但这还远远不够，正如您将（通常通过惨痛教训）了解到的那样。==

  

Correct memory management has been such a problem, in fact, that many newer languages have support for automatic memory management.

==事实上，正确的内存管理一直是个大问题，以至于许多较新的语言都支持自动内存管理。==

  

In such languages, while you call something akin to malloc() to allocate memory (usually new or something similar to allocate a new object), you never have to call something to free space.

==在这些语言中，虽然您调用类似于 malloc() 的东西来分配内存（通常是 new 或类似的命令来分配新对象），但您永远不必调用某种东西来释放空间。==

  

Rather, a garbage collector runs and figures out what memory you no longer have references to and frees it for you.

==相反，垃圾收集器（garbage collector）会运行并找出您不再引用的内存，并为您释放它。==

  

Forgetting To Allocate Memory

==忘记分配内存==

  

Many routines expect memory to be allocated before you call them.

==许多例程期望在您调用它们之前已经分配了内存。==

  

For example, the routine strcpy(dst, src) copies a string from a source pointer to a destination pointer.

==例如，例程 strcpy(dst, src) 将字符串从源指针复制到目标指针。==

  

However, if you are not careful, you might do this:

==但是，如果您不小心，您可能会这样做：==

  

char *src = "hello";

char *src = "hello";

  

char *dst;

char *dst;

  

// oops! unallocated

==// 哎呀！未分配==

  

strcpy(dst, src); // segfault and die

==strcpy(dst, src); // 段错误并崩溃==

  

TIP: IT COMPILED OR IT RAN != IT IS CORRECT

==提示：它编译通过了或它运行了 != 它是正确的==

  

Just because a program compiled(!) or even ran once or many times correctly does not mean the program is correct.

==仅仅因为一个程序编译通过了（！），甚至正确运行了一次或多次，并不意味着该程序是正确的。==

  

Many events may have conspired to get you to a point where you believe it works, but then something changes and it stops.

==许多事件可能凑巧让您认为它能工作，但随后某些事情发生了变化，它就停止工作了。==

  

A common student reaction is to say (or yell) "But it worked before!" and then blame the compiler, operating system, hardware, or even (dare we say it) the professor.

==学生常见的反应是说（或大喊）“但它以前是好的！”，然后责怪编译器、操作系统、硬件，甚至（我们要说出来吗）教授。==

  

But the problem is usually right where you think it would be, in your code.

==但问题通常就在您认为可能存在的地方，即您的代码中。==

  

Get to work and debug it before you blame those other components.

==在责怪其他组件之前，先去调试它。==

  

When you run this code, it will likely lead to a segmentation fault, which is a fancy term for YOU DID SOMETHING WRONG WITH MEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY.

==当您运行此代码时，它可能会导致分段错误（segmentation fault），这是一个花哨的术语，意思是“你这个愚蠢的程序员在内存上做错了什么，我很生气”。==

  

In this case, the proper code might instead look like this:

==在这种情况下，正确的代码可能如下所示：==

  

char *src = "hello";

char *src = "hello";

  

char *dst = (char *) malloc(strlen(src) + 1);

char *dst = (char *) malloc(strlen(src) + 1);

  

strcpy(dst, src); // work properly

==strcpy(dst, src); // 正常工作==

  

Alternately, you could use strdup() and make your life even easier.

==或者，您可以使用 strdup() 让您的生活更轻松。==

  

Read the strdup man page for more information.

==阅读 strdup 手册页以获取更多信息。==

  

Not Allocating Enough Memory

==未分配足够的内存==

  

A related error is not allocating enough memory, sometimes called a buffer overflow.

==一个相关的错误是未分配足够的内存，有时称为缓冲区溢出。==

  

In the example above, a common error is to make almost enough room for the destination buffer.

==在上面的例子中，一个常见的错误是为目标缓冲区分配了“几乎”足够的空间。==

  

char *src = "hello";

char *src = "hello";

  

char *dst = (char *) malloc(strlen(src)); // too small!

==char *dst = (char *) malloc(strlen(src)); // 太小了！==

  

strcpy(dst, src); // work properly

==strcpy(dst, src); // 正常工作==

  

Oddly enough, depending on how malloc is implemented and many other details, this program will often run seemingly correctly.

==奇怪的是，取决于 malloc 的实现方式以及许多其他细节，该程序通常看起来运行正常。==

  

In some cases, when the string copy executes, it writes one byte too far past the end of the allocated space, but in some cases this is harmless, perhaps overwriting a variable that isn't used anymore.

==在某些情况下，当字符串复制执行时，它会写到超出分配空间末尾一个字节的地方，但在某些情况下这是无害的，也许覆盖了一个不再使用的变量。==

  

In some cases, these overflows can be incredibly harmful, and in fact are the source of many security vulnerabilities in systems [W06].

==在某些情况下，这些溢出可能极具危害性，实际上是系统中许多安全漏洞的根源 [W06]。==

  

In other cases, the malloc library allocated a little extra space anyhow, and thus your program actually doesn't scribble on some other variable's value and works quite fine.

==在其他情况下，malloc 库无论如何都分配了一点额外的空间，因此您的程序实际上并没有在其他变量的值上乱写，并且运行得很好。==

  

In even other cases, the program will indeed fault and crash.

==在另外一些情况下，程序确实会出错并崩溃。==

  

And thus we learn another valuable lesson: even though it ran correctly once, doesn't mean it's correct.

==因此我们学到了另一个宝贵的教训：即使它正确运行了一次，并不意味着它是正确的。==

  

Forgetting to Initialize Allocated Memory

==忘记初始化分配的内存==

  

With this error, you call malloc() properly, but forget to fill in some values into your newly-allocated data type.

==对于这个错误，您正确调用了 malloc()，但忘记在您新分配的数据类型中填入一些值。==

  

Don't do this!

==不要这样做！==

  

If you do forget, your program will eventually encounter an uninitialized read, where it reads from the heap some data of unknown value.

==如果您真的忘记了，您的程序最终会遇到未初始化读取，即从堆中读取一些未知值的数据。==

  

Who knows what might be in there?

==谁知道里面会有什么？==

  

If you're lucky, some value such that the program still works (e.g., zero).

==如果您幸运的话，可能是一些仍然让程序工作的值（例如，零）。==

  

If you're not lucky, something random and harmful.

==如果您不幸的话，可能是一些随机且有害的东西。==

  

Forgetting To Free Memory

==忘记释放内存==

  

Another common error is known as a memory leak, and it occurs when you forget to free memory.

==另一个常见的错误被称为内存泄漏，当您忘记释放内存时就会发生。==

  

In long-running applications or systems (such as the OS itself), this is a huge problem, as slowly leaking memory eventually leads one to run out of memory, at which point a restart is required.

==在长时间运行的应用程序或系统（如操作系统本身）中，这是一个巨大的问题，因为缓慢的内存泄漏最终会导致内存耗尽，此时就需要重启。==

  

Thus, in general, when you are done with a chunk of memory, you should make sure to free it.

==因此，一般来说，当您用完一块内存时，您应该确保释放它。==

  

Note that using a garbage-collected language doesn't help here: if you still have a reference to some chunk of memory, no garbage collector will ever free it, and thus memory leaks remain a problem even in more modern languages.

==请注意，使用垃圾收集语言在这里并没有帮助：如果您仍然保留对某块内存的引用，垃圾收集器就永远不会释放它，因此即使在更现代的语言中，内存泄漏仍然是一个问题。==

  

In some cases, it may seem like not calling free() is reasonable.

==在某些情况下，不调用 free() 似乎是合理的。==

  

For example, your program is short-lived, and will soon exit; in this case, when the process dies, the OS will clean up all of its allocated pages and thus no memory leak will take place per se.

==例如，您的程序是短命的，很快就会退出；在这种情况下，当进程死亡时，操作系统将清理其所有分配的页面，因此本质上不会发生内存泄漏。==

  

While this certainly "works" (see the aside on page 7), it is probably a bad habit to develop, so be wary of choosing such a strategy.

==虽然这肯定“行得通”（参见第 7 页的旁注），但这可能是一个坏习惯，所以要警惕选择这种策略。==

  

In the long run, one of your goals as a programmer is to develop good habits; one of those habits is understanding how you are managing memory, and (in languages like C), freeing the blocks you have allocated.

==从长远来看，作为一名程序员，您的目标之一是养成良好的习惯；其中一个习惯是了解您如何管理内存，并（在像 C 这样的语言中）释放您分配的块。==

  

Even if you can get away with not doing so, it is probably good to get in the habit of freeing each and every byte you explicitly allocate.

==即使您可以侥幸不这样做，养成释放您显式分配的每一个字节的习惯可能也是好的。==

  

Freeing Memory Before You Are Done With It

==在用完之前释放内存==

  

Sometimes a program will free memory before it is finished using it; such a mistake is called a dangling pointer, and it, as you can guess, is also a bad thing.

==有时程序会在用完内存之前释放它；这种错误被称为悬空指针，正如您所猜测的，这也是一件坏事。==

  

The subsequent use can crash the program, or overwrite valid memory (e.g., you called free(), but then called malloc() again to allocate something else, which then recycles the errantly-freed memory).

==随后的使用可能会使程序崩溃，或覆盖有效的内存（例如，您调用了 free()，但随后又调用 malloc() 分配其他东西，这将重新使用被错误释放的内存）。==

  

ASIDE: WHY NO MEMORY IS LEAKED ONCE YOUR PROCESS EXITS

==旁注：为什么一旦进程退出就不会有内存泄漏==

  

When you write a short-lived program, you might allocate some space using malloc().

==当您编写一个短命的程序时，您可能会使用 malloc() 分配一些空间。==

  

The program runs and is about to complete: is there need to call free() a bunch of times just before exiting?

==程序运行并即将完成：是否有必要在退出前调用一堆 free()？==

  

While it seems wrong not to, no memory will be "lost" in any real sense.

==虽然不这样做似乎是错误的，但实际上没有任何内存会“丢失”。==

  

The reason is simple: there are really two levels of memory management in the system.

==原因很简单：系统中实际上有两个层级的内存管理。==

  

The first level of memory management is performed by the OS, which hands out memory to processes when they run, and takes it back when processes exit (or otherwise die).

==第一级内存管理由操作系统执行，它在进程运行时将内存分发给进程，并在进程退出（或以其他方式死亡）时将其收回。==

  

The second level of management is within each process, for example within the heap when you call malloc() and free().

==第二级管理是在每个进程内部，例如当您调用 malloc() 和 free() 时的堆内部。==

  

Even if you fail to call free() (and thus leak memory in the heap), the operating system will reclaim all the memory of the process (including those pages for code, stack, and, as relevant here, heap) when the program is finished running.

==即使您没有调用 free()（从而在堆中泄漏内存），操作系统也会在程序运行结束时回收进程的所有内存（包括代码、栈以及这里相关的堆页面）。==

  

No matter what the state of your heap in your address space, the OS takes back all of those pages when the process dies, thus ensuring that no memory is lost despite the fact that you didn't free it.

==无论您的地址空间中的堆处于什么状态，当进程死亡时，操作系统都会收回所有这些页面，从而确保尽管您没有释放它，也没有内存丢失。==

  

Thus, for short-lived programs, leaking memory often does not cause any operational problems (though it may be considered poor form).

==因此，对于短命的程序，内存泄漏通常不会导致任何操作问题（尽管这可能被认为是糟糕的编程风格）。==

  

When you write a long-running server (such as a web server or database management system, which never exit), leaked memory is a much bigger issue, and will eventually lead to a crash when the application runs out of memory.

==当您编写一个长时间运行的服务器（例如 Web 服务器或数据库管理系统，它们从不退出）时，内存泄漏是一个更大的问题，最终会导致应用程序在内存耗尽时崩溃。==

  

And of course, leaking memory is an even larger issue inside one particular program: the operating system itself.

==当然，在某个特定程序内部，内存泄漏是一个更大的问题：那就是操作系统本身。==

  

Showing us once again: those who write the kernel code have the toughest job of all...

==这再次向我们表明：编写内核代码的人拥有最艰巨的工作……==

  

Freeing Memory Repeatedly

==重复释放内存==

  

Programs also sometimes free memory more than once; this is known as the double free.

==程序有时也会多次释放内存；这被称为双重释放（double free）。==

  

The result of doing so is undefined.

==这样做的结果是未定义的。==

  

As you can imagine, the memory-allocation library might get confused and do all sorts of weird things; crashes are a common outcome.

==正如您可以想象的那样，内存分配库可能会感到困惑并做出各种奇怪的事情；崩溃是常见的结果。==

  

Calling free() Incorrectly

==错误地调用 free()==

  

One last problem we discuss is the call of free() incorrectly.

==我们讨论的最后一个问题是错误地调用 free()。==

  

After all, free() expects you only to pass to it one of the pointers you received from malloc() earlier.

==毕竟，free() 期望您只传递给它之前从 malloc() 接收到的指针之一。==

  

When you pass in some other value, bad things can (and do) happen.

==当您传入其他值时，坏事可能（并且确实）会发生。==

  

Thus, such invalid frees are dangerous and of course should also be avoided.

==因此，这种无效的释放是危险的，当然也应该避免。==

  

Summary

==总结==

  

As you can see, there are lots of ways to abuse memory.

==正如您所见，有很多滥用内存的方法。==

  

Because of frequent errors with memory, a whole ecosphere of tools have developed to help find such problems in your code.

==由于内存错误频繁发生，整个工具生态系统已经发展起来，以帮助查找代码中的此类问题。==

  

Check out both purify [HJ92] and valgrind [SN05]; both are excellent at helping you locate the source of your memory-related problems.

==查看 purify [HJ92] 和 valgrind [SN05]；两者都非常擅长帮助您定位内存相关问题的根源。==

  

Once you become accustomed to using these powerful tools, you will wonder how you survived without them.

==一旦您习惯了使用这些强大的工具，您会想知道没有它们您是如何生存下来的。==

  

14.5 Underlying OS Support

==14.5 底层操作系统支持==

  

You might have noticed that we haven't been talking about system calls when discussing malloc() and free().

==您可能已经注意到，在讨论 malloc() 和 free() 时，我们没有谈论系统调用。==

  

The reason for this is simple: they are not system calls, but rather library calls.

==原因很简单：它们不是系统调用，而是库调用。==

  

Thus the malloc library manages space within your virtual address space, but itself is built on top of some system calls which call into the OS to ask for more memory or release some back to the system.

==因此，malloc 库管理您的虚拟地址空间内的空间，但其本身是构建在一些系统调用之上的，这些系统调用调用操作系统以请求更多内存或将部分内存释放回系统。==

  

One such system call is called brk, which is used to change the location of the program's break: the location of the end of the heap.

==一种这样的系统调用称为 brk，用于更改程序断点（break）的位置：即堆结束的位置。==

  

It takes one argument (the address of the new break), and thus either increases or decreases the size of the heap based on whether the new break is larger or smaller than the current break.

==它接受一个参数（新断点的地址），因此根据新断点是大于还是小于当前断点，来增加或减少堆的大小。==

  

An additional call sbrk is passed an increment but otherwise serves a similar purpose.

==另一个调用 sbrk 传递的是增量，但除此之外用途相似。==

  

Note that you should never directly call either brk or sbrk.

==请注意，您永远不应该直接调用 brk 或 sbrk。==

  

They are used by the memory-allocation library; if you try to use them, you will likely make something go (horribly) wrong.

==它们由内存分配库使用；如果您尝试使用它们，您可能会让事情变得（非常）糟糕。==

  

Stick to malloc() and free() instead.

==坚持使用 malloc() 和 free() 吧。==

  

Finally, you can also obtain memory from the operating system via the mmap() call.

==最后，您还可以通过 mmap() 调用从操作系统获取内存。==

  

By passing in the correct arguments, mmap() can create an anonymous memory region within your program - a region which is not associated with any particular file but rather with swap space, something we'll discuss in detail later on in virtual memory.

==通过传入正确的参数，mmap() 可以在您的程序中创建一个匿名内存区域——一个不与任何特定文件关联，而是与交换空间关联的区域，我们稍后将在虚拟内存中详细讨论这一点。==

  

This memory can then also be treated like a heap and managed as such.

==然后，这块内存也可以像堆一样被对待和管理。==

  

Read the manual page of mmap() for more details.

==阅读 mmap() 的手册页以获取更多详细信息。==

  

14.6 Other Calls

==14.6 其他调用==

  

There are a few other calls that the memory-allocation library supports.

==内存分配库还支持其他一些调用。==

  

For example, calloc() allocates memory and also zeroes it before returning; this prevents some errors where you assume that memory is zeroed and forget to initialize it yourself (see the paragraph on "uninitialized reads" above).

==例如，calloc() 分配内存并在返回前将其清零；这可以防止一些错误，即您假设内存已清零但忘记自己初始化它（参见上面关于“未初始化读取”的段落）。==

  

The routine realloc() can also be useful, when you've allocated space for something (say, an array), and then need to add something to it: realloc() makes a new larger region of memory, copies the old region into it, and returns the pointer to the new region.

==例程 realloc() 也很有用，当您为某物（比如数组）分配了空间，然后需要向其中添加内容时：realloc() 会创建一个新的更大的内存区域，将旧区域复制到其中，并返回指向新区域的指针。==

  

14.7 Summary

==14.7 总结==

  

We have introduced some of the APIs dealing with memory allocation.

==我们介绍了一些处理内存分配的 API。==

  

As always, we have just covered the basics; more details are available elsewhere.

==一如既往，我们只涵盖了基础知识；更多详细信息可在其他地方找到。==

  

Read the C book [KR88] and Stevens [SR05] (Chapter 7) for more information.

==阅读 C 语言书籍 [KR88] 和 Stevens [SR05]（第 7 章）以获取更多信息。==

  

For a cool modern paper on how to detect and correct many of these problems automatically, see Novark et al. [N+07]; this paper also contains a nice summary of common problems and some neat ideas on how to find and fix them.

==若想了解一篇关于如何自动检测和纠正许多此类问题的很酷的现代论文，请参阅 Novark 等人的文章 [N+07]；这篇论文还包含对常见问题的精彩总结，以及关于如何发现和修复这些问题的一些巧妙想法。==

  

Mechanism: Address Translation

==机制：地址转换==

  

In developing the virtualization of the CPU, we focused on a general mechanism known as limited direct execution (or LDE).

==在开发 CPU 虚拟化时，我们关注了一种称为受限直接执行（Limited Direct Execution，或 LDE）的通用机制。==

  

The idea behind LDE is simple: for the most part, let the program run directly on the hardware; however, at certain key points in time (such as when a process issues a system call, or a timer interrupt occurs), arrange so that the OS gets involved and makes sure the "right" thing happens.

==LDE 背后的想法很简单：在大多数情况下，让程序直接在硬件上运行；但是，在某些关键时间点（例如当进程发出系统调用或发生定时器中断时），安排操作系统介入并确保发生“正确”的事情。==

  

Thus, the OS, with a little hardware support, tries its best to get out of the way of the running program, to deliver an efficient virtualization; however, by interposing at those critical points in time, the OS ensures that it maintains control over the hardware.

==因此，操作系统在少量硬件支持下，尽力不干扰正在运行的程序，以提供高效的虚拟化；然而，通过在这些关键时间点介入，操作系统确保其保持对硬件的控制。==

  

Efficiency and control together are two of the main goals of any modern operating system.

==效率和控制是任何现代操作系统的两个主要目标。==

  

In virtualizing memory, we will pursue a similar strategy, attaining both efficiency and control while providing the desired virtualization.

==在虚拟化内存时，我们将采取类似的策略，在提供所需的虚拟化的同时实现效率和控制。==

  

Efficiency dictates that we make use of hardware support, which at first will be quite rudimentary (e.g., just a few registers) but will grow to be fairly complex (e.g., TLBs, page-table support, and so forth, as you will see).

==效率要求我们利用硬件支持，起初这些支持将非常基础（例如，仅几个寄存器），但随后将变得相当复杂（例如，TLB、页表支持等，正如您将看到的）。==

  

Control implies that the OS ensures that no application is allowed to access any memory but its own; thus, to protect applications from one another, and the OS from applications, we will need help from the hardware here too.

==控制意味着操作系统确保不允许任何应用程序访问除其自身以外的任何内存；因此，为了保护应用程序互不干扰，以及保护操作系统免受应用程序的影响，我们也需要硬件的帮助。==

  

Finally, we will need a little more from the VM system, in terms of flexibility; specifically, we'd like for programs to be able to use their address spaces in whatever way they would like, thus making the system easier to program.

==最后，我们需要虚拟机系统提供更多一点的灵活性；具体来说，我们希望程序能够以它们喜欢的任何方式使用其地址空间，从而使系统更易于编程。==

  

And thus we arrive at the refined crux:

==因此，我们得出了改进后的关键问题：==

  

THE CRUX: HOW TO EFFICIENTLY AND FLEXIBLY VIRTUALIZE MEMORY

==关键问题：如何高效且灵活地虚拟化内存==

  

How can we build an efficient virtualization of memory?

==我们如何构建高效的内存虚拟化？==

  

How do we provide the flexibility needed by applications?

==我们如何提供应用程序所需的灵活性？==

  

How do we maintain control over which memory locations an application can access, and thus ensure that application memory accesses are properly restricted?

==我们如何保持对应用程序可以访问哪些内存位置的控制，从而确保应用程序的内存访问受到适当限制？==

  

How do we do all of this efficiently?

==我们如何高效地完成这一切？==

  

The generic technique we will use, which you can consider an addition to our general approach of limited direct execution, is something that is referred to as hardware-based address translation, or just address translation for short.

==我们将使用的通用技术，您可以将其视为我们受限直接执行通用方法的补充，被称为基于硬件的地址转换，或者简称为地址转换。==

  

With address translation, the hardware transforms each memory access (e.g., an instruction fetch, load, or store), changing the virtual address provided by the instruction to a physical address where the desired information is actually located.

==通过地址转换，硬件转换每次内存访问（例如，指令获取、加载或存储），将指令提供的虚拟地址更改为所需信息实际所在的物理地址。==

  

Thus, on each and every memory reference, an address translation is performed by the hardware to redirect application memory references to their actual locations in memory.

==因此，对于每一次内存引用，硬件都会执行地址转换，将应用程序的内存引用重定向到它们在内存中的实际位置。==

  

Of course, the hardware alone cannot virtualize memory, as it just provides the low-level mechanism for doing so efficiently.

==当然，仅靠硬件无法虚拟化内存，因为它只是提供了高效执行此操作的底层机制。==

  

The OS must get involved at key points to set up the hardware so that the correct translations take place; it must thus manage memory, keeping track of which locations are free and which are in use, and judiciously intervening to maintain control over how memory is used.

==操作系统必须在关键点介入以设置硬件，以便进行正确的转换；因此，它必须管理内存，跟踪哪些位置是空闲的，哪些正在使用，并明智地进行干预以保持对内存使用方式的控制。==

  

Once again the goal of all of this work is to create a beautiful illusion: that the program has its own private memory, where its own code and data reside.

==所有这些工作的目标再次是创造一个美丽的错觉：程序拥有自己的私有内存，其自己的代码和数据驻留其中。==

  

Behind that virtual reality lies the ugly physical truth: that many programs are actually sharing memory at the same time, as the CPU (or CPUs) switches between running one program and the next.

==在这个虚拟现实背后隐藏着丑陋的物理真相：实际上许多程序同时共享内存，因为 CPU（或多个 CPU）在运行一个程序和下一个程序之间切换。==

  

Through virtualization, the OS (with the hardware's help) turns the ugly machine reality into a useful, powerful, and easy to use abstraction.

==通过虚拟化，操作系统（在硬件的帮助下）将丑陋的机器现实转化为有用、强大且易于使用的抽象。==

  

15.1 Assumptions

==15.1 假设==

  

Our first attempts at virtualizing memory will be very simple, almost laughably so.

==我们要进行的第一次内存虚拟化尝试将非常简单，简直有些可笑。==

  

Go ahead, laugh all you want; pretty soon it will be the OS laughing at you, when you try to understand the ins and outs of TLBs, multi-level page tables, and other technical wonders.

==来吧，尽管笑吧；很快，当您试图理解 TLB、多级页表和其他技术奇迹的来龙去脉时，就轮到操作系统嘲笑您了。==

  

Don't like the idea of the OS laughing at you?

==不喜欢操作系统嘲笑您的想法？==

  

Well, you may be out of luck then; that's just how the OS rolls.

==好吧，那您可能倒霉了；操作系统就是这么行事的。==

  

Specifically, we will assume for now that the user's address space must be placed contiguously in physical memory.

==具体来说，我们暂时假设用户的地址空间必须连续地放置在物理内存中。==

  

We will also assume, for simplicity, that the size of the address space is not too big; specifically, that it is less than the size of physical memory.

==为了简单起见，我们还将假设地址空间的大小不要太大；具体来说，它小于物理内存的大小。==

  

Finally, we will also assume that each address space is exactly the same size.

==最后，我们还将假设每个地址空间的大小完全相同。==

  

Don't worry if these assumptions sound unrealistic; we will relax them as we go, thus achieving a realistic virtualization of memory.

==如果这些假设听起来不切实际，请不要担心；我们将逐步放宽这些假设，从而实现逼真的内存虚拟化。==

  

15.2 An Example

==15.2 一个例子==

  

To understand better what we need to do to implement address translation, and why we need such a mechanism, let's look at a simple example.

==为了更好地理解我们需要做什么来实现地址转换，以及为什么我们需要这种机制，让我们看一个简单的例子。==

  

Imagine there is a process whose address space is as indicated in Figure 15.1.

==想象一下有一个进程，其地址空间如图 15.1 所示。==

  

What we are going to examine here is a short code sequence that loads a value from memory, increments it by three, and then stores the value back into memory.

==我们要在这里检查的是一段简短的代码序列，它从内存中加载一个值，将其增加 3，然后将该值存回内存。==

  

You can imagine the C-language representation of this code might look like this:

==您可以想象这段代码的 C 语言表示可能如下所示：==

  

void func() {

void func() {

  

int x = 3000; // thanks, Perry.

==int x = 3000; // 谢谢，Perry。==

  

x = x + 3; // line of code we are interested in

==x = x + 3; // 我们感兴趣的代码行==

  

The compiler turns this line of code into assembly, which might look something like this (in x86 assembly).

==编译器将这行代码转换为汇编代码，可能看起来像这样（在 x86 汇编中）。==

  

Use objdump on Linux or otool on a Mac to disassemble it:

==在 Linux 上使用 objdump 或在 Mac 上使用 otool 来反汇编它：==

  

128: movl 0x0(%ebx), %eax ; load 0+ebx into eax

==128: movl 0x0(%ebx), %eax ; 将 0+ebx 加载到 eax==

  

132: addl $0x03, %eax ; add 3 to eax register

==132: addl $0x03, %eax ; 将 3 加到 eax 寄存器==

  

135: movl %eax, 0x0(%ebx) ; store eax back to mem

==135: movl %eax, 0x0(%ebx) ; 将 eax 存回内存==

  

This code snippet is relatively straightforward; it presumes that the address of x has been placed in the register ebx, and then loads the value at that address into the general-purpose register eax using the movl instruction (for "longword" move).

==这段代码片段相对简单；它假定 x 的地址已放入寄存器 ebx 中，然后使用 movl 指令（用于“长字”移动）将该地址处的值加载到通用寄存器 eax 中。==

  

The next instruction adds 3 to eax, and the final instruction stores the value in eax back into memory at that same location.

==下一条指令将 3 加到 eax，最后一条指令将 eax 中的值存回内存中的同一位置。==

  

In Figure 15.1 (page 4), observe how both the code and data are laid out in the process's address space; the three-instruction code sequence is located at address 128 (in the code section near the top), and the value of the variable x at address 15 KB (in the stack near the bottom).

==在图 15.1（第 4 页）中，观察代码和数据是如何在进程的地址空间中布局的；三条指令的代码序列位于地址 128 处（在顶部的代码段附近），变量 x 的值位于地址 15 KB 处（在底部的栈附近）。==

  

In the figure, the initial value of x is 3000, as shown in its location on the stack.

==在图中，x 的初始值为 3000，如其在栈上的位置所示。==

  

When these instructions run, from the perspective of the process, the following memory accesses take place.

==当这些指令运行时，从进程的角度来看，会发生以下内存访问。==

  

• Fetch instruction at address 128

==• 获取地址 128 处的指令==

  

• Execute this instruction (load from address 15 KB)

==• 执行此指令（从地址 15 KB 加载）==

  

• Fetch instruction at address 132

==• 获取地址 132 处的指令==

  

• Execute this instruction (no memory reference)

==• 执行此指令（无内存引用）==

  

• Fetch the instruction at address 135

==• 获取地址 135 处的指令==

  

• Execute this instruction (store to address 15 KB)

==• 执行此指令（存储到地址 15 KB）==

  

TIP: INTERPOSITION IS POWERFUL

==提示：介入（Interposition）是强大的==

  

Interposition is a generic and powerful technique that is often used to great effect in computer systems.

==介入是一种通用且强大的技术，通常在计算机系统中发挥巨大作用。==

  

In virtualizing memory, the hardware will interpose on each memory access, and translate each virtual address issued by the process to a physical address where the desired information is actually stored.

==在虚拟化内存时，硬件会介入每一次内存访问，并将进程发出的每个虚拟地址转换为所需信息实际存储的物理地址。==

  

However, the general technique of interposition is much more broadly applicable; indeed, almost any well-defined interface can be interposed upon, to add new functionality or improve some other aspect of the system.

==然而，介入这一通用技术的适用范围要广得多；实际上，几乎任何定义良好的接口都可以被介入，以添加新功能或改进系统的其他方面。==

  

One of the usual benefits of such an approach is transparency; the interposition often is done without changing the interface of the client, thus requiring no changes to said client.

==这种方法的通常好处之一是透明性；介入通常在不改变客户端接口的情况下完成，因此无需对所述客户端进行任何更改。==

  

From the program's perspective, its address space starts at address 0 and grows to a maximum of 16 KB; all memory references it generates should be within these bounds.

==从程序的角度来看，其地址空间从地址 0 开始，最大增长到 16 KB；它生成的所有内存引用都应在这些范围内。==

  

However, to virtualize memory, the OS wants to place the process somewhere else in physical memory, not necessarily at address 0.

==然而，为了虚拟化内存，操作系统希望将进程放置在物理内存的其他位置，不一定是在地址 0。==

  

Thus, we have the problem: how can we relocate this process in memory in a way that is transparent to the process?

==因此，我们遇到了一个问题：我们如何才能以对进程透明的方式在内存中重新定位该进程？==

  

How can we provide the illusion of a virtual address space starting at 0, when in reality the address space is located at some other physical address?

==当地址空间实际上位于其他物理地址时，我们如何提供从 0 开始的虚拟地址空间的错觉？==

  

An example of what physical memory might look like once this process's address space has been placed in memory is found in Figure 15.2.

==图 15.2 显示了该进程的地址空间被放置在内存中后，物理内存可能的样子。==

  

In the figure, you can see the OS using the first slot of physical memory for itself, and that it has relocated the process from the example above into the slot starting at physical memory address 32 KB.

==在图中，您可以看到操作系统使用物理内存的第一个槽位供自己使用，并且它已将上述示例中的进程重新定位到从物理内存地址 32 KB 开始的槽位中。==

  

The other two slots are free (16 KB-32 KB and 48 KB-64 KB).

==其他两个槽位是空闲的（16 KB-32 KB 和 48 KB-64 KB）。==

  

15.3 Dynamic (Hardware-based) Relocation

==15.3 动态（基于硬件的）重定位==

  

To gain some understanding of hardware-based address translation, we'll first discuss its first incarnation.

==为了对基于硬件的地址转换有所了解，我们将首先讨论它的最初形式。==

  

Introduced in the first time-sharing machines of the late 1950's is a simple idea referred to as base and bounds; the technique is also referred to as dynamic relocation; we'll use both terms interchangeably [SS74].

==在 20 世纪 50 年代末的第一批分时机器中引入了一个简单的想法，称为基址和界限（base and bounds）；该技术也被称为动态重定位（dynamic relocation）；我们将互换使用这两个术语 [SS74]。==

  

Specifically, we'll need two hardware registers within each CPU: one is called the base register, and the other the bounds (sometimes called a limit register).

==具体来说，我们在每个 CPU 中需要两个硬件寄存器：一个称为基址寄存器（base register），另一个称为界限寄存器（bounds register，有时称为限长寄存器）。==

  

This base-and-bounds pair is going to allow us to place the address space anywhere we'd like in physical memory, and do so while ensuring that the process can only access its own address space.

==这对基址和界限寄存器将允许我们将地址空间放置在物理内存中的任何位置，并且在这样做的同时确保进程只能访问其自己的地址空间。==

  

In this setup, each program is written and compiled as if it is loaded at address zero.

==在这种设置下，每个程序的编写和编译都好像它是加载在地址零处一样。==

  

However, when a program starts running, the OS decides where in physical memory it should be loaded and sets the base register to that value.

==然而，当程序开始运行时，操作系统决定它应该加载到物理内存的哪个位置，并将基址寄存器设置为该值。==

  

In the example above, the OS decides to load the process at physical address 32 KB and thus sets the base register to this value.

==在上面的例子中，操作系统决定将进程加载到物理地址 32 KB 处，因此将基址寄存器设置为该值。==

  

Interesting things start to happen when the process is running.

==当进程运行时，有趣的事情开始发生。==

  

Now, when any memory reference is generated by the process, it is translated by the processor in the following manner:

==现在，当进程生成任何内存引用时，处理器会按以下方式对其进行转换：==

  

physical address = virtual address + base

==物理地址 = 虚拟地址 + 基址==

  

Each memory reference generated by the process is a virtual address; the hardware in turn adds the contents of the base register to this address and the result is a physical address that can be issued to the memory system.

==进程生成的每个内存引用都是一个虚拟地址；硬件随即把基址寄存器的内容加到这个地址上，结果是可以发送给内存系统的物理地址。==

  

To understand this better, let's trace through what happens when a single instruction is executed.

==为了更好地理解这一点，让我们追踪一下执行一条指令时会发生什么。==

  

Specifically, let's look at one instruction from our earlier sequence:

==具体来说，让我们看看我们之前序列中的一条指令：==

  

128: movl 0x0(%ebx), %eax

128: movl 0x0(%ebx), %eax

  

The program counter (PC) is set to 128; when the hardware needs to fetch this instruction, it first adds the value to the base register value of 32 KB (32768) to get a physical address of 32896; the hardware then fetches the instruction from that physical address.

==程序计数器 (PC) 设置为 128；当硬件需要获取此指令时，它首先将该值与 32 KB (32768) 的基址寄存器值相加，得到物理地址 32896；然后硬件从该物理地址获取指令。==

  

Next, the processor begins executing the instruction.

==接下来，处理器开始执行指令。==

MECHANISM: ADDRESS TRANSLATION

==机制：地址转换==

  

The following table outlines the OS and Hardware actions at boot time.

==下表概述了启动时操作系统和硬件的动作。==

  

OS @ boot (kernel mode)

==操作系统 @ 启动（内核模式）==

  

Hardware

==硬件==

  

(No Program Yet)

==（尚无程序）==

  

initialize trap table

==初始化陷阱表==

  

remember addresses of system call handler

==记住系统调用处理程序的地址==

  

remember addresses of timer handler

==记住时钟处理程序的地址==

  

remember addresses of illegal mem-access handler

==记住非法内存访问处理程序的地址==

  

remember addresses of illegal instruction handler

==记住非法指令处理程序的地址==

  

start interrupt timer

==启动中断计时器==

  

initialize process table

==初始化进程表==

  

initialize free list

==初始化空闲列表==

  

start timer; interrupt after X ms

==启动计时器；X 毫秒后中断==

  

Figure 15.5: Limited Direct Execution (Dynamic Relocation) @ Boot

==图 15.5：受限直接执行（动态重定位）@ 启动==

  

execute instructions that it shouldn't.

==执行它不该执行的指令。==

  

Bye bye, misbehaving process; it's been nice knowing you.

==再见，行为不端的进程；很高兴认识你。==

  

Figures 15.5 and 15.6 (page 12) illustrate much of the hardware/OS interaction in a timeline.

==图 15.5 和 15.6（第 12 页）在时间轴上展示了大部分硬件与操作系统的交互。==

  

The first figure shows what the OS does at boot time to ready the machine for use, and the second shows what happens when a process (Process A) starts running.

==第一张图展示了操作系统在启动时为机器做好准备所做的工作，第二张图展示了当一个进程（进程 A）开始运行时发生的情况。==

  

Note how its memory translations are handled by the hardware with no OS intervention.

==请注意，其内存转换是如何由硬件处理的，而无需操作系统的干预。==

  

At some point (middle of second figure), a timer interrupt occurs, and the OS switches to Process B, which executes a "bad load" (to an illegal memory address).

==在某个时刻（第二张图的中间），发生了时钟中断，操作系统切换到进程 B，该进程执行了一个“错误的加载”（针对一个非法的内存地址）。==

  

At that point, the OS must get involved, terminating the process and cleaning up by freeing B's memory and removing its entry from the process table.

==此时，操作系统必须介入，终止该进程，并通过释放 B 的内存及其在进程表中的条目来进行清理。==

  

As you can see from the figures, we are still following the basic approach of limited direct execution.

==正如你从图中看到的，我们要依然遵循受限直接执行的基本方法。==

  

In most cases, the OS just sets up the hardware appropriately and lets the process run directly on the CPU.

==在大多数情况下，操作系统只是适当地设置硬件，并让进程直接在 CPU 上运行。==

  

Only when the process misbehaves does the OS have to become involved.

==只有当进程行为不端时，操作系统才必须介入。==

  

15.6 Summary

==15.6 小结==

  

In this chapter, we have extended the concept of limited direct execution with a specific mechanism used in virtual memory, known as address translation.

==在本章中，我们利用虚拟内存中使用的一种特定机制——即地址转换，扩展了受限直接执行的概念。==

  

With address translation, the OS can control each and every memory access from a process, ensuring the accesses stay within the bounds of the address space.

==通过地址转换，操作系统可以控制进程的每一次内存访问，确保访问保持在地址空间的边界内。==

  

Key to the efficiency of this technique is hardware support, which performs the translation quickly for each access, turning virtual addresses (the process's view of memory) into physical ones (the actual view).

==这项技术效率的关键在于硬件支持，它为通过将虚拟地址（进程的内存视图）转换为物理地址（实际视图），快速地对每次访问执行转换。==

  

All of this is performed in a way that is transparent to the process that has been relocated.

==所有这些操作对被重定位的进程来说都是透明的。==

  

The process has no idea its memory references are being translated, making for a wonderful illusion.

==进程不知道它的内存引用正在被转换，从而制造了一种美妙的错觉。==

  

We have also seen one particular form of virtualization, known as base and bounds or dynamic relocation.

==我们也看到了一种特定的虚拟化形式，称为基址和界限或动态重定位。==

  

Base-and-bounds virtualization is quite efficient, as only a little more hardware logic is required to add a base register to the virtual address and check that the address generated by the process is in bounds.

==基址和界限虚拟化非常高效，因为只需要增加少量的硬件逻辑，用于将基址寄存器加到虚拟地址上，并检查进程生成的地址是否在界限内。==

  

The following table outlines the OS and Hardware actions at runtime.

==下表概述了运行时操作系统和硬件的动作。==

  

OS @ run (kernel mode)

==操作系统 @ 运行（内核模式）==

  

Hardware

==硬件==

  

Program (user mode)

==程序（用户模式）==

  

To start process A:

==启动进程 A：==

  

allocate entry in process table

==在进程表中分配条目==

  

alloc memory for process

==为进程分配内存==

  

set base/bound registers

==设置基址/界限寄存器==

  

return-from-trap (into A)

==从陷阱返回（进入 A）==

  

restore registers of A

==恢复 A 的寄存器==

  

move to user mode

==切换到用户模式==

  

jump to A's (initial) PC

==跳转到 A 的（初始）PC==

  

Process A runs

==进程 A 运行==

  

Fetch instruction

==取指令==

  

translate virtual address

==转换虚拟地址==

  

perform fetch

==执行取指==

  

Execute instruction (A runs...)

==执行指令（A 运行...）==

  

Timer interrupt

==时钟中断==

  

move to kernel mode

==切换到内核模式==

  

jump to handler

==跳转到处理程序==

  

Handle timer

==处理计时器==

  

decide: stop A, run B

==决定：停止 A，运行 B==

  

call switch() routine

==调用 switch() 例程==

  

save regs(A) to proc-struct(A) (including base/bounds)

==保存 regs(A) 到 proc-struct(A)（包括基址/界限）==

  

restore regs(B) from proc-struct(B) (including base/bounds)

==从 proc-struct(B) 恢复 regs(B)（包括基址/界限）==

  

return-from-trap (into B)

==从陷阱返回（进入 B）==

  

restore registers of B

==恢复 B 的寄存器==

  

move to user mode

==切换到用户模式==

  

jump to B's PC

==跳转到 B 的 PC==

  

Process B runs

==进程 B 运行==

  

Execute bad load

==执行错误的加载==

  

if explicit load/store:

==如果是显式加载/存储：==

  

ensure address is legal

==确保地址合法==

  

translate virtual address

==转换虚拟地址==

  

perform load/store

==执行加载/存储==

  

Load is out-of-bounds;

==加载越界；==

  

move to kernel mode

==切换到内核模式==

  

jump to trap handler

==跳转到陷阱处理程序==

  

Handle the trap

==处理陷阱==

  

decide to kill process B

==决定杀死进程 B==

  

deallocate B's memory

==释放 B 的内存==

  

free B's entry in process table

==释放 B 在进程表中的条目==

  

MECHANISM: ADDRESS TRANSLATION

==机制：地址转换==

  

Figure 15.6: Limited Direct Execution (Dynamic Relocation) @ Runtime

==图 15.6：受限直接执行（动态重定位）@ 运行==

  

Base-and-bounds also offers protection; the OS and hardware combine to ensure no process can generate memory references outside its own address space.

==基址和界限也提供了保护；操作系统和硬件结合起来，确保没有进程可以生成超出其自身地址空间的内存引用。==

  

Protection is certainly one of the most important goals of the OS.

==保护无疑是操作系统最重要的目标之一。==

  

Without it, the OS could not control the machine (if processes were free to overwrite memory, they could easily do nasty things like overwrite the trap table and take over the system).

==没有它，操作系统将无法控制机器（如果进程可以随意覆盖内存，它们很容易做出一些恶劣的事情，比如覆盖陷阱表并接管系统）。==

  

Unfortunately, this simple technique of dynamic relocation does have its inefficiencies.

==不幸的是，这种简单的动态重定位技术确实有其低效之处。==

  

For example, as you can see in Figure 15.2 (page 5), the relocated process is using physical memory from 32 KB to 48 KB.

==例如，正如你在图 15.2（第 5 页）中看到的，重定位后的进程正在使用从 32 KB 到 48 KB 的物理内存。==

  

However, because the process stack and heap are not too big, all of the space between the two is simply wasted.

==然而，由于进程的栈和堆都不是很大，两者之间的所有空间都被白白浪费了。==

  

This type of waste is usually called internal fragmentation, as the space inside the allocated unit is not all used (i.e., is fragmented) and thus wasted.

==这种类型的浪费通常被称为内部碎片，因为分配单元内部的空间并未被全部使用（即被分割了），因此被浪费了。==

  

In our current approach, although there might be enough physical memory for more processes, we are currently restricted to placing an address space in a fixed-sized slot and thus internal fragmentation can arise.

==在我们当前的方法中，尽管物理内存可能足够容纳更多的进程，但我们目前受限于将地址空间放置在固定大小的槽位中，因此可能会产生内部碎片。==

  

Thus, we are going to need more sophisticated machinery, to try to better utilize physical memory and avoid internal fragmentation.

==因此，我们需要更复杂的机制，以尝试更好地利用物理内存并避免内部碎片。==

  

Our first attempt will be a slight generalization of base and bounds known as segmentation, which we will discuss next.

==我们的第一次尝试将是基址和界限的一个轻微的泛化，称为分段，我们将在接下来讨论它。==

  

2A different solution might instead place a fixed-sized stack within the address space, just below the code region, and a growing heap below that.

==脚注 2：另一种解决方案可能是将固定大小的栈放置在地址空间内，紧靠代码区域的下方，并在其下方放置一个增长的堆。==

  

However, this limits flexibility by making recursion and deeply-nested function calls challenging, and thus is something we hope to avoid.

==然而，这限制了灵活性，使得递归和深度嵌套的函数调用变得充满挑战，因此这是我们希望避免的事情。==

  

References

==参考文献==

  

[M65] "On Dynamic Program Relocation" by W.C. McGee. IBM Systems Journal, Volume 4:3, 1965, pages 184-199.

[M65] "On Dynamic Program Relocation" by W.C. McGee. IBM Systems Journal, Volume 4:3, 1965, pages 184-199.

  

This paper is a nice summary of early work on dynamic relocation, as well as some basics on static relocation.

==这篇论文很好地总结了关于动态重定位的早期工作，以及关于静态重定位的一些基础知识。==

  

[P90] "Relocating loader for MS-DOS.EXE executable files" by Kenneth D. A. Pillay.

[P90] "Relocating loader for MS-DOS.EXE executable files" by Kenneth D. A. Pillay.

  

Microprocessors & Microsystems archive, Volume 14:7 (September 1990). An example of a relocating loader for MS-DOS.

==Microprocessors & Microsystems archive, Volume 14:7 (September 1990). 一个用于 MS-DOS 的重定位加载器的例子。==

  

Not the first one, but just a relatively modern example of how such a system works.

==虽然不是第一个，但它是一个展示此类系统如何工作的相对现代的例子。==

  

[SS74] "The Protection of Information in Computer Systems" by J. Saltzer and M. Schroeder. CACM, July 1974.

[SS74] "The Protection of Information in Computer Systems" by J. Saltzer and M. Schroeder. CACM, July 1974.

  

From this paper: "The concepts of base-and-bound register and hardware-interpreted descriptors appeared, apparently independently, between 1957 and 1959 on three projects with diverse goals."

==摘自该论文：“基址和界限寄存器以及硬件解释描述符的概念，显然是在 1957 年至 1959 年间，在三个目标各异的项目中独立出现的。”==

  

We found this quote on Mark Smotherman's cool history pages [S04]; see them for more information.

==我们在 Mark Smotherman 很酷的历史页面 [S04] 上发现了这段引文；更多信息请参阅该页面。==

  

[S04] "System Call Support" by Mark Smotherman. May 2004. people.cs.clemson.edu/~mark/syscall.html. A neat history of system call support.

==[S04] "System Call Support" by Mark Smotherman. May 2004. people.cs.clemson.edu/~mark/syscall.html. 一段关于系统调用支持的精彩历史。==

  

Smotherman has also collected some early history on items like interrupts and other fun aspects of computing history.

==Smotherman 还收集了一些关于中断等项目的早期历史，以及计算历史中其他有趣的方面。==

  

See his web pages for more details.

==请查看他的网页以获取更多详情。==

  

[WL+93] "Efficient Software-based Fault Isolation" by Robert Wahbe, Steven Lucco, Thomas E. Anderson, Susan L. Graham. SOSP '93.

[WL+93] "Efficient Software-based Fault Isolation" by Robert Wahbe, Steven Lucco, Thomas E. Anderson, Susan L. Graham. SOSP '93.

  

A terrific paper about how you can use compiler support to bound memory references from a program, without hardware support.

==一篇极好的论文，介绍了如何在没有硬件支持的情况下，利用编译器支持来限制程序的内存引用。==

  

The paper sparked renewed interest in software techniques for isolation of memory references.

==这篇论文重新引发了人们对用于隔离内存引用的软件技术的兴趣。==

  

[W17] Answer to footnote: "Is there anything other than havoc that can be wreaked?" by Waciuma Wanjohi.

==[W17] 回答脚注：“除了浩劫（havoc），还有什么可以被造成（wreaked）吗？” by Waciuma Wanjohi.==

  

October 2017. Amazingly, this enterprising reader found the answer via google's Ngram viewing tool.

==2017 年 10 月。令人惊讶的是，这位富有进取心的读者通过 Google 的 Ngram 查看工具找到了答案。==

  

The answer, thanks to Mr. Wanjohi: "It's only since about 1970 that 'wreak havoc' has been more popular than 'wreak vengeance'. In the 1800s, the word wreak was almost always followed by 'his/their vengeance'."

==感谢 Wanjohi 先生提供的答案：“直到 1970 年左右，‘wreak havoc’（造成浩劫）才比 ‘wreak vengeance’（实施报复）更流行。在 19 世纪，wreak 这个词几乎总是跟在 ‘his/their vengeance’ 后面。”==

  

Apparently, when you wreak, you are up to no good, but at least wreakers have some options now.

==显然，当你在 wreak 时，你肯定没干好事，但至少现在的 wreaker 们有了一些选择。==

  

Homework (Simulation)

==作业（模拟）==

  

The program relocation.py allows you to see how address translations are performed in a system with base and bounds registers.

==程序 relocation.py 允许你查看在具有基址和界限寄存器的系统中是如何执行地址转换的。==

  

See the README for details.

==详情请参阅 README。==

  

Questions

==问题==

  

1. Run with seeds 1, 2, and 3, and compute whether each virtual address generated by the process is in or out of bounds.

==2. 使用种子 1、2 和 3 运行，并计算进程生成的每个虚拟地址是在界限内还是界限外。==

  

If in bounds, compute the translation.

==如果在界限内，计算转换结果。==

  

2. Run with these flags: s0n 10. What value do you have to set -l (the bounds register) to in order to ensure that all the generated virtual addresses are within bounds?

==3. 使用这些标志运行：s0n 10。你需要将 -l（界限寄存器）设置为多少，才能确保所有生成的虚拟地址都在界限内？==

4. Run with these flags: s 1n 101 100. What is the maximum value that base can be set to, such that the address space still fits into physical memory in its entirety?

==5. 使用这些标志运行：s 1n 101 100。基址最大可以设置为多少，才能使地址空间仍然完整地放入物理内存中？==

6. Run some of the same problems above, but with larger address spaces (-a) and physical memories (-p).

==7. 运行上述相同的一些问题，但使用更大的地址空间 (-a) 和物理内存 (-p)。==

8. What fraction of randomly-generated virtual addresses are valid, as a function of the value of the bounds register?

==9. 作为界限寄存器值的函数，随机生成的虚拟地址中有效的比例是多少？==

  

Make a graph from running with different random seeds, with limit values ranging from 0 up to the maximum size of the address space.

==通过使用不同的随机种子运行，并使用从 0 到地址空间最大范围的限制值，绘制一张图表。==

  

Segmentation

==分段==

  

So far we have been putting the entire address space of each process in memory.

==到目前为止，我们将每个进程的整个地址空间都放入了内存中。==

  

With the base and bounds registers, the OS can easily relocate processes to different parts of physical memory.

==有了基址和界限寄存器，操作系统可以轻松地将进程重定位到物理内存的不同部分。==

  

However, you might have noticed something interesting about these address spaces of ours: there is a big chunk of "free" space right in the middle, between the stack and the heap.

==然而，你可能已经注意到关于我们要讨论的这些地址空间的一些有趣之处：在栈和堆之间，正中间有一大块“空闲”空间。==

  

As you can imagine from Figure 16.1, although the space between the stack and heap is not being used by the process, it is still taking up physical memory when we relocate the entire address space somewhere in physical memory.

==你可以从图 16.1 中想象到，尽管栈和堆之间的空间没有被进程使用，但当我们把整个地址空间重定位到物理内存的某个地方时，它仍然占用了物理内存。==

  

Thus, the simple approach of using a base and bounds register pair to virtualize memory is wasteful.

==因此，简单地使用一对基址和界限寄存器来虚拟化内存是浪费的。==

  

It also makes it quite hard to run a program when the entire address space doesn't fit into memory.

==当整个地址空间无法装入内存时，这也使得运行程序变得非常困难。==

  

Thus, base and bounds is not as flexible as we would like. And thus:

==因此，基址和界限并不像我们希望的那样灵活。因此：==

  

THE CRUX: HOW TO SUPPORT A LARGE ADDRESS SPACE

==关键问题：如何支持大地址空间==

  

How do we support a large address space with (potentially) a lot of free space between the stack and the heap?

==我们如何支持一个在栈和堆之间（可能）有大量空闲空间的大地址空间？==

  

Note that in our examples, with tiny (pretend) address spaces, the waste doesn't seem too bad.

==请注意，在我们的例子中，使用微小的（假想的）地址空间，这种浪费看起来并不太严重。==

  

Imagine, however, a 32-bit address space (4 GB in size).

==然而，想象一个 32 位的地址空间（大小为 4 GB）。==

  

A typical program will only use megabytes of memory, but still would demand that the entire address space be resident in memory.

==一个典型的程序只会使用几兆字节的内存，但仍然会要求整个地址空间驻留在内存中。==

  

16.1 Segmentation: Generalized Base/Bounds

==16.1 分段：广义的基址/界限==

  

To solve this problem, an idea was born, and it is called segmentation.

==为了解决这个问题，诞生了一个想法，它被称为分段。==

  

It is quite an old idea, going at least as far back as the very early 1960's [H61, G62].

==这是一个相当古老的想法，至少可以追溯到 20 世纪 60 年代初 [H61, G62]。==

  

The idea is simple: instead of having just one base and bounds pair in our MMU, why not have a base and bounds pair per logical segment of the address space?

==这个想法很简单：与其在我们的 MMU 中只有一对基址和界限，为什么不为地址空间的每个逻辑段都配备一对基址和界限呢？==

  

A segment is just a contiguous portion of the address space of a particular length, and in our canonical address space, we have three logically-different segments: code, stack, and heap.

==段只是地址空间中特定长度的连续部分，在我们的标准地址空间中，我们有三个逻辑上不同的段：代码段、栈段和堆段。==

  

Figure 16.1: An Address Space (Again)

==图 16.1：地址空间（再次展示）==

  

What segmentation allows the OS to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space.

==分段允许操作系统做的是将这些段中的每一个放置在物理内存的不同部分，从而避免用未使用的虚拟地址空间填满物理内存。==

  

Let's look at an example. Assume we want to place the address space from Figure 16.1 into physical memory.

==让我们看一个例子。假设我们想把图 16.1 中的地址空间放入物理内存。==

  

With a base and bounds pair per segment, we can place each segment independently in physical memory.

==通过为每个段配备一对基址和界限，我们可以将每个段独立地放置在物理内存中。==

  

For example, see Figure 16.2 (page 3); there you see a 64KB physical memory with those three segments in it (and 16KB reserved for the OS).

==例如，见图 16.2（第 3 页）；在那里你可以看到一个 64KB 的物理内存，其中包含那三个段（以及为操作系统保留的 16KB）。==

  

Figure 16.2: Placing Segments In Physical Memory

==图 16.2：将段放置在物理内存中==

  

As you can see in the diagram, only used memory is allocated space in physical memory, and thus large address spaces with large amounts of unused address space (which we sometimes call sparse address spaces) can be accommodated.

==正如你在图中看到的，只有被使用的内存才会在物理内存中分配空间，因此可以容纳具有大量未使用地址空间（我们有时称之为稀疏地址空间）的大地址空间。==

  

The hardware structure in our MMU required to support segmentation is just what you'd expect: in this case, a set of three base and bounds register pairs.

==我们的 MMU 中支持分段所需的硬件结构正是你所期望的：在这种情况下，是一组三对基址和界限寄存器。==

  

Figure 16.3 below shows the register values for the example above.

==下面的图 16.3 显示了上述示例的寄存器值。==

  

Each bounds register holds the size of a segment.

==每个界限寄存器保存一个段的大小。==

  

Figure 16.3: Segment Register Values

==图 16.3：段寄存器值==

  

You can see from the figure that the code segment is placed at physical address 32KB and has a size of 2KB and the heap segment is placed at 34KB and has a size of 3KB.

==你可以从图中看到，代码段被放置在物理地址 32KB 处，大小为 2KB，堆段被放置在 34KB 处，大小为 3KB。==

  

The size segment here is exactly the same as the bounds register introduced previously.

==这里的段大小与之前介绍的界限寄存器完全相同。==

  

It tells the hardware exactly how many bytes are valid in this segment (and thus, enables the hardware to determine when a program has made an illegal access outside of those bounds).

==它确切地告诉硬件该段中有多少字节是有效的（因此，使硬件能够确定程序何时进行了超出这些界限的非法访问）。==

  

Let's do an example translation, using the address space in Figure 16.1.

==让我们使用图 16.1 中的地址空间做一个转换示例。==

  

Assume a reference is made to virtual address 100 (which is in the code segment, as you can see visually in Figure 16.1, page 2).

==假设引用了虚拟地址 100（正如你在第 2 页图 16.1 中直观看到的，它位于代码段中）。==

  

ASIDE: THE SEGMENTATION FAULT

==旁注：分段错误==

  

The term segmentation fault or violation arises from a memory access on a segmented machine to an illegal address.

==术语“分段错误”或“违规”源于分段机器上对非法地址的内存访问。==

  

Humorously, the term persists, even on machines with no support for segmentation at all.

==有趣的是，即使在完全不支持分段的机器上，这个术语依然存在。==

  

Or not so humorously, if you can't figure out why your code keeps faulting.

==或者不那么有趣的是，如果你搞不清楚为什么你的代码一直出错。==

  

When the reference takes place (say, on an instruction fetch), the hardware will add the base value to the offset into this segment (100 in this case) to arrive at the desired physical address: , or 32868.

==当引用发生时（比如，在取指令时），硬件会将基址值加到该段内的偏移量（本例中为 100）上，得出所需的物理地址：，即 32868。==

  

It will then check that the address is within bounds (100 is less than 2KB), find that it is, and issue the reference to physical memory address 32868.

==然后它会检查地址是否在界限内（100 小于 2KB），发现确实在界限内，于是向物理内存地址 32868 发出引用。==

  

Now let's look at an address in the heap, virtual address 4200 (again refer to Figure 16.1).

==现在让我们看看堆中的一个地址，虚拟地址 4200（再次参考图 16.1）。==

  

If we just add the virtual address 4200 to the base of the heap (34KB), we get a physical address of 39016, which is not the correct physical address.

==如果我们只是将虚拟地址 4200 加到堆的基址（34KB）上，我们会得到物理地址 39016，这不是正确的物理地址。==

  

What we need to first do is extract the offset into the heap, i.e., which byte(s) in this segment the address refers to.

==我们需要先做的是提取堆内的偏移量，即该地址指的是该段中的哪个（些）字节。==

  

Because the heap starts at virtual address 4KB (4096), the offset of 4200 is actually 4200 minus 4096, or 104.

==因为堆从虚拟地址 4KB（4096）开始，所以 4200 的偏移量实际上是 4200 减去 4096，即 104。==

  

We then take this offset (104) and add it to the base register physical address (34K) to get the desired result: 34920.

==然后我们要取这个偏移量（104）并将它加到基址寄存器的物理地址（34K）上，得到想要的结果：34920。==

  

What if we tried to refer to an illegal address (i.e., a virtual address of 7KB or greater), which is beyond the end of the heap?

==如果我们试图引用一个非法地址（即 7KB 或更大的虚拟地址），即超出了堆的末尾，会发生什么？==

  

You can imagine what will happen: the hardware detects that the address is out of bounds, traps into the OS, likely leading to the termination of the offending process.

==你可以想象会发生什么：硬件检测到地址越界，陷入操作系统，很可能导致违规进程被终止。==

  

And now you know the origin of the famous term that all C programmers learn to dread: the segmentation violation or segmentation fault.

==现在你知道了所有 C 程序员都学会恐惧的那个著名术语的起源：分段违规或分段错误。==

  

16.2 Which Segment Are We Referring To?

==16.2 我们引用的是哪个段？==

  

The hardware uses segment registers during translation.

==硬件在转换过程中使用段寄存器。==

  

How does it know the offset into a segment, and to which segment an address refers?

==它是如何知道段内的偏移量，以及一个地址引用的是哪个段的？==

  

One common approach, sometimes referred to as an explicit approach, is to chop up the address space into segments based on the top few bits of the virtual address.

==一种常见的方法，有时被称为显式方法，是根据虚拟地址的高几位将地址空间分割成段。==

  

This technique was used in the VAX/VMS system [LL82].

==VAX/VMS 系统 [LL82] 使用了这种技术。==

  

In our example above, we have three segments; thus we need two bits to accomplish our task.

==在上面的例子中，我们有三个段；因此我们需要两位来完成任务。==

  

If we use the top two bits of our 14-bit virtual address to select the segment, our virtual address looks like this:

==如果我们使用 14 位虚拟地址的前两位来选择段，我们的虚拟地址看起来是这样的：==

  

In our example, then, if the top two bits are 00, the hardware knows the virtual address is in the code segment, and thus uses the code base and bounds pair to relocate the address to the correct physical location.

==在我们的例子中，如果前两位是 00，硬件就知道虚拟地址在代码段中，因此使用代码基址和界限对将地址重定位到正确的物理位置。==

  

If the top two bits are 01, the hardware knows the address is in the heap, and thus uses the heap base and bounds.

==如果前两位是 01，硬件就知道地址在堆中，因此使用堆的基址和界限。==

  

Let's take our example heap virtual address from above (4200) and translate it, just to make sure this is clear.

==让我们拿上面的堆虚拟地址示例（4200）进行转换，以确保这一点很清楚。==

  

The virtual address 4200, in binary form, can be seen here: 01 0000 0110 1000.

==虚拟地址 4200 的二进制形式如下所示：01 0000 0110 1000。==

  

As you can see from the picture, the top two bits (01) tell the hardware which segment we are referring to.

==从图中可以看出，前两位（01）告诉硬件我们引用的是哪个段。==

  

The bottom 12 bits are the offset into the segment: 0000 0110 1000, or hex 0x068, or 104 in decimal.

==后 12 位是段内的偏移量：0000 0110 1000，或十六进制 0x068，或十进制 104。==

  

Thus, the hardware simply takes the first two bits to determine which segment register to use, and then takes the next 12 bits as the offset into the segment.

==因此，硬件只需取前两位来确定使用哪个段寄存器，然后取接下来的 12 位作为段内的偏移量。==

  

By adding the base register to the offset, the hardware arrives at the final physical address.

==通过将基址寄存器加到偏移量上，硬件得出了最终的物理地址。==

  

Note the offset eases the bounds check too: we can simply check if the offset is less than the bounds; if not, the address is illegal.

==注意，偏移量也简化了界限检查：我们可以简单地检查偏移量是否小于界限；如果不是，则地址非法。==

  

Thus, if base and bounds were arrays (with one entry per segment), the hardware would be doing something like this to obtain the desired physical address:

==因此，如果基址和界限是数组（每个段一个条目），硬件将执行类似以下的操作来获取所需的物理地址：==

  

Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT

Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT

  

Offset = VirtualAddress & OFFSET_MASK

Offset = VirtualAddress & OFFSET_MASK

  

if (Offset >= Bounds[Segment]) RaiseException(PROTECTION_FAULT)

if (Offset >= Bounds[Segment]) RaiseException(PROTECTION_FAULT)

  

else PhysAddr = Base[Segment] + Offset

else PhysAddr = Base[Segment] + Offset

  

Register = AccessMemory(PhysAddr)

Register = AccessMemory(PhysAddr)

  

In our running example, we can fill in values for the constants above.

==在我们正在运行的示例中，我们可以填入上述常数的值。==

  

Specifically, SEG_MASK would be set to 0x3000, SEG_SHIFT to 12, and OFFSET_MASK to 0xFFF.

==具体来说，SEG_MASK 将被设置为 0x3000，SEG_SHIFT 设置为 12，OFFSET_MASK 设置为 0xFFF。==

  

You may also have noticed that when we use the top two bits, and we only have three segments (code, heap, stack), one segment of the address space goes unused.

==你可能也注意到了，当我们使用前两位，而只有三个段（代码、堆、栈）时，地址空间的一个段未被使用。==

  

To fully utilize the virtual address space (and avoid an unused segment), some systems put code in the same segment as the heap and thus use only one bit to select which segment to use [LL82].

==为了充分利用虚拟地址空间（并避免未使用的段），有些系统将代码放在与堆相同的段中，因此只使用一位来选择使用哪个段 [LL82]。==

  

Another issue with using the top so many bits to select a segment is that it limits use of the virtual address space.

==使用前几位来选择段的另一个问题是，它限制了虚拟地址空间的使用。==

  

Specifically, each segment is limited to a maximum size, which in our example is 4KB (using the top two bits to choose segments implies the 16KB address space gets chopped into four pieces, or 4KB in this example).

==具体来说，每个段都被限制在最大尺寸内，在我们的例子中是 4KB（使用前两位选择段意味着 16KB 的地址空间被切成四块，本例中即为 4KB）。==

  

If a running program wishes to grow a segment (say the heap, or the stack) beyond that maximum, the program is out of luck.

==如果一个正在运行的程序希望将某个段（比如堆或栈）增长到超过该最大值，那它就没运气了。==

  

There are other ways for the hardware to determine which segment a particular address is in.

==还有其他方法可以让硬件确定特定地址位于哪个段中。==

  

In the implicit approach, the hardware determines the segment by noticing how the address was formed.

==在隐式方法中，硬件通过注意地址是如何形成的来确定段。==

  

If, for example, the address was generated from the program counter (i.e., it was an instruction fetch), then the address is within the code segment.

==例如，如果地址是从程序计数器生成的（即这是一次取指令），那么该地址就在代码段内。==

  

If the address is based off of the stack or base pointer, it must be in the stack segment.

==如果地址是基于栈指针或基址指针的，它一定在栈段内。==

  

Any other address must be in the heap.

==任何其他地址一定在堆中。==

  

16.3 What About The Stack?

==16.3 栈呢？==

  

Thus far, we've left out one important component of the address space: the stack.

==到目前为止，我们遗漏了地址空间的一个重要组成部分：栈。==

  

The stack has been relocated to physical address 28KB in the diagram above, but with one critical difference: it grows backwards (i.e., towards lower addresses).

==在上面的图中，栈已被重定位到物理地址 28KB，但有一个关键的区别：它是向后增长的（即向低地址方向增长）。==

  

In physical memory, it "starts" at  and grows back to 26KB, corresponding to virtual addresses 16KB to 14KB.

==在物理内存中，它“开始”于  并向后增长到 26KB，对应于虚拟地址 16KB 到 14KB。==

  

Translation must proceed differently.

==转换必须以不同的方式进行。==

  

The first thing we need is a little extra hardware support.

==我们首先需要的是一点额外的硬件支持。==

  

Instead of just base and bounds values, the hardware also needs to know which way the segment grows (a bit, for example, that is set to 1 when the segment grows in the positive direction, and 0 for negative).

==硬件不仅需要基址和界限值，还需要知道段的增长方向（例如，用一个位，当段向正方向增长时设为 1，负方向设为 0）。==

  

Our updated view of what the hardware tracks is seen in Figure 16.4.

==我们在图 16.4 中看到了硬件跟踪内容的更新视图。==

  

Figure 16.4: Segment Registers (With Negative-Growth Support)

==图 16.4：段寄存器（支持负向增长）==

  

With the hardware understanding that segments can grow in the negative direction, the hardware must now translate such virtual addresses slightly differently.

==随着硬件理解了段可以向负方向增长，硬件现在必须以略微不同的方式转换此类虚拟地址。==

  

Let's take an example stack virtual address and translate it to understand the process.

==让我们举一个栈虚拟地址的例子并进行转换，以理解这个过程。==

  

In this example, assume we wish to access virtual address 15KB, which should map to physical address 27KB.

==在这个例子中，假设我们希望访问虚拟地址 15KB，它应该映射到物理地址 27KB。==

  

Our virtual address, in binary form, thus looks like this: 11 1100 0000 0000 (hex 0x3C00).

==因此，我们的虚拟地址的二进制形式如下：11 1100 0000 0000（十六进制 0x3C00）。==

  

The hardware uses the top two bits (11) to designate the segment, but then we are left with an offset of 3KB.

==硬件使用前两位（11）来指定段，但我们剩下的偏移量是 3KB。==

  

To obtain the correct negative offset, we must subtract the maximum segment size from 3KB: in this example, a segment can be 4KB, and thus the correct negative offset is 3KB minus 4KB which equals -1KB.

==为了获得正确的负偏移量，我们必须从 3KB 中减去最大段大小：在本例中，一个段可以是 4KB，因此正确的负偏移量是 3KB 减去 4KB，等于 -1KB。==

  

We simply add the negative offset (-1KB) to the base (28KB) to arrive at the correct physical address: 27KB.

==我们只需将负偏移量（-1KB）加到基址（28KB）上，即可得出正确的物理地址：27KB。==

  

The bounds check can be calculated by ensuring the absolute value of the negative offset is less than or equal to the segment's current size (in this case, 2KB).

==可以通过确保负偏移量的绝对值小于或等于段的当前大小（本例中为 2KB）来计算界限检查。==

  

Although we say, for simplicity, that the stack "starts" at 28KB, this value is actually the byte just below the location of the backward growing region.

==虽然为了简单起见，我们说栈“开始”于 28KB，但这个值实际上是紧靠向后增长区域下方的字节。==

  

The first valid byte is actually 28KB minus 1.

==第一个有效字节实际上是 28KB 减 1。==

  

In contrast, forward-growing regions start at the address of the first byte of the segment.

==相比之下，向前增长的区域从段的第一个字节的地址开始。==

  

We take this approach because it makes the math to compute the physical address straightforward: the physical address is just the base plus the negative offset.

==我们采用这种方法是因为它使得计算物理地址的数学运算变得简单直观：物理地址就是基址加上负偏移量。==

  

16.4 Support for Sharing

==16.4 支持共享==

  

As support for segmentation grew, system designers soon realized that they could realize new types of efficiencies with a little more hardware support.

==随着对分段支持的增加，系统设计者很快意识到，只需增加一点硬件支持，他们就可以实现新型的效率。==

  

Specifically, to save memory, sometimes it is useful to share certain memory segments between address spaces.

==具体来说，为了节省内存，有时在地址空间之间共享某些内存段是很有用的。==

  

In particular, code sharing is common and still in use in systems today.

==特别是代码共享，这在当今的系统中仍然很常见并被使用。==

  

To support sharing, we need a little extra support from the hardware, in the form of protection bits.

==为了支持共享，我们需要硬件提供一点额外的支持，形式为保护位。==

  

Basic support adds a few bits per segment, indicating whether or not a program can read or write a segment, or perhaps execute code that lies within the segment.

==基本支持为每个段增加几个位，指示程序是否可以读取或写入某个段，或者是否可以执行位于该段内的代码。==

  

By setting a code segment to read-only, the same code can be shared across multiple processes, without worry of harming isolation.

==通过将代码段设置为只读，同一代码可以在多个进程之间共享，而无需担心破坏隔离性。==

  

While each process still thinks that it is accessing its own private memory, the OS is secretly sharing memory which cannot be modified by the process, and thus the illusion is preserved.

==虽然每个进程仍然认为它在访问自己的私有内存，但操作系统正在秘密地共享该进程无法修改的内存，从而保留了这种错觉。==

  

An example of the additional information tracked by the hardware (and OS) is shown in Figure 16.5.

==图 16.5 显示了硬件（和操作系统）跟踪的附加信息的一个示例。==

  

As you can see, the code segment is set to read and execute, and thus the same physical segment in memory could be mapped into multiple virtual address spaces.

==正如你所看到的，代码段被设置为读取和执行，因此内存中的同一个物理段可以映射到多个虚拟地址空间中。==

  

Figure 16.5: Segment Register Values (with Protection)

==图 16.5：段寄存器值（带保护）==

  

With protection bits, the hardware algorithm described earlier would also have to change.

==有了保护位，前面描述的硬件算法也必须改变。==

  

In addition to checking whether a virtual address is within bounds, the hardware also has to check whether a particular access is permissible.

==除了检查虚拟地址是否在界限内，硬件还必须检查特定的访问是否被允许。==

  

If a user process tries to write to a read-only segment, or execute from a non-executable segment, the hardware should raise an exception, and thus let the OS deal with the offending process.

==如果用户进程试图写入只读段，或从不可执行段执行代码，硬件应引发异常，从而让操作系统处理违规进程。==

  

16.5 Fine-grained vs. Coarse-grained Segmentation

==16.5 细粒度与粗粒度分段==

  

Most of our examples thus far have focused on systems with just a few segments (i.e., code, stack, heap).

==到目前为止，我们的大多数示例都集中在只有几个段（即代码、栈、堆）的系统上。==

  

We can think of this segmentation as coarse-grained, as it chops up the address space into relatively large, coarse chunks.

==我们可以将这种分段视为粗粒度的，因为它将地址空间切分成相对较大、粗糙的块。==

  

However, some early systems (e.g., Multics [CV65,DD68]) were more flexible and allowed for address spaces to consist of a large number of smaller segments, referred to as fine-grained segmentation.

==然而，一些早期系统（如 Multics [CV65,DD68]）更加灵活，允许地址空间由大量较小的段组成，这被称为细粒度分段。==

  

Supporting many segments requires even further hardware support, with a segment table of some kind stored in memory.

==支持许多段需要更进一步的硬件支持，即在内存中存储某种段表。==

  

Such segment tables usually support the creation of a very large number of segments, and thus enable a system to use segments in more flexible ways than we have thus far discussed.

==此类段表通常支持创建非常多的段，从而使系统能够以比我们目前讨论的更灵活的方式使用段。==

  

For example, early machines like the Burroughs B5000 had support for thousands of segments, and expected a compiler to chop code and data into separate segments which the OS and hardware would then support [RK68].

==例如，像 Burroughs B5000 这样的早期机器支持数千个段，并期望编译器将代码和数据切分成单独的段，然后由操作系统和硬件提供支持 [RK68]。==

  

The thinking at the time was that by having fine-grained segments, the OS could better learn about which segments are in use and which are not and thus utilize main memory more effectively.

==当时的想法是，通过拥有细粒度的段，操作系统可以更好地了解哪些段正在使用，哪些没有，从而更有效地利用主存。==

  

Figure 16.6: Non-compacted and Compacted Memory

==图 16.6：未压缩和压缩的内存==

  

16.6 OS Support

==16.6 操作系统支持==

  

You now should have a basic idea as to how segmentation works.

==你现在应该对分段的工作原理有了基本的了解。==

  

Pieces of the address space are relocated into physical memory as the system runs, and thus a huge savings of physical memory is achieved relative to our simpler approach with just a single base/bounds pair for the entire address space.

==地址空间的片段在系统运行时被重定位到物理内存中，因此相对于我们只为整个地址空间使用一对基址/界限的简单方法，实现了物理内存的巨大节省。==

  

Specifically, all the unused space between the stack and the heap need not be allocated in physical memory, allowing us to fit more address spaces into physical memory and support a large and sparse virtual address space per process.

==具体来说，栈和堆之间所有未使用的空间不需要在物理内存中分配，这使我们能够将更多的地址空间装入物理内存，并支持每个进程拥有一个巨大且稀疏的虚拟地址空间。==

  

However, segmentation raises a number of new issues for the operating system.

==然而，分段给操作系统带来了一些新问题。==

  

The first is an old one: what should the OS do on a context switch?

==第一个是一个老问题：在上下文切换时操作系统应该做什么？==

  

You should have a good guess by now: the segment registers must be saved and restored.

==你现在应该已经猜到了：必须保存和恢复段寄存器。==

  

Clearly, each process has its own virtual address space, and the OS must make sure to set up these registers correctly before letting the process run again.

==显然，每个进程都有自己的虚拟地址空间，操作系统必须确保在让进程再次运行之前正确设置这些寄存器。==

  

The second is OS interaction when segments grow (or perhaps shrink).

==第二个是当段增长（或者可能收缩）时的操作系统交互。==

  

For example, a program may call malloc() to allocate an object.

==例如，程序可能会调用 malloc() 来分配一个对象。==

  

In some cases, the existing heap will be able to service the request, and thus malloc() will find free space for the object and return a pointer to it to the caller.

==在某些情况下，现有的堆将能够满足请求，因此 malloc() 将为对象找到空闲空间并将指向它的指针返回给调用者。==

  

In others, however, the heap segment itself may need to grow.

==然而在其他情况下，堆段本身可能需要增长。==

  

In this case, the memory-allocation library will perform a system call to grow the heap (e.g., the traditional UNIX sbrk() system call).

==在这种情况下，内存分配库将执行系统调用以增长堆（例如，传统的 UNIX sbrk() 系统调用）。==

  

The OS will then (usually) provide more space, updating the segment size register to the new (bigger) size, and informing the library of success.

==然后操作系统（通常）将提供更多空间，将段大小寄存器更新为新的（更大的）大小，并通知库成功。==

  

The library can then allocate space for the new object and return successfully to the calling program.

==然后库可以为新对象分配空间并成功返回给调用程序。==

  

Do note that the OS could reject the request, if no more physical memory is available, or if it decides that the calling process already has too much.

==请注意，如果没有更多可用的物理内存，或者如果操作系统判定调用进程已经占用了太多内存，它可以拒绝该请求。==

  

The last, and perhaps most important, issue is managing free space in physical memory.

==最后一个，也许是最重要的问题，是管理物理内存中的空闲空间。==

  

When a new address space is created, the OS has to be able to find space in physical memory for its segments.

==当创建一个新的地址空间时，操作系统必须能够为其段在物理内存中找到空间。==

  

Previously, we assumed that each address space was the same size, and thus physical memory could be thought of as a bunch of slots where processes would fit in.

==以前，我们假设每个地址空间的大小相同，因此物理内存可以被视为一堆供进程放入的槽位。==

  

Now, we have a number of segments per process, and each segment might be a different size.

==现在，每个进程有多个段，并且每个段的大小可能不同。==

  

The general problem that arises is that physical memory quickly becomes full of little holes of free space, making it difficult to allocate new segments, or to grow existing ones.

==随之而来的一般问题是，物理内存很快就会充满小块的空闲空间，使得分配新段或增长现有段变得困难。==

  

We call this problem external fragmentation [R69]; see Figure 16.6 (left).

==我们将此问题称为外部碎片 [R69]；见图 16.6（左）。==

  

In the example, a process comes along and wishes to allocate a 20KB segment.

==在这个例子中，一个进程出现并希望分配一个 20KB 的段。==

  

In that example, there is 24KB free, but not in one contiguous segment (rather, in three non-contiguous chunks).

==在该示例中，有 24KB 的空闲空间，但不是在一个连续的段中（而是分成三个不连续的块）。==

  

Thus, the OS cannot satisfy the 20KB request.

==因此，操作系统无法满足 20KB 的请求。==

  

Similar problems could occur when a request to grow a segment arrives.

==当增长段的请求到达时，也可能发生类似的问题。==

  

If the next so many bytes of physical space are not available, the OS will have to reject the request, even though there may be free bytes available elsewhere in physical memory.

==如果接下来的若干字节物理空间不可用，操作系统将不得不拒绝该请求，即使物理内存的其他地方可能有可用的空闲字节。==

  

One solution to this problem would be to compact physical memory by rearranging the existing segments.

==解决此问题的一种方案是通过重新排列现有段来压缩物理内存。==

  

For example, the OS could stop whichever processes are running, copy their data to one contiguous region of memory, change their segment register values to point to the new physical locations, and thus have a large free extent of memory with which to work.

==例如，操作系统可以停止任何正在运行的进程，将其数据复制到一块连续的内存区域，更改其段寄存器值以指向新的物理位置，从而获得一大块空闲内存来进行工作。==

  

By doing so, the OS enables the new allocation request to succeed.

==通过这样做，操作系统使新的分配请求得以成功。==

  

However, compaction is expensive, as copying segments is memory-intensive and generally uses a fair amount of processor time; see Figure 16.6 (right) for a diagram of compacted physical memory.

==然而，压缩是昂贵的，因为复制段是内存密集型的，通常会占用大量的处理器时间；有关压缩后的物理内存图示，请参见图 16.6（右）。==

  

Compaction also (ironically) makes requests to grow existing segments hard to serve, and may thus cause further rearrangement to accommodate such requests.

==压缩也（讽刺地）使得增长现有段的请求难以被满足，因此可能导致进一步的重新排列以适应此类请求。==

  

A simpler approach might instead be to use a free-list management algorithm that tries to keep large extents of memory available for allocation.

==一种更简单的方法可能是使用空闲列表管理算法，试图保留大块的内存以供分配。==

  

There are literally hundreds of approaches that people have taken, including classic algorithms like best-fit (which keeps a list of free spaces and returns the one closest in size that satisfies the desired allocation to the requester), worst-fit, first-fit, and more complex schemes like the buddy algorithm [K68].

==人们实际上已经采取了数百种方法，包括经典算法如最佳适应（best-fit，它保留一个空闲空间列表，并返回大小最接近且满足所需分配的空间给请求者）、最差适应（worst-fit）、首次适应（first-fit），以及像伙伴算法（buddy algorithm）[K68] 这样更复杂的方案。==

  

An excellent survey by Wilson et al. is a good place to start if you want to learn more about such algorithms  or you can wait until we cover some of the basics in a later chapter.

==如果你想了解更多关于此类算法的信息，Wilson 等人的出色调查是一个很好的起点 ，或者你可以等到我们在后面的章节中介绍一些基础知识。==

  

Unfortunately, though, no matter how smart the algorithm, external fragmentation will still exist; a good algorithm attempts to minimize it.

==然而不幸的是，无论算法多么聪明，外部碎片仍然会存在；好的算法只是试图将其最小化。==

  

TIP: IF 1000 SOLUTIONS EXIST, NO GREAT ONE DOES

==提示：如果存在 1000 种解决方案，那就没有一个是完美的==

  

The fact that so many different algorithms exist to try to minimize external fragmentation is indicative of a stronger underlying truth: there is no one "best" way to solve the problem.

==存在这么多不同的算法试图最小化外部碎片，这一事实表明了一个更强烈的潜在真理：没有一种“最好”的方法来解决这个问题。==

  

Thus, we settle for something reasonable and hope it is good enough.

==因此，我们满足于某种合理的方案，并希望它足够好。==

  

The only real solution (as we will see in forthcoming chapters) is to avoid the problem altogether, by never allocating memory in variable-sized chunks.

==唯一的真正解决方案（正如我们将在后续章节中看到的）是完全避免这个问题，即永远不以可变大小的块来分配内存。==

  

16.7 Summary

==16.7 小结==

  

Segmentation solves a number of problems, and helps us build a more effective virtualization of memory.

==分段解决了许多问题，并帮助我们构建更有效的内存虚拟化。==

  

Beyond just dynamic relocation, segmentation can better support sparse address spaces, by avoiding the huge potential waste of memory between logical segments of the address space.

==除了动态重定位之外，分段通过避免地址空间逻辑段之间巨大的潜在内存浪费，可以更好地支持稀疏地址空间。==

  

It is also fast, as doing the arithmetic segmentation requires is easy and well-suited to hardware.

==它也很快，因为执行分段所需的算术运算很简单，非常适合硬件实现。==

  

The overheads of translation are minimal.

==转换的开销极小。==

  

A fringe benefit arises too: code sharing.

==还产生了一个附带的好处：代码共享。==

  

If code is placed within a separate segment, such a segment could potentially be shared across multiple running programs.

==如果代码被放置在一个单独的段中，这样的段可能在多个正在运行的程序之间共享。==

  

However, as we learned, allocating variable-sized segments in memory leads to some problems that we'd like to overcome.

==然而，正如我们所学到的，在内存中分配可变大小的段会导致一些我们希望克服的问题。==

  

The first, as discussed above, is external fragmentation.

==第一个问题，如上所述，是外部碎片。==

  

Because segments are variable-sized, free memory gets chopped up into odd-sized pieces, and thus satisfying a memory-allocation request can be difficult.

==因为段是可变大小的，空闲内存被切成大小不一的碎片，因此满足内存分配请求可能会很困难。==

  

One can try to use smart algorithms [W+95] or periodically compact memory, but the problem is fundamental and hard to avoid.

==人们可以尝试使用智能算法 [W+95] 或定期压缩内存，但这个问题是根本性的，很难避免。==

  

The second and perhaps more important problem is that segmentation still isn't flexible enough to support our fully generalized, sparse address space.

==第二个或许更重要的问题是，分段仍然不够灵活，无法支持我们需要完全通用的、稀疏的地址空间。==

  

For example, if we have a large but sparsely-used heap all in one logical segment, the entire heap must still reside in memory in order to be accessed.

==例如，如果我们有一个很大但使用稀疏的堆都在一个逻辑段中，整个堆仍然必须驻留在内存中才能被访问。==

  

In other words, if our model of how the address space is being used doesn't exactly match how the underlying segmentation has been designed to support it, segmentation doesn't work very well.

==换句话说，如果我们对地址空间使用方式的模型与底层分段设计所支持的方式不完全匹配，分段就不能很好地工作。==

  

We thus need to find some new solutions. Ready to find them?

==因此我们需要寻找一些新的解决方案。准备好去发现它们了吗？==

  

References

==参考文献==

  

[CV65] "Introduction and Overview of the Multics System" by F. J. Corbato, V. A. Vyssotsky. Fall Joint Computer Conference, 1965.

[CV65] "Introduction and Overview of the Multics System" by F. J. Corbato, V. A. Vyssotsky. Fall Joint Computer Conference, 1965.

  

One of five papers presented on Multics at the Fall Joint Computer Conference; oh to be a fly on the wall in that room that day!

==在秋季联合计算机会议上发表的关于 Multics 的五篇论文之一；真希望那天能在那间屋子里旁听啊！==

  

[DD68] "Virtual Memory, Processes, and Sharing in Multics" by Robert C. Daley and Jack B. Dennis. Communications of the ACM, Volume 11:5, May 1968.

[DD68] "Virtual Memory, Processes, and Sharing in Multics" by Robert C. Daley and Jack B. Dennis. Communications of the ACM, Volume 11:5, May 1968.

  

An early paper on how to perform dynamic linking in Multics, which was way ahead of its time.

==一篇关于如何在 Multics 中执行动态链接的早期论文，这在当时是遥遥领先的。==

  

Dynamic linking finally found its way back into systems about 20 years later, as the large X-windows libraries demanded it.

==动态链接最终在大约 20 年后回到了系统中，因为庞大的 X-windows 库需要它。==

  

Some say that these large X11 libraries were MIT's revenge for removing support for dynamic linking in early versions of UNIX!

==有人说，这些庞大的 X11 库是 MIT 对早期 UNIX 版本移除动态链接支持的报复！==

  

[G62] "Fact Segmentation" by M. N. Greenfield. Proceedings of the SJCC, Volume 21, May 1962.

[G62] "Fact Segmentation" by M. N. Greenfield. Proceedings of the SJCC, Volume 21, May 1962.

  

Another early paper on segmentation; so early that it has no references to other work.

==另一篇关于分段的早期论文；太早了，以至于没有引用其他工作。==

  

[H61] "Program Organization and Record Keeping for Dynamic Storage" by A. W. Holt. Communications of the ACM, Volume 4:10, October 1961.

[H61] "Program Organization and Record Keeping for Dynamic Storage" by A. W. Holt. Communications of the ACM, Volume 4:10, October 1961.

  

An incredibly early and difficult to read paper about segmentation and some of its uses.

==一篇极其早期且难以阅读的关于分段及其用途的论文。==

  

[I09] "Intel 64 and IA-32 Architectures Software Developer's Manuals" by Intel. 2009.

[I09] "Intel 64 and IA-32 Architectures Software Developer's Manuals" by Intel. 2009.

  

Try reading about segmentation in here (Chapter 3 in Volume 3a); it'll hurt your head, at least a little bit.

==试着在这里读读关于分段的内容（第 3a 卷第 3 章）；这会让你头疼，至少会有一点点。==

  

[K68] "The Art of Computer Programming: Volume I" by Donald Knuth. Addison-Wesley, 1968.

[K68] "The Art of Computer Programming: Volume I" by Donald Knuth. Addison-Wesley, 1968.

  

Knuth is famous not only for his early books on the Art of Computer Programming but for his typesetting system TeX which is still a powerhouse typesetting tool used by professionals today, and indeed to typeset this very book.

==Knuth 不仅因其早期的《计算机程序设计艺术》一书而闻名，还因其排版系统 TeX 而闻名，该系统至今仍是专业人士使用的强大排版工具，实际上也用于排版本书。==

  

His tomes on algorithms are a great early reference to many of the algorithms that underlie computing systems today.

==他的算法巨著是当今计算系统基础的许多算法的绝佳早期参考。==

  

[L83] "Hints for Computer Systems Design" by Butler Lampson. ACM Operating Systems Review, 15:5, October 1983.

[L83] "Hints for Computer Systems Design" by Butler Lampson. ACM Operating Systems Review, 15:5, October 1983.

  

A treasure-trove of sage advice on how to build systems.

==关于如何构建系统的明智建议的宝库。==

  

Hard to read in one sitting; take it in a little at a time, like a fine wine, or a reference manual.

==很难一次读完；像品尝美酒或查阅参考手册一样，一次读一点。==

  

[LL82] "Virtual Memory Management in the VAX/VMS Operating System" by Henry M. Levy, Peter H. Lipman. IEEE Computer, Volume 15:3, March 1982.

[LL82] "Virtual Memory Management in the VAX/VMS Operating System" by Henry M. Levy, Peter H. Lipman. IEEE Computer, Volume 15:3, March 1982.

  

A classic memory management system, with lots of common sense in its design.

==一个经典的内存管理系统，其设计中蕴含了许多常识。==

  

We'll study it in more detail in a later chapter.

==我们将在后面的章节中更详细地研究它。==

  

[RK68] "Dynamic Storage Allocation Systems" by B. Randell and C.J. Kuehner. Communications of the ACM, Volume 11:5, May 1968.

[RK68] "Dynamic Storage Allocation Systems" by B. Randell and C.J. Kuehner. Communications of the ACM, Volume 11:5, May 1968.

  

A nice overview of the differences between paging and segmentation, with some historical discussion of various machines.

==一篇很好的概述，介绍了分页和分段的区别，并对各种机器进行了一些历史性讨论。==

  

[R69] "A note on storage fragmentation and program segmentation" by Brian Randell. Communications of the ACM, Volume 12:7, July 1969.

[R69] "A note on storage fragmentation and program segmentation" by Brian Randell. Communications of the ACM, Volume 12:7, July 1969.

  

One of the earliest papers to discuss fragmentation.

==最早讨论碎片的论文之一。==

  

[W+95] "Dynamic Storage Allocation: A Survey and Critical Review" by Paul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles. International Workshop on Memory Management, Scotland, UK, September 1995.

[W+95] "Dynamic Storage Allocation: A Survey and Critical Review" by Paul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles. International Workshop on Memory Management, Scotland, UK, September 1995.

  

A great survey paper on memory allocators.

==一篇关于内存分配器的精彩综述论文。==

  

Homework (Simulation)

==作业（模拟）==

  

This program allows you to see how address translations are performed in a system with segmentation.

==该程序允许你查看在具有分段的系统中是如何执行地址转换的。==

  

See the README for details.

==详情请参阅 README。==

  

Questions

==问题==

  

1. First let's use a tiny address space to translate some addresses.

==2. 首先，让我们使用一个微小的地址空间来转换一些地址。==

  

Here's a simple set of parameters with a few different random seeds; can you translate the addresses?

==这里有一组简单的参数和几个不同的随机种子；你能转换这些地址吗？==

  

2. Now, let's see if we understand this tiny address space we've constructed (using the parameters from the question above).

==3. 现在，让我们看看我们是否理解了我们构建的这个微小地址空间（使用上面问题中的参数）。==

  

What is the highest legal virtual address in segment 0?

==段 0 中最高的合法虚拟地址是多少？==

  

What about the lowest legal virtual address in segment 1?

==段 1 中最低的合法虚拟地址呢？==

  

What are the lowest and highest illegal addresses in this entire address space?

==这整个地址空间中最低和最高的非法地址是多少？==

  

Finally, how would you run segmentation.py with the -A flag to test if you are right?

==最后，你应该如何使用 -A 标志运行 segmentation.py 来测试你是否正确？==

  

3. Let's say we have a tiny 16-byte address space in a 128-byte physical memory.

==4. 假设我们在 128 字节的物理内存中有一个 16 字节的微小地址空间。==

  

What base and bounds would you set up so as to get the simulator to generate the following translation results for the specified address stream: valid, valid, violation, ..., violation, valid, valid?

==你会设置什么样的基址和界限，以便让模拟器为指定的地址流生成以下转换结果：有效、有效、违规、......、违规、有效、有效？==

  

4. Assume we want to generate a problem where roughly 90% of the randomly-generated virtual addresses are valid (not segmentation violations).

==5. 假设我们想生成一个问题，其中大约 90% 的随机生成的虚拟地址是有效的（没有分段违规）。==

  

How should you configure the simulator to do so?

==你应该如何配置模拟器来做到这一点？==

  

Which parameters are important to getting this outcome?

==哪些参数对获得此结果很重要？==

  

5. Can you run the simulator such that no virtual addresses are valid? How?

==6. 你能运行模拟器使得没有虚拟地址是有效的吗？如何做到？==

  

Free-Space Management

==空闲空间管理==

  

In this chapter, we take a small detour from our discussion of virtualizing memory to discuss a fundamental aspect of any memory management system, whether it be a malloc library (managing pages of a process's heap) or the OS itself (managing portions of the address space of a process).

==在本章中，我们在讨论虚拟化内存的过程中稍作偏离，转而讨论任何内存管理系统的一个基本方面，无论是 malloc 库（管理进程堆的页面）还是操作系统本身（管理进程地址空间的部分）。==

  

Specifically, we will discuss the issues surrounding free-space management.

==具体来说，我们将讨论围绕空闲空间管理的问题。==

  

Let us make the problem more specific.

==让我们把问题具体化。==

  

Managing free space can certainly be easy, as we will see when we discuss the concept of paging.

==管理空闲空间当然可以很容易，正如我们在讨论分页概念时将看到的那样。==

  

It is easy when the space you are managing is divided into fixed-sized units.

==当你管理的每个空间被划分为固定大小的单元时，这很容易。==

  

In such a case, you just keep a list of these fixed-sized units; when a client requests one of them, return the first entry.

==在这种情况下，你只需要保留这些固定大小单元的列表；当客户端请求其中一个时，返回第一个条目。==

  

Where free-space management becomes more difficult (and interesting) is when the free space you are managing consists of variable-sized units.

==当你要管理的空闲空间由可变大小的单元组成时，空闲空间管理就变得更加困难（也更有趣）。==

  

This arises in a user-level memory-allocation library (as in malloc() and free()) and in an OS managing physical memory when using segmentation to implement virtual memory.

==这出现在用户级内存分配库（如 malloc() 和 free()）中，以及在使用分段实现虚拟内存时管理物理内存的操作系统中。==

  

In either case, the problem that exists is known as external fragmentation: the free space gets chopped into little pieces of different sizes and is thus fragmented.

==在任何一种情况下，存在的问题被称为外部碎片：空闲空间被切成不同大小的小块，从而变得支离破碎。==

  

Subsequent requests may fail because there is no single contiguous space that can satisfy the request, even though the total amount of free space exceeds the size of the request.

==随后的请求可能会失败，因为没有单个连续的空间可以满足请求，即使空闲空间的总量超过了请求的大小。==

  

The figure shows an example of this problem.

==图示展示了这个问题的一个例子。==

  

In this case, the total free space available is 20 bytes; unfortunately, it is fragmented into two chunks of size 10 each.

==在这种情况下，可用的总空闲空间是 20 字节；不幸的是，它被分割成两个各 10 字节的块。==

  

As a result, a request for 15 bytes will fail even though there are 20 bytes free.

==结果，即使有 20 字节空闲，15 字节的请求也会失败。==

  

And thus we arrive at the problem addressed in this chapter.

==因此，我们引出了本章要解决的问题。==

  

CRUX: HOW TO MANAGE FREE SPACE

==关键问题：如何管理空闲空间==

  

How should free space be managed, when satisfying variable-sized requests?

==在满足可变大小的请求时，应如何管理空闲空间？==

  

What strategies can be used to minimize fragmentation?

==可以使用什么策略来最小化碎片？==

  

What are the time and space overheads of alternate approaches?

==替代方案的时间和空间开销是多少？==

  

17.1 Assumptions

==17.1 假设==

  

Most of this discussion will focus on the great history of allocators found in user-level memory-allocation libraries.

==大部分讨论将集中在用户级内存分配库中发现的分配器的伟大历史上。==

  

We draw on Wilson's excellent survey [W+95] but encourage interested readers to go to the source document itself for more details¹.

==我们借鉴了 Wilson 的精彩调查 [W+95]，但鼓励感兴趣的读者去查阅源文档以获取更多细节¹。==

  

1 It is nearly 80 pages long; thus, you really have to be interested!

==脚注 1：它将近 80 页长；因此，你真的必须很感兴趣！==

  

We assume a basic interface such as that provided by malloc() and free().

==我们假设有一个基本的接口，如 malloc() 和 free() 提供的接口。==

  

Specifically, void malloc(size_t size) takes a single parameter, size, which is the number of bytes requested by the application.

==具体来说，void malloc(size_t size) 接受一个参数 size，即应用程序请求的字节数。==

  

It hands back a pointer (of no particular type, or a void pointer in C lingo) to a region of that size (or greater).

==它返回一个指向该大小（或更大）区域的指针（没有特定类型，或者用 C 语言术语说是 void 指针）。==

  

The complementary routine void free(void *ptr) takes a pointer and frees the corresponding chunk.

==互补的例程 void free(void *ptr) 接受一个指针并释放相应的块。==

  

Note the implication of the interface: the user, when freeing the space, does not inform the library of its size.

==注意该接口的含义：用户在释放空间时，不通知库它的大小。==

  

Thus, the library must be able to figure out how big a chunk of memory is when handed just a pointer to it.

==因此，库必须能够在其仅被传递一个指针时，弄清楚一块内存有多大。==

  

We'll discuss how to do this a bit later on in the chapter.

==稍后我们将在本章讨论如何做到这一点。==

  

The space that this library manages is known historically as the heap, and the generic data structure used to manage free space in the heap is some kind of free list.

==该库管理的空间在历史上被称为堆，用于管理堆中空闲空间的通用数据结构是某种空闲列表。==

  

This structure contains references to all of the free chunks of space in the managed region of memory.

==该结构包含对受管内存区域中所有空闲空间块的引用。==

  

Of course, this data structure need not be a list per se, but just some kind of data structure to track free space.

==当然，这个数据结构本身不必是一个列表，而只是某种用于跟踪空闲空间的数据结构。==

  

We further assume that primarily we are concerned with external fragmentation, as described above.

==我们进一步假设，如上所述，我们主要关注外部碎片。==

  

Allocators could of course also have the problem of internal fragmentation.

==分配器当然也可能存在内部碎片的问题。==

  

If an allocator hands out chunks of memory bigger than that requested, any unasked for (and thus unused) space in such a chunk is considered internal fragmentation (because the waste occurs inside the allocated unit) and is another example of space waste.

==如果分配器分发的内存块大于请求的内存块，那么该块中任何未被请求（因此未被使用）的空间都被视为内部碎片（因为浪费发生在分配单元内部），这是空间浪费的另一个例子。==

  

However, for the sake of simplicity, and because it is the more interesting of the two types of fragmentation, we'll mostly focus on external fragmentation.

==然而，为了简单起见，也因为它是两种碎片类型中更有趣的一种，我们将主要关注外部碎片。==

  

We'll also assume that once memory is handed out to a client, it cannot be relocated to another location in memory.

==我们还假设，一旦内存被分发给客户端，它就不能被重定位到内存中的另一个位置。==

  

For example, if a program calls malloc() and is given a pointer to some space within the heap, that memory region is essentially "owned" by the program (and cannot be moved by the library) until the program returns it via a corresponding call to free().

==例如，如果程序调用 malloc() 并获得指向堆内某个空间的指针，则该内存区域本质上由程序“拥有”（并且库不能移动它），直到程序通过相应的 free() 调用将其归还。==

  

Thus, no compaction of free space is possible, which would be useful to combat fragmentation².

==因此，不可能对空闲空间进行压缩，而压缩本可用于对抗碎片²。==

  

2 Once you hand a pointer to a chunk of memory to a C program, it is generally difficult to determine all references (pointers) to that region, which may be stored in other variables or even in registers at a given point in execution.

==脚注 2：一旦你将指向内存块的指针传递给 C 程序，通常很难确定对该区域的所有引用（指针），这些引用可能存储在其他变量中，甚至在执行的某个时刻存储在寄存器中。==

  

This may not be the case in more strongly-typed, garbage-collected languages, which would thus enable compaction as a technique to combat fragmentation.

==在更强类型、有垃圾回收的语言中情况可能并非如此，这使得压缩成为一种对抗碎片的技术。==

  

Compaction could, however, be used in the OS to deal with fragmentation when implementing segmentation (as discussed in said chapter on segmentation).

==然而，在实现分段时（如在分段一章中所讨论的），操作系统可以使用压缩来处理碎片。==

  

Finally, we'll assume that the allocator manages a contiguous region of bytes.

==最后，我们假设分配器管理一个连续的字节区域。==

  

In some cases, an allocator could ask for that region to grow.

==在某些情况下，分配器可以要求该区域增长。==

  

For example, a user-level memory-allocation library might call into the kernel to grow the heap (via a system call such as sbrk) when it runs out of space.

==例如，当用户级内存分配库空间耗尽时，它可能会调用内核来增加堆（通过系统调用，如 sbrk）。==

  

However, for simplicity, we'll just assume that the region is a single fixed size throughout its life.

==然而，为了简单起见，我们仅假设该区域在其整个生命周期中都是单一的固定大小。==

  

17.2 Low-level Mechanisms

==17.2 低级机制==

  

Before delving into some policy details, we'll first cover some common mechanisms used in most allocators.

==在深入研究一些策略细节之前，我们将首先介绍大多数分配器中使用的一些常见机制。==

  

First, we'll discuss the basics of splitting and coalescing, common techniques in most any allocator.

==首先，我们将讨论分割和合并的基础知识，这是大多数分配器中的常用技术。==

  

Second, we'll show how one can track the size of allocated regions quickly and with relative ease.

==其次，我们将展示如何快速且相对轻松地跟踪已分配区域的大小。==

  

Finally, we'll discuss how to build a simple list inside the free space to keep track of what is free and what isn't.

==最后，我们将讨论如何在空闲空间内建立一个简单的列表，以跟踪哪些是空闲的，哪些不是。==

  

Splitting and Coalescing

==分割与合并==

  

A free list contains a set of elements that describe the free space still remaining in the heap.

==空闲列表包含一组描述堆中剩余空闲空间的元素。==

  

Thus, assume the following 30-byte heap:

==因此，假设有以下 30 字节的堆：==

  

The free list for this heap would have two elements on it.

==该堆的空闲列表将有两个元素。==

  

One entry describes the first 10-byte free segment (bytes 0-9), and one entry describes the other free segment (bytes 20-29).

==一个条目描述第一个 10 字节的空闲段（字节 0-9），另一个条目描述另一个空闲段（字节 20-29）。==

  

head -> addr:0 len:10 -> addr:20 len:10 -> NULL

==头 -> 地址:0 长度:10 -> 地址:20 长度:10 -> NULL==

  

As described above, a request for anything greater than 10 bytes will fail (returning NULL); there just isn't a single contiguous chunk of memory of that size available.

==如上所述，任何大于 10 字节的请求都将失败（返回 NULL）；因为根本没有那么大的单个连续内存块可用。==

  

A request for exactly that size (10 bytes) could be satisfied easily by either of the free chunks.

==正好该大小（10 字节）的请求可以很容易地由任一空闲块满足。==

  

But what happens if the request is for something smaller than 10 bytes?

==但是如果请求小于 10 字节会发生什么？==

  

Assume we have a request for just a single byte of memory.

==假设我们只请求一个字节的内存。==

  

In this case, the allocator will perform an action known as splitting: it will find a free chunk of memory that can satisfy the request and split it into two.

==在这种情况下，分配器将执行一种称为分割的操作：它将找到一个可以满足请求的空闲内存块，并将其分成两部分。==


FREE-SPACE MANAGEMENT

==空闲空间管理==

  

a free chunk of memory that can satisfy the request and split it into two.

==一个能满足请求的空闲内存块，并将其分割为两部分。==

  

The first chunk it will return to the caller; the second chunk will remain on the list.

==第一块将返回给调用者；第二块将保留在列表中。==

  

Thus, in our example above, if a request for 1 byte were made, and the allocator decided to use the second of the two elements on the list to satisfy the request, the call to malloc() would return 20 (the address of the 1-byte allocated region) and the list would end up looking like this:

==因此，在上文的示例中，如果有一个 1 字节的请求，且分配器决定使用列表中的第二个元素来满足该请求，那么对 malloc() 的调用将返回 20（即 1 字节已分配区域的地址），列表最终将变成这样：==

  

In the picture, you can see the list basically stays intact;

==在图中，你可以看到列表基本保持原样；==

  

the only change is that the free region now starts at 21 instead of 20, and the length of that free region is now just 9.

==唯一的变化是，空闲区域现在从 21 开始，而不是 20，且该空闲区域的长度现在仅为 9。==

  

Thus, the split is commonly used in allocators when requests are smaller than the size of any particular free chunk.

==因此，当请求的大小小于任何特定空闲块的大小时，分配器通常使用“分割”机制。==

  

A corollary mechanism found in many allocators is known as coalescing of free space.

==许多分配器中存在的一个推论机制被称为空闲空间的合并。==

  

Take our example from above once more (free 10 bytes, used 10 bytes, and another free 10 bytes).

==再次使用上面的例子（空闲 10 字节，已用 10 字节，以及另一个空闲 10 字节）。==

  

Given this (tiny) heap, what happens when an application calls free(10), thus returning the space in the middle of the heap?

==给定这个（微小的）堆，当应用程序调用 free(10) 从而归还堆中间的空间时，会发生什么？==

  

If we simply add this free space back into our list without too much thinking, we might end up with a list that looks like this:

==如果我们不假思索地简单将此空闲空间加回列表，我们可能会得到一个如下所示的列表：==

  

Note the problem: while the entire heap is now free, it is seemingly divided into three chunks of 10 bytes each.

==注意这个问题：虽然整个堆现在都是空闲的，但它似乎被分割成了三个各为 10 字节的块。==

  

Thus, if a user requests 20 bytes, a simple list traversal will not find such a free chunk, and return failure.

==因此，如果用户请求 20 字节，简单的列表遍历将找不到这样的空闲块，并返回失败。==

  

What allocators do in order to avoid this problem is coalesce free space when a chunk of memory is freed.

==为了避免这个问题，分配器所做的是在释放内存块时合并空闲空间。==

  

The idea is simple: when returning a free chunk in memory, look carefully at the addresses of the chunk you are returning as well as the nearby chunks of free space;

==思路很简单：当归还内存中的一个空闲块时，仔细查看你正在归还的块的地址以及附近的空闲空间块；==

  

if the newly-freed space sits right next to one (or two, as in this example) existing free chunks, merge them into a single larger free chunk.

==如果新释放的空间紧邻一个（或两个，如本例所示）现有的空闲块，则将它们合并为一个更大的空闲块。==

  

Thus, with coalescing, our final list should look like this:

==因此，经过合并，我们最终的列表应该如下所示：==

  

Indeed, this is what the heap list looked like at first, before any allocations were made.

==确实，这正是堆列表起初的样子，在进行任何分配之前。==

  

With coalescing, an allocator can better ensure that large free extents are available for the application.

==通过合并，分配器可以更好地确保应用程序有可用的连续大块空闲空间。==

  

This discussion assumes that there are no headers, an unrealistic but simplifying assumption we make for now.

==此讨论假设没有头部信息（headers），这是一个不切实际但为了简化我们目前讨论的假设。==

  

Tracking The Size Of Allocated Regions

==跟踪已分配区域的大小==

  

You might have noticed that the interface to free (void *ptr) does not take a size parameter;

==你可能已经注意到，free (void *ptr) 接口不接受大小参数；==

  

thus it is assumed that given a pointer, the malloc library can quickly determine the size of the region of memory being freed and thus incorporate the space back into the free list.

==因此可以假设，给定一个指针，malloc 库可以快速确定正在释放的内存区域的大小，从而将该空间并回空闲列表。==

  

To accomplish this task, most allocators store a little bit of extra information in a header block which is kept in memory, usually just before the handed-out chunk of memory.

==为了完成这项任务，大多数分配器在内存中保留的头部块（header block）里存储少量额外信息，该头部块通常刚好位于分发出的内存块之前。==

  

Let's look at an example again.

==让我们再看一个例子。==

  

In this example, we are examining an allocated block of size 20 bytes, pointed to by ptr;

==在这个例子中，我们要检查一个由 ptr 指向的大小为 20 字节的已分配块；==

  

imagine the user called malloc() and stored the results in ptr, e.g., ptr = malloc(20);.

==想象一下用户调用了 malloc() 并将结果存储在 ptr 中，例如 ptr = malloc(20);。==

  

The header minimally contains the size of the allocated region (in this case, 20);

==头部至少包含已分配区域的大小（在本例中为 20）；==

  

it may also contain additional pointers to speed up deallocation, a magic number to provide additional integrity checking, and other information.

==它还可能包含用于加速释放过程的额外指针、用于提供额外完整性检查的幻数（magic number）以及其他信息。==

  

Let's assume a simple header which contains the size of the region and a magic number, like this:

==让我们假设一个简单的头部，它包含区域的大小和一个幻数，如下所示：==

  

When the user calls free (ptr), the library then uses simple pointer arithmetic to figure out where the header begins:

==当用户调用 free (ptr) 时，库便使用简单的指针算术运算来计算头部的起始位置：==

  

After obtaining such a pointer to the header, the library can easily determine whether the magic number matches the expected value as a sanity check (assert (hptr->magic == 1234567)) and calculate the total size of the newly-freed region via simple math (i.e., adding the size of the header to size of the region).

==在获得指向头部的指针后，库可以轻松地确定幻数是否与预期值匹配以作为完整性检查（assert (hptr->magic == 1234567)），并通过简单的数学运算（即，将头部的大小加上区域的大小）计算新释放区域的总大小。==

  

Note the small but critical detail in the last sentence: the size of the free region is the size of the header plus the size of the space allocated to the user.

==注意最后一句中微小但关键的细节：空闲区域的大小是头部的大小加上分配给用户的空间大小。==

  

Thus, when a user requests N bytes of memory, the library does not search for a free chunk of size N;

==因此，当用户请求 N 字节的内存时，库不会搜索大小为 N 的空闲块；==

  

rather, it searches for a free chunk of size N plus the size of the header.

==相反，它搜索的是大小为 N 加上头部大小的空闲块。==

  

Embedding A Free List

==嵌入空闲列表==

  

Thus far we have treated our simple free list as a conceptual entity;

==到目前为止，我们将简单的空闲列表视为一个概念实体；==

  

it is just a list describing the free chunks of memory in the heap.

==它只是一个描述堆中空闲内存块的列表。==

  

But how do we build such a list inside the free space itself?

==但是，我们如何在空闲空间本身内部构建这样一个列表呢？==

  

In a more typical list, when allocating a new node, you would just call malloc() when you need space for the node.

==在一个更典型的列表中，当分配一个新节点时，你会在需要节点空间时直接调用 malloc()。==

  

Unfortunately, within the memory-allocation library, you can't do this!

==不幸的是，在内存分配库内部，你不能这样做！==

  

Instead, you need to build the list inside the free space itself.

==相反，你需要在空闲空间内部构建该列表。==

  

Don't worry if this sounds a little weird; it is, but not so weird that you can't do it!

==如果这听起来有点奇怪，别担心；它确实有点怪，但还没怪到让你无法做到的地步！==

  

Assume we have a 4096-byte chunk of memory to manage (i.e., the heap is 4KB).

==假设我们要管理一个 4096 字节的内存块（即堆大小为 4KB）。==

  

To manage this as a free list, we first have to initialize said list;

==为了将其作为空闲列表进行管理，我们首先必须初始化该列表；==

  

initially, the list should have one entry, of size 4096 (minus the header size).

==最初，列表中应该有一个条目，大小为 4096（减去头部大小）。==

  

Now let's look at some code that initializes the heap and puts the first element of the free list inside that space.

==现在让我们看一些代码，这些代码用于初始化堆并将空闲列表的第一个元素放入该空间内。==

  

We are assuming that the heap is built within some free space acquired via a call to the system call mmap();

==我们假设堆是建立在通过调用系统调用 mmap() 获取的空闲空间内的；==

  

this is not the only way to build such a heap but serves us well in this example.

==这不是构建此类堆的唯一方法，但在本例中很适用。==

  

After running this code, the status of the list is that it has a single entry, of size 4088.

==运行此代码后，列表的状态是它只有一个条目，大小为 4088。==

  

Yes, this is a tiny heap, but it serves as a fine example for us here.

==是的，这是一个微小的堆，但它在这里作为一个很好的例子为我们服务。==

  

The head pointer contains the beginning address of this range;

==头指针包含此范围的起始地址；==

  

let's assume it is 16KB (though any virtual address would be fine).

==让我们假设它是 16KB（尽管任何虚拟地址都可以）。==

  

Visually, the heap thus looks like what you see in Figure 17.3.

==直观地说，堆看起来如图 17.3 所示。==

  

Now, let's imagine that a chunk of memory is requested, say of size 100 bytes.

==现在，让我们想象有一个内存块请求，比如大小为 100 字节。==

  

To service this request, the library will first find a chunk that is large enough to accommodate the request;

==为了满足此请求，库将首先找到一个足以容纳该请求的块；==

  

because there is only one free chunk (size: 4088), this chunk will be chosen.

==因为只有一个空闲块（大小：4088），所以将选择该块。==

  

Then, the chunk will be split into two: one chunk big enough to service the request (and header, as described above), and the remaining free chunk.

==然后，该块将被分成两部分：一块大到足以满足请求（以及头部，如上所述），以及剩余的空闲块。==

  

Assuming an 8-byte header (an integer size and an integer magic number), the space in the heap now looks like what you see in Figure 17.4.

==假设有一个 8 字节的头部（一个整数大小和一个整数幻数），堆中的空间现在看起来如图 17.4 所示。==

  

Thus, upon the request for 100 bytes, the library allocated 108 bytes out of the existing one free chunk, returns a pointer (marked ptr in the figure above) to it, stashes the header information immediately before the allocated space for later use upon free (), and shrinks the one free node in the list to 3980 bytes (4088 minus 108).

==因此，在请求 100 字节时，库从现有的一个空闲块中分配了 108 字节，返回指向它的指针（在上图中标记为 ptr），将头部信息存储在已分配空间之前以便稍后在 free() 时使用，并将列表中的那个空闲节点缩小为 3980 字节（4088 减去 108）。==

  

Now let's look at the heap when there are three allocated regions, each of 100 bytes (or 108 including the header).

==现在让我们看看当有三个已分配区域时的堆，每个区域为 100 字节（或包含头部为 108 字节）。==

  

A visualization of this heap is shown in Figure 17.5.

==此堆的可视化如图 17.5 所示。==

  

As you can see therein, the first 324 bytes of the heap are now allocated, and thus we see three headers in that space as well as three 100-byte regions being used by the calling program.

==正如你在其中看到的，堆的前 324 个字节现在已分配，因此我们在该空间中看到了三个头部，以及正在被调用程序使用的三个 100 字节区域。==

  

The free list remains uninteresting: just a single node (pointed to by head), but now only 3764 bytes in size after the three splits.

==空闲列表仍然没什么特别：只有一个节点（由 head 指向），但在三次分割后大小仅为 3764 字节。==

  

But what happens when the calling program returns some memory via free ()?

==但是当调用程序通过 free() 归还一些内存时会发生什么？==

  

In this example, the application returns the middle chunk of allocated memory, by calling free (16500) (the value 16500 is arrived upon by adding the start of the memory region, 16384, to the 108 of the previous chunk and the 8 bytes of the header for this chunk).

==在此示例中，应用程序通过调用 free(16500) 归还已分配内存的中间块（值 16500 是通过将内存区域的起始位置 16384 加上前一个块的 108 和此块的 8 字节头部得出的）。==

  

The library immediately figures out the size of the free region, and then adds the free chunk back onto the free list.

==库立即计算出空闲区域的大小，然后将空闲块加回空闲列表。==

  

Assuming we insert at the head of the free list, the space now looks like this (Figure 17.6).

==假设我们在空闲列表的头部插入，空间现在看起来像这样（图 17.6）。==

  

Now we have a list that starts with a small free chunk (100 bytes, pointed to by the head of the list) and a large free chunk (3764 bytes).

==现在我们有了一个列表，它以一个小空闲块（100 字节，由列表的 head 指向）开始，后面跟着一个大空闲块（3764 字节）。==

  

Our list finally has more than one element on it!

==我们的列表终于包含不止一个元素了！==

  

And yes, the free space is fragmented, an unfortunate but common occurrence.

==是的，空闲空间被碎片化了，这是一种不幸但常见的情况。==

  

One last example: let's assume now that the last two in-use chunks are freed.

==最后一个例子：让我们假设现在最后两个使用中的块被释放了。==

  

Without coalescing, you end up with fragmentation (Figure 17.7).

==如果不进行合并，你最终会得到碎片（图 17.7）。==

  

As you can see from the figure, we now have a big mess!

==从图中可以看出，我们现在遇到大麻烦了！==

  

Why?

==为什么？==

  

Simple, we forgot to coalesce the list.

==很简单，我们忘记合并列表了。==

  

Although all of the memory is free, it is chopped up into pieces, thus appearing as a fragmented memory despite not being one.

==虽然所有的内存都是空闲的，但它被切成了碎片，因此尽管它本质上不是碎片化的，但表现得像是碎片化内存。==

  

The solution is simple: go through the list and merge neighboring chunks;

==解决方案很简单：遍历列表并合并相邻的块；==

  

when finished, the heap will be whole again.

==完成后，堆将再次变回一个整体。==

  

Growing The Heap

==扩展堆==

  

We should discuss one last mechanism found within many allocation libraries.

==我们应该讨论许多分配库中存在的最后一个机制。==

  

Specifically, what should you do if the heap runs out of space?

==具体来说，如果堆空间耗尽，你应该怎么做？==

  

The simplest approach is just to fail.

==最简单的方法就是直接失败。==

  

In some cases this is the only option, and thus returning NULL is an honorable approach.

==在某些情况下，这是唯一的选择，因此返回 NULL 是一个可敬的做法。==

  

Don't feel bad!

==别难过！==

  

You tried, and though you failed, you fought the good fight.

==你尝试了，虽然失败了，但你已经尽力了。==

  

Most traditional allocators start with a small-sized heap and then request more memory from the OS when they run out.

==大多数传统的分配器从一个小规模的堆开始，然后在耗尽时向操作系统请求更多内存。==

  

Typically, this means they make some kind of system call (e.g., sbrk in most UNIX systems) to grow the heap, and then allocate the new chunks from there.

==通常，这意味着它们进行某种系统调用（例如大多数 UNIX 系统中的 sbrk）来扩展堆，然后从那里分配新的块。==

  

To service the sbrk request, the OS finds free physical pages, maps them into the address space of the requesting process, and then returns the value of the end of the new heap;

==为了服务 sbrk 请求，操作系统找到空闲的物理页面，将它们映射到请求进程的地址空间中，然后返回新堆的末尾值；==

  

at that point, a larger heap is available, and the request can be successfully serviced.

==此时，一个更大的堆可用了，请求可以被成功满足。==

  

17.3 Basic Strategies

==17.3 基本策略==

  

Now that we have some machinery under our belt, let's go over some basic strategies for managing free space.

==现在我们已经掌握了一些机制，让我们复习一些管理空闲空间的基本策略。==

  

These approaches are mostly based on pretty simple policies that you could think up yourself;

==这些方法大多基于你自己就能想出来的非常简单的策略；==

  

try it before reading and see if you come up with all of the alternatives (or maybe some new ones!).

==在阅读之前试一试，看看你是否能想出所有的替代方案（或者也许是一些新的方案！）。==

  

The ideal allocator is both fast and minimizes fragmentation.

==理想的分配器既快速又能最大限度地减少碎片。==

  

Unfortunately, because the stream of allocation and free requests can be arbitrary (after all, they are determined by the programmer), any particular strategy can do quite badly given the wrong set of inputs.

==不幸的是，由于分配和释放请求流可能是任意的（毕竟，它们是由程序员决定的），任何特定的策略在错误的输入集下都可能表现得很差。==

  

Thus, we will not describe a "best" approach, but rather talk about some basics and discuss their pros and cons.

==因此，我们将不描述“最佳”方法，而是谈论一些基础知识并讨论它们的优缺点。==

  

Best Fit

==最佳适应 (Best Fit)==

  

The best fit strategy is quite simple: first, search through the free list and find chunks of free memory that are as big or bigger than the requested size.

==最佳适应策略非常简单：首先，搜索空闲列表并找到大小等于或大于请求大小的空闲内存块。==

  

Then, return the one that is the smallest in that group of candidates;

==然后，返回该候选组中最小的一个；==

  

this is the so called best-fit chunk (it could be called smallest fit too).

==这就是所谓的最佳适应块（也可以称为最小适应）。==

  

One pass through the free list is enough to find the correct block to return.

==遍历一遍空闲列表就足以找到要返回的正确块。==

  

The intuition behind best fit is simple: by returning a block that is close to what the user asks, best fit tries to reduce wasted space.

==最佳适应背后的直觉很简单：通过返回接近用户要求的块，最佳适应试图减少浪费的空间。==

  

However, there is a cost; naive implementations pay a heavy performance penalty when performing an exhaustive search for the correct free block.

==然而，这是有代价的；朴素的实现在执行穷举搜索以寻找正确空闲块时会付出沉重的性能代价。==

  

Worst Fit

==最差适应 (Worst Fit)==

  

The worst fit approach is the opposite of best fit; find the largest chunk and return the requested amount;

==最差适应方法与最佳适应相反；找到最大的块并返回请求的数量；==

  

keep the remaining (large) chunk on the free list.

==将剩余的（大）块保留在空闲列表中。==

  

Worst fit tries to thus leave big chunks free instead of lots of small chunks that can arise from a best-fit approach.

==最差适应试图以此保留大的空闲块，而不是像最佳适应方法那样产生许多小块。==

  

Once again, however, a full search of free space is required, and thus this approach can be costly.

==然而，这再一次需要对空闲空间进行全面搜索，因此这种方法的代价可能很高。==

  

Worse, most studies show that it performs badly, leading to excess fragmentation while still having high overheads.

==更糟糕的是，大多数研究表明它表现不佳，导致过多的碎片，同时仍具有很高的开销。==

  

First Fit

==首次适应 (First Fit)==

  

The first fit method simply finds the first block that is big enough and returns the requested amount to the user.

==首次适应方法只是简单地找到第一个足够大的块，并将请求的数量返回给用户。==

  

As before, the remaining free space is kept free for subsequent requests.

==如前所述，剩余的空闲空间保留为空闲状态以供后续请求使用。==

  

First fit has the advantage of speed no exhaustive search of all the free spaces are necessary - but sometimes pollutes the beginning of the free list with small objects.

==首次适应具有速度优势——不需要对所有空闲空间进行详尽搜索——但有时会用小对象污染空闲列表的开头。==

  

Thus, how the allocator manages the free list's order becomes an issue.

==因此，分配器如何管理空闲列表的顺序成为一个问题。==

  

One approach is to use address-based ordering;

==一种方法是使用基于地址的排序；==

  

by keeping the list ordered by the address of the free space, coalescing becomes easier, and fragmentation tends to be reduced.

==通过保持列表按空闲空间的地址排序，合并变得更容易，碎片化也倾向于减少。==

  

Next Fit

==下次适应 (Next Fit)==

  

Instead of always beginning the first-fit search at the beginning of the list, the next fit algorithm keeps an extra pointer to the location within the list where one was looking last.

==下次适应算法不总是从列表的开头开始首次适应搜索，而是保留一个额外的指针，指向上次查找结束的列表位置。==

  

The idea is to spread the searches for free space throughout the list more uniformly, thus avoiding splintering of the beginning of the list.

==其想法是将对空闲空间的搜索更均匀地分布在整个列表中，从而避免列表开头的碎片化。==

  

The performance of such an approach is quite similar to first fit, as an exhaustive search is once again avoided.

==这种方法的性能与首次适应非常相似，因为再次避免了穷举搜索。==

  

Examples

==示例==

  

Here are a few examples of the above strategies.

==以下是上述策略的几个示例。==

  

Envision a free list with three elements on it, of sizes 10, 30, and 20 (we'll ignore headers and other details here, instead just focusing on how strategies operate):

==设想一个空闲列表，上面有三个元素，大小分别为 10、30 和 20（我们这里忽略头部和其他细节，仅关注策略如何运作）：==

  

Assume an allocation request of size 15.

==假设有一个大小为 15 的分配请求。==

  

A best-fit approach would search the entire list and find that 20 was the best fit, as it is the smallest free space that can accommodate the request.

==最佳适应方法将搜索整个列表，并发现 20 是最佳匹配，因为它是能容纳请求的最小空闲空间。==

  

As happens in this example, and often happens with a best-fit approach, a small free chunk is now left over.

==正如在这个例子中发生的，以及在最佳适应方法中经常发生的那样，现在留下了一个小的空闲块。==

  

A worst-fit approach is similar but instead finds the largest chunk, in this example 30.

==最差适应方法类似，但它是寻找最大的块，在本例中为 30。==

  

The first-fit strategy, in this example, does the same thing as worst-fit, also finding the first free block that can satisfy the request.

==在此示例中，首次适应策略与最差适应做同样的事情，也是找到第一个能满足请求的空闲块。==

  

The difference is in the search cost; both best-fit and worst-fit look through the entire list;

==区别在于搜索成本；最佳适应和最差适应都要浏览整个列表；==

  

first-fit only examines free chunks until it finds one that fits, thus reducing search cost.

==首次适应仅检查空闲块直到找到一个合适的块，从而降低了搜索成本。==

  

These examples just scratch the surface of allocation policies.

==这些例子仅仅触及了分配策略的皮毛。==

  

More detailed analysis with real workloads and more complex allocator behaviors (e.g., coalescing) are required for a deeper understanding.

==需要对真实工作负载和更复杂的分配器行为（例如合并）进行更详细的分析，才能有更深入的理解。==

  

17.4 Other Approaches

==17.4 其他方法==

  

Beyond the basic approaches described above, there have been a host of suggested techniques and algorithms to improve memory allocation in some way.

==除了上述基本方法外，还有许多建议的技术和算法可以在某些方面改进内存分配。==

  

We list a few of them here for your consideration (i.e., to make you think about a little more than just best-fit allocation).

==我们在这里列出其中一些供你参考（即，让你不仅仅思考最佳适应分配）。==

  

Segregated Lists

==分离空闲列表 (Segregated Lists)==

  

One interesting approach that has been around for some time is the use of segregated lists.

==一个已经存在一段时间的有趣方法是使用分离空闲列表。==

  

The basic idea is simple: if a particular application has one (or a few) popular-sized request that it makes, keep a separate list just to manage objects of that size;

==基本思想很简单：如果特定应用程序有一个（或几个）它经常请求的特定大小，就保留一个单独的列表来专门管理该大小的对象；==

  

all other requests are forwarded to a more general memory allocator.

==所有其他请求都转发给更通用的内存分配器。==

  

The benefits of such an approach are obvious.

==这种方法的好处是显而易见的。==

  

By having a chunk of memory dedicated for one particular size of requests, fragmentation is much less of a concern;

==通过拥有一块专用于特定大小请求的内存，碎片化就不再是一个大问题；==

  

moreover, allocation and free requests can be served quite quickly when they are of the right size, as no complicated search of a list is required.

==此外，当分配和释放请求的大小合适时，可以非常快速地提供服务，因为不需要对列表进行复杂的搜索。==

  

Just like any good idea, this approach introduces new complications into a system as well.

==就像任何好主意一样，这种方法也给系统引入了新的复杂性。==

  

For example, how much memory should one dedicate to the pool of memory that serves specialized requests of a given size, as opposed to the general pool?

==例如，应该将多少内存专门用于服务给定大小的特殊请求的内存池，而不是通用池？==

  

One particular allocator, the slab allocator by uber-engineer Jeff Bonwick (which was designed for use in the Solaris kernel), handles this issue in a rather nice way.

==由超级工程师 Jeff Bonwick 开发的一种特定分配器——Slab 分配器（专为在 Solaris 内核中使用而设计），以一种相当不错的方式处理了这个问题。==

  

Specifically, when the kernel boots up, it allocates a number of object caches for kernel objects that are likely to be requested frequently (such as locks, file-system inodes, etc.);

==具体来说，当内核启动时，它为可能被频繁请求的内核对象（如锁、文件系统 inode 等）分配许多对象缓存；==

  

the object caches thus are each segregated free lists of a given size and serve memory allocation and free requests quickly.

==这些对象缓存因此是各自特定大小的分离空闲列表，并能快速服务内存分配和释放请求。==

  

When a given cache is running low on free space, it requests some slabs of memory from a more general memory allocator (the total amount requested being a multiple of the page size and the object in question).

==当给定的缓存空闲空间不足时，它会向更通用的内存分配器请求一些内存 slab（请求的总量是页面大小和相关对象的倍数）。==

  

Conversely, when the reference counts of the objects within a given slab all go to zero, the general allocator can reclaim them from the specialized allocator, which is often done when the VM system needs more memory.

==相反，当给定 slab 内对象的引用计数全都变为零时，通用分配器可以从专用分配器回收它们，这通常在虚拟机系统需要更多内存时进行。==

  

The slab allocator also goes beyond most segregated list approaches by keeping free objects on the lists in a pre-initialized state.

==Slab 分配器还超越了大多数分离列表方法，因为它将列表上的空闲对象保持在预初始化状态。==

  

Bonwick shows that initialization and destruction of data structures is costly;

==Bonwick 表明，数据结构的初始化和销毁是昂贵的；==

  

by keeping freed objects in a particular list in their initialized state, the slab allocator thus avoids frequent initialization and destruction cycles per object and thus lowers overheads noticeably.

==通过将已释放的对象保持在特定列表中的初始化状态，Slab 分配器从而避免了每个对象的频繁初始化和销毁周期，从而显著降低了开销。==

  

Buddy Allocation

==伙伴分配 (Buddy Allocation)==

  

Because coalescing is critical for an allocator, some approaches have been designed around making coalescing simple.

==由于合并对于分配器至关重要，因此一些方法被设计为使合并变得简单。==

  

One good example is found in the binary buddy allocator.

==二进制伙伴分配器就是一个很好的例子。==

  

In such a system, free memory is first conceptually thought of as one big space of size .

==在这样的系统中，空闲内存首先在概念上被视为一个大小为  的大空间。==

  

When a request for memory is made, the search for free space recursively divides free space by two until a block that is big enough to accommodate the request is found (and a further split into two would result in a space that is too small).

==当发出内存请求时，对空闲空间的搜索会递归地将空闲空间一分为二，直到找到一个大到足以容纳请求的块（并且进一步一分为二会导致空间太小）。==

  

At this point, the requested block is returned to the user.

==此时，请求的块被返回给用户。==

  

Here is an example of a 64KB free space getting divided in the search for a 7KB block (Figure 17.8, page 15).

==这是一个 64KB 空闲空间在搜索 7KB 块时被分割的示例（图 17.8，第 15 页）。==

  

In the example, the leftmost 8KB block is allocated (as indicated by the darker shade of gray) and returned to the user;

==在示例中，最左边的 8KB 块被分配（如较深的灰色所示）并返回给用户；==

  

note that this scheme can suffer from internal fragmentation, as you are only allowed to give out power-of-two-sized blocks.

==请注意，此方案可能会遭受内部碎片的影响，因为你只被允许分发 2 的幂次大小的块。==

  

The beauty of buddy allocation is found in what happens when that block is freed.

==伙伴分配的妙处在于当该块被释放时发生的事情。==

  

When returning the 8KB block to the free list, the allocator checks whether the "buddy" 8KB is free;

==当将 8KB 块归还给空闲列表时，分配器会检查“伙伴” 8KB 块是否空闲；==

  

if so, it coalesces the two blocks into a 16KB block.

==如果是，它将这两个块合并为一个 16KB 块。==

  

The allocator then checks if the buddy of the 16KB block is still free;

==然后分配器检查 16KB 块的伙伴是否仍然空闲；==

  

if so, it coalesces those two blocks.

==如果是，它将这两个块合并。==

  

This recursive coalescing process continues up the tree, either restoring the entire free space or stopping when a buddy is found to be in use.

==这个递归合并过程继续沿树向上进行，要么恢复整个空闲空间，要么在发现伙伴正在使用时停止。==

  

The reason buddy allocation works so well is that it is simple to determine the buddy of a particular block.

==伙伴分配之所以如此有效，是因为确定特定块的伙伴很简单。==

  

How, you ask? Think about the addresses of the blocks in the free space above.

==你可能会问，怎么做到的？想想上面空闲空间中块的地址。==

  

If you think carefully enough, you'll see that the address of each buddy pair only differs by a single bit;

==如果你仔细思考，你会发现每对伙伴的地址仅相差一位；==

  

which bit is determined by the level in the buddy tree.

==具体是哪一位由伙伴树中的层级决定。==

  

And thus you have a basic idea of how binary buddy allocation schemes work.

==因此，你已经基本了解了二进制伙伴分配方案是如何工作的。==

  

Other Ideas

==其他想法==

  

One major problem with many of the approaches described above is their lack of scaling.

==上述许多方法的一个主要问题是它们缺乏扩展性。==

  

Specifically, searching lists can be quite slow.

==具体来说，搜索列表可能非常慢。==

  

Thus, advanced allocators use more complex data structures to address these costs, trading simplicity for performance.

==因此，高级分配器使用更复杂的数据结构来解决这些成本问题，以牺牲简单性换取性能。==

  

Examples include balanced binary trees, splay trees, or partially-ordered trees.

==例子包括平衡二叉树、伸展树或偏序树。==

  

Given that modern systems often have multiple processors and run multi-threaded workloads (something you'll learn about in great detail in the section of the book on Concurrency), it is not surprising that a lot of effort has been spent making allocators work well on multiprocessor-based systems.

==鉴于现代系统通常拥有多个处理器并运行多线程工作负载（你将在本书关于并发的部分详细了解这一点），人们花费大量精力使分配器在基于多处理器的系统上良好运行也就不足为奇了。==

  

These are but two of the thousands of ideas people have had over time about memory allocators;

==这只是人们随着时间的推移关于内存分配器的数千个想法中的两个；==

  

read on your own if you are curious.

==如果你好奇，可以自己阅读。==

  

Failing that, read about how the glibc allocator works, to give you a sense of what the real world is like.

==如果不想读那些，可以读读 glibc 分配器是如何工作的，以便对现实世界的情况有所了解。==

  

17.5 Summary

==17.5 总结==

  

In this chapter, we've discussed the most rudimentary forms of memory allocators.

==在本章中，我们讨论了最基本的内存分配器形式。==

  

Such allocators exist everywhere, linked into every C program you write, as well as in the underlying OS which is managing memory for its own data structures.

==此类分配器无处不在，链接到你编写的每个 C 程序中，以及存在于为自身数据结构管理内存的底层操作系统中。==

  

As with many systems, there are many trade-offs to be made in building such a system, and the more you know about the exact workload presented to an allocator, the more you could do to tune it to work better for that workload.

==与许多系统一样，在构建此类系统时需要进行许多权衡，而且你对呈现给分配器的确切工作负载了解得越多，你就越能调整它以更好地在该工作负载下工作。==

  

Making a fast, space-efficient, scalable allocator that works well for a broad range of workloads remains an on-going challenge in modern computer systems.

==制作一个快速、节省空间、可扩展且能很好地适用于广泛工作负载的分配器，仍然是现代计算机系统中一个持续存在的挑战。==

  

Paging: Introduction

==分页：介绍==

  

It is sometimes said that the operating system takes one of two approaches when solving most any space-management problem.

==有时人们会说，操作系统在解决几乎所有空间管理问题时都会采取两种方法之一。==

  

The first approach is to chop things up into variable-sized pieces, as we saw with segmentation in virtual memory.

==第一种方法是将事物切分成大小可变的块，就像我们在虚拟内存的分段中看到的那样。==

  

Unfortunately, this solution has inherent difficulties.

==不幸的是，这种解决方案有其固有的困难。==

  

In particular, when dividing a space into different-size chunks, the space itself can become fragmented, and thus allocation becomes more challenging over time.

==特别是，当将空间划分为不同大小的块时，空间本身可能会变得碎片化，因此随着时间的推移，分配变得更具挑战性。==

  

Thus, it may be worth considering the second approach: to chop up space into fixed-sized pieces.

==因此，值得考虑第二种方法：将空间切分成固定大小的块。==

  

In virtual memory, we call this idea paging, and it goes back to an early and important system, the Atlas.

==在虚拟内存中，我们将这种想法称为分页，它追溯到一个早期且重要的系统——Atlas。==

  

Instead of splitting up a process's address space into some number of variable-sized logical segments (e.g., code, heap, stack), we divide it into fixed-sized units, each of which we call a page.

==我们不再将进程的地址空间分割成若干个大小可变的逻辑段（例如代码、堆、栈），而是将其划分为固定大小的单元，每个单元我们称为一个页（page）。==

  

Correspondingly, we view physical memory as an array of fixed-sized slots called page frames;

==相应地，我们将物理内存视为固定大小槽位的数组，称为页帧（page frames）；==

  

each of these frames can contain a single virtual-memory page.

==这些帧中的每一个都可以包含一个虚拟内存页。==

  

Our challenge:

==我们的挑战：==

  

THE CRUX: HOW TO VIRTUALIZE MEMORY WITH PAGES

==关键问题：如何用页来虚拟化内存==

  

How can we virtualize memory with pages, so as to avoid the problems of segmentation?

==我们要如何利用页来虚拟化内存，从而避免分段的问题？==

  

What are the basic techniques?

==基本技术有哪些？==

  

How do we make those techniques work well, with minimal space and time overheads?

==我们如何使这些技术良好运作，并将空间和时间开销降至最低？==

  

18.1 A Simple Example And Overview

==18.1 一个简单的例子和概述==

  

To help make this approach more clear, let's illustrate it with a simple example.

==为了帮助理清这种方法，让我们用一个简单的例子来说明。==

  

Figure 18.1 (page 2) presents an example of a tiny address space, only 64 bytes total in size, with four 16-byte pages (virtual pages 0, 1, 2, and 3).

==图 18.1（第 2 页）展示了一个微小地址空间的例子，总大小仅为 64 字节，包含四个 16 字节的页（虚拟页 0、1、2 和 3）。==

  

Real address spaces are much bigger, of course, commonly 32 bits and thus 4-GB of address space, or even .

==当然，真实的地址空间要大得多，通常是 32 位，因此有 4GB 的地址空间，甚至是 64 位。==

  

In the book, we'll often use tiny examples to make them easier to digest.

==在本书中，我们将经常使用微小的例子以便于消化。==

  

A 64-bit address space is hard to imagine, it is so amazingly large.

==64 位地址空间很难想象，它大得惊人。==

  

An analogy might help: if you think of a 32-bit address space as the size of a tennis court, a 64-bit address space is about the size of Europe(!).

==打个比方可能会有帮助：如果你将 32 位地址空间想象成网球场的大小，那么 64 位地址空间大约有欧洲那么大（！）。==

  

Physical memory, as shown in Figure 18.2, also consists of a number of fixed-sized slots, in this case eight page frames (making for a 128-byte physical memory, also ridiculously small).

==如图 18.2 所示，物理内存也由许多固定大小的槽位组成，在这个例子中是 8 个页帧（构成 128 字节的物理内存，同样小得可笑）。==

  

As you can see in the diagram, the pages of the virtual address space have been placed at different locations throughout physical memory;

==正如你在图中所见，虚拟地址空间的页被放置在物理内存的不同位置；==

  

the diagram also shows the OS using some of physical memory for itself.

==该图还显示操作系统将部分物理内存用于其自身。==

  

Paging, as we will see, has a number of advantages over our previous approaches.

==正如我们将看到的，分页比我们之前的方法具有许多优势。==

  

Probably the most important improvement will be flexibility: with a fully-developed paging approach, the system will be able to support the abstraction of an address space effectively, regardless of how a process uses the address space;

==最重要的改进可能是灵活性：通过完善的分页方法，系统将能够有效地支持地址空间的抽象，无论进程如何使用地址空间；==

  

we won't, for example, make assumptions about the direction the heap and stack grow and how they are used.

==例如，我们不会假设堆和栈的增长方向以及它们的使用方式。==

  

Another advantage is the simplicity of free-space management that paging affords.

==另一个优点是分页提供的空闲空间管理的简单性。==

  

For example, when the OS wishes to place our tiny 64-byte address space into our eight-page physical memory, it simply finds four free pages;

==例如，当操作系统希望将我们微小的 64 字节地址空间放入八个页面的物理内存中时，它只需找到四个空闲页面；==

  

perhaps the OS keeps a free list of all free pages for this, and just grabs the first four free pages off of this list.

==也许操作系统为此保留了一个所有空闲页面的空闲列表，并直接从该列表中抓取前四个空闲页面。==

  

In the example, the OS has placed virtual page 0 of the address space (AS) in physical frame 3, virtual page 1 of the AS in physical frame 7, page 2 in frame 5, and page 3 in frame 2.

==在示例中，操作系统将地址空间（AS）的虚拟页 0 放置在物理帧 3 中，AS 的虚拟页 1 放置在物理帧 7 中，页 2 在帧 5 中，页 3 在帧 2 中。==

  

Page frames 1, 4, and 6 are currently free.

==页帧 1、4 和 6 目前是空闲的。==

  

To record where each virtual page of the address space is placed in physical memory, the operating system usually keeps a per-process data structure known as a page table.

==为了记录地址空间的每个虚拟页放置在物理内存的哪个位置，操作系统通常保留一个称为页表的每个进程独有的数据结构。==

  

The major role of the page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides.

==页表的主要作用是存储地址空间中每个虚拟页的地址转换，从而让我们知道每个页驻留在物理内存的何处。==

  

For our simple example (Figure 18.2, page 2), the page table would thus have the following four entries: (Virtual Page 0 -> Physical Frame 3), (VP 1 -> PF 7), (VP 2 -> PF 5), and (VP 3 -> PF 2).

==对于我们的简单示例（图 18.2，第 2 页），页表将因此具有以下四个条目：（虚拟页 0 -> 物理帧 3），（VP 1 -> PF 7），（VP 2 -> PF 5），和（VP 3 -> PF 2）。==

  

It is important to remember that this page table is a per-process data structure (most page table structures we discuss are per-process structures; an exception we'll touch on is the inverted page table).

==重要的是要记住，这个页表是一个每个进程独有的数据结构（我们讨论的大多数页表结构都是每个进程的结构；我们将触及的一个例外是反向页表）。==

  

If another process were to run in our example above, the OS would have to manage a different page table for it, as its virtual pages obviously map to different physical pages (modulo any sharing going on).

==如果在我们上面的例子中运行另一个进程，操作系统将不得不为它管理一个不同的页表，因为它的虚拟页显然映射到不同的物理页（排除任何正在进行的共享）。==

  

Now, we know enough to perform an address-translation example.

==现在，我们已经了解足够多的知识来执行一个地址转换示例。==

  

Let's imagine the process with that tiny address space (64 bytes) is performing a memory access:

==让我们想象那个拥有微小地址空间（64 字节）的进程正在执行一次内存访问：==

  

movl <virtual address>, %eax

movl <virtual address>, %eax

  

Specifically, let's pay attention to the explicit load of the data from address <virtual address> into the register eax (and thus ignore the instruction fetch that must have happened prior).

==具体来说，让我们关注从地址 <virtual address> 将数据显式加载到寄存器 eax 的过程（从而忽略之前必然发生的指令获取）。==

  

To translate this virtual address that the process generated, we have to first split it into two components: the virtual page number (VPN), and the offset within the page.

==为了转换进程生成的这个虚拟地址，我们必须首先将其拆分为两个部分：虚拟页号 (VPN) 和页内偏移量。==

  

For this example, because the virtual address space of the process is 64 bytes, we need 6 bits total for our virtual address ().

==对于这个例子，因为进程的虚拟地址空间是 64 字节，我们的虚拟地址总共需要 6 位（）。==

  

Thus, our virtual address can be conceptualized as follows:

==因此，我们的虚拟地址可以概念化如下：==

  

In this diagram, Va5 is the highest-order bit of the virtual address, and Va0 the lowest-order bit.

==在此图中，Va5 是虚拟地址的最高位，Va0 是最低位。==

  

Because we know the page size (16 bytes), we can further divide the virtual address as follows:

==因为我们知道页面大小（16 字节），我们可以进一步划分虚拟地址如下：==

  

The page size is 16 bytes in a 64-byte address space;

==在 64 字节的地址空间中，页面大小为 16 字节；==

  

thus we need to be able to select 4 pages, and the top 2 bits of the address do just that.

==因此我们需要能够选择 4 个页面，而地址的高 2 位正好可以做到这一点。==

  

Thus, we have a 2-bit virtual page number (VPN).

==因此，我们有一个 2 位的虚拟页号（VPN）。==

  

The remaining bits tell us which byte of the page we are interested in, 4 bits in this case;

==剩余的位告诉我们要访问页面中的哪个字节，在本例中为 4 位；==

  

we call this the offset.

==我们称之为偏移量。==

  

When a process generates a virtual address, the OS and hardware must combine to translate it into a meaningful physical address.

==当进程生成虚拟地址时，操作系统和硬件必须结合起来将其转换为有意义的物理地址。==

  

For example, let us assume the load above was to virtual address 21:

==例如，让我们假设上面的加载是针对虚拟地址 21：==

  

movl 21, %eax

movl 21, %eax

  

Turning "21" into binary form, we get "010101", and thus we can examine this virtual address and see how it breaks down into a virtual page number (VPN) and offset:

==将“21”转换为二进制形式，我们得到“010101”，因此我们可以检查这个虚拟地址，看看它是如何分解为虚拟页号 (VPN) 和偏移量的：==

  

Thus, the virtual address "21" is on the 5th ("0101"th) byte of virtual page "01" (or 1).

==因此，虚拟地址“21”位于虚拟页“01”（或 1）的第 5 个（“0101”）字节。==

  

With our virtual page number, we can now index our page table and find which physical frame virtual page 1 resides within.

==有了虚拟页号，我们现在可以索引页表，找到虚拟页 1 驻留在哪个物理帧中。==

  

In the page table above the physical frame number (PFN) (also sometimes called the physical page number or PPN) is 7 (binary 111).

==在上方的页表中，物理帧号 (PFN)（有时也称为物理页号或 PPN）为 7（二进制 111）。==

  

Thus, we can translate this virtual address by replacing the VPN with the PFN and then issue the load to physical memory (Figure 18.3).

==因此，我们可以通过用 PFN 替换 VPN 来转换此虚拟地址，然后向物理内存发出加载指令（图 18.3）。==

  

Note the offset stays the same (i.e., it is not translated), because the offset just tells us which byte within the page we want.

==注意偏移量保持不变（即，它不被转换），因为偏移量只是告诉我们想要页面内的哪个字节。==

  

Our final physical address is 1110101 (117 in decimal), and is exactly where we want our load to fetch data from (Figure 18.2, page 2).

==我们要的最终物理地址是 1110101（十进制为 117），这正是我们希望加载指令从中获取数据的位置（图 18.2，第 2 页）。==

  

With this basic overview in mind, we can now ask (and hopefully, answer) a few basic questions you may have about paging.

==牢记这一基本概述，我们现在可以提出（并希望能回答）一些你可能对分页有的基本问题。==

  

For example, where are these page tables stored?

==例如，这些页表存储在哪里？==

  

What are the typical contents of the page table, and how big are the tables?

==页表的典型内容是什么，表有多大？==

  

Does paging make the system (too) slow?

==分页会让系统（太）慢吗？==

  

These and other beguiling questions are answered, at least in part, in the text below.

==这些以及其他令人着迷的问题将在下面的文本中得到部分回答。==

  

Read on!

==继续阅读！==

  

18.2 Where Are Page Tables Stored?

==18.2 页表存在哪里？==

  

Page tables can get terribly large, much bigger than the small segment table or base/bounds pair we have discussed previously.

==页表可能会变得非常大，比我们之前讨论的小段表或基址/界限对大得多。==

  

For example, imagine a typical 32-bit address space, with 4KB pages.

==例如，想象一个典型的 32 位地址空间，拥有 4KB 的页面。==

  

This virtual address splits into a 20-bit VPN and 12-bit offset (recall that 10 bits would be needed for a 1KB page size, and just add two more to get to 4KB).

==此虚拟地址分为 20 位 VPN 和 12 位偏移量（回想一下，1KB 页面大小需要 10 位，只需再加两位即可达到 4KB）。==

  

A 20-bit VPN implies that there are  translations that the OS would have to manage for each process (that's roughly a million);

==20 位 VPN 意味着操作系统必须为每个进程管理  个转换（大约一百万个）；==

  

assuming we need 4 bytes per page table entry (PTE) to hold the physical translation plus any other useful stuff, we get an immense 4MB of memory needed for each page table!

==假设我们需要每个页表项 (PTE) 4 个字节来保存物理转换以及任何其他有用的东西，那么每个页表就需要巨大的 4MB 内存！==

  

That is pretty large.

==这相当大。==

  

Now imagine there are 100 processes running: this means the OS would need 400MB of memory just for all those address translations!

==现在想象有 100 个进程在运行：这意味着操作系统仅为了所有这些地址转换就需要 400MB 的内存！==

  

Even in the modern era, where...

==即使在现代，...==

PAGING: INTRODUCTION

==分页：介绍==

  

ASIDE: DATA STRUCTURE - THE PAGE TABLE

==补充说明：数据结构——页表==

  

One of the most important data structures in the memory management subsystem of a modern OS is the page table.

==现代操作系统内存管理子系统中最重要的数据结构之一就是页表。==

  

In general, a page table stores virtual-to-physical address translations, thus letting the system know where each page of an address space actually resides in physical memory.

==通常，页表存储虚拟地址到物理地址的转换，从而让系统知道地址空间的每一页实际驻留在物理内存的何处。==

  

Because each address space requires such translations, in general there is one page table per process in the system.

==因为每个地址空间都需要这种转换，所以通常系统中每个进程都有一个页表。==

  

The exact structure of the page table is either determined by the hardware (older systems) or can be more flexibly managed by the OS (modern systems).

==页表的具体结构要么由硬件决定（旧系统），要么可以由操作系统更灵活地管理（现代系统）。==

  

To map a typical 32-bit address space (4GB) with 4KB pages, we would need a large number of translations.

==为了映射一个典型的 32 位地址空间（4GB）并使用 4KB 的页大小，我们需要大量的转换条目。==

  

Even if machines have gigabytes of memory, it seems a little crazy to use a large chunk of it just for translations, no?

==即使机器有千兆字节的内存，用这么大一块内存仅仅来做地址转换似乎有点疯狂，不是吗？==

  

And we won't even think about how big such a page table would be for a 64-bit address space.

==我们甚至不敢想象对于 64 位地址空间，这样的页表会有多大。==

  

That would be too gruesome and perhaps scare you off entirely.

==那太可怕了，可能会把你彻底吓跑。==

  

Because page tables are so big, we don't keep any special on-chip hardware in the MMU to store the page table of the currently-running process.

==因为页表太大，我们在 MMU（内存管理单元）中不保留任何特殊的片上硬件来存储当前运行进程的页表。==

  

Instead, we store the page table for each process in memory somewhere.

==相反，我们将每个进程的页表存储在内存的某个地方。==

  

Let's assume for now that the page tables live in physical memory that the OS manages.

==我们暂时假设页表驻留在操作系统管理的物理内存中。==

  

Later we'll see that much of OS memory itself can be virtualized, and thus page tables can be stored in OS virtual memory (and even swapped to disk), but that is too confusing right now, so we'll ignore it.

==稍后我们会看到，操作系统的大部分内存本身也可以被虚拟化，因此页表可以存储在操作系统的虚拟内存中（甚至可以交换到磁盘），但现在这太令人困惑了，所以我们先忽略它。==

  

In Figure 18.4 is a picture of a page table in OS memory; see the tiny set of translations in there?

==图 18.4 展示了操作系统内存中的页表；看到里面那一小组转换条目了吗？==

  

18.3 What's Actually In The Page Table?

==18.3 页表里实际上有什么？==

  

Let's talk a little about page table organization.

==让我们稍微谈谈页表的组织结构。==

  

The page table is just a data structure that is used to map virtual addresses (or really, virtual page numbers) to physical addresses (physical frame numbers).

==页表只是一个数据结构，用于将虚拟地址（或者确切地说是虚拟页号）映射到物理地址（物理帧号）。==

  

Thus, any data structure could work.

==因此，任何数据结构都可以。==

  

The simplest form is called a linear page table, which is just an array.

==最简单的形式称为线性页表，它就是一个数组。==

  

The OS indexes the array by the virtual page number (VPN), and looks up the page-table entry (PTE) at that index in order to find the desired physical frame number (PFN).

==操作系统通过虚拟页号 (VPN) 索引数组，并在该索引处查找页表项 (PTE)，以找到所需的物理帧号 (PFN)。==

  

For now, we will assume this simple linear structure; in later chapters, we will make use of more advanced data structures to help solve some problems with paging.

==现在，我们将假设这种简单的线性结构；在后面的章节中，我们将利用更高级的数据结构来帮助解决分页的一些问题。==

  

As for the contents of each PTE, we have a number of different bits in there worth understanding at some level.

==至于每个 PTE 的内容，里面有许多不同的位值得在一定程度上理解。==

  

A valid bit is common to indicate whether the particular translation is valid.

==有效位 (Valid Bit) 很常见，用于指示特定的转换是否有效。==

  

For example, when a program starts running, it will have code and heap at one end of its address space, and the stack at the other.

==例如，当程序开始运行时，它的代码和堆位于地址空间的一端，而栈位于另一端。==

  

All the unused space in-between will be marked invalid, and if the process tries to access such memory, it will generate a trap to the OS which will likely terminate the process.

==中间所有未使用的空间都将被标记为无效，如果进程尝试访问此类内存，它将生成一个陷阱 (Trap) 给操作系统，这通常会导致进程终止。==

  

Thus, the valid bit is crucial for supporting a sparse address space.

==因此，有效位对于支持稀疏地址空间至关重要。==

  

By simply marking all the unused pages in the address space invalid, we remove the need to allocate physical frames for those pages and thus save a great deal of memory.

==通过简单地将地址空间中所有未使用的页面标记为无效，我们消除了为这些页面分配物理帧的需要，从而节省了大量内存。==

  

Figure 18.5: An x86 Page Table Entry (PTE)

==图 18.5：x86 页表项 (PTE)==

  

We also might have protection bits, indicating whether the page could be read from, written to, or executed from.

==我们可能还有保护位 (Protection Bits)，指示该页面是否可读、可写或可执行。==

  

Again, accessing a page in a way not allowed by these bits will generate a trap to the OS.

==同样，以这些位不允许的方式访问页面将生成给操作系统的陷阱。==

  

There are a couple of other bits that are important but we won't talk about much for now.

==还有其他几个重要的位，但我们将暂时不作详细讨论。==

  

A present bit indicates whether this page is in physical memory or on disk (i.e., it has been swapped out).

==存在位 (Present Bit) 指示此页面是在物理内存中还是在磁盘上（即，它已被换出）。==

  

We will understand this machinery further when we study how to swap parts of the address space to disk to support address spaces that are larger than physical memory.

==当我们研究如何将部分地址空间交换到磁盘以支持大于物理内存的地址空间时，我们将进一步理解这一机制。==

  

Swapping allows the OS to free up physical memory by moving rarely-used pages to disk.

==交换技术允许操作系统通过将很少使用的页面移动到磁盘来释放物理内存。==

  

A dirty bit is also common, indicating whether the page has been modified since it was brought into memory.

==脏位 (Dirty Bit) 也很常见，指示页面自调入内存以来是否已被修改。==

  

A reference bit (a.k.a. accessed bit) is sometimes used to track whether a page has been accessed, and is useful in determining which pages are popular and thus should be kept in memory.

==引用位 (Reference Bit)（也称为访问位）有时用于跟踪页面是否已被访问，这在确定哪些页面是常用的并因此应保留在内存中时非常有用。==

  

Such knowledge is critical during page replacement, a topic we will study in great detail in subsequent chapters.

==这些知识在页面置换期间至关重要，这是我们将在后续章节中详细研究的主题。==

  

Figure 18.5 shows an example page table entry from the x86 architecture.

==图 18.5 显示了 x86 架构中的一个页表项示例。==

  

It contains a present bit (P); a read/write bit (R/W) which determines if writes are allowed to this page; a user/supervisor bit (U/S) which determines if user-mode processes can access the page.

==它包含一个存在位 (P)；一个决定是否允许写入该页面的读/写位 (R/W)；一个决定用户模式进程是否可以访问该页面的用户/超级用户位 (U/S)。==

  

It also contains a few bits (PWT, PCD, PAT, and G) that determine how hardware caching works for these pages; an accessed bit (A) and a dirty bit (D); and finally, the page frame number (PFN) itself.

==它还包含几个决定这些页面的硬件缓存如何工作的位（PWT、PCD、PAT 和 G）；一个访问位 (A) 和一个脏位 (D)；最后是页帧号 (PFN) 本身。==

  

Read the Intel Architecture Manuals for more details on x86 paging support.

==请阅读英特尔架构手册以了解有关 x86 分页支持的更多详细信息。==

  

Be forewarned, however; reading manuals such as these, while quite informative (and certainly necessary for those who write code to use such page tables in the OS), can be challenging at first.

==但请注意；阅读此类手册虽然信息量很大（对于编写代码以在操作系统中使用此类页表的人来说当然是必要的），但起初可能会很有挑战性。==

  

A little patience, and a lot of desire, is required.

==这需要一点耐心和大量的求知欲。==

  

ASIDE: WHY NO VALID BIT?

==补充说明：为什么没有有效位？==

  

You may notice that in the Intel example, there are no separate valid and present bits, but rather just a present bit (P).

==您可能注意到在英特尔的示例中，没有单独的有效位和存在位，而只有一个存在位 (P)。==

  

If that bit is set (P=1) it means the page is both present and valid.

==如果该位被置位 (P=1)，则意味着该页面既存在又有效。==

  

If not (P=0), it means that the page may not be present in memory (but is valid), or may not be valid.

==如果未置位 (P=0)，则意味着该页面可能不存在于内存中（但是有效的），或者可能根本无效。==

  

An access to a page with P=0 will trigger a trap to the OS.

==访问 P=0 的页面将触发给操作系统的陷阱。==

  

The OS must then use additional structures it keeps to determine whether the page is valid (and thus perhaps should be swapped back in) or not (and thus the program is attempting to access memory illegally).

==然后，操作系统必须使用它保留的额外结构来确定页面是否有效（因此可能应该换回），或者无效（因此程序正在尝试非法访问内存）。==

  

This sort of judiciousness is common in hardware, which often just provide the minimal set of features upon which the OS can build a full service.

==这种审慎设计在硬件中很常见，硬件通常只提供最小的功能集，操作系统可在此基础上构建完整的服务。==

  

18.4 Paging: Also Too Slow

==18.4 分页：还是太慢==

  

With page tables in memory, we already know that they might be too big.

==既然页表存放在内存中，我们已经知道它们可能太大了。==

  

As it turns out, they can slow things down too.

==事实证明，它们也会拖慢运行速度。==

  

For example, take our simple instruction: `movl 21, %eax`.

==例如，看看我们这个简单的指令：`movl 21, %eax`。==

  

Again, let's just examine the explicit reference to address 21 and not worry about the instruction fetch.

==同样，让我们只检查对地址 21 的显式引用，而不必担心指令的获取。==

  

In this example, we'll assume the hardware performs the translation for us.

==在这个例子中，我们将假设硬件为我们执行转换。==

  

To fetch the desired data, the system must first translate the virtual address (21) into the correct physical address (117).

==为了获取所需的数据，系统必须首先将虚拟地址 (21) 转换为正确的物理地址 (117)。==

  

Thus, before fetching the data from address 117, the system must first fetch the proper page table entry from the process's page table, perform the translation, and then load the data from physical memory.

==因此，在从地址 117 获取数据之前，系统必须首先从进程的页表中获取适当的页表项，执行转换，然后从物理内存加载数据。==

  

To do so, the hardware must know where the page table is for the currently-running process.

==为此，硬件必须知道当前运行进程的页表在哪里。==

  

Let's assume for now that a single page-table base register contains the physical address of the starting location of the page table.

==我们暂且假设有一个单独的页表基址寄存器，其中包含页表起始位置的物理地址。==

  

To find the location of the desired PTE, the hardware will thus perform the following functions:

==为了找到所需 PTE 的位置，硬件将执行以下功能：==

  

VPN = (VirtualAddress & VPN_MASK) >> SHIFT

==VPN = (虚拟地址 & VPN_MASK) >> SHIFT==

  

PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))

==PTE地址 = 页表基址寄存器 + (VPN * PTE大小)==

  

In our example, VPN_MASK would be set to 0x30 (hex 30, or binary 110000) which picks out the VPN bits from the full virtual address.

==在我们的例子中，VPN_MASK 将被设置为 0x30（十六进制 30，或二进制 110000），它从完整的虚拟地址中选取 VPN 位。==

  

SHIFT is set to 4 (the number of bits in the offset), such that we move the VPN bits down to form the correct integer virtual page number.

==SHIFT 设置为 4（偏移量中的位数），这样我们将 VPN 位下移以形成正确的整数虚拟页号。==

  

For example, with virtual address 21 (010101), and masking turns this value into 010000.

==例如，对于虚拟地址 21 (010101)，掩码操作将其变为 010000。==

  

The shift turns it into 01, or virtual page 1, as desired.

==移位操作将其变为 01，即虚拟页 1，正如我们所愿。==

  

We then use this value as an index into the array of PTEs pointed to by the page table base register.

==然后，我们使用此值作为页表基址寄存器指向的 PTE 数组的索引。==

  

Once this physical address is known, the hardware can fetch the PTE from memory, extract the PFN, and concatenate it with the offset from the virtual address to form the desired physical address.

==一旦知道了这个物理地址，硬件就可以从内存中获取 PTE，提取 PFN，并将其与虚拟地址的偏移量拼接，以形成所需的物理地址。==

  

Specifically, you can think of the PFN being left-shifted by SHIFT, and then bitwise OR'd with the offset to form the final address as follows:

==具体来说，你可以认为 PFN 被左移了 SHIFT 位，然后与偏移量进行按位或运算，从而形成最终地址，如下所示：==

  

offset = VirtualAddress & OFFSET_MASK

==offset = 虚拟地址 & OFFSET_MASK==

  

PhysAddr = (PFN << SHIFT) | offset

==物理地址 = (PFN << SHIFT) | offset==

  

Finally, the hardware can fetch the desired data from memory and put it into register eax.

==最后，硬件可以从内存中获取所需的数据并将其放入寄存器 eax 中。==

  

The program has now succeeded at loading a value from memory!

==程序现在已成功从内存加载了一个值！==

  

To summarize, we now describe the initial protocol for what happens on each memory reference.

==总而言之，我们现在描述每次内存引用时发生的初始协议。==

  

Figure 18.6 shows the approach.

==图 18.6 展示了这种方法。==

  

For every memory reference (whether an instruction fetch or an explicit load or store), paging requires us to perform one extra memory reference in order to first fetch the translation from the page table.

==对于每一次内存引用（无论是指令获取还是显式的加载或存储），分页都要求我们执行一次额外的内存引用，以便首先从页表中获取转换信息。==

  

That is a lot of work!

==这是很大的工作量！==

  

Extra memory references are costly, and in this case will likely slow down the process by a factor of two or more.

==额外的内存引用代价高昂，在这种情况下，可能会使进程速度减慢两倍或更多。==

  

And now you can hopefully see that there are two real problems that we must solve.

==现在你可以看到，有两个必须解决的实际问题。==

  

Without careful design of both hardware and software, page tables will cause the system to run too slowly, as well as take up too much memory.

==如果不对硬件和软件进行精心设计，页表将导致系统运行过慢，并占用过多内存。==

  

While seemingly a great solution for our memory virtualization needs, these two crucial problems must first be overcome.

==虽然这看似是我们内存虚拟化需求的一个极佳解决方案，但必须首先克服这两个关键问题。==

  

18.5 A Memory Trace

==18.5 内存追踪==

  

Before closing, we now trace through a simple memory access example to demonstrate all of the resulting memory accesses that occur when using paging.

==在结束之前，我们要追踪一个简单的内存访问示例，以演示使用分页时产生的所有内存访问。==

  

The code snippet (in C, in a file called array.c) that we are interested in is as follows:

==我们要关注的代码片段（C 语言，在一个名为 array.c 的文件中）如下：==

  

`int array[1000];`

`int array[1000];`

  

`for (i = 0; i < 1000; i++) array[i] = 0;`

`for (i = 0; i < 1000; i++) array[i] = 0;`

  

We compile array.c and run it with the following commands:

==我们编译 array.c 并使用以下命令运行它：==

  

`prompt> gcc -o array array.c -Wall -O`

`prompt> gcc -o array array.c -Wall -O`

  

`prompt> ./array`

`prompt> ./array`

  

Of course, to truly understand what memory accesses this code snippet (which simply initializes an array) will make, we'll have to know (or assume) a few more things.

==当然，要真正理解这个代码片段（仅仅是初始化一个数组）将进行哪些内存访问，我们必须知道（或假设）更多的事情。==

  

First, we'll have to disassemble the resulting binary (using objdump on Linux, or otool on a Mac) to see what assembly instructions are used to initialize the array in a loop.

==首先，我们必须反汇编生成的二进制文件（在 Linux 上使用 objdump，或在 Mac 上使用 otool），以查看循环中初始化数组使用了哪些汇编指令。==

  

Here is the resulting assembly code:

==以下是生成的汇编代码：==

  

`1024 movl $0x0, (%edi, %eax, 4)`

`1024 movl $0x0, (%edi, %eax, 4)`

  

`1028 incl %eax`

`1028 incl %eax`

  

`1032 cmpl $0x03e8, %eax`

`1032 cmpl $0x03e8, %eax`

  

`1036 jne 1024`

`1036 jne 1024`

  

The code, if you know a little x86, is actually quite easy to understand.

==如果你懂一点 x86，这段代码其实很容易理解。==

  

The first instruction moves the value zero (shown as $0x0) into the virtual memory address of the location of the array.

==第一条指令将值 0（显示为 $0x0）移入数组位置的虚拟内存地址。==

  

This address is computed by taking the contents of %edi and adding %eax multiplied by four to it.

==该地址是通过取 %edi 的内容并加上 %eax 乘以 4 计算得出的。==

  

Thus, %edi holds the base address of the array, whereas %eax holds the array index (i).

==因此，%edi 保存数组的基地址，而 %eax 保存数组索引 (i)。==

  

We multiply by four because the array is an array of integers, each of size four bytes.

==我们乘以 4 是因为该数组是整数数组，每个整数大小为 4 字节。==

  

The second instruction increments the array index held in %eax, and the third instruction compares the contents of that register to the hex value 0x03e8, or decimal 1000.

==第二条指令递增 %eax 中保存的数组索引，第三条指令将该寄存器的内容与十六进制值 0x03e8（即十进制 1000）进行比较。==

  

If the comparison shows that two values are not yet equal (which is what the jne instruction tests), the fourth instruction jumps back to the top of the loop.

==如果比较表明两个值尚未相等（这正是 jne 指令测试的内容），则第四条指令跳回循环顶部。==

  

To understand which memory accesses this instruction sequence makes (at both the virtual and physical levels), we'll have to assume something about where in virtual memory the code snippet and array are found, as well as the contents and location of the page table.

==为了理解该指令序列进行了哪些内存访问（在虚拟和物理级别），我们必须假设代码片段和数组在虚拟内存中的位置，以及页表的内容和位置。==

  

For this example, we assume a virtual address space of size 64KB (unrealistically small).

==在这个例子中，我们假设虚拟地址空间大小为 64KB（不切实际地小）。==

  

We also assume a page size of 1KB.

==我们还假设页大小为 1KB。==

  

All we need to know now are the contents of the page table, and its location in physical memory.

==我们要知道的只是页表的内容及其在物理内存中的位置。==

  

Let's assume we have a linear (array-based) page table and that it is located at physical address 1KB (1024).

==让我们假设有一个线性（基于数组的）页表，且位于物理地址 1KB (1024) 处。==

  

As for its contents, there are just a few virtual pages we need to worry about having mapped for this example.

==至于它的内容，在这个例子中我们只需要关心少数几个已映射的虚拟页面。==

  

First, there is the virtual page the code lives on.

==首先是代码所在的虚拟页面。==

  

Because the page size is 1KB, virtual address 1024 resides on the second page of the virtual address space (VPN=1, as VPN=0 is the first page).

==因为页大小是 1KB，虚拟地址 1024 位于虚拟地址空间的第二页（VPN=1，因为 VPN=0 是第一页）。==

  

Let's assume this virtual page maps to physical frame 4 (VPN 1 -> PFN 4).

==让我们假设这个虚拟页映射到物理帧 4 (VPN 1 -> PFN 4)。==

  

Next, there is the array itself.

==接下来是数组本身。==

  

Its size is 4000 bytes (1000 integers), and we assume that it resides at virtual addresses 40000 through 44000 (not including the last byte).

==它的大小为 4000 字节（1000 个整数），我们假设它位于虚拟地址 40000 到 44000（不包括最后一个字节）。==

  

The virtual pages for this decimal range are VPN=39 ... VPN=42.

==这个十进制范围对应的虚拟页是 VPN=39 ... VPN=42。==

  

Thus, we need mappings for these pages.

==因此，我们需要这些页面的映射。==

  

Let's assume these virtual-to-physical mappings for the example: (VPN 39 -> PFN 7), (VPN 40 -> PFN 8), (VPN 41 -> PFN 9), (VPN 42 -> PFN 10).

==让我们为这个例子假设如下虚拟到物理的映射：(VPN 39 -> PFN 7), (VPN 40 -> PFN 8), (VPN 41 -> PFN 9), (VPN 42 -> PFN 10)。==

  

We are now ready to trace the memory references of the program.

==我们现在准备好追踪程序的内存引用了。==

  

When it runs, each instruction fetch will generate two memory references: one to the page table to find the physical frame that the instruction resides within, and one to the instruction itself to fetch it to the CPU for processing.

==当它运行时，每次指令获取都会产生两次内存引用：一次访问页表以查找指令所在的物理帧，另一次访问指令本身以将其获取到 CPU 进行处理。==

  

In addition, there is one explicit memory reference in the form of the mov instruction.

==此外，mov 指令还会产生一次显式的内存引用。==

  

This adds another page table access first (to translate the array virtual address to the correct physical one) and then the array access itself.

==这会先增加另一次页表访问（将数组虚拟地址转换为正确的物理地址），然后再访问数组本身。==

  

The entire process, for the first five loop iterations, is depicted in Figure 18.7.

==图 18.7 描绘了前五次循环迭代的整个过程。==

  

The bottom most graph shows the instruction memory references on the y-axis in black (with virtual addresses on the left, and the actual physical addresses on the right).

==最底部的图表在 y 轴上用黑色显示了指令内存引用（左侧是虚拟地址，右侧是实际物理地址）。==

  

The middle graph shows array accesses in dark gray (again with virtual on left and physical on right).

==中间的图表用深灰色显示了数组访问（同样左侧是虚拟，右侧是物理）。==

  

Finally, the topmost graph shows page table memory accesses in light gray (just physical, as the page table in this example resides in physical memory).

==最后，最顶部的图表用浅灰色显示了页表内存访问（只有物理地址，因为本例中的页表驻留在物理内存中）。==

  

The x-axis, for the entire trace, shows memory accesses across the first five iterations of the loop.

==整个追踪图的 x 轴显示了循环前五次迭代中的内存访问。==

  

There are 10 memory accesses per loop, which includes four instruction fetches, one explicit update of memory, and five page table accesses to translate those four fetches and one explicit update.

==每次循环有 10 次内存访问，其中包括 4 次指令获取，1 次显式内存更新，以及 5 次用于转换这 4 次获取和 1 次显式更新的页表访问。==

  

Figure 18.7: A Virtual (And Physical) Memory Trace

==图 18.7：虚拟（和物理）内存追踪==

  

See if you can make sense of the patterns that show up in this visualization.

==看看你能否理解这个可视化中出现的模式。==

  

In particular, what will change as the loop continues to run beyond these first five iterations?

==特别是，随着循环继续运行超过这前五次迭代，会发生什么变化？==

  

Which new memory locations will be accessed?

==将访问哪些新的内存位置？==

  

Can you figure it out?

==你能算出来吗？==

  

This has just been the simplest of examples (only a few lines of C code), and yet you might already be able to sense the complexity of understanding the actual memory behavior of real applications.

==这只是最简单的例子（只有几行 C 代码），但你可能已经能感觉到理解实际应用程序的真实内存行为有多复杂。==

  

Don't worry: it definitely gets worse, because the mechanisms we are about to introduce only complicate this already complex machinery.

==别担心：情况肯定会变得更糟，因为我们将要介绍的机制只会让这个本已复杂的机制变得更加复杂。==

  

Sorry!

==抱歉！==

  

18.6 Summary

==18.6 总结==

  

We have introduced the concept of paging as a solution to our challenge of virtualizing memory.

==我们介绍了分页的概念，作为解决内存虚拟化挑战的方案。==

  

Paging has many advantages over previous approaches (such as segmentation).

==分页相比以前的方法（如分段）有许多优势。==

  

First, it does not lead to external fragmentation, as paging (by design) divides memory into fixed-sized units.

==首先，它不会导致外部碎片，因为分页（在设计上）将内存划分为固定大小的单元。==

  

Second, it is quite flexible, enabling the sparse use of virtual address spaces.

==其次，它非常灵活，能够支持虚拟地址空间的稀疏使用。==

  

However, implementing paging support without care will lead to a slower machine (with many extra memory accesses to access the page table) as well as memory waste (with memory filled with page tables instead of useful application data).

==然而，如果不小心实现分页支持，将导致机器变慢（因为访问页表需要许多额外的内存访问），以及内存浪费（内存中充满了页表而不是有用的应用程序数据）。==

  

We'll thus have to think a little harder to come up with a paging system that not only works, but works well.

==因此，我们将不得不更加深入地思考，以提出一个不仅有效，而且高效的分页系统。==

  

The next two chapters, fortunately, will show us how to do so.

==幸运的是，接下来的两章将向我们展示如何做到这一点。==

  

We're not really sorry.

==我们其实并不是真的感到抱歉。==

  

But, we are sorry about not being sorry, if that makes sense.

==但是，如果这能说得通的话，我们对不感到抱歉这件事感到抱歉。==

  

Paging: Faster Translations (TLBs)

==分页：更快的转换 (TLB)==

  

Using paging as the core mechanism to support virtual memory can lead to high performance overheads.

==使用分页作为支持虚拟内存的核心机制可能会导致高昂的性能开销。==

  

By chopping the address space into small, fixed-sized units (i.e., pages), paging requires a large amount of mapping information.

==通过将地址空间切分成小的、固定大小的单元（即页面），分页需要大量的映射信息。==

  

Because that mapping information is generally stored in physical memory, paging logically requires an extra memory lookup for each virtual address generated by the program.

==因为这些映射信息通常存储在物理内存中，所以从逻辑上讲，分页要求程序生成的每个虚拟地址都要进行一次额外的内存查找。==

  

Going to memory for translation information before every instruction fetch or explicit load or store is prohibitively slow.

==在每次指令获取或显式加载、存储之前都要去内存获取转换信息，这实在是太慢了。==

  

And thus our problem:

==这就是我们的问题：==

  

THE CRUX: HOW TO SPEED UP ADDRESS TRANSLATION

==关键问题：如何加速地址转换==

  

How can we speed up address translation, and generally avoid the extra memory reference that paging seems to require?

==我们如何加速地址转换，并总体上避免分页似乎需要的额外内存引用？==

  

What hardware support is required?

==需要什么硬件支持？==

  

What OS involvement is needed?

==需要操作系统如何参与？==

  

When we want to make things fast, the OS usually needs some help.

==当我们想让事情变快时，操作系统通常需要一些帮助。==

  

And help often comes from the OS's old friend: the hardware.

==而帮助通常来自操作系统的老朋友：硬件。==

  

To speed address translation, we are going to add what is called (for historical reasons) a translation-lookaside buffer, or TLB.

==为了加速地址转换，我们要增加一种称为（由于历史原因）旁路转换缓冲，或简称 TLB 的东西。==

  

A TLB is part of the chip's memory-management unit (MMU), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache.

==TLB 是芯片内存管理单元 (MMU) 的一部分，它只是热门虚拟地址到物理地址转换的硬件缓存；因此，一个更好的名字应该是地址转换缓存。==

  

Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein.

==对于每次虚拟内存引用，硬件首先检查 TLB 以查看其中是否包含所需的转换。==

  

If so, the translation is performed (quickly) without having to consult the page table (which has all translations).

==如果是，则（快速）执行转换，而无需查询页表（页表包含所有转换）。==

  

Because of their tremendous performance impact, TLBs in a real sense make virtual memory possible.

==由于其巨大的性能影响，TLB 在真正意义上使虚拟内存成为可能。==

  

Figure 19.1: TLB Control Flow Algorithm

==图 19.1：TLB 控制流算法==

  

19.1 TLB Basic Algorithm

==19.1 TLB 基本算法==

  

Figure 19.1 shows a rough sketch of how hardware might handle a virtual address translation, assuming a simple linear page table (i.e., the page table is an array) and a hardware-managed TLB.

==图 19.1 大致勾勒了硬件如何处理虚拟地址转换，假设使用简单的线性页表（即页表是一个数组）和硬件管理的 TLB。==

  

The algorithm the hardware follows works like this: first, extract the virtual page number (VPN) from the virtual address (Line 1 in Figure 19.1), and check if the TLB holds the translation for this VPN (Line 2).

==硬件遵循的算法如下：首先，从虚拟地址中提取虚拟页号 (VPN)（图 19.1 中的第 1 行），并检查 TLB 是否保存了此 VPN 的转换（第 2 行）。==

  

If it does, we have a TLB hit, which means the TLB holds the translation. Success!

==如果有，我们就有了 TLB 命中，这意味着 TLB 保存了该转换。成功！==

  

We can now extract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory (Lines 5-7), assuming protection checks do not fail (Line 4).

==我们要现在可以从相关的 TLB 条目中提取页帧号 (PFN)，将其拼接到原始虚拟地址的偏移量上，形成所需的物理地址 (PA)，并访问内存（第 5-7 行），假设保护检查没有失败（第 4 行）。==

  

If the CPU does not find the translation in the TLB (a TLB miss), we have some more work to do.

==如果 CPU 在 TLB 中没有找到转换（TLB 未命中），我们还有更多工作要做。==

  

In this example, the hardware accesses the page table to find the translation (Lines 11-12), and, assuming that the virtual memory reference generated by the process is valid and accessible (Lines 13, 15), updates the TLB with the translation (Line 18).

==在这个例子中，硬件访问页表以查找转换（第 11-12 行），并假设进程生成的虚拟内存引用是有效且可访问的（第 13、15 行），用该转换更新 TLB（第 18 行）。==

  

These set of actions are costly, primarily because of the extra memory reference needed to access the page table (Line 12).

==这一系列操作代价高昂，主要是因为需要额外的内存引用来访问页表（第 12 行）。==

  

Finally, once the TLB is updated, the hardware retries the instruction; this time, the translation is found in the TLB, and the memory reference is processed quickly.

==最后，一旦 TLB 更新完毕，硬件会重试指令；这一次，转换在 TLB 中被找到，内存引用被快速处理。==

  

The TLB, like all caches, is built on the premise that in the common case, translations are found in the cache (i.e., are hits).

==TLB 像所有缓存一样，建立在这样的前提下：在常见情况下，转换可以在缓存中找到（即命中）。==

  

If so, little overhead is added, as the TLB is found near the processing core and is designed to be quite fast.

==如果是这样，增加的开销很小，因为 TLB 位于处理核心附近，并且设计得非常快。==

  

When a miss occurs, the high cost of paging is incurred; the page table must be accessed to find the translation, and an extra memory reference (or more, with more complex page tables) results.

==当未命中发生时，就会产生高昂的分页成本；必须访问页表来查找转换，从而导致额外的内存引用（如果是更复杂的页表，则更多）。==

  

If this happens often, the program will likely run noticeably more slowly; memory accesses, relative to most CPU instructions, are quite costly, and TLB misses lead to more memory accesses.

==如果这种情况经常发生，程序的运行速度可能会明显变慢；相对于大多数 CPU 指令，内存访问相当昂贵，而 TLB 未命中会导致更多的内存访问。==

  

Thus, it is our hope to avoid TLB misses as much as we can.

==因此，我们要希望尽可能避免 TLB 未命中。==

  

19.2 Example: Accessing An Array

==19.2 示例：访问数组==

  

To make clear the operation of a TLB, let's examine a simple virtual address trace and see how a TLB can improve its performance.

==为了清楚说明 TLB 的操作，让我们检查一个简单的虚拟地址追踪，看看 TLB 如何提高其性能。==

  

In this example, let's assume we have an array of 10 4-byte integers in memory, starting at virtual address 100.

==在这个例子中，让我们假设内存中有一个包含 10 个 4 字节整数的数组，起始于虚拟地址 100。==

  

Assume further that we have a small 8-bit virtual address space, with 16-byte pages.

==进一步假设我们有一个小的 8 位虚拟地址空间，页大小为 16 字节。==

  

Thus, a virtual address breaks down into a 4-bit VPN (there are 16 virtual pages) and a 4-bit offset (there are 16 bytes on each of those pages).

==因此，虚拟地址分解为 4 位 VPN（有 16 个虚拟页）和 4 位偏移量（每页有 16 个字节）。==

  

Figure 19.2 shows the array laid out on the 16 16-byte pages of the system.

==图 19.2 显示了分布在系统 16 个 16 字节页面上的数组。==

  

As you can see, the array's first entry (a[0]) begins on (VPN=06, offset=04); only three 4-byte integers fit onto that page.

==如你所见，数组的第一个条目 (a[0]) 开始于 (VPN=06, offset=04)；该页面只能容纳三个 4 字节整数。==

  

The array continues onto the next page (VPN=07), where the next four entries (a[3] ... a[6]) are found.

==数组延续到下一页 (VPN=07)，那里有接下来的四个条目 (a[3] ... a[6])。==

  

Finally, the last three entries of the 10-entry array (a[7] ... a[9]) are located on the next page of the address space (VPN=08).

==最后，这个 10 条目数组的最后三个条目 (a[7] ... a[9]) 位于地址空间的下一页 (VPN=08)。==

  

Now let's consider a simple loop that accesses each array element, something that would look like this in C:

==现在让我们考虑一个访问每个数组元素的简单循环，在 C 语言中如下所示：==

  

`int i, sum = 0;`

`int i, sum = 0;`

  

`for (i = 0; i < 10; i++) { sum += a[i]; }`

`for (i = 0; i < 10; i++) { sum += a[i]; }`

  

For the sake of simplicity, we will pretend that the only memory accesses the loop generates are to the array (ignoring the variables i and sum, as well as the instructions themselves).

==为简单起见，我们将假装循环产生的唯一内存访问是对数组的访问（忽略变量 i 和 sum，以及指令本身）。==

  

When the first array element (a[0]) is accessed, the CPU will see a load to virtual address 100.

==当访问第一个数组元素 (a[0]) 时，CPU 将看到对虚拟地址 100 的加载。==

  

The hardware extracts the VPN from this (VPN=06), and uses that to check the TLB for a valid translation.

==硬件从中提取 VPN (VPN=06)，并使用它来检查 TLB 是否有有效的转换。==

  

Assuming this is the first time the program accesses the array, the result will be a TLB miss.

==假设这是程序第一次访问数组，结果将是 TLB 未命中。==

  

The next access is to a[1], and there is some good news here: a TLB hit!

==下一次访问是 a[1]，这里有好消息：TLB 命中！==

  

Because the second element of the array is packed next to the first, it lives on the same page.

==因为数组的第二个元素紧挨着第一个元素，它位于同一页面上。==

  

Because we've already accessed this page when accessing the first element of the array, the translation is already loaded into the TLB.

==因为我们在访问数组的第一个元素时已经访问过该页面，所以转换已经加载到 TLB 中。==

  

And hence the reason for our success.

==这就是我们成功的原因。==

  

Access to a[2] encounters similar success (another hit), because it too lives on the same page as a[0] and a[1].

==访问 a[2] 也会遇到类似的成功（再次命中），因为它也与 a[0] 和 a[1] 位于同一页面上。==

  

Unfortunately, when the program accesses a[3], we encounter another TLB miss.

==不幸的是，当程序访问 a[3] 时，我们遇到另一次 TLB 未命中。==

  

However, once again, the next entries (a[4] ... a[6]) will hit in the TLB, as they all reside on the same page in memory.

==但是，接下来的条目 (a[4] ... a[6]) 再次在 TLB 中命中，因为它们都驻留在内存中的同一页面上。==

  

Finally, access to a[7] causes one last TLB miss.

==最后，访问 a[7] 导致最后一次 TLB 未命中。==

  

The hardware once again consults the page table to figure out the location of this virtual page in physical memory, and updates the TLB accordingly.

==硬件再次查询页表以找出该虚拟页面在物理内存中的位置，并相应地更新 TLB。==

  

The final two accesses (a[8] and a[9]) receive the benefits of this TLB update; when the hardware looks in the TLB for their translations, two more hits result.

==最后两次访问（a[8] 和 a[9]）受益于此 TLB 更新；当硬件在 TLB 中查找它们的转换时，又产生了两次命中。==

  

Let us summarize TLB activity during our ten accesses to the array: miss, hit, hit, miss, hit, hit, hit, miss, hit, hit.

==让我们总结一下访问数组十次期间的 TLB 活动：未命中、命中、命中、未命中、命中、命中、命中、未命中、命中、命中。==

  

Thus, our TLB hit rate, which is the number of hits divided by the total number of accesses, is 70%.

==因此，我们的 TLB 命中率（即命中次数除以总访问次数）为 70%。==

  

Although this is not too high (indeed, we desire hit rates that approach 100%), it is non-zero, which may be a surprise.

==虽然这个比例不是太高（实际上，我们希望命中率接近 100%），但它是非零的，这可能令人惊讶。==

  

Even though this is the first time the program accesses the array, the TLB improves performance due to spatial locality.

==即使这是程序第一次访问数组，TLB 也因为空间局部性而提高了性能。==

  

The elements of the array are packed tightly into pages (i.e., they are close to one another in space), and thus only the first access to an element on a page yields a TLB miss.

==数组元素被紧密地打包到页面中（即，它们在空间上彼此靠近），因此只有对页面上元素的第一次访问才会产生 TLB 未命中。==

  

Also note the role that page size plays in this example.

==还要注意页大小在此示例中扮演的角色。==

  

If the page size had simply been twice as big (32 bytes, not 16), the array access would suffer even fewer misses.

==如果页大小大一倍（32 字节，而不是 16 字节），数组访问遭遇的未命中次数会更少。==

  

As typical page sizes are more like 4KB, these types of dense, array-based accesses achieve excellent TLB performance, encountering only a single miss per page of accesses.

==由于典型的页大小更像是 4KB，这些类型的密集、基于数组的访问可以实现极佳的 TLB 性能，每页访问只会遇到一次未命中。==

  

One last point about TLB performance: if the program, soon after this loop completes, accesses the array again, we'd likely see an even better result, assuming that we have a big enough TLB to cache the needed translations: hit, hit, hit, hit, hit, hit, hit, hit, hit, hit.

==关于 TLB 性能的最后一点：如果程序在该循环完成后不久再次访问数组，我们可能会看到更好的结果，假设我们有足够大的 TLB 来缓存所需的转换：命中、命中、命中……（全部命中）。==

  

In this case, the TLB hit rate would be high because of temporal locality, i.e., the quick re-referencing of memory items in time.

==在这种情况下，TLB 命中率会很高，因为时间局部性，即在时间上快速重新引用内存项。==

  

Like any cache, TLBs rely upon both spatial and temporal locality for success, which are program properties.

==像任何缓存一样，TLB 依靠空间局部性和时间局部性来获得成功，这是程序的属性。==

  

If the program of interest exhibits such locality (and many programs do), the TLB hit rate will likely be high.

==如果感兴趣的程序表现出这种局部性（许多程序都是如此），TLB 命中率可能会很高。==

  

TIP: USE CACHING WHEN POSSIBLE

==提示：尽可能使用缓存==

  

Caching is one of the most fundamental performance techniques in computer systems, one that is used again and again to make the "common case fast".

==缓存是计算机系统中最基本的性能技术之一，它被反复用于使“常见情况变快”。==

  

The idea behind hardware caches is to take advantage of locality in instruction and data references.

==硬件缓存背后的思想是利用指令和数据引用中的局部性。==

  

There are usually two types of locality: temporal locality and spatial locality.

==通常有两种类型的局部性：时间局部性和空间局部性。==

  

With temporal locality, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future.

==对于时间局部性，其思想是最近访问过的指令或数据项很可能在不久的将来被再次访问。==

  

Think of loop variables or instructions in a loop; they are accessed repeatedly over time.

==想想循环变量或循环中的指令；它们随着时间的推移被反复访问。==

  

With spatial locality, the idea is that if a program accesses memory at address x, it will likely soon access memory near x.

==对于空间局部性，其思想是如果程序访问地址 x 处的内存，它很可能很快就会访问 x 附近的内存。==

  

Imagine here streaming through an array of some kind, accessing one element and then the next.

==想象一下流式处理某种数组，访问一个元素然后访问下一个。==

  

Of course, these properties depend on the exact nature of the program, and thus are not hard-and-fast laws but more like rules of thumb.

==当然，这些属性取决于程序的具体性质，因此不是硬性定律，而更像是经验法则。==

  

Hardware caches, whether for instructions, data, or address translations (as in our TLB) take advantage of locality by keeping copies of memory in small, fast on-chip memory.

==硬件缓存，无论是用于指令、数据还是地址转换（如我们的 TLB），都利用局部性，将内存副本保存在小型、快速的片上内存中。==

  

Instead of having to go to a (slow) memory to satisfy a request, the processor can first check if a nearby copy exists in a cache.

==处理器不必去（慢速）内存满足请求，而是可以首先检查缓存中是否存在附近的副本。==

  

If it does, the processor can access it quickly (i.e., in a few CPU cycles) and avoid spending the costly time it takes to access memory (many nanoseconds).

==如果有，处理器可以快速访问它（即在几个 CPU 周期内），并避免花费昂贵的时间去访问内存（许多纳秒）。==

  

You might be wondering: if caches (like the TLB) are so great, why don't we just make bigger caches and keep all of our data in them?

==你可能想知道：如果缓存（如 TLB）如此之好，为什么我们不把缓存做得更大，把所有数据都放在里面呢？==

  

Unfortunately, this is where we run into more fundamental laws like those of physics.

==不幸的是，这是我们遇到更基本的定律（如物理定律）的地方。==

  

If you want a fast cache, it has to be small, as issues like the speed-of-light and other physical constraints become relevant.

==如果你想要一个快速的缓存，它必须很小，因为光速和其他物理限制等问题变得相关。==

  

Any large cache by definition is slow, and thus defeats the purpose.

==任何大型缓存根据定义都是慢的，因此违背了初衷。==

  

Thus, we are stuck with small, fast caches; the question that remains is how to best use them to improve performance.

==因此，我们只能使用小型、快速的缓存；剩下的问题是如何最好地利用它们来提高性能。==

  

19.3 Who Handles The TLB Miss?

==19.3 谁来处理 TLB 未命中？==

  

One question that we must answer: who handles a TLB miss?

==我们必须回答的一个问题是：谁来处理 TLB 未命中？==

  

Two answers are possible: the hardware, or the software (OS).

==有两种可能的答案：硬件或软件（操作系统）。==

  

In the olden days, the hardware had complex instruction sets (sometimes called CISC, for complex-instruction set computers) and the people who built the hardware didn't much trust those sneaky OS people.

==在过去，硬件具有复杂的指令集（有时称为 CISC，即复杂指令集计算机），制造硬件的人不太信任那些鬼鬼祟祟的操作系统人员。==

  

Thus, the hardware would handle the TLB miss entirely.

==因此，硬件将完全处理 TLB 未命中。==

  

To do this, the hardware has to know exactly where the page tables are located in memory (via a page-table base register), as well as their exact format.

==为此，硬件必须确切知道页表在内存中的位置（通过页表基址寄存器），以及它们的确切格式。==

  

On a miss, the hardware would "walk" the page table, find the correct page-table entry and extract the desired translation, update the TLB with the translation, and retry the instruction.

==在未命中时，硬件会“遍历”页表，找到正确的页表项并提取所需的转换，用该转换更新 TLB，并重试指令。==

  

An example of an "older" architecture that has hardware-managed TLBs is the Intel x86 architecture, which uses a fixed multi-level page table.

==具有硬件管理 TLB 的“较旧”架构的一个例子是 Intel x86 架构，它使用固定的多级页表。==

  

The current page table is pointed to by the CR3 register.

==当前页表由 CR3 寄存器指向。==

  

More modern architectures (e.g., MIPS R10k or Sun's SPARC v9, both RISC or reduced-instruction set computers) have what is known as a software-managed TLB.

==更现代的架构（例如 MIPS R10k 或 Sun 的 SPARC v9，均为 RISC 或精简指令集计算机）具有所谓的软件管理 TLB。==

  

On a TLB miss, the hardware simply raises an exception, which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a trap handler.

==在 TLB 未命中时，硬件只是引发一个异常，暂停当前指令流，将特权级别提升为内核模式，并跳转到陷阱处理程序。==

  

As you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB misses.

==正如你可能猜到的那样，这个陷阱处理程序是操作系统内的代码，其编写目的是为了处理 TLB 未命中。==

  

When run, the code will lookup the translation in the page table, use special "privileged" instructions to update the TLB, and return from the trap.

==运行时，代码将在页表中查找转换，使用特殊的“特权”指令更新 TLB，并从陷阱返回。==

  

At this point, the hardware retries the instruction (resulting in a TLB hit).

==此时，硬件重试指令（导致 TLB 命中）。==

  

Let's discuss a couple of important details.

==让我们讨论几个重要的细节。==

  

First, the return-from-trap instruction needs to be a little different than the return-from-trap we saw before when servicing a system call.

==首先，从陷阱返回的指令需要与我们之前在处理系统调用时看到的从陷阱返回指令略有不同。==

  

In the latter case, the return-from-trap should resume execution at the instruction after the trap into the OS, just as a return from a procedure call returns to the instruction immediately following the call into the procedure.

==在后一种情况下（系统调用），从陷阱返回应在进入操作系统的陷阱之后的指令处恢复执行，就像过程调用返回时回到过程调用之后的指令一样。==

  

In the former case, when returning from a TLB miss-handling trap, the hardware must resume execution at the instruction that caused the trap.

==在前一种情况下，当从 TLB 未命中处理陷阱返回时，硬件必须在导致陷阱的指令处恢复执行。==

  

This retry thus lets the instruction run again, this time resulting in a TLB hit.

==这种重试让指令再次运行，这次会产生 TLB 命中。==

  

Thus, depending on how a trap or exception was caused, the hardware must save a different PC when trapping into the OS, in order to resume properly when the time to do so arrives.

==因此，根据导致陷阱或异常的原因，硬件必须在陷入操作系统时保存不同的 PC（程序计数器），以便在需要时正确恢复。==

  

Second, when running the TLB miss-handling code, the OS needs to be extra careful not to cause an infinite chain of TLB misses to occur.

==其次，在运行 TLB 未命中处理代码时，操作系统需要格外小心，以免导致 TLB 未命中的无限循环。==

  

Many solutions exist; for example, you could keep TLB miss handlers in physical memory (where they are unmapped and not subject to address translation).

==存在许多解决方案；例如，你可以将 TLB 未命中处理程序保留在物理内存中（那里未映射且不受地址转换的影响）。==

  

Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself.

==或者在 TLB 中保留一些条目用于永久有效的转换，并将其中一些永久转换槽用于处理程序代码本身。==

  

These wired translations always hit in the TLB.

==这些固定的转换总是会在 TLB 中命中。==

  

The primary advantage of the software-managed approach is flexibility: the OS can use any data structure it wants to implement the page table, without necessitating hardware change.

==软件管理方法的主要优点是灵活性：操作系统可以使用它想要的任何数据结构来实现页表，而无需更改硬件。==

  

Another advantage is simplicity, as seen in the TLB control flow.

==另一个优点是简单性，如 TLB 控制流所示。==

  

The hardware doesn't do much on a miss: just raise an exception and let the OS TLB miss handler do the rest.

==硬件在未命中时做不了多少事：只是引发异常并让操作系统 TLB 未命中处理程序完成其余工作。==

  

ASIDE: RISC vs. CISC

==补充说明：RISC 与 CISC==

  

In the 1980's, a great battle took place in the computer architecture community.

==在 20 世纪 80 年代，计算机架构界发生了一场大战。==

  

On one side was the CISC camp, which stood for Complex Instruction Set Computing; on the other side was RISC, for Reduced Instruction Set Computing.

==一方是 CISC 阵营，代表复杂指令集计算；另一方是 RISC，代表精简指令集计算。==

  

The RISC side was spear-headed by David Patterson at Berkeley and John Hennessy at Stanford (who are also co-authors of some famous books).

==RISC 方面由伯克利的 David Patterson 和斯坦福的 John Hennessy 领头（他们也是一些著名书籍的合著者）。==

  

CISC instruction sets tend to have a lot of instructions in them, and each instruction is relatively powerful.

==CISC 指令集往往包含大量指令，每条指令都相对强大。==

  

For example, you might see a string copy, which takes two pointers and a length and copies bytes from source to destination.

==例如，你可能会看到字符串复制指令，它接受两个指针和一个长度，并将字节从源复制到目标。==

  

The idea behind CISC was that instructions should be high-level primitives, to make the assembly language itself easier to use, and to make code more compact.

==CISC 背后的理念是指令应该是高级原语，以使汇编语言本身更易于使用，并使代码更紧凑。==

  

RISC instruction sets are exactly the opposite.

==RISC 指令集恰恰相反。==

  

A key observation behind RISC is that instruction sets are really compiler targets, and all compilers really want are a few simple primitives that they can use to generate high-performance code.

==RISC 背后是一个关键观察结果：指令集实际上是编译器的目标，所有编译器真正想要的只是几个简单的原语，以便用来生成高性能代码。==

  

Thus, RISC proponents argued, let's rip out as much from the hardware as possible (especially the microcode), and make what's left simple, uniform, and fast.

==因此，RISC 支持者主张，让我们尽可能从硬件中剥离内容（特别是微代码），并使剩下的部分简单、统一且快速。==

  

In the early days, RISC chips made a huge impact, as they were noticeably faster.

==在早期，RISC 芯片产生了巨大的影响，因为它们明显更快。==

  

However, as time progressed, CISC manufacturers such as Intel incorporated many RISC techniques into the core of their processors.

==然而，随着时间的推移，像 Intel 这样的 CISC 制造商将许多 RISC 技术融入了其处理器核心。==

  

For example by adding early pipeline stages that transformed complex instructions into micro-instructions which could then be processed in a RISC-like manner.

==例如，通过添加早期流水线阶段，将复杂指令转换为微指令，然后可以以类似 RISC 的方式进行处理。==

  

These innovations, plus a growing number of transistors on each chip, allowed CISC to remain competitive.

==这些创新加上每个芯片上晶体管数量的增加，使得 CISC 能够保持竞争力。==

  

The end result is that the debate died down, and today both types of processors can be made to run fast.

==最终结果是争论平息了，今天这两种类型的处理器都可以运行得很快。==

  

19.4 TLB Contents: What's In There?

==19.4 TLB 内容：里面有什么？==

  

Let's look at the contents of the hardware TLB in more detail.

==让我们更详细地查看硬件 TLB 的内容。==

  

A typical TLB might have 32, 64, or 128 entries and be what is called fully associative.

==典型的 TLB 可能有 32、64 或 128 个条目，并且被称为全相联的。==

  

Basically, this just means that any given translation can be anywhere in the TLB, and that the hardware will search the entire TLB in parallel to find the desired translation.

==基本上，这只是意味着任何给定的转换都可以位于 TLB 中的任何位置，并且硬件将并行搜索整个 TLB 以查找所需的转换。==

  

A TLB entry might look like this: VPN | PFN | other bits.

==TLB 条目可能如下所示：VPN | PFN | 其他位。==

  

Note that both the VPN and PFN are present in each entry, as a translation could end up in any of these locations (in hardware terms, the TLB is known as a fully-associative cache).

==请注意，VPN 和 PFN 都存在于每个条目中，因为转换可能最终出现在这些位置中的任何一个（在硬件术语中，TLB 被称为全相联缓存）。==

  

The hardware searches the entries in parallel to see if there is a match.

==硬件并行搜索条目以查看是否有匹配项。==

  

More interesting are the "other bits".

==更有趣的是“其他位”。==

  

For example, the TLB commonly has a valid bit, which says whether the entry has a valid translation or not.

==例如，TLB 通常有一个有效位，用于说明该条目是否具有有效的转换。==

  

Also common are protection bits, which determine how a page can be accessed (as in the page table).

==同样常见的是保护位，它决定了如何访问页面（如在页表中一样）。==

  

For example, code pages might be marked read and execute, whereas heap pages might be marked read and write.

==例如，代码页可能被标记为读取和执行，而堆页可能被标记为读取和写入。==

  

There may also be a few other fields, including an address-space identifier, a dirty bit, and so forth.

==可能还有其他几个字段，包括地址空间标识符、脏位等。==

  

ASIDE: TLB VALID BIT != PAGE TABLE VALID BIT

==补充说明：TLB 有效位 != 页表有效位==

  

A common mistake is to confuse the valid bits found in a TLB with those found in a page table.

==一个常见的错误是将 TLB 中的有效位与页表中的有效位混淆。==

  

In a page table, when a page-table entry (PTE) is marked invalid, it means that the page has not been allocated by the process, and should not be accessed by a correctly-working program.

==在页表中，当页表项 (PTE) 被标记为无效时，意味着该页面尚未由进程分配，并且不应由正常工作的程序访问。==

  

The usual response when an invalid page is accessed is to trap to the OS, which will respond by killing the process.

==当访问无效页面时，通常的反应是陷入操作系统，操作系统将通过终止进程来响应。==

  

A TLB valid bit, in contrast, simply refers to whether a TLB entry has a valid translation within it.

==相比之下，TLB 有效位仅指 TLB 条目中是否包含有效的转换。==

  

When a system boots, for example, a common initial state for each TLB entry is to be set to invalid, because no address translations are yet cached there.

==例如，当系统启动时，每个 TLB 条目的常见初始状态是设置为无效，因为那里尚未缓存任何地址转换。==

  

Once virtual memory is enabled, and once programs start running and accessing their virtual address spaces, the TLB is slowly populated, and thus valid entries soon fill the TLB.

==一旦启用了虚拟内存，并且程序开始运行并访问其虚拟地址空间，TLB 就会慢慢填充，因此有效条目很快就会填满 TLB。==

  

The TLB valid bit is quite useful when performing a context switch too, as we'll discuss further below.

==在执行上下文切换时，TLB 有效位也非常有用，我们将要在下面进一步讨论。==

  

By setting all TLB entries to invalid, the system can ensure that the about-to-be-run process does not accidentally use a virtual-to-physical translation from a previous process.

==通过将所有 TLB 条目设置为无效，系统可以确保即将运行的进程不会意外使用来自先前进程的虚拟到物理转换。==

  

19.5 TLB Issue: Context Switches

==19.5 TLB 问题：上下文切换==

  

With TLBs, new issues arise when switching between processes (and hence address spaces).

==使用 TLB 后，在进程（以及地址空间）之间切换时会出现新问题。==

  

Specifically, the TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes.

==具体来说，TLB 包含仅对当前运行的进程有效的虚拟到物理转换；这些转换对其他进程没有意义。==

  

As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.

==结果，当从一个进程切换到另一个进程时，硬件或操作系统（或两者）必须小心，以确保即将运行的进程不会意外使用来自先前运行进程的转换。==

  

To understand this situation better, let's look at an example.

==为了更好地理解这种情况，让我们看一个例子。==

  

When one process (P1) is running, it assumes the TLB might be caching translations that are valid for it, i.e., that come from P1's page table.

==当一个进程 (P1) 运行时，它假设 TLB 可能缓存了对它有效的转换，即来自 P1 页表的转换。==

  

Assume, for this example, that the 10th virtual page of P1 is mapped to physical frame 100.

==假设在这个例子中，P1 的第 10 个虚拟页映射到物理帧 100。==

  

In this example, assume another process (P2) exists, and the OS soon might decide to perform a context switch and run it.

==在这个例子中，假设存在另一个进程 (P2)，并且操作系统很快可能决定执行上下文切换并运行它。==

  

Assume here that the 10th virtual page of P2 is mapped to physical frame 170.

==这里假设 P2 的第 10 个虚拟页映射到物理帧 170。==

  

If entries for both processes were in the TLB, the contents of the TLB would be:

==如果两个进程的条目都在 TLB 中，则 TLB 的内容将是：==

  

(VPN 10, PFN 100, Valid 1, Prot rwx)

==(VPN 10, PFN 100, 有效 1, 保护 rwx)==

  

(VPN 10, PFN 170, Valid 1, Prot rwx)

==(VPN 10, PFN 170, 有效 1, 保护 rwx)==

  

In the TLB above, we clearly have a problem: VPN 10 translates to either PFN 100 (P1) or PFN 170 (P2), but the hardware can't distinguish which entry is meant for which process.

==在上面的 TLB 中，我们显然有一个问题：VPN 10 转换为 PFN 100 (P1) 或 PFN 170 (P2)，但硬件无法区分哪个条目对应哪个进程。==

  

Thus, we need to do some more work in order for the TLB to correctly and efficiently support virtualization across multiple processes.

==因此，我们需要做更多工作，以便 TLB 能够正确有效地支持跨多个进程的虚拟化。==

  

THE CRUX: HOW TO MANAGE TLB CONTENTS ON A CONTEXT SWITCH

==关键问题：如何在上下文切换时管理 TLB 内容==

  

When context-switching between processes, the translations in the TLB for the last process are not meaningful to the about-to-be-run process.

==当在进程之间进行上下文切换时，TLB 中上一个进程的转换对即将运行的进程没有意义。==

  

What should the hardware or OS do in order to solve this problem?

==为了解决这个问题，硬件或操作系统应该做什么？==

  

There are a number of possible solutions to this problem.

==针对这个问题有多种可能的解决方案。==

  

One approach is to simply flush the TLB on context switches, thus emptying it before running the next process.

==一种方法是在上下文切换时简单地刷新 TLB，从而在运行下一个进程之前将其清空。==

  

On a software-based system, this can be accomplished with an explicit (and privileged) hardware instruction.

==在基于软件的系统上，这可以通过显式的（特权）硬件指令来完成。==

  

With a hardware-managed TLB, the flush could be enacted when the page-table base register is changed (note the OS must change the PTBR on a context switch anyhow).

==对于硬件管理的 TLB，可以在更改页表基址寄存器时执行刷新（注意，操作系统无论如何都必须在上下文切换时更改 PTBR）。==

  

In either case, the flush operation simply sets all valid bits to 0, essentially clearing the contents of the TLB.

==在任何一种情况下，刷新操作只是将所有有效位设置为 0，本质上清除了 TLB 的内容。==

  

By flushing the TLB on each context switch, we now have a working solution, as a process will never accidentally encounter the wrong translations in the TLB.

==通过在每次上下文切换时刷新 TLB，我们现在有了一个可行的解决方案，因为进程永远不会在 TLB 中意外遇到错误的转换。==

  

However, there is a cost: each time a process runs, it must incur TLB misses as it touches its data and code pages.

==但是，这是有代价的：每次进程运行时，当它触及数据和代码页时，必然会引发 TLB 未命中。==

  

If the OS switches between processes frequently, this cost may be high.

==如果操作系统频繁地在进程之间切换，这个代价可能会很高。==

  

To reduce this overhead, some systems add hardware support to enable sharing of the TLB across context switches.

==为了减少这种开销，一些系统添加了硬件支持，以实现跨上下文切换共享 TLB。==

  

In particular, some hardware systems provide an address space identifier (ASID) field in the TLB.

==特别是，一些硬件系统在 TLB 中提供了地址空间标识符 (ASID) 字段。==

  

You can think of the ASID as a process identifier (PID), but usually it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID).

==你可以将 ASID 视为进程标识符 (PID)，但通常它的位数较少（例如，ASID 为 8 位，而 PID 为 32 位）。==

  

If we take our example TLB from above and add ASIDs, it is clear processes can readily share the TLB: only the ASID field is needed to differentiate otherwise identical translations.

==如果我们采用上面的 TLB 示例并添加 ASID，很明显进程可以随时共享 TLB：只需要 ASID 字段来区分原本相同的转换。==

  

Thus, with address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion.

==因此，有了地址空间标识符，TLB 可以同时保存来自不同进程的转换，而不会产生任何混淆。==

  

Of course, the hardware also needs to know which process is currently running in order to perform translations, and thus the OS must, on a context switch, set some privileged register to the ASID of the current process.

==当然，硬件还需要知道当前正在运行哪个进程以执行转换，因此操作系统必须在上下文切换时将某个特权寄存器设置为当前进程的 ASID。==

  

As an aside, you may also have thought of another case where two entries of the TLB are remarkably similar.

==顺便说一句，你可能还想到了另一种情况，即 TLB 的两个条目非常相似。==

  

In this example, there are two entries for two different processes with two different VPNs that point to the same physical page.

==在这个例子中，有两个不同进程的两个条目，它们具有两个指向同一物理页面的不同 VPN。==

  

This situation might arise, for example, when two processes share a page (a code page, for example).

==这种情况可能会出现，例如当两个进程共享一个页面（例如代码页）时。==

  

In the example above, Process 1 is sharing physical page 101 with Process 2; P1 maps this page into the 10th page of its address space, whereas P2 maps it to the 50th page of its address space.

==在上面的例子中，进程 1 与进程 2 共享物理页 101；P1 将此页面映射到其地址空间的第 10 页，而 P2 将其映射到其地址空间的第 50 页。==

  

Sharing of code pages (in binaries, or shared libraries) is useful as it reduces the number of physical pages in use, thus reducing memory overheads.

==共享代码页（在二进制文件或共享库中）很有用，因为它减少了使用的物理页面数量，从而减少了内存开销。==

PAGING: FASTER TRANSLATIONS (TLBS)

==分页：更快的地址转换 (TLB)==

  

19.6 Issue: Replacement Policy

==19.6 问题：替换策略==

  

As with any cache, and thus also with the TLB, one more issue that we must consider is cache replacement.

==正如任何缓存一样，TLB 也是如此，我们需要考虑的另一个问题是缓存替换。==

  

Specifically, when we are installing a new entry in the TLB, we have to replace an old one, and thus the question: which one to replace?

==具体来说，当我们在 TLB 中安装一个新条目时，我们必须替换一个旧条目，因此问题来了：应该替换哪一个？==

  

THE CRUX: HOW TO DESIGN TLB REPLACEMENT POLICY

==关键问题：如何设计 TLB 替换策略==

  

Which TLB entry should be replaced when we add a new TLB entry?

==当我们要添加一个新的 TLB 条目时，应该替换哪一个 TLB 条目？==

  

The goal, of course, being to minimize the miss rate (or increase hit rate) and thus improve performance.

==当然，目标是最小化未命中率（或提高命中率），从而提高性能。==

  

We will study such policies in some detail when we tackle the problem of swapping pages to disk;

==当我们解决将页面交换到磁盘的问题时，我们将详细研究这些策略；==

  

here we'll just highlight a few typical policies.

==在这里，我们只强调几种典型的策略。==

  

One common approach is to evict the least-recently-used or LRU entry.

==一种常见的方法是驱逐最近最少使用（LRU）的条目。==

  

LRU tries to take advantage of locality in the memory-reference stream, assuming it is likely that an entry that has not recently been used is a good candidate for eviction.

==LRU 试图利用内存引用流中的局部性，假设最近未被使用的条目很可能是被驱逐的好人选。==

  

Another typical approach is to use a random policy, which evicts a TLB mapping at random.

==另一种典型的方法是使用随机策略，即随机驱逐一个 TLB 映射。==

  

Such a policy is useful due to its simplicity and ability to avoid corner-case behaviors;

==这种策略因其简单性和避免极端情况行为的能力而非常有用；==

  

for example, a "reasonable" policy such as LRU behaves quite unreasonably when a program loops over  pages with a TLB of size n;

==例如，当一个程序在 TLB 大小为 n 的情况下循环访问  个页面时，像 LRU 这样“合理”的策略表现得相当不合理；==

  

in this case, LRU misses upon every access, whereas random does much better.

==在这种情况下，LRU 每次访问都会未命中，而随机策略则表现得好得多。==

  

19.7 A Real TLB Entry

==19.7 一个真实的 TLB 条目==

  

Finally, let's briefly look at a real TLB.

==最后，让我们简要地看一个真实的 TLB。==

  

This example is from the MIPS R4000 [H93], a modern system that uses software-managed TLBs;

==这个例子来自 MIPS R4000 [H93]，这是一个使用软件管理 TLB 的现代系统；==

  

a slightly simplified MIPS TLB entry can be seen in Figure 19.4.

==图 19.4 展示了一个稍微简化的 MIPS TLB 条目。==

  

The MIPS R4000 supports a 32-bit address space with 4KB pages.

==MIPS R4000 支持具有 4KB 页面的 32 位地址空间。==

  

Thus, we would expect a 20-bit VPN and 12-bit offset in our typical virtual address.

==因此，我们在典型的虚拟地址中期望有一个 20 位的 VPN 和 12 位的偏移量。==

  

However, as you can see in the TLB, there are only 19 bits for the VPN;

==然而，正如你在 TLB 中看到的，VPN 只有 19 位；==

  

as it turns out, user addresses will only come from half the address space (the rest reserved for the kernel) and hence only 19 bits of VPN are needed.

==事实证明，用户地址只来自地址空间的一半（其余部分保留给内核），因此只需要 19 位的 VPN。==

  

The VPN translates to up to a 24-bit physical frame number (PFN), and hence can support systems with up to 64GB of (physical) main memory ( 4KB pages).

==VPN 转换为高达 24 位的物理帧号（PFN），因此可以支持高达 64GB 的（物理）主内存（ 个 4KB 页面）。==

  

There are a few other interesting bits in the MIPS TLB.

==MIPS TLB 中还有其他几个有趣的位。==

  

We see a global bit (G), which is used for pages that are globally-shared among processes.

==我们看到一个全局位（G），用于在进程之间全局共享的页面。==

  

Thus, if the global bit is set, the ASID is ignored.

==因此，如果设置了全局位，则忽略 ASID。==

  

We also see the 8-bit ASID, which the OS can use to distinguish between address spaces (as described above).

==我们还看到了 8 位的 ASID，操作系统可以使用它来区分地址空间（如上所述）。==

  

One question for you: what should the OS do if there are more than 256 () processes running at a time?

==问你一个问题：如果同时运行的进程超过 256 () 个，操作系统应该怎么做？==

  

Finally, we see 3 Coherence (C) bits, which determine how a page is cached by the hardware (a bit beyond the scope of these notes);

==最后，我们看到 3 个一致性（C）位，它们决定了硬件如何缓存页面（这有点超出了本笔记的范围）；==

  

a dirty bit which is marked when the page has been written to (we'll see the use of this later);

==一个脏位（dirty bit），当页面被写入时会被标记（我们稍后会看到它的用途）；==

  

a valid bit which tells the hardware if there is a valid translation present in the entry.

==以及一个有效位（valid bit），告诉硬件该条目中是否存在有效的转换。==

  

There is also a page mask field (not shown), which supports multiple page sizes;

==还有一个页面掩码字段（未显示），它支持多种页面大小；==

  

we'll see later why having larger pages might be useful.

==我们稍后会看到为什么拥有更大的页面可能很有用。==

  

Figure 19.4: A MIPS TLB Entry

==图 19.4：一个 MIPS TLB 条目==

  

TIP: RAM ISN'T ALWAYS RAM (CULLER'S LAW)

==提示：RAM 并不总是 RAM（库勒定律）==

  

The term random-access memory, or RAM, implies that you can access any part of RAM just as quickly as another.

==术语随机存取存储器（RAM）意味着你可以像访问其他部分一样快地访问 RAM 的任何部分。==

  

While it is generally good to think of RAM in this way, because of hardware/OS features such as the TLB, accessing a particular page of memory may be costly, particularly if that page isn't currently mapped by your TLB.

==虽然这样看待 RAM 通常是好的，但由于 TLB 等硬件/操作系统特性的存在，访问内存的特定页面可能代价高昂，特别是如果该页面当前未被 TLB 映射。==

  

Thus, it is always good to remember the implementation tip: RAM isn't always RAM.

==因此，最好始终记住这个实现提示：RAM 并不总是 RAM。==

  

Sometimes randomly accessing your address space, particularly if the number of pages accessed exceeds the TLB coverage, can lead to severe performance penalties.

==有时，随机访问你的地址空间，特别是当访问的页面数量超过 TLB 覆盖范围时，会导致严重的性能损失。==

  

Because one of our advisors, David Culler, used to always point to the TLB as the source of many performance problems, we name this law in his honor: Culler's Law.

==因为我们的导师之一 David Culler 过去总是指出 TLB 是许多性能问题的根源，我们以他的名字命名这条定律：库勒定律。==

  

MIPS TLBs usually have 32 or 64 of these entries, most of which are used by user processes as they run.

==MIPS TLB 通常有 32 或 64 个这样的条目，其中大多数由用户进程在运行时使用。==

  

However, a few are reserved for the OS.

==然而，有几个是为操作系统保留的。==

  

A wired register can be set by the OS to tell the hardware how many slots of the TLB to reserve for the OS;

==操作系统可以设置一个固定寄存器（wired register），告诉硬件为操作系统保留多少个 TLB 插槽；==

  

the OS uses these reserved mappings for code and data that it wants to access during critical times, where a TLB miss would be problematic (e.g., in the TLB miss handler).

==操作系统将这些保留的映射用于它希望在关键时刻访问的代码和数据，在这些时刻 TLB 未命中将会造成问题（例如，在 TLB 未命中处理程序中）。==

  

Because the MIPS TLB is software managed, there needs to be instructions to update the TLB.

==由于 MIPS TLB 是软件管理的，因此需要有指令来更新 TLB。==

  

The MIPS provides four such instructions:

==MIPS 提供了四条这样的指令：==

  

TLBP, which probes the TLB to see if a particular translation is in there;

==TLBP，用于探测 TLB 以查看其中是否存在特定的转换；==

  

TLBR, which reads the contents of a TLB entry into registers;

==TLBR，用于将 TLB 条目的内容读取到寄存器中；==

  

TLBWI, which replaces a specific TLB entry;

==TLBWI，用于替换特定的 TLB 条目；==

  

and TLBWR, which replaces a random TLB entry.

==以及 TLBWR，用于替换随机的 TLB 条目。==

  

The OS uses these instructions to manage the TLB's contents.

==操作系统使用这些指令来管理 TLB 的内容。==

  

It is of course critical that these instructions are privileged;

==当然，至关重要的是这些指令必须是特权指令；==

  

imagine what a user process could do if it could modify the contents of the TLB (hint: just about anything, including take over the machine, run its own malicious "OS", or even make the Sun disappear).

==想象一下，如果用户进程可以修改 TLB 的内容，它能做什么（提示：几乎任何事情，包括接管机器、运行自己的恶意“操作系统”，甚至让太阳消失）。==

  

19.8 Summary

==19.8 总结==

  

We have seen how hardware can help us make address translation faster.

==我们已经看到了硬件如何帮助我们加快地址转换。==

  

By providing a small, dedicated on-chip TLB as an address-translation cache, most memory references will hopefully be handled without having to access the page table in main memory.

==通过提供一个小型的、专用的片上 TLB 作为地址转换缓存，大多数内存引用有望在无需访问主内存中页表的情况下得到处理。==

  

Thus, in the common case, the performance of the program will be almost as if memory isn't being virtualized at all, an excellent achievement for an operating system, and certainly essential to the use of paging in modern systems.

==因此，在通常情况下，程序的性能几乎就像内存根本没有被虚拟化一样，这对操作系统来说是一个卓越的成就，当然也是现代系统中使用分页必不可少的。==

  

However, TLBs do not make the world rosy for every program that exists.

==然而，TLB 并没有让每个现存程序的世界都变得美好。==

  

In particular, if the number of pages a program accesses in a short period of time exceeds the number of pages that fit into the TLB, the program will generate a large number of TLB misses, and thus run quite a bit more slowly.

==特别是，如果一个程序在短时间内访问的页面数量超过了 TLB 能够容纳的页面数量，该程序将产生大量的 TLB 未命中，从而运行得相当缓慢。==

  

We refer to this phenomenon as exceeding the TLB coverage, and it can be quite a problem for certain programs.

==我们将这种现象称为超出 TLB 覆盖范围，对于某些程序来说，这可能是一个相当大的问题。==

  

One solution, as we'll discuss in the next chapter, is to include support for larger page sizes;

==正如我们将在下一章讨论的那样，一种解决方案是增加对更大页面大小的支持；==

  

by mapping key data structures into regions of the program's address space that are mapped by larger pages, the effective coverage of the TLB can be increased.

==通过将关键数据结构映射到由更大页面映射的程序地址空间区域，可以增加 TLB 的有效覆盖范围。==

  

Support for large pages is often exploited by programs such as a database management system (a DBMS), which have certain data structures that are both large and randomly-accessed.

==对大页面的支持通常被数据库管理系统（DBMS）等程序所利用，这些程序拥有既庞大又被随机访问的特定数据结构。==

  

One other TLB issue worth mentioning: TLB access can easily become a bottleneck in the CPU pipeline, in particular with what is called a physically-indexed cache.

==另一个值得一提的 TLB 问题是：TLB 访问很容易成为 CPU 流水线中的瓶颈，特别是对于所谓的物理索引缓存。==

  

With such a cache, address translation has to take place before the cache is accessed, which can slow things down quite a bit.

==对于这种缓存，地址转换必须在访问缓存之前发生，这可能会使速度变慢不少。==

  

Because of this potential problem, people have looked into all sorts of clever ways to access caches with virtual addresses, thus avoiding the expensive step of translation in the case of a cache hit.

==由于这个潜在的问题，人们已经研究了各种巧妙的方法来使用虚拟地址访问缓存，从而在缓存命中的情况下避免昂贵的转换步骤。==

  

Such a virtually-indexed cache solves some performance problems, but introduces new issues into hardware design as well.

==这种虚拟索引缓存解决了一些性能问题，但也给硬件设计带来了新的问题。==

  

Homework (Measurement)

==家庭作业（测量）==

  

In this homework, you are to measure the size and cost of accessing a TLB.

==在这个作业中，你要测量 TLB 的大小和访问成本。==

  

The idea is based on work by Saavedra-Barrera [SB92], who developed a simple but beautiful method to measure numerous aspects of cache hierarchies, all with a very simple user-level program.

==这个想法基于 Saavedra-Barrera [SB92] 的工作，他开发了一种简单而优美的方法来测量缓存层次结构的许多方面，所有这些都只需要一个非常简单的用户级程序。==

  

The basic idea is to access some number of pages within a large data structure (e.g., an array) and to time those accesses.

==基本的想法是访问大型数据结构（例如数组）中的一定数量的页面，并对这些访问进行计时。==

  

For example, let's say the TLB size of a machine happens to be 4 (which would be very small, but useful for the purposes of this discussion).

==例如，假设一台机器的 TLB 大小恰好是 4（这非常小，但对于本次讨论很有用）。==

  

If you write a program that touches 4 or fewer pages, each access should be a TLB hit, and thus relatively fast.

==如果你编写一个程序访问 4 个或更少的页面，每次访问都应该是 TLB 命中，因此相对较快。==

  

However, once you touch 5 pages or more, repeatedly in a loop, each access will suddenly jump in cost, to that of a TLB miss.

==然而，一旦你在循环中重复访问 5 个或更多的页面，每次访问的成本将突然跳升至 TLB 未命中的成本。==

  

Figure 19.5: Discovering TLB Sizes and Miss Costs

==图 19.5：发现 TLB 大小和未命中成本==

  

Figure 19.5 (page 15) shows the average time per access as the number of pages accessed in the loop is increased.

==图 19.5（第 15 页）显示了随着循环中访问页面数量的增加，每次访问的平均时间。==

  

As you can see in the graph, when just a few pages are accessed (8 or fewer), the average access time is roughly 5 nanoseconds.

==正如你在图表中看到的，当只访问少数几个页面（8 个或更少）时，平均访问时间大约是 5 纳秒。==

  

When 16 or more pages are accessed, there is a sudden jump to about 20 nanoseconds per access.

==当访问 16 个或更多页面时，每次访问的时间突然跳升至约 20 纳秒。==

  

A final jump in cost occurs at around 1024 pages, at which point each access takes around 70 nanoseconds.

==成本的最后一次跳跃发生在约 1024 个页面时，此时每次访问大约需要 70 纳秒。==

  

From this data, we can conclude that there is a two-level TLB hierarchy;

==从这些数据中，我们可以得出结论，存在一个两级 TLB 层次结构；==

  

the first is quite small (probably holding between 8 and 16 entries);

==第一级相当小（可能包含 8 到 16 个条目）；==

  

the second is larger but slower (holding roughly 512 entries).

==第二级较大但较慢（大约包含 512 个条目）。==

  

The overall difference between hits in the first-level TLB and misses is quite large, roughly a factor of fourteen.

==一级 TLB 命中与未命中之间的总体差异相当大，大约是十四倍。==

  

TLB performance matters!

==TLB 性能很重要！==

  

Paging: Smaller Tables

==分页：更小的表==

  

We now tackle the second problem that paging introduces: page tables are too big and thus consume too much memory.

==我们现在解决分页引入的第二个问题：页表太大，因此消耗太多内存。==

  

Let's start out with a linear page table.

==让我们从线性页表开始。==

  

As you might recall, linear page tables get pretty big.

==正如你可能记得的那样，线性页表变得非常大。==

  

Assume again a 32-bit address space ( bytes), with 4KB ( byte) pages and a 4-byte page-table entry.

==再次假设一个 32 位地址空间（ 字节），具有 4KB（ 字节）页面和 4 字节的页表项。==

  

An address space thus has roughly one million virtual pages in it ;

==因此，一个地址空间中大约有一百万个虚拟页面 ；==

  

multiply by the page-table entry size and you see that our page table is 4MB in size.

==乘以页表项的大小，你会看到我们的页表大小为 4MB。==

  

Recall also: we usually have one page table for every process in the system!

==还请回想一下：我们通常为系统中的每个进程都有一个页表！==

  

With a hundred active processes (not uncommon on a modern system), we will be allocating hundreds of megabytes of memory just for page tables!

==如果有一百个活动进程（在现代系统中并不罕见），我们将仅为页表分配数亿字节的内存！==

  

As a result, we are in search of some techniques to reduce this heavy burden.

==因此，我们正在寻找一些技术来减轻这一沉重负担。==

  

CRUX: HOW TO MAKE PAGE TABLES SMALLER?

==关键问题：如何让页表变小？==

  

Simple array-based page tables (usually called linear page tables) are too big, taking up far too much memory on typical systems.

==简单的基于数组的页表（通常称为线性页表）太大，在典型系统上占用了太多内存。==

  

How can we make page tables smaller?

==我们要如何让页表变小？==

  

What are the key ideas?

==关键思路是什么？==

  

What inefficiencies arise as a result of these new data structures?

==这些新数据结构会导致什么效率低下的问题？==

  

20.1 Simple Solution: Bigger Pages

==20.1 简单的解决方案：更大的页面==

  

We could reduce the size of the page table in one simple way: use bigger pages.

==我们可以用一种简单的方法来减小页表的大小：使用更大的页面。==

  

Take our 32-bit address space again, but this time assume 16KB pages.

==再次以我们的 32 位地址空间为例，但这次假设页面大小为 16KB。==

  

We would thus have an 18-bit VPN plus a 14-bit offset.

==因此我们将拥有一个 18 位的 VPN 加上一个 14 位的偏移量。==

  

Assuming the same size for each PTE (4 bytes), we now have  entries in our linear page table and thus a total size of 1MB per page table, a factor of four reduction in size of the page table (not surprisingly, the reduction exactly mirrors the factor of four increase in page size).

==假设每个 PTE 的大小相同（4 字节），我们的线性页表中现在有  个条目，因此每个页表的总大小为 1MB，页表大小减少了四倍（不足为奇的是，这种减少完全反映了页面大小增加的四倍）。==

  

The major problem with this approach, however, is that big pages lead to waste within each page, a problem known as internal fragmentation (as the waste is internal to the unit of allocation).

==然而，这种主要方法的问题在于，大页面会导致每个页面内部的浪费，这个问题被称为内部碎片（因为浪费发生在分配单元的内部）。==

  

Applications thus end up allocating pages but only using little bits and pieces of each, and memory quickly fills up with these overly-large pages.

==因此，应用程序最终分配了页面，但只使用了每个页面的一小部分，内存很快就被这些过大的页面填满了。==

  

Thus, most systems use relatively small page sizes in the common case: 4KB (as in x86) or 8KB (as in SPARCv9).

==因此，大多数系统在通常情况下使用相对较小的页面大小：4KB（如 x86）或 8KB（如 SPARCv9）。==

  

ASIDE: MULTIPLE PAGE SIZES

==旁白：多种页面大小==

  

As an aside, do note that many architectures (e.g., MIPS, SPARC, x86-64) now support multiple page sizes.

==顺便提一下，请注意许多架构（例如 MIPS、SPARC、x86-64）现在支持多种页面大小。==

  

Usually, a small (4KB or 8KB) page size is used.

==通常使用较小的（4KB 或 8KB）页面大小。==

  

However, if a "smart" application requests it, a single large page (e.g., of size 4MB) can be used for a specific portion of the address space, enabling such applications to place a frequently-used (and large) data structure in such a space while consuming only a single TLB entry.

==然而，如果一个“智能”应用程序请求，可以为地址空间的特定部分使用单个大页面（例如，大小为 4MB），使此类应用程序能够将常用（且大型）的数据结构放置在这样的空间中，同时仅消耗一个 TLB 条目。==

  

This type of large page usage is common in database management systems and other high-end commercial applications.

==这种大页面的使用在数据库管理系统和其他高端商业应用中很常见。==

  

The main reason for multiple page sizes is not to save page table space, however;

==然而，多种页面大小的主要原因并不是为了节省页表空间；==

  

it is to reduce pressure on the TLB, enabling a program to access more of its address space without suffering from too many TLB misses.

==而是为了减轻 TLB 的压力，使程序能够访问更多的地址空间，而不会遭遇太多的 TLB 未命中。==

  

20.2 Hybrid Approach: Paging and Segments

==20.2 混合方法：分页和分段==

  

Whenever you have two reasonable but different approaches to something in life, you should always examine the combination of the two to see if you can obtain the best of both worlds.

==无论何时，如果你对生活中的某件事有两种合理但不同的方法，你都应该检查这两者的结合，看看是否能两全其美。==

  

We call such a combination a hybrid.

==我们称这种组合为混合体。==

  

Years ago, the creators of Multics (in particular Jack Dennis) chanced upon such an idea in the construction of the Multics virtual memory system [M07].

==多年前，Multics 的创造者（特别是 Jack Dennis）在构建 Multics 虚拟内存系统时偶然发现了这样一个想法 [M07]。==

  

Specifically, Dennis had the idea of combining paging and segmentation in order to reduce the memory overhead of page tables.

==具体来说，Dennis 提出了结合分页和分段的想法，以减少页表的内存开销。==

  

We can see why this might work by examining a typical linear page table in more detail.

==我们可以通过更详细地检查典型的线性页表来了解为什么这可能行得通。==

  

Assume we have an address space in which the used portions of the heap and stack are small.

==假设我们要处理一个地址空间，其中堆和栈的已使用部分很小。==

  

For the example, we use a tiny 16KB address space with 1KB pages (Figure 20.1);

==在这个例子中，我们使用一个具有 1KB 页面的微型 16KB 地址空间（图 20.1）；==

  

the page table for this address space is in Figure 20.2.

==该地址空间的页表如图 20.2 所示。==

  

Figure 20.1: A 16KB Address Space With 1KB Pages

==图 20.1：具有 1KB 页面的 16KB 地址空间==

  

Figure 20.2: A Page Table For 16KB Address Space

==图 20.2：16KB 地址空间的页表==

  

This example assumes the single code page (VPN 0) is mapped to physical page 10, the single heap page (VPN 4) to physical page 23, and the two stack pages at the other end of the address space (VPNs 14 and 15) are mapped to physical pages 28 and 4, respectively.

==该示例假设单个代码页（VPN 0）映射到物理页 10，单个堆页（VPN 4）映射到物理页 23，地址空间另一端的两个栈页（VPN 14 和 15）分别映射到物理页 28 和 4。==

  

As you can see from the picture, most of the page table is unused, full of invalid entries.

==从图中可以看出，页表的大部分未被使用，充满了无效条目。==

  

What a waste!

==真是浪费！==

  

And this is for a tiny 16KB address space.

==这还是针对一个微小的 16KB 地址空间。==

  

Imagine the page table of a 32-bit address space and all the potential wasted space in there!

==想象一下 32 位地址空间的页表以及其中所有潜在的浪费空间！==

  

Thus, our hybrid approach: instead of having a single page table for the entire address space of the process, why not have one per logical segment?

==因此，我们的混合方法是：与其为进程的整个地址空间保留单个页表，为什么不为每个逻辑段保留一个页表呢？==

  

In this example, we might thus have three page tables, one for the code, heap, and stack parts of the address space.

==在这个例子中，我们因此可能有三个页表，分别用于地址空间的代码、堆和栈部分。==

  

Now, remember with segmentation, we had a base register that told us where each segment lived in physical memory, and a bound or limit register that told us the size of said segment.

==现在，请记住在分段中，我们有一个基址寄存器告诉我们每个段在物理内存中的位置，还有一个界限或限制寄存器告诉我们该段的大小。==

  

In our hybrid, we still have those structures in the MMU;

==在我们的混合方案中，我们在 MMU 中仍然有这些结构；==

  

here, we use the base not to point to the segment itself but rather to hold the physical address of the page table of that segment.

==在这里，我们使用基址不是指向段本身，而是保存该段页表的物理地址。==

  

The bounds register is used to indicate the end of the page table (i.e., how many valid pages it has).

==界限寄存器用于指示页表的结束（即它有多少个有效页面）。==

  

Let's do a simple example to clarify.

==让我们做一个简单的例子来阐明。==

  

Assume a 32-bit virtual address space with 4KB pages, and an address space split into four segments.

==假设一个具有 4KB 页面的 32 位虚拟地址空间，并且地址空间被分成四个段。==

  

We'll only use three segments for this example: one for code, one for heap, and one for stack.

==在这个例子中我们只使用三个段：一个用于代码，一个用于堆，一个用于栈。==

  

To determine which segment an address refers to, we'll use the top two bits of the address space.

==为了确定一个地址指的是哪个段，我们将使用地址空间的前两位。==

  

Let's assume 00 is the unused segment, with 01 for code, 10 for the heap, and 11 for the stack.

==假设 00 是未使用段，01 是代码段，10 是堆段，11 是栈段。==

  

Thus, a virtual address looks like this: Seg | VPN | Offset

==因此，虚拟地址看起来像这样：段号 | VPN | 偏移量==

  

In the hardware, assume that there are thus three base/bounds pairs, one each for code, heap, and stack.

==在硬件中，假设因此有三对基址/界限寄存器，分别用于代码、堆和栈。==

  

When a process is running, the base register for each of these segments contains the physical address of a linear page table for that segment;

==当进程运行时，这些段的每个基址寄存器都包含该段线性页表的物理地址；==

  

thus, each process in the system now has three page tables associated with it.

==因此，系统中的每个进程现在都有三个与其关联的页表。==

  

On a context switch, these registers must be changed to reflect the location of the page tables of the newly-running process.

==在上下文切换时，必须更改这些寄存器以反映新运行进程的页表位置。==

  

On a TLB miss (assuming a hardware-managed TLB, i.e., where the hardware is responsible for handling TLB misses), the hardware uses the segment bits (SN) to determine which base and bounds pair to use.

==在 TLB 未命中时（假设是硬件管理的 TLB，即硬件负责处理 TLB 未命中），硬件使用段位（SN）来确定使用哪一对基址和界限。==

  

TIP: USE HYBRIDS

==提示：使用混合体==

  

When you have two good and seemingly opposing ideas, you should always see if you can combine them into a hybrid that manages to achieve the best of both worlds.

==当你有两个好的且看似对立的想法时，你应该总是看看是否可以将它们结合成一个混合体，从而设法两全其美。==

  

The critical difference in our hybrid scheme is the presence of a bounds register per segment;

==我们要混合方案的关键区别在于每个段都有一个界限寄存器；==

  

each bounds register holds the value of the maximum valid page in the segment.

==每个界限寄存器保存段中最大有效页面的值。==

  

For example, if the code segment is using its first three pages (0, 1, and 2), the code segment page table will only have three entries allocated to it and the bounds register will be set to 3;

==例如，如果代码段使用其前三个页面（0、1 和 2），则代码段页表将只为其分配三个条目，并且界限寄存器将被设置为 3；==

  

memory accesses beyond the end of the segment will generate an exception and likely lead to the termination of the process.

==超出段末尾的内存访问将产生异常，并可能导致进程终止。==

  

In this manner, our hybrid approach realizes a significant memory savings compared to the linear page table;

==通过这种方式，与线性页表相比，我们的混合方法实现了显著的内存节省；==

  

unallocated pages between the stack and the heap no longer take up space in a page table (just to mark them as not valid).

==堆栈和堆之间的未分配页面不再在页表中占用空间（仅仅为了将它们标记为无效）。==

  

However, as you might notice, this approach is not without problems.

==然而，你可能已经注意到，这种方法并非没有问题。==

  

First, it still requires us to use segmentation;

==首先，它仍然要求我们使用分段；==

  

as we discussed before, segmentation is not quite as flexible as we would like, as it assumes a certain usage pattern of the address space;

==正如我们要之前讨论的那样，分段并不像我们希望的那样灵活，因为它假设了地址空间的某种使用模式；==

  

if we have a large but sparsely-used heap, for example, we can still end up with a lot of page table waste.

==例如，如果我们有一个很大但使用稀疏的堆，我们仍然可能最终造成大量的页表浪费。==

  

Second, this hybrid causes external fragmentation to arise again.

==其次，这种混合方法会导致外部碎片再次出现。==

  

While most of memory is managed in page-sized units, page tables now can be of arbitrary size (in multiples of PTEs).

==虽然大多数内存是以页面大小为单位进行管理的，但页表现在可以是任意大小（PTE 的倍数）。==

  

Thus, finding free space for them in memory is more complicated.

==因此，在内存中为它们寻找空闲空间变得更加复杂。==

  

For these reasons, people continued to look for better ways to implement smaller page tables.

==由于这些原因，人们继续寻找实现更小页表的更好方法。==

  

20.3 Multi-level Page Tables

==20.3 多级页表==

  

A different approach doesn't rely on segmentation but attacks the same problem: how to get rid of all those invalid regions in the page table instead of keeping them all in memory?

==另一种不同的方法不依赖于分段，而是解决同样的问题：如何去掉页表中所有那些无效区域，而不是将它们全部保留在内存中？==

  

We call this approach a multi-level page table, as it turns the linear page table into something like a tree.

==我们将这种方法称为多级页表，因为它将线性页表变成了类似树的结构。==

  

This approach is so effective that many modern systems employ it (e.g., x86 [BOH10]).

==这种方法非常有效，以至于许多现代系统都采用它（例如，x86 [BOH10]）。==

  

We now describe this approach in detail.

==我们现在详细描述这种方法。==

  

The basic idea behind a multi-level page table is simple.

==多级页表背后的基本思想很简单。==

  

First, chop up the page table into page-sized units;

==首先，将页表切分成页面大小的单元；==

  

then, if an entire page of page-table entries (PTEs) is invalid, don't allocate that page of the page table at all.

==然后，如果整页的页表项（PTE）都是无效的，就根本不分配该页的页表。==

  

To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the page directory.

==为了跟踪页表的一个页面是否有效（如果有效，它在内存中的位置），使用一种新的结构，称为页目录。==

  

The page directory thus either can be used to tell you where a page of the page table is, or that the entire page of the page table contains no valid pages.

==因此，页目录要么可以用来告诉你页表的一个页面在哪里，要么告诉你页表的整个页面都不包含有效页面。==

  

Figure 20.3: Linear (Left) And Multi-Level (Right) Page Tables

==图 20.3：线性（左）和多级（右）页表==

  

Figure 20.3 shows an example.

==图 20.3 展示了一个例子。==

  

On the left of the figure is the classic linear page table;

==图的左边是经典的线性页表；==

  

even though most of the middle regions of the address space are not valid, we still require page-table space allocated for those regions (i.e., the middle two pages of the page table).

==即使地址空间的大部分中间区域是无效的，我们仍然需要为这些区域分配页表空间（即页表的中间两页）。==

  

On the right is a multi-level page table.

==右边是一个多级页表。==

  

The page directory marks just two pages of the page table as valid (the first and last);

==页目录仅将页表的两个页面标记为有效（第一个和最后一个）；==

  

thus, just those two pages of the page table reside in memory.

==因此，只有页表的这两个页面驻留在内存中。==

  

And thus you can see one way to visualize what a multi-level table is doing: it just makes parts of the linear page table disappear (freeing those frames for other uses), and tracks which pages of the page table are allocated with the page directory.

==因此，你可以看到一种可视化多级页表作用的方法：它只是让线性页表的一部分消失（释放这些帧供其他用途），并使用页目录跟踪分配了哪些页表的页面。==

  

The page directory, in a simple two-level table, contains one entry per page of the page table.

==在一个简单的二级表中，页目录包含对应于页表每一页的一个条目。==

  

It consists of a number of page directory entries (PDE).

==它由许多页目录项（PDE）组成。==

  

A PDE (minimally) has a valid bit and a page frame number (PFN), similar to a PTE.

==一个 PDE（最小限度地）拥有一个有效位和一个物理帧号（PFN），类似于 PTE。==

  

However, as hinted at above, the meaning of this valid bit is slightly different:

==然而，如上文暗示的那样，这个有效位的含义略有不同：==

  

if the PDE is valid, it means that at least one of the pages of the page table that the entry points to (via the PFN) is valid, i.e., in at least one PTE on that page pointed to by this PDE, the valid bit in that PTE is set to one.

==如果 PDE 是有效的，这意味着该条目（通过 PFN）指向的页表页面中至少有一个是有效的，即在该 PDE 指向的页面上的至少一个 PTE 中，该 PTE 的有效位被设置为 1。==

  

If the PDE is not valid (i.e., equal to zero), the rest of the PDE is not defined.

==如果 PDE 无效（即等于零），则 PDE 的其余部分未定义。==

  

Multi-level page tables have some obvious advantages over approaches we've seen thus far.

==多级页表比我们目前看到的方法具有一些明显的优势。==

  

First, and perhaps most obviously, the multi-level table only allocates page-table space in proportion to the amount of address space you are using;

==首先，也许最明显的是，多级表仅根据你使用的地址空间量来分配页表空间；==

  

thus it is generally compact and supports sparse address spaces.

==因此它通常很紧凑，并支持稀疏的地址空间。==

  

Second, if carefully constructed, each portion of the page table fits neatly within a page, making it easier to manage memory;

==其次，如果构建得当，页表的每一部分都可以整齐地放入一个页面中，从而更容易管理内存；==

  

the OS can simply grab the next free page when it needs to allocate or grow a page table.

==当操作系统需要分配或增长页表时，它可以简单地抓取下一个空闲页面。==

  

Contrast this to a simple (non-paged) linear page table, which is just an array of PTEs indexed by VPN;

==将其与简单的（非分页）线性页表进行对比，后者只是一个由 VPN 索引的 PTE 数组；==

  

with such a structure, the entire linear page table must reside contiguously in physical memory.

==对于这种结构，整个线性页表必须连续地驻留在物理内存中。==

  

For a large page table (say 4MB), finding such a large chunk of unused contiguous free physical memory can be quite a challenge.

==对于一个大型页表（例如 4MB），找到如此大块未使用的连续空闲物理内存可能是一个相当大的挑战。==

  

With a multi-level structure, we add a level of indirection through use of the page directory, which points to pieces of the page table;

==通过多级结构，我们通过使用指向页表片段的页目录增加了一级间接性；==

  

that indirection allows us to place page-table pages wherever we would like in physical memory.

==这种间接性允许我们将页表页面放置在物理内存中我们想要的任何位置。==

  

It should be noted that there is a cost to multi-level tables;

==应该注意的是，多级表是有代价的；==

  

on a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the page directory, and one for the PTE itself), in contrast to just one load with a linear page table.

==在 TLB 未命中时，需要从内存中加载两次才能从页表中获取正确的转换信息（一次用于页目录，一次用于 PTE 本身），而线性页表只需要加载一次。==

  

Thus, the multi-level table is a small example of a time-space trade-off.

==因此，多级表是时间-空间权衡的一个小例子。==

  

We wanted smaller tables (and got them), but not for free;

==我们想要更小的表（并且得到了），但不是免费的；==

  

although in the common case (TLB hit), performance is obviously identical, a TLB miss suffers from a higher cost with this smaller table.

==虽然在通常情况下（TLB 命中），性能显然是相同的，但使用这种较小的表，TLB 未命中的成本更高。==

  

Another obvious negative is complexity.

==另一个明显的缺点是复杂性。==

  

Whether it is the hardware or OS handling the page-table lookup (on a TLB miss), doing so is undoubtedly more involved than a simple linear page-table lookup.

==无论是硬件还是操作系统处理页表查找（在 TLB 未命中时），这样做无疑比简单的线性页表查找更复杂。==

  

A Detailed Multi-Level Example

==一个详细的多级示例==

  

To understand the idea behind multi-level page tables better, let's do an example.

==为了更好地理解多级页表背后的思想，让我们做一个示例。==

  

Imagine a small address space of size 16KB, with 64-byte pages.

==想象一个大小为 16KB、页面大小为 64 字节的小型地址空间。==

  

Thus, we have a 14-bit virtual address space, with 8 bits for the VPN and 6 bits for the offset.

==因此，我们有一个 14 位的虚拟地址空间，其中 8 位用于 VPN，6 位用于偏移量。==

  

A linear page table would have  (256) entries, even if only a small portion of the address space is in use.

==线性页表将有  (256) 个条目，即使只有一小部分地址空间被使用。==

  

Figure 20.4: A 16KB Address Space With 64-byte Pages

==图 20.4：具有 64 字节页面的 16KB 地址空间==

  

To build a two-level page table for this address space, we start with our full linear page table and break it up into page-sized units.

==为了为此地址空间构建二级页表，我们从完整的线性页表开始，并将其分解为页面大小的单元。==

  

Recall our full table (in this example) has 256 entries;

==回想一下我们的完整表（在这个例子中）有 256 个条目；==

  

assume each PTE is 4 bytes in size.

==假设每个 PTE 的大小为 4 字节。==

  

Thus, our page table is 1KB ( bytes) in size.

==因此，我们的页表大小为 1KB ( 字节)。==

  

Given that we have 64-byte pages, the 1KB page table can be divided into 16 64-byte pages;

==鉴于我们有 64 字节的页面，1KB 的页表可以分为 16 个 64 字节的页面；==

  

each page can hold 16 PTEs.

==每个页面可以容纳 16 个 PTE。==

  

What we need to understand now is how to take a VPN and use it to index first into the page directory and then into the page of the page table.

==我们现在需要理解的是如何获取 VPN 并使用它首先索引到页目录，然后索引到页表的页面。==

  

Remember that each is an array of entries; thus, all we need to figure out is how to construct the index for each from pieces of the VPN.

==请记住，每个都是条目数组；因此，我们只需要弄清楚如何从 VPN 的片段中为每个构建索引。==

  

Let's first index into the page directory.

==让我们首先索引到页目录。==

  

Our page table in this example is small: 256 entries, spread across 16 pages.

==我们在这个例子中的页表很小：256 个条目，分布在 16 个页面上。==

  

The page directory needs one entry per page of the page table; thus, it has 16 entries.

==页目录需要页表的每一页有一个条目；因此，它有 16 个条目。==

  

As a result, we need four bits of the VPN to index into the directory;

==结果，我们需要 VPN 的 4 位来索引目录；==

  

we use the top four bits of the VPN, as follows:

==我们使用 VPN 的前四位，如下所示：==

  

VPN: 13 12 11 10 | 9 8 7 6 5 4 | 3 2 1 0

VPN: 13 12 11 10 | 9 8 7 6 5 4 | 3 2 1 0

  

Page Directory Index: 3 2 1 0

==页目录索引：3 2 1 0==

  

Once we extract the page-directory index (PDIndex for short) from the VPN, we can use it to find the address of the page-directory entry (PDE) with a simple calculation: .

==一旦我们从 VPN 中提取了页目录索引（简称 PDIndex），我们可以通过一个简单的计算来找到页目录项（PDE）的地址：。==

  

This results in our page directory, which we now examine to make further progress in our translation.

==这产生了我们的页目录，我们现在检查它以便在转换中取得进一步进展。==

  

If the page-directory entry is marked invalid, we know that the access is invalid, and thus raise an exception.

==如果页目录项被标记为无效，我们就知道访问是无效的，因此引发异常。==

  

If, however, the PDE is valid, we have more work to do.

==然而，如果 PDE 是有效的，我们要有更多的工作要做。==

  

Specifically, we now have to fetch the page-table entry (PTE) from the page of the page table pointed to by this page-directory entry.

==具体来说，我们现在必须从该页目录项指向的页表页面中获取页表项（PTE）。==

  

To find this PTE, we have to index into the portion of the page table using the remaining bits of the VPN:

==为了找到这个 PTE，我们必须使用 VPN 的剩余位索引到页表的那一部分：==

  

This page-table index (PTIndex for short) can then be used to index into the page table itself, giving us the address of our PTE:

==这个页表索引（简称 PTIndex）然后可以用来索引到页表本身，给我们提供 PTE 的地址：==

  
  
  
  

Note that the page-frame number obtained from the page-directory entry must be left-shifted into place before combining it with the page-table index to form the address of the PTE.

==请注意，从页目录项获取的页帧号必须左移到位，然后才能与页表索引组合以形成 PTE 的地址。==

PAGING: SMALLER TABLES

==分页：较小的页表==

  

Homework (Simulation)

==作业（模拟）==

  

This fun little homework tests if you understand how a multi-level page table works.

==这个有趣的小作业是为了测试你是否理解多级页表是如何工作的。==

  

And yes, there is some debate over the use of the term "fun" in the previous sentence.

==当然，对于上一句话中“有趣”这个词的使用，确实存在一些争议。==

  

The program is called, perhaps unsurprisingly: paging-multilevel-translate.py; see the README for details.

==该程序的名字或许并不令人意外：paging-multilevel-translate.py；详情请参阅 README 文件。==

  

Questions

==问题==

  

1. With a linear page table, you need a single register to locate the page table, assuming that hardware does the lookup upon a TLB miss.

==2. 对于线性页表，假设硬件在 TLB 未命中时执行查找，你需要一个寄存器来定位页表。==

  

How many registers do you need to locate a two-level page table? A three-level table?

==定位一个二级页表需要多少个寄存器？三级页表呢？==

  

2. Use the simulator to perform translations given random seeds 0, 1, and 2, and check your answers using the -c flag.

==3. 使用模拟器对随机种子 0、1 和 2 执行地址转换，并使用 -c 标志检查你的答案。==

  

How many memory references are needed to perform each lookup?

==每次查找需要多少次内存引用？==

  

3. Given your understanding of how cache memory works, how do you think memory references to the page table will behave in the cache?

==4. 基于你对缓存（cache）工作原理的理解，你认为对页表的内存引用在缓存中会表现如何？==

  

Will they lead to lots of cache hits (and thus fast accesses?) Or lots of misses (and thus slow accesses)?

==它们会导致大量的缓存命中（从而实现快速访问）吗？还是会有大量的未命中（从而导致访问缓慢）？==

  

Beyond Physical Memory: Mechanisms

==超越物理内存：机制==

  

Thus far, we've assumed that an address space is unrealistically small and fits into physical memory.

==到目前为止，我们假设地址空间非常小，并且能够放入物理内存中，这在现实中是不切实际的。==

  

In fact, we've been assuming that every address space of every running process fits into memory.

==事实上，我们一直假设每个运行进程的每个地址空间都能装入内存。==

  

We will now relax these big assumptions, and assume that we wish to support many concurrently-running large address spaces.

==现在我们将放宽这些主要的假设，假设我们希望支持许多并发运行的大型地址空间。==

  

To do so, we require an additional level in the memory hierarchy.

==为此，我们需要在内存层次结构中增加一个层级。==

  

Thus far, we have assumed that all pages reside in physical memory.

==到目前为止，我们一直假设所有页面都驻留在物理内存中。==

  

However, to support large address spaces, the OS will need a place to stash away portions of address spaces that currently aren't in great demand.

==然而，为了支持大型地址空间，操作系统需要一个地方来存放当前需求不大的部分地址空间。==

  

In general, the characteristics of such a location are that it should have more capacity than memory;

==一般来说，这个位置的特征是它应该比内存拥有更大的容量；==

  

as a result, it is generally slower (if it were faster, we would just use it as memory, no?).

==因此，它通常速度较慢（如果它更快，我们就直接把它当作内存用了，不是吗？）。==

  

In modern systems, this role is usually served by a hard disk drive.

==在现代系统中，这个角色通常由硬盘驱动器来承担。==

  

Thus, in our memory hierarchy, big and slow hard drives sit at the bottom, with memory just above.

==因此，在我们的内存层次结构中，大而慢的硬盘位于底部，内存就在其上方。==

  

And thus we arrive at the crux of the problem:

==因此，我们来到了问题的关键：==

  

**THE CRUX: HOW TO GO BEYOND PHYSICAL MEMORY**

==**关键问题：如何超越物理内存**==

  

How can the OS make use of a larger, slower device to transparently provide the illusion of a large virtual address space?

==操作系统如何利用一个更大、更慢的设备来透明地提供巨大虚拟地址空间的假象？==

  

One question you might have: why do we want to support a single large address space for a process?

==你可能会有一个问题：为什么我们要为一个进程支持单个巨大的地址空间？==

  

Once again, the answer is convenience and ease of use.

==再一次，答案是为了方便和易用性。==

  

With a large address space, you don't have to worry about if there is enough room in memory for your program's data structures;

==有了巨大的地址空间，你不必担心内存中是否有足够的空间来容纳程序的数据结构；==

  

rather, you just write the program naturally, allocating memory as needed.

==相反，你只需自然地编写程序，根据需要分配内存。==

  

It is a powerful illusion that the OS provides, and makes your life vastly simpler.

==这是操作系统提供的一种强大的假象，它让你的生活变得简单多了。==

  

You're welcome!

==不客气！==

  

A contrast is found in older systems that used memory overlays, which required programmers to manually move pieces of code or data in and out of memory as they were needed [D97].

==与之形成对比的是使用内存覆盖（overlays）的旧系统，它要求程序员在需要时手动将代码或数据片段移入和移出内存 [D97]。==

  

Try imagining what this would be like: before calling a function or accessing some data, you need to first arrange for the code or data to be in memory;

==试着想象一下这会是什么样子：在调用函数或访问某些数据之前，你需要先安排将代码或数据放入内存中；==

  

yuck!

==糟透了！==

  

**ASIDE: STORAGE TECHNOLOGIES**

==**旁注：存储技术**==

  

We'll delve much more deeply into how I/O devices actually work later (see the chapter on I/O devices).

==我们稍后将深入探讨 I/O 设备实际上是如何工作的（请参阅 I/O 设备一章）。==

  

So be patient!

==所以请耐心等待！==

  

And of course the slower device need not be a hard disk, but could be something more modern such as a Flash-based SSD.

==当然，较慢的设备不一定非得是硬盘，也可以是更现代的设备，比如基于闪存的 SSD。==

  

We'll talk about those things too.

==我们也会讨论这些东西。==

  

For now, just assume we have a big and relatively-slow device which we can use to help us build the illusion of a very large virtual memory, even bigger than physical memory itself.

==现在，只需假设我们要利用一个相对较慢的大型设备，来帮助构建一个比物理内存本身还要大的虚拟内存假象。==

  

Beyond just a single process, the addition of swap space allows the OS to support the illusion of a large virtual memory for multiple concurrently-running processes.

==除了支持单个进程外，增加交换空间还允许操作系统为多个并发运行的进程提供大容量虚拟内存的假象。==

  

The invention of multiprogramming (running multiple programs "at once", to better utilize the machine) almost demanded the ability to swap out some pages, as early machines clearly could not hold all the pages needed by all processes at once.

==多道程序设计（“同时”运行多个程序，以更好地利用机器）的发明几乎强制要求具备换出某些页面的能力，因为早期的机器显然无法同时容纳所有进程所需的所有页面。==

  

Thus, the combination of multiprogramming and ease-of-use leads us to want to support using more memory than is physically available.

==因此，多道程序设计和易用性的结合，促使我们要支持使用超出物理可用范围的内存。==

  

It is something that all modern VM systems do; it is now something we will learn more about.

==这是所有现代虚拟内存系统都会做的事情；这也是我们现在要进一步学习的内容。==

  

**21.1 Swap Space**

==**21.1 交换空间**==

  

The first thing we will need to do is to reserve some space on the disk for moving pages back and forth.

==我们需要做的第一件事是在磁盘上预留一些空间，以便来回移动页面。==

  

In operating systems, we generally refer to such space as swap space, because we swap pages out of memory to it and swap pages into memory from it.

==在操作系统中，我们通常将这种空间称为交换空间（swap space），因为我们将页面从内存交换到其中，并将页面从其中交换回内存。==

  

Thus, we will simply assume that the OS can read from and write to the swap space, in page-sized units.

==因此，我们简单地假设操作系统可以按页面大小为单位对交换空间进行读写。==

  

To do so, the OS will need to remember the disk address of a given page.

==为此，操作系统需要记住给定页面的磁盘地址。==

  

The size of the swap space is important, as ultimately it determines the maximum number of memory pages that can be in use by a system at a given time.

==交换空间的大小很重要，因为它最终决定了系统在给定时间内可以使用的最大内存页面数量。==

  

Let us assume for simplicity that it is very large for now.

==为了简单起见，我们暂时假设它非常大。==

  

In the tiny example (Figure 21.1), you can see a little example of a 4-page physical memory and an 8-page swap space.

==在这个微小的例子（图 21.1）中，你可以看到一个拥有 4 页物理内存和 8 页交换空间的示例。==

  

In the example, three processes (Proc 0, Proc 1, and Proc 2) are actively sharing physical memory;

==在该示例中，三个进程（进程 0、进程 1 和进程 2）正在主动共享物理内存；==

  

each of the three, however, only have some of their valid pages in memory, with the rest located in swap space on disk.

==然而，这三个进程中每一个都只有部分有效页面在内存中，其余的则位于磁盘上的交换空间中。==

  

A fourth process (Proc 3) has all of its pages swapped out to disk, and thus clearly isn't currently running.

==第四个进程（进程 3）的所有页面都已换出到磁盘，因此显然当前没有运行。==

  

One block of swap remains free.

==交换空间中保留了一个空闲块。==

  

Even from this tiny example, hopefully you can see how using swap space allows the system to pretend that memory is larger than it actually is.

==即使从这个微小的例子中，希望你也能看出使用交换空间是如何让系统假装内存比实际要大的。==

  

We should note that swap space is not the only on-disk location for swapping traffic.

==我们需要注意的是，交换空间并不是交换流量在磁盘上的唯一位置。==

  

For example, assume you are running a program binary (e.g., ls, or your own compiled main program).

==例如，假设你正在运行一个程序二进制文件（例如 ls，或者你自己编译的主程序）。==

  

The code pages from this binary are initially found on disk, and when the program runs, they are loaded into memory (either all at once when the program starts execution, or, as in modern systems, one page at a time when needed).

==这个二进制文件的代码页最初位于磁盘上，当程序运行时，它们被加载到内存中（要么在程序开始执行时一次性全部加载，要么像现代系统那样，在需要时一次加载一页）。==

  

However, if the system needs to make room in physical memory for other needs, it can safely re-use the memory space for these code pages, knowing that it can later swap them in again from the on-disk binary in the file system.

==但是，如果系统需要腾出物理内存空间以满足其他需求，它可以安全地重用这些代码页占用的内存空间，因为它知道稍后可以从文件系统中的磁盘二进制文件再次将它们交换进来。==

  

**21.2 The Present Bit**

==**21.2 存在位**==

  

Now that we have some space on the disk, we need to add some machinery higher up in the system in order to support swapping pages to and from the disk.

==既然我们在磁盘上有了一些空间，我们需要在系统更高层添加一些机制，以支持将页面交换到磁盘或从磁盘换出。==

  

Let us assume, for simplicity, that we have a system with a hardware-managed TLB.

==为了简单起见，让我们假设我们要处理一个拥有硬件管理 TLB 的系统。==

  

Recall first what happens on a memory reference.

==首先回顾一下内存引用时会发生什么。==

  

The running process generates virtual memory references (for instruction fetches, or data accesses), and, in this case, the hardware translates them into physical addresses before fetching the desired data from memory.

==运行中的进程生成虚拟内存引用（用于指令获取或数据访问），在这种情况下，硬件在从内存中获取所需数据之前，将其转换为物理地址。==

  

Remember that the hardware first extracts the VPN from the virtual address, checks the TLB for a match (a TLB hit), and if a hit, produces the resulting physical address and fetches it from memory.

==请记住，硬件首先从虚拟地址中提取 VPN（虚拟页号），检查 TLB 是否匹配（TLB 命中），如果命中，则生成相应的物理地址并从内存中获取数据。==

  

This is hopefully the common case, as it is fast (requiring no additional memory accesses).

==这通常是常见的情况，因为它很快（不需要额外的内存访问）。==

  

If the VPN is not found in the TLB (i.e., a TLB miss), the hardware locates the page table in memory (using the page table base register) and looks up the page table entry (PTE) for this page using the VPN as an index.

==如果 VPN 未在 TLB 中找到（即 TLB 未命中），硬件会（使用页表基址寄存器）在内存中定位页表，并使用 VPN 作为索引查找该页的页表项（PTE）。==

  

If the page is valid and present in physical memory, the hardware extracts the PFN from the PTE, installs it in the TLB, and retries the instruction, this time generating a TLB hit;

==如果页面有效且存在于物理内存中，硬件会从 PTE 中提取 PFN（物理帧号），将其安装到 TLB 中，并重试该指令，这次会产生 TLB 命中；==

  

so far, so good.

==到目前为止，一切顺利。==

  

If we wish to allow pages to be swapped to disk, however, we must add even more machinery.

==然而，如果我们希望允许页面被交换到磁盘，我们必须添加更多的机制。==

  

Specifically, when the hardware looks in the PTE, it may find that the page is not present in physical memory.

==具体来说，当硬件查看 PTE 时，它可能会发现该页面不存在于物理内存中。==

  

The way the hardware (or the OS, in a software-managed TLB approach) determines this is through a new piece of information in each page-table entry, known as the present bit.

==硬件（或者在软件管理 TLB 方法中的操作系统）确定这一点的方式是通过每个页表项中的一条新信息，称为存在位（present bit）。==

  

If the present bit is set to one, it means the page is present in physical memory and everything proceeds as above;

==如果存在位设置为 1，则表示该页面存在于物理内存中，一切照常进行；==

  

if it is set to zero, the page is not in memory but rather on disk somewhere.

==如果它设置为 0，则该页面不在内存中，而是在磁盘的某个地方。==

  

**ASIDE: SWAPPING TERMINOLOGY AND OTHER THINGS**

==**旁注：交换术语及其他事项**==

  

Terminology in virtual memory systems can be a little confusing and variable across machines and operating systems.

==虚拟内存系统中的术语可能有点令人困惑，并且在不同的机器和操作系统之间会有所变化。==

  

For example, a page fault more generally could refer to any reference to a page table that generates a fault of some kind: this could include the type of fault we are discussing here, i.e., a page-not-present fault, but sometimes can refer to illegal memory accesses.

==例如，页错误（page fault）通常可以指代任何产生某种错误的页表引用：这可能包括我们这里讨论的错误类型，即“页面不存在”错误，但有时也可以指非法内存访问。==

  

Indeed, it is odd that we call what is definitely a legal access (to a page mapped into the virtual address space of a process, but simply not in physical memory at the time) a "fault" at all;

==确实，将一个绝对合法的访问（访问映射到进程虚拟地址空间但当时仅是不在物理内存中的页面）称为“错误（fault）”是很奇怪的；==

  

really, it should be called a page miss.

==实际上，它应该被称为页面未命中（page miss）。==

  

But often, when people say a program is "page faulting", they mean that it is accessing parts of its virtual address space that the OS has swapped out to disk.

==但通常情况下，当人们说程序正在发生“页错误”时，他们的意思是它正在访问操作系统已换出到磁盘的那部分虚拟地址空间。==

  

We suspect the reason that this behavior became known as a "fault" relates to the machinery in the operating system to handle it.

==我们怀疑这种行为被称为“错误”的原因与操作系统处理它的机制有关。==

  

When something unusual happens, i.e., when something the hardware doesn't know how to handle occurs, the hardware simply transfers control to the OS, hoping it can make things better.

==当发生异常情况时，即当发生硬件不知道如何处理的事情时，硬件只是将控制权转移给操作系统，希望它能改善状况。==

  

In this case, a page that a process wants to access is missing from memory;

==在这种情况下，进程想要访问的页面从内存中丢失了；==

  

the hardware does the only thing it can, which is raise an exception, and the OS takes over from there.

==硬件做了它唯一能做的事情，即引发一个异常，然后由操作系统接管。==

  

As this is identical to what happens when a process does something illegal, it is perhaps not surprising that we term the activity a "fault."

==由于这与进程执行非法操作时发生的情况完全相同，因此我们将此活动称为“错误”也许并不令人惊讶。==

  

The act of accessing a page that is not in physical memory is commonly referred to as a page fault.

==访问不在物理内存中的页面的行为通常被称为页错误（page fault）。==

  

Upon a page fault, the OS is invoked to service the page fault.

==发生页错误时，操作系统会被调用来处理该页错误。==

  

A particular piece of code, known as a page-fault handler, runs, and must service the page fault, as we now describe.

==一段特定的代码，称为页错误处理程序（page-fault handler），会运行并必须处理该页错误，正如我们要描述的那样。==

  

**21.3 The Page Fault**

==**21.3 页错误**==

  

Recall that with TLB misses, we have two types of systems: hardware-managed TLBs (where the hardware looks in the page table to find the desired translation) and software-managed TLBs (where the OS does).

==回想一下，对于 TLB 未命中，我们有两种类型的系统：硬件管理的 TLB（硬件在页表中查找所需的转换）和软件管理的 TLB（操作系统进行查找）。==

  

In either type of system, if a page is not present, the OS is put in charge to handle the page fault.

==在这两种类型的系统中，如果页面不存在，操作系统都负责处理页错误。==

  

The appropriately-named OS page-fault handler runs to determine what to do.

==名称恰当的操作系统页错误处理程序会运行以决定做什么。==

  

Virtually all systems handle page faults in software;

==几乎所有的系统都在软件中处理页错误；==

  

even with a hardware-managed TLB, the hardware trusts the OS to manage this important duty.

==即使是硬件管理的 TLB，硬件也信任操作系统来管理这一重要职责。==

  

If a page is not present and has been swapped to disk, the OS will need to swap the page into memory in order to service the page fault.

==如果页面不存在且已被交换到磁盘，操作系统将需要把该页面交换到内存中以处理页错误。==

  

Thus, a question arises: how will the OS know where to find the desired page?

==因此，出现了一个问题：操作系统如何知道在哪里可以找到所需的页面？==

  

In many systems, the page table is a natural place to store such information.

==在许多系统中，页表是存储此类信息的天然场所。==

  

Thus, the OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address.

==因此，操作系统可以使用 PTE 中通常用于存储页面 PFN 等数据的位来存储磁盘地址。==

  

When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory.

==当操作系统收到某个页面的页错误时，它会查看 PTE 以找到该地址，并向磁盘发出请求以将该页面获取到内存中。==

Figure 22.6: The No-Locality Workload

==图 22.6：无局部性负载==

  

Figure 22.6 plots the results of the experiment for optimal, LRU, Random, and FIFO.

==图 22.6 绘制了最佳（Optimal）、LRU、随机（Random）和 FIFO 策略的实验结果。==

  

The y-axis of the figure shows the hit rate that each policy achieves;

==图的 y 轴显示了每种策略实现的命中率；==

  

the x-axis varies the cache size as described above.

==x 轴如上所述改变缓存大小。==

  

We can draw a number of conclusions from the graph.

==我们可以从图中得出许多结论。==

  

First, when there is no locality in the workload, it doesn't matter much which realistic policy you are using;

==首先，当工作负载中没有局部性时，使用哪种现实策略并不重要；==

  

LRU, FIFO, and Random all perform the same, with the hit rate exactly determined by the size of the cache.

==LRU、FIFO 和随机策略的表现都相同，命中率完全由缓存的大小决定。==

  

Second, when the cache is large enough to fit the entire workload, it also doesn't matter which policy you use;

==其次，当缓存大到足以容纳整个工作负载时，使用哪种策略也不重要；==

  

all policies (even Random) converge to a 100% hit rate when all the referenced blocks fit in cache.

==当所有被引用的块都放入缓存时，所有策略（甚至是随机策略）都会收敛到 100% 的命中率。==

  

Finally, you can see that optimal performs noticeably better than the realistic policies;

==最后，你可以看到最佳策略的表现明显优于现实策略；==

  

peeking into the future, if it were possible, does a much better job of replacement.

==如果可能的话，窥探未来在替换方面能做得更好。==

  

The next workload we examine is called the "80-20" workload, which exhibits locality: 80% of the references are made to 20% of the pages (the "hot" pages);

==我们要检查的下一个负载称为“80-20”负载，它表现出局部性：80% 的引用是指向 20% 的页面（即“热”页）；==

  

the remaining 20% of the references are made to the remaining 80% of the pages (the "cold" pages).

==剩余 20% 的引用是指向剩余 80% 的页面（即“冷”页）。==

  

In our workload, there are a total 100 unique pages again;

==在我们的负载中，总共有 100 个唯一的页面；==

  

thus, "hot" pages are referred to most of the time, and "cold" pages the remainder.

==因此，“热”页在大部分时间被引用，“冷”页在其余时间被引用。==

  

Figure 22.7 shows how the policies perform with this workload.

==图 22.7 显示了各策略在该负载下的表现。==

  

As you can see from the figure, while both random and FIFO do reasonably well, LRU does better, as it is more likely to hold onto the hot pages;

==正如你从图中看到的，虽然随机和 FIFO 表现尚可，但 LRU 表现更好，因为它更有可能保留热页；==

  

as those pages have been referred to frequently in the past, they are likely to be referred to again in the near future.

==由于这些页面在过去被频繁引用，它们很可能在不久的将来再次被引用。==

  

Optimal once again does better, showing that LRU's historical information is not perfect.

==最佳策略再次表现更好，这表明 LRU 的历史信息并不完美。==

  

Figure 22.7: The 80-20 Workload

==图 22.7：80-20 负载==

  

You might now be wondering: is LRU's improvement over Random and FIFO really that big of a deal?

==你现在可能想知道：LRU 对比随机和 FIFO 的改进真的那么重要吗？==

  

The answer, as usual, is "it depends."

==答案通常是“视情况而定”。==

  

If each miss is very costly (not uncommon), then even a small increase in hit rate (reduction in miss rate) can make a huge difference on performance.

==如果每次未命中的代价都很高（这并不罕见），那么即使是命中率的小幅增加（未命中率的降低）也能对性能产生巨大影响。==

  

If misses are not so costly, then of course the benefits possible with LRU are not nearly as important.

==如果未命中的代价不高，那么 LRU 可能带来的好处当然就不那么重要了。==

  

Let's look at one final workload.

==让我们看看最后一个负载。==

  

We call this one the "looping sequential" workload, as in it, we refer to 50 pages in sequence, starting at 0, then 1, ..., up to page 49, and then we loop, repeating those accesses, for a total of 10,000 accesses to 50 unique pages.

==我们将此称为“循环顺序”负载，因为在其中，我们要按顺序引用 50 个页面，从 0 开始，然后是 1，……，直到第 49 页，然后我们循环，重复这些访问，总共对 50 个唯一页面进行 10,000 次访问。==

  

The last graph in Figure 22.8 shows the behavior of the policies under this workload.

==图 22.8 中的最后一张图表显示了在该负载下各策略的行为。==

  

This workload, common in many applications (including important commercial applications such as databases), represents a worst-case for both LRU and FIFO.

==这种在许多应用程序（包括重要的商业应用程序，如数据库）中常见的负载，代表了 LRU 和 FIFO 的最坏情况。==

  

These algorithms, under a looping-sequential workload, kick out older pages;

==在循环顺序负载下，这些算法会踢出较旧的页面；==

  

unfortunately, due to the looping nature of the workload, these older pages are going to be accessed sooner than the pages that the policies prefer to keep in cache.

==不幸的是，由于负载的循环特性，这些较旧的页面将比策略倾向于保留在缓存中的页面更早被访问。==

  

Indeed, even with a cache of size 49, a looping-sequential workload of 50 pages results in a 0% hit rate.

==确实，即使缓存大小为 49，50 个页面的循环顺序负载也会导致 0% 的命中率。==

  

Interestingly, Random fares notably better, not quite approaching optimal, but at least achieving a non-zero hit rate.

==有趣的是，随机策略的表现明显更好，虽然没有接近最佳策略，但至少实现了非零的命中率。==

  

Turns out that random has some nice properties; one such property is not having weird corner-case behaviors.

==事实证明，随机策略具有一些不错的属性；其中一个属性就是没有奇怪的极端情况行为。==

  

Figure 22.8: The Looping Workload

==图 22.8：循环负载==

  

22.7 Implementing Historical Algorithms

==22.7 实现历史算法==

  

As you can see, an algorithm such as LRU can generally do a better job than simpler policies like FIFO or Random, which may throw out important pages.

==正如你所看到的，像 LRU 这样的算法通常比 FIFO 或随机等更简单的策略表现更好，后者可能会丢弃重要的页面。==

  

Unfortunately, historical policies present us with a new challenge: how do we implement them?

==不幸的是，基于历史的策略给我们带来了新的挑战：我们该如何实现它们？==

  

Let's take, for example, LRU.

==以 LRU 为例。==

  

To implement it perfectly, we need to do a lot of work.

==为了完美地实现它，我们需要做很多工作。==

  

Specifically, upon each page access (i.e., each memory access, whether an instruction fetch or a load or store), we must update some data structure to move this page to the front of the list (i.e., the MRU side).

==具体来说，在每次页面访问（即每次内存访问，无论是指令提取还是加载或存储）时，我们必须更新某个数据结构，将此页面移动到列表的前端（即最近最常使用的一端）。==

  

Contrast this to FIFO, where the FIFO list of pages is only accessed when a page is evicted (by removing the first-in page) or when a new page is added to the list (to the last-in side).

==与之形成对比的是 FIFO，其中 FIFO 页面列表仅在页面被驱逐（通过移除最早进入的页面）或新页面添加到列表（添加到最后进入的一端）时才被访问。==

  

To keep track of which pages have been least- and most-recently used, the system has to do some accounting work on every memory reference.

==为了跟踪哪些页面是最近最少使用和最近最常使用的，系统必须在每次内存引用时做一些记账工作。==

  

Clearly, without great care, such accounting could greatly reduce performance.

==显然，如果不小心处理，这种记账工作可能会大大降低性能。==

  

One method that could help speed this up is to add a little bit of hardware support.

==一种有助于加速此过程的方法是增加少量的硬件支持。==

  

For example, a machine could update, on each page access, a time field in memory (for example, this could be in the per-process page table, or just in some separate array in memory, with one entry per physical page of the system).

==例如，机器可以在每次页面访问时更新内存中的时间字段（例如，这可以在每个进程的页表中，或者就在内存中的某个单独数组中，系统的每个物理页面对应一个条目）。==

  

Thus, when a page is accessed, the time field would be set, by hardware, to the current time.

==因此，当访问页面时，硬件会将时间字段设置为当前时间。==

  

Then, when replacing a page, the OS could simply scan all the time fields in the system to find the least-recently-used page.

==然后，在替换页面时，操作系统只需扫描系统中的所有时间字段，即可找到最近最少使用的页面。==

  

Unfortunately, as the number of pages in a system grows, scanning a huge array of times just to find the absolute least-recently-used page is prohibitively expensive.

==不幸的是，随着系统中页面数量的增加，仅仅为了找到绝对的最近最少使用页面而扫描巨大的时间数组，其代价高得令人望而却步。==

  

Imagine a modern machine with 4GB of memory, chopped into 4KB pages.

==想象一台拥有 4GB 内存的现代机器，被切分成 4KB 的页面。==

  

This machine has 1 million pages, and thus finding the LRU page will take a long time, even at modern CPU speeds.

==这台机器有 100 万个页面，因此即使以现代 CPU 的速度，找到 LRU 页面也需要很长时间。==

  

Which begs the question: do we really need to find the absolute oldest page to replace?

==这就引出了一个问题：我们要替换时真的需要找到绝对最老的页面吗？==

  

Can we instead survive with an approximation?

==我们能否改用一种近似方法并仍能正常运行？==

  

CRUX: HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY

==关键问题：如何实现 LRU 替换策略==

  

Given that it will be expensive to implement perfect LRU, can we approximate it in some way, and still obtain the desired behavior?

==鉴于实现完美的 LRU 代价昂贵，我们能否以某种方式近似它，并仍然获得期望的行为？==

  

22.8 Approximating LRU

==22.8 近似 LRU==

  

As it turns out, the answer is yes: approximating LRU is more feasible from a computational-overhead standpoint, and indeed it is what many modern systems do.

==事实证明，答案是肯定的：从计算开销的角度来看，近似 LRU 更可行，而且确实是许多现代系统所做的。==

  

The idea requires some hardware support, in the form of a use bit (sometimes called the reference bit), the first of which was implemented in the first system with paging, the Atlas one-level store.

==这个想法需要一些硬件支持，形式是一个使用位（有时称为引用位），最早是在第一个具有分页功能的系统——Atlas 一级存储中实现的。==

  

There is one use bit per page of the system, and the use bits live in memory somewhere (they could be in the per-process page tables, for example, or just in an array somewhere).

==系统的每个页面都有一个使用位，使用位存在于内存的某个地方（例如，它们可以在每个进程的页表中，或者只是在某处的数组中）。==

  

Whenever a page is referenced (i.e., read or written), the use bit is set by hardware to 1.

==每当页面被引用（即读取或写入）时，硬件会将使用位设置为 1。==

  

The hardware never clears the bit, though (i.e., sets it to 0);

==不过，硬件从不清除该位（即将其设置为 0）；==

  

that is the responsibility of the OS.

==那是操作系统的责任。==

  

How does the OS employ the use bit to approximate LRU?

==操作系统如何利用使用位来近似 LRU？==

  

Well, there could be a lot of ways, but with the clock algorithm, one simple approach was suggested.

==嗯，可能有很多方法，但通过时钟算法，人们提出了一种简单的方法。==

  

Imagine all the pages of the system arranged in a circular list.

==想象一下系统的所有页面被排列成一个循环列表。==

  

A clock hand points to some particular page to begin with (it doesn't really matter which).

==时钟指针起初指向某个特定的页面（具体是哪个并不重要）。==

  

When a replacement must occur, the OS checks if the currently-pointed to page P has a use bit of 1 or 0.

==当必须进行替换时，操作系统检查当前指向的页面 P 的使用位是 1 还是 0。==

  

If 1, this implies that page P was recently used and thus is not a good candidate for replacement.

==如果是 1，这意味着页面 P 最近被使用过，因此不是替换的好候选者。==

  

Thus, the use bit for P is set to 0 (cleared), and the clock hand is incremented to the next page (P + 1).

==因此，P 的使用位被设置为 0（清除），时钟指针增加到下一页 (P + 1)。==

  

The algorithm continues until it finds a use bit that is set to 0, implying this page has not been recently used (or, in the worst case, that all pages have been and that we have now searched through the entire set of pages, clearing all the bits).

==该算法持续进行，直到找到一个设置为 0 的使用位，这意味着该页面最近没有被使用过（或者，在最坏的情况下，所有页面都被使用过，我们现在已经搜索了整个页面集，清除了所有的位）。==

  

Note that this approach is not the only way to employ a use bit to approximate LRU.

==请注意，这种方法并不是利用使用位来近似 LRU 的唯一方法。==

  

Indeed, any approach which periodically clears the use bits and then differentiates between which pages have use bits of 1 versus 0 to decide which to replace would be fine.

==事实上，任何定期清除使用位，然后区分哪些页面的使用位为 1 或 0 来决定替换哪些页面的方法都是可以的。==

  

The clock algorithm of Corbato's was just one early approach which met with some success, and had the nice property of not repeatedly scanning through all of memory looking for an unused page.

==Corbato 的时钟算法只是早期取得一定成功的一种方法，它具有不必重复扫描整个内存来寻找未使用的页面的良好属性。==

  

Figure 22.9: The 80-20 Workload With Clock

==图 22.9：使用时钟算法的 80-20 负载==

  

The behavior of a clock algorithm variant is shown in Figure 22.9.

==图 22.9 显示了时钟算法变体的行为。==

  

This variant randomly scans pages when doing a replacement;

==该变体在进行替换时随机扫描页面；==

  

when it encounters a page with a reference bit set to 1, it clears the bit (i.e., sets it to 0);

==当它遇到引用位设置为 1 的页面时，它清除该位（即将其设置为 0）；==

  

when it finds a page with the reference bit set to 0, it chooses it as its victim.

==当它找到引用位设置为 0 的页面时，它选择该页面作为受害者（被替换）。==

  

As you can see, although it doesn't do quite as well as perfect LRU, it does better than approaches that don't consider history at all.

==正如你所看到的，虽然它的表现不如完美的 LRU，但它比根本不考虑历史的方法要好。==

  

22.9 Considering Dirty Pages

==22.9 考虑脏页==

  

One small modification to the clock algorithm (also originally suggested by Corbato) that is commonly made is the additional consideration of whether a page has been modified or not while in memory.

==对时钟算法常做的一个小修改（最初也是由 Corbato 建议的）是额外考虑页面在内存中是否被修改过。==

  

The reason for this: if a page has been modified and is thus dirty, it must be written back to disk to evict it, which is expensive.

==这样做的原因是：如果一个页面被修改过，因此是“脏”的，那么为了驱逐它，必须将其写回磁盘，这很昂贵。==

  

If it has not been modified (and is thus clean), the eviction is free;

==如果它没有被修改（因此是“干净”的），则驱逐是免费的；==

  

the physical frame can simply be reused for other purposes without additional I/O.

==物理帧可以简单地重用于其他目的，而无需额外的 I/O。==

  

Thus, some VM systems prefer to evict clean pages over dirty pages.

==因此，一些虚拟机系统更倾向于驱逐干净的页面，而不是脏页。==

  

To support this behavior, the hardware should include a modified bit (a.k.a. dirty bit).

==为了支持这种行为，硬件应包含一个修改位（又名脏位）。==

  

This bit is set any time a page is written, and thus can be incorporated into the page-replacement algorithm.

==任何时候写入页面时都会设置此位，因此可以将其纳入页面替换算法中。==

  

The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict first;

==例如，可以更改时钟算法，以优先扫描既未使用又干净的页面进行驱逐；==

  

failing to find those, then for unused pages that are dirty, and so forth.

==如果找不到这些页面，则扫描未使用的脏页，依此类推。==

  

22.10 Other VM Policies

==22.10 其他虚拟机策略==

  

Page replacement is not the only policy the VM subsystem employs (though it may be the most important).

==页面替换并不是虚拟机子系统采用的唯一策略（尽管它可能是最重要的）。==

  

For example, the OS also has to decide when to bring a page into memory.

==例如，操作系统还必须决定何时将页面带入内存。==

  

This policy, sometimes called the page selection policy (as it was called by Denning), presents the OS with some different options.

==这种策略有时称为页面选择策略（正如 Denning 所称），它为操作系统提供了一些不同的选项。==

  

For most pages, the OS simply uses demand paging, which means the OS brings the page into memory when it is accessed, "on demand" as it were.

==对于大多数页面，操作系统只是使用按需分页，这意味着操作系统在页面被访问时将其带入内存，即所谓的“按需”。==

  

Of course, the OS could guess that a page is about to be used, and thus bring it in ahead of time;

==当然，操作系统可以猜测某个页面即将被使用，从而提前将其带入；==

  

this behavior is known as prefetching and should only be done when there is reasonable chance of success.

==这种行为称为预取，只有在有合理的成功几率时才应该这样做。==

  

For example, some systems will assume that if a code page P is brought into memory, that code page P + 1 will likely soon be accessed and thus should be brought into memory too.

==例如，一些系统会假设如果代码页 P 被带入内存，那么代码页 P + 1 很快也会被访问，因此也应该被带入内存。==

  

Another policy determines how the OS writes pages out to disk.

==另一个策略决定操作系统如何将页面写出到磁盘。==

  

Of course, they could simply be written out one at a time;

==当然，它们可以简单地一次写出一个；==

  

however, many systems instead collect a number of pending writes together in memory and write them to disk in one (more efficient) write.

==然而，许多系统反而会在内存中收集许多待处理的写入，并在一次（更高效的）写入中将它们写入磁盘。==

  

This behavior is usually called clustering or simply grouping of writes, and is effective because of the nature of disk drives, which perform a single large write more efficiently than many small ones.

==这种行为通常称为聚类或简称为写入分组，它之所以有效，是因为磁盘驱动器的特性，即执行单个大写入比许多小写入更有效。==

  

22.11 Thrashing

==22.11 抖动==

  

Before closing, we address one final question: what should the OS do when memory is simply oversubscribed, and the memory demands of the set of running processes simply exceeds the available physical memory?

==在结束之前，我们要解决最后一个问题：当内存完全超额认购，即运行进程集的内存需求完全超过可用的物理内存时，操作系统应该做什么？==

  

In this case, the system will constantly be paging, a condition sometimes referred to as thrashing.

==在这种情况下，系统将不断地进行分页，这种状况有时被称为抖动。==

  

Some earlier operating systems had a fairly sophisticated set of mechanisms to both detect and cope with thrashing when it took place.

==一些早期的操作系统有一套相当复杂的机制来检测和应对发生的抖动。==

  

For example, given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes' working sets (the pages that they are using actively) fit in memory and thus can make progress.

==例如，给定一组进程，系统可以决定不运行其中的一个子集，希望剩余进程集的工作集（它们正在活跃使用的页面）能放入内存，从而能够取得进展。==

  

This approach, generally known as admission control, states that it is sometimes better to do less work well than to try to do everything at once poorly, a situation we often encounter in real life as well as in modern computer systems (sadly).

==这种通常被称为准入控制的方法指出，有时少做但做好，比试图一次性糟糕地做完所有事情要好，这是一种我们在现实生活以及现代计算机系统中经常遇到的情况（可悲的是）。==

  

Some current systems take more a draconian approach to memory overload.

==一些当前的系统对内存过载采取了更严厉的方法。==

  

For example, some versions of Linux run an out-of-memory killer when memory is oversubscribed;

==例如，某些版本的 Linux 在内存超额认购时会运行一个“内存不足杀手”（OOM killer）；==

  

this daemon chooses a memory-intensive process and kills it, thus reducing memory in a none-too-subtle manner.

==这个守护进程会选择一个内存密集型进程并将其杀死，从而以一种毫不微妙的方式减少内存占用。==

  

While successful at reducing memory pressure, this approach can have problems, if, for example, it kills the X server and thus renders any applications requiring the display unusable.

==虽然这种方法在减轻内存压力方面是成功的，但它可能会带来问题，例如，如果它杀死了 X server（显示服务器），从而导致任何需要显示的应用程序无法使用。==

  

22.12 Summary

==22.12 总结==

  

We have seen the introduction of a number of page-replacement (and other) policies, which are part of the VM subsystem of all modern operating systems.

==我们已经看到了许多页面替换（及其他）策略的介绍，它们是所有现代操作系统虚拟机子系统的一部分。==

  

Modern systems add some tweaks to straightforward LRU approximations like clock;

==现代系统对像时钟算法这样直截了当的 LRU 近似方法添加了一些调整；==

  

for example, scan resistance is an important part of many modern algorithms, such as ARC.

==例如，扫描抗性是许多现代算法（如 ARC）的重要组成部分。==

  

Scan-resistant algorithms are usually LRU-like but also try to avoid the worst-case behavior of LRU, which we saw with the looping-sequential workload.

==具有扫描抗性的算法通常类似于 LRU，但也试图避免 LRU 的最坏情况行为，即我们在循环顺序负载中看到的那样。==

  

Thus, the evolution of page-replacement algorithms continues.

==因此，页面替换算法的演变仍在继续。==

  

For many years, the importance of replacement algorithms had decreased, as the discrepancy between memory-access and disk-access times was so large.

==多年来，由于内存访问时间和磁盘访问时间之间的差异如此之大，替换算法的重要性已经下降。==

  

Specifically, because paging to disk was so expensive, the cost of frequent paging was prohibitive;

==具体来说，因为分页到磁盘非常昂贵，频繁分页的成本高得令人望而却步；==

  

simply put, no matter how good your replacement algorithm was, if you were performing frequent replacements, your system became unbearably slow.

==简单地说，无论你的替换算法有多好，如果你频繁进行替换，你的系统就会变得无法忍受的慢。==

  

Thus, the best solution was a simple (if intellectually unsatisfying) one: buy more memory.

==因此，最好的解决方案是一个简单（尽管在智力上不能令人满意）的方案：购买更多内存。==

  

However, recent innovations in much faster storage devices (e.g., Flash-based SSDs) have changed these performance ratios yet again, leading to a renaissance in page replacement algorithms.

==然而，最近速度更快的存储设备（例如基于闪存的 SSD）的创新再次改变了这些性能比率，导致了页面替换算法的复兴。==

  

Complete Virtual Memory Systems

==完整的虚拟内存系统==

  

23

23

  

Before we end our study of virtualizing memory, let us take a closer look at how entire virtual memory systems are put together.

==在结束对虚拟化内存的研究之前，让我们仔细看看整个虚拟内存系统是如何组合在一起的。==

  

We've seen key elements of such systems, including numerous page-table designs, interactions with the TLB (sometimes, even handled by the OS itself), and strategies for deciding which pages to keep in memory and which to kick out.

==我们已经看到了此类系统的关键要素，包括众多的页表设计、与 TLB 的交互（有时甚至由操作系统本身处理），以及决定哪些页面保留在内存中以及将哪些页面踢出的策略。==

  

However, there are many other features that comprise a complete virtual memory system, including numerous features for performance, functionality, and security.

==然而，还有许多其他功能构成了一个完整的虚拟内存系统，包括许多用于性能、功能和安全性的特性。==

  

And thus, our crux:

==因此，我们的关键问题是：==

  

THE CRUX: How To BUILD A COMPLETE VM SYSTEM

==关键问题：如何构建一个完整的虚拟机系统==

  

What features are needed to realize a complete virtual memory system?

==实现一个完整的虚拟内存系统需要哪些功能？==

  

How do they improve performance, increase security, or otherwise improve the system?

==它们如何提高性能、增加安全性或以其他方式改进系统？==

  

We'll do this by covering two systems.

==我们将通过介绍两个系统来做到这一点。==

  

The first is one of the earliest examples of a "modern" virtual memory manager, that found in the VAX/VMS operating system, as developed in the 1970's and early 1980's;

==第一个是“现代”虚拟内存管理器最早的例子之一，即 20 世纪 70 年代和 80 年代初开发的 VAX/VMS 操作系统中的管理器；==

  

a surprising number of techniques and approaches from this system survive to this day, and thus it is well worth studying.

==该系统中有惊人数量的技术和方法至今仍然存在，因此非常值得研究。==

  

Some ideas, even those that are 50 years old, are still worth knowing, a thought that is well known to those in most other fields (e.g., Physics), but has to be stated in technology-driven disciplines (e.g., Computer Science).

==有些思想，即使已有 50 年历史，仍然值得了解，这在大多数其他领域（例如物理学）中是众所周知的，但在技术驱动的学科（例如计算机科学）中却必须被特别指出。==

  

The second is that of Linux, for reasons that should be obvious.

==第二个是 Linux，原因应该是显而易见的。==

  

Linux is a widely used system, and runs effectively on systems as small and underpowered as phones to the most scalable multicore systems found in modern datacenters.

==Linux 是一个广泛使用的系统，并且可以在小到手机这样性能有限的系统，大到现代数据中心中最具扩展性的多核系统上有效地运行。==

  

Thus, its VM system must be flexible enough to run successfully in all of those scenarios.

==因此，它的虚拟机系统必须足够灵活，才能在所有这些场景中成功运行。==

  

We will discuss each system to illustrate how concepts brought forth in earlier chapters come together in a complete memory manager.

==我们将讨论每个系统，以说明前面章节中提出的概念是如何在一个完整的内存管理器中结合在一起的。==

  

23.1 VAX/VMS Virtual Memory

==23.1 VAX/VMS 虚拟内存==

  

The VAX-11 minicomputer architecture was introduced in the late 1970's by Digital Equipment Corporation (DEC).

==VAX-11 小型计算机架构是由数字设备公司 (DEC) 在 20 世纪 70 年代末推出的。==

  

DEC was a massive player in the computer industry during the era of the mini-computer;

==DEC 是小型计算机时代计算机行业的巨头；==

  

unfortunately, a series of bad decisions and the advent of the PC slowly (but surely) led to their demise.

==不幸的是，一系列错误的决定和个人电脑的出现缓慢（但肯定）地导致了他们的灭亡。==

  

The architecture was realized in a number of implementations, including the VAX-11/780 and the less powerful VAX-11/750.

==该架构在许多实现中得以体现，包括 VAX-11/780 和功能较弱的 VAX-11/750。==

  

The OS for the system was known as VAX/VMS (or just plain VMS), one of whose primary architects was Dave Cutler, who later led the effort to develop Microsoft's Windows NT.

==该系统的操作系统被称为 VAX/VMS（或简称 VMS），其主要架构师之一是 Dave Cutler，他后来领导了微软 Windows NT 的开发工作。==

  

VMS had the general problem that it would be run on a broad range of machines, including very inexpensive VAXen (yes, that is the proper plural) to extremely high-end and powerful machines in the same architecture family.

==VMS 面临的一个普遍问题是，它需要在各种机器上运行，包括同一架构家族中非常便宜的 VAXen（是的，这是正确的复数形式）到极其高端和强大的机器。==

  

Thus, the OS had to have mechanisms and policies that worked (and worked well) across this huge range of systems.

==因此，操作系统必须拥有能在如此巨大的系统范围内工作（并且工作良好）的机制和策略。==

  

As an additional issue, VMS is an excellent example of software innovations used to hide some of the inherent flaws of the architecture.

==作为一个额外的问题，VMS 是利用软件创新来掩盖架构本身缺陷的一个极好例子。==

  

Although the OS often relies on the hardware to build efficient abstractions and illusions, sometimes the hardware designers don't quite get everything right;

==尽管操作系统通常依赖硬件来构建高效的抽象和幻觉，但有时硬件设计者并不能把所有事情都做对；==

  

in the VAX hardware, we'll see a few examples of this, and what the VMS operating system does to build an effective, working system despite these hardware flaws.

==在 VAX 硬件中，我们将看到这方面的一些例子，以及 VMS 操作系统如何在这个硬件缺陷的情况下构建一个有效的、可工作的系统。==

  

Memory Management Hardware

==内存管理硬件==

  

The VAX-11 provided a 32-bit virtual address space per process, divided into 512-byte pages.

==VAX-11 为每个进程提供了一个 32 位的虚拟地址空间，分为 512 字节的页面。==

  

Thus, a virtual address consisted of a 23-bit VPN and a 9-bit offset.

==因此，虚拟地址由 23 位 VPN 和 9 位偏移量组成。==

  

Further, the upper two bits of the VPN were used to differentiate which segment the page resided within;

==此外，VPN 的高两位用于区分页面所在的段；==

  

thus, the system was a hybrid of paging and segmentation, as we saw previously.

==因此，正如我们之前看到的，该系统是分页和分段的混合体。==

  

The lower-half of the address space was known as "process space" and is unique to each process.

==地址空间的下半部分被称为“进程空间”，对每个进程来说是唯一的。==

  

In the first half of process space (known as P0), the user program is found, as well as a heap which grows downward.

==在进程空间的前半部分（称为 P0），可以找到用户程序，以及一个向下增长的堆。==

  

In the second half of process space (P1), we find the stack, which grows upwards.

==在进程空间的后半部分（P1），我们找到了向上增长的栈。==

  

The upper-half of the address space is known as system space (S), although only half of it is used.

==地址空间的上半部分被称为系统空间 (S)，尽管只使用了其中的一半。==

  

Protected OS code and data reside here, and the OS is in this way shared across processes.

==受保护的操作系统代码和数据驻留在那里，操作系统以这种方式在进程间共享。==

  

One major concern of the VMS designers was the incredibly small size of pages in the VAX hardware (512 bytes).

==VMS 设计者主要关心的一个问题是 VAX 硬件中极小的页面大小（512 字节）。==

  

This size, chosen for historical reasons, has the fundamental problem of making simple linear page tables excessively large.

==这个出于历史原因选择的大小，有一个根本问题，即使得简单的线性页表变得过大。==

  

Thus, one of the first goals of the VMS designers was to ensure that VMS would not overwhelm memory with page tables.

==因此，VMS 设计者的首要目标之一是确保 VMS 不会让页表淹没内存。==

  

The system reduced the pressure page tables place on memory in two ways.

==该系统通过两种方式减少了页表对内存的压力。==

  

First, by segmenting the user address space into two, the VAX-11 provides a page table for each of these regions (P0 and P1) per process;

==首先，通过将用户地址空间分成两部分，VAX-11 为每个进程的这些区域（P0 和 P1）分别提供了一个页表；==

  

thus, no page-table space is needed for the unused portion of the address space between the stack and the heap.

==因此，栈和堆之间未使用的地址空间部分不需要页表空间。==

  

The base and bounds registers are used as you would expect;

==基址和界限寄存器的使用正如你所预期的那样；==

  

a base register holds the address of the page table for that segment, and the bounds holds its size (i.e., number of page-table entries).

==基址寄存器保存该段页表的地址，界限寄存器保存其大小（即页表条目的数量）。==

  

Second, the OS reduces memory pressure even further by placing user page tables (for P0 and P1, thus two per process) in kernel virtual memory.

==其次，操作系统通过将用户页表（针对 P0 和 P1，因此每个进程两个）放置在内核虚拟内存中，进一步降低了内存压力。==

  

Thus, when allocating or growing a page table, the kernel allocates space out of its own virtual memory, in segment S.

==因此，当分配或增加页表时，内核会从其自己的虚拟内存（在段 S 中）中分配空间。==

  

If memory comes under severe pressure, the kernel can swap pages of these page tables out to disk, thus making physical memory available for other uses.

==如果内存面临严重压力，内核可以将这些页表的页面交换到磁盘，从而腾出物理内存用于其他用途。==

  

Putting page tables in kernel virtual memory means that address translation is even further complicated.

==将页表放在内核虚拟内存中意味着地址转换变得更加复杂。==

  

For example, to translate a virtual address in P0 or P1, the hardware has to first try to look up the page-table entry for that page in its page table (the P0 or P1 page table for that process);

==例如，为了转换 P0 或 P1 中的虚拟地址，硬件必须首先尝试在其页表（该进程的 P0 或 P1 页表）中查找该页面的页表条目；==

  

in doing so, however, the hardware may first have to consult the system page table (which lives in physical memory);

==然而，在这样做时，硬件可能首先必须查询系统页表（它驻留在物理内存中）；==

  

with that translation complete, the hardware can learn the address of the page of the page table, and then finally learn the address of the desired memory access.

==完成该转换后，硬件可以获知页表页面的地址，然后最终获知所需内存访问的地址。==

  

All of this, fortunately, is made faster by the VAX's hardware-managed TLBs, which usually (hopefully) circumvent this laborious lookup.

==幸运的是，VAX 的硬件管理 TLB 加快了所有这些过程，它通常（希望如此）可以规避这种费力的查找。==

  

A Real Address Space

==一个真实的地址空间==

  

One neat aspect of studying VMS is that we can see how a real address space is constructed (Figure 23.1).

==研究 VMS 的一个极好方面是我们可以看到一个真实的地址空间是如何构建的（图 23.1）。==

  

Thus far, we have assumed a simple address space of just user code, user data, and user heap, but as we can see above, a real address space is notably more complex.

==到目前为止，我们假设了一个只有用户代码、用户数据和用户堆的简单地址空间，但正如我们在上面看到的，真实的地址空间明显更复杂。==

  

For example, the code segment never begins at page 0.

==例如，代码段从不从第 0 页开始。==

  

This page, instead, is marked inaccessible, in order to provide some support for detecting null-pointer accesses.

==相反，该页面被标记为不可访问，以便为检测空指针访问提供一些支持。==

  

Thus, one concern when designing an address space is support for debugging, which the inaccessible zero page provides here in some form.

==因此，设计地址空间时的一个考虑因素是支持调试，而不可访问的零页在这里以某种形式提供了这种支持。==

  

Perhaps more importantly, the kernel virtual address space (i.e., its data structures and code) is a part of each user address space.

==也许更重要的是，内核虚拟地址空间（即其数据结构和代码）是每个用户地址空间的一部分。==

  

Figure 23.1: The VAX/VMS Address Space

==图 23.1：VAX/VMS 地址空间==

  

On a context switch, the OS changes the P0 and P1 registers to point to the appropriate page tables of the soon-to-be-run process;

==在上下文切换时，操作系统更改 P0 和 P1 寄存器以指向即将运行的进程的相应页表；==

  

however, it does not change the S base and bound registers, and as a result the "same" kernel structures are mapped into each user address space.

==然而，它不会更改 S 基址和界限寄存器，结果是“相同的”内核结构被映射到每个用户地址空间。==

  

The kernel is mapped into each address space for a number of reasons.

==内核被映射到每个地址空间有许多原因。==

  

This construction makes life easier for the kernel; when, for example, the OS is handed a pointer from a user program (e.g., on a write() system call), it is easy to copy data from that pointer to its own structures.

==这种结构使内核的生活更轻松；例如，当操作系统从用户程序收到一个指针时（例如，在 write() 系统调用中），很容易将数据从该指针复制到它自己的结构中。==

  

The OS is naturally written and compiled, without worry of where the data it is accessing comes from.

==操作系统的编写和编译都很自然，不用担心它访问的数据来自哪里。==

  

If in contrast the kernel were located entirely in physical memory, it would be quite hard to do things like swap pages of the page table to disk;

==相比之下，如果内核完全位于物理内存中，那么像将页表的页面交换到磁盘这样的事情就会很难做；==

  

if the kernel were given its own address space, moving data between user applications and the kernel would again be complicated and painful.

==如果给内核自己的地址空间，在用户应用程序和内核之间移动数据又会变得复杂和痛苦。==

  

With this construction (now used widely), the kernel appears almost as a library to applications, albeit a protected one.

==通过这种结构（现在被广泛使用），内核几乎就像应用程序的一个库，尽管是一个受保护的库。==

  

One last point about this address space relates to protection.

==关于这个地址空间的最后一点与保护有关。==

  

Clearly, the OS does not want user applications reading or writing OS data or code.

==显然，操作系统不希望用户应用程序读取或写入操作系统的数据或代码。==

  

Thus, the hardware must support different protection levels for pages to enable this.

==因此，硬件必须支持页面的不同保护级别才能实现这一点。==

  

The VAX did so by specifying, in protection bits in the page table, what privilege level the CPU must be at in order to access a particular page.

==VAX 通过在页表中的保护位中指定 CPU 必须处于什么特权级别才能访问特定页面来做到这一点。==

  

Thus, system data and code are set to a higher level of protection than user data and code;

==因此，系统数据和代码被设置为比用户数据和代码更高的保护级别；==

  

an attempted access to such information from user code will generate a trap into the OS, and (you guessed it) the likely termination of the offending process.

==试图从用户代码访问此类信息将产生一个进入操作系统的陷阱，并且（你猜对了）可能会终止违规进程。==

  

Page Replacement

==页面替换==

  

The page table entry (PTE) in VAX contains the following bits: a valid bit, a protection field (4 bits), a modify (or dirty) bit, a field reserved for OS use (5 bits), and finally a physical frame number (PFN) to store the location of the page in physical memory.

==VAX 中的页表条目 (PTE) 包含以下位：一个有效位、一个保护字段（4 位）、一个修改（或脏）位、一个为操作系统使用保留的字段（5 位），最后是一个物理帧号 (PFN) 以存储页面在物理内存中的位置。==

  

The astute reader might note: no reference bit!

==敏锐的读者可能会注意到：没有引用位！==

  

Thus, the VMS replacement algorithm must make do without hardware support for determining which pages are active.

==因此，VMS 替换算法必须在没有硬件支持的情况下确定哪些页面是活跃的。==

  

The developers were also concerned about memory hogs, programs that use a lot of memory and make it hard for other programs to run.

==开发人员还担心内存独占程序，即那些使用大量内存并使其他程序难以运行的程序。==

  

Most of the policies we have looked at thus far are susceptible to such hogging;

==我们目前看到的大多数策略都容易受到这种独占的影响；==

  

for example, LRU is a global policy that doesn't share memory fairly among processes.

==例如，LRU 是一种全局策略，它不能在进程之间公平地共享内存。==

  

ASIDE: EMULATING REFERENCE BITS

==旁白：模拟引用位==

  

As it turns out, you don't need a hardware reference bit in order to get some notion of which pages are in use in a system.

==事实证明，你不需要硬件引用位就能获得系统中哪些页面正在使用的一些概念。==

  

In fact, in the early 1980's, Babaoglu and Joy showed that protection bits on the VAX can be used to emulate reference bits.

==事实上，在 20 世纪 80 年代初，Babaoglu 和 Joy 展示了 VAX 上的保护位可以用来模拟引用位。==

  

The basic idea: if you want to gain some understanding of which pages are actively being used in a system, mark all of the pages in the page table as inaccessible (but keep around the information as to which pages are really accessible by the process, perhaps in the "reserved OS field" portion of the page table entry).

==基本思路是：如果你想了解系统中哪些页面正在被积极使用，就将页表中的所有页面标记为不可访问（但保留关于哪些页面实际上可被进程访问的信息，也许在页表条目的“保留操作系统字段”部分）。==

  

When a process accesses a page, it will generate a trap into the OS;

==当进程访问一个页面时，它会生成一个进入操作系统的陷阱；==

  

the OS will then check if the page really should be accessible, and if so, revert the page to its normal protections (e.g., read-only, or read-write).

==然后操作系统会检查该页面是否真的应该可访问，如果是，则将该页面恢复为其正常的保护（例如，只读或读写）。==

  

At the time of a replacement, the OS can check which pages remain marked inaccessible, and thus get an idea of which pages have not been recently used.

==在替换时，操作系统可以检查哪些页面仍然被标记为不可访问，从而了解哪些页面最近没有被使用。==

  

The key to this "emulation" of reference bits is reducing overhead while still obtaining a good idea of page usage.

==这种引用位“模拟”的关键是在降低开销的同时，仍然获得良好的页面使用概念。==

  

The OS must not be too aggressive in marking pages inaccessible, or overhead would be too high.

==操作系统在标记页面不可访问时不能太激进，否则开销会太高。==

  

The OS also must not be too passive in such marking, or all pages will end up referenced;

==操作系统在标记时也不能太被动，否则所有页面最终都会被引用；==

  

the OS will again have no good idea which page to evict.

==操作系统将再次不知道该驱逐哪个页面。==

  

To address these two problems, the developers came up with the segmented FIFO replacement policy.

==为了解决这两个问题，开发人员提出了分段 FIFO 替换策略。==

  

The idea is simple: each process has a maximum number of pages it can keep in memory, known as its resident set size (RSS).

==这个想法很简单：每个进程都有一个它可以保留在内存中的最大页面数，称为其驻留集大小 (RSS)。==

  

Each of these pages is kept on a FIFO list;

==这些页面中的每一个都保存在一个 FIFO 列表中；==

  

when a process exceeds its RSS, the "first-in" page is evicted.

==当一个进程超过其 RSS 时，“最早进入”的页面将被驱逐。==

  

FIFO clearly does not need any support from the hardware, and is thus easy to implement.

==FIFO 显然不需要硬件的任何支持，因此易于实现。==

  

Of course, pure FIFO does not perform particularly well, as we saw earlier.

==当然，纯 FIFO 的表现并不是特别好，正如我们之前所见。==

  

To improve FIFO's performance, VMS introduced two second-chance lists where pages are placed before getting evicted from memory, specifically a global clean-page free list and dirty-page list.

==为了提高 FIFO 的性能，VMS 引入了两个二次机会列表，页面在从内存中被驱逐之前会被放置在这里，具体来说是一个全局干净页空闲列表和脏页列表。==

  

When a process P exceeds its RSS, a page is removed from its per-process FIFO;

==当进程 P 超过其 RSS 时，一个页面将从其每进程 FIFO 中移除；==

  

if clean (not modified), it is placed on the end of the clean-page list;

==如果是干净的（未修改），它被放置在干净页列表的末尾；==

  

if dirty (modified), it is placed on the end of the dirty-page list.

==如果是脏的（已修改），它被放置在脏页列表的末尾。==

  

If another process Q needs a free page, it takes the first free page off of the global clean list.

==如果另一个进程 Q 需要一个空闲页面，它会从全局干净列表中取出第一个空闲页面。==

  

However, if the original process P faults on that page before it is reclaimed, P reclaims it from the free (or dirty) list, thus avoiding a costly disk access.

==然而，如果原始进程 P 在该页面被回收之前对其发生缺页错误，P 会从空闲（或脏）列表中收回它，从而避免昂贵的磁盘访问。==

  

The bigger these global second-chance lists are, the closer the segmented FIFO algorithm performs to LRU.

==这些全局二次机会列表越大，分段 FIFO 算法的表现就越接近 LRU。==

  

Another optimization used in VMS also helps overcome the small page size in VMS.

==VMS 中使用的另一个优化也有助于克服 VMS 中较小的页面大小问题。==

  

Specifically, with such small pages, disk I/O during swapping could be highly inefficient, as disks do better with large transfers.

==具体来说，由于页面如此之小，交换期间的磁盘 I/O 可能会非常低效，因为磁盘在处理大传输时表现更好。==

  

To make swapping I/O more efficient, VMS adds a number of optimizations, but most important is clustering.

==为了使交换 I/O 更高效，VMS 添加了许多优化，但最重要的是聚类。==

  

With clustering, VMS groups large batches of pages together from the global dirty list, and writes them to disk in one fell swoop (thus making them clean).

==通过聚类，VMS 将全局脏列表中的大量页面分组在一起，并一下子将它们写入磁盘（从而使它们变干净）。==

  

Clustering is used in most modern systems, as the freedom to place pages anywhere within swap space lets the OS group pages, perform fewer and bigger writes, and thus improve performance.

==聚类在大多数现代系统中都有使用，因为可以将页面放置在交换空间内任何位置的自由度允许操作系统对页面进行分组，执行更少但更大的写入，从而提高性能。==

  

Other Neat Tricks

==其他巧妙的技巧==

  

VMS had two other now-standard tricks: demand zeroing and copy-on-write.

==VMS 还有另外两个现在已成为标准的技巧：按需零化和写时复制。==

  

We now describe these lazy optimizations.

==我们要描述这些懒惰优化。==

  

One form of laziness in VMS (and most modern systems) is demand zeroing of pages.

==VMS（以及大多数现代系统）中懒惰的一种形式是页面的按需零化。==

  

To understand this better, let's consider the example of adding a page to your address space, say in your heap.

==为了更好地理解这一点，让我们考虑一个将页面添加到地址空间的例子，比如在你的堆中。==

  

In a naive implementation, the OS responds to a request to add a page to your heap by finding a page in physical memory, zeroing it (required for security; otherwise you'd be able to see what was on the page from when some other process used it!), and then mapping it into your address space (i.e., setting up the page table to refer to that physical page as desired).

==在一个简单的实现中，操作系统响应向堆添加页面的请求的方式是：在物理内存中找到一个页面，将其归零（这是安全所需的；否则你就能看到其他进程使用该页面时留下的内容！），然后将其映射到你的地址空间（即设置页表以根据需要引用该物理页面）。==

  

But the naive implementation can be costly, particularly if the page does not get used by the process.

==但是，这种简单的实现可能代价高昂，特别是如果该页面没有被进程使用的话。==

  

With demand zeroing, the OS instead does very little work when the page is added to your address space;

==通过按需零化，当页面添加到你的地址空间时，操作系统反而只做很少的工作；==

  

it puts an entry in the page table that marks the page inaccessible.

==它在页表中放入一个条目，将该页面标记为不可访问。==

  

If the process then reads or writes the page, a trap into the OS takes place.

==如果进程随后读取或写入该页面，就会发生进入操作系统的陷阱。==

  

When handling the trap, the OS notices (usually through some bits marked in the "reserved for OS" portion of the page table entry) that this is actually a demand-zero page;

==在处理陷阱时，操作系统注意到（通常通过页表条目中“为操作系统保留”部分标记的一些位）这实际上是一个按需零化页面；==

  

at this point, the OS does the needed work of finding a physical page, zeroing it, and mapping it into the process's address space.

==此时，操作系统执行所需的工作：找到一个物理页面，将其归零，并将其映射到进程的地址空间中。==

  

If the process never accesses the page, all such work is avoided, and thus the virtue of demand zeroing.

==如果进程从未访问该页面，则避免了所有此类工作，这就是按需零化的优点。==

  

Another cool optimization found in VMS (and again, in virtually every modern OS) is copy-on-write (COW for short).

==VMS（同样，实际上在每个现代操作系统中）中发现的另一个很酷的优化是写时复制（简称 COW）。==

  

The idea, which goes at least back to the TENEX operating system, is simple: when the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the target address space and mark it read-only in both address spaces.

==这个至少可以追溯到 TENEX 操作系统的想法很简单：当操作系统需要将页面从一个地址空间复制到另一个地址空间时，它可以将其映射到目标地址空间并在两个地址空间中将其标记为只读，而不是复制它。==

  

If both address spaces only read the page, no further action is taken, and thus the OS has realized a fast copy without actually moving any data.

==如果两个地址空间都只读取该页面，则不采取进一步行动，因此操作系统实现了一个快速复制，而实际上没有移动任何数据。==

  

If, however, one of the address spaces does indeed try to write to the page, it will trap into the OS.

==然而，如果其中一个地址空间确实试图写入该页面，它将陷入操作系统。==

  

The OS will then notice that the page is a COW page, and thus (lazily) allocate a new page, fill it with the data, and map this new page into the address space of the faulting process.

==操作系统随后会注意到该页面是一个 COW 页面，因此（懒惰地）分配一个新页面，用数据填充它，并将这个新页面映射到发生错误的进程的地址空间中。==

  

The process then continues and now has its own private copy of the page.

==然后进程继续运行，现在它拥有该页面的私有副本。==

  

COW is useful for a number of reasons.

==COW 有用有许多原因。==

  

Certainly any sort of shared library can be mapped copy-on-write into the address spaces of many processes, saving valuable memory space.

==当然，任何类型的共享库都可以通过写时复制映射到许多进程的地址空间中，从而节省宝贵的内存空间。==

  

In UNIX systems, COW is even more critical, due to the semantics of fork() and exec().

==在 UNIX 系统中，由于 fork() 和 exec() 的语义，COW 甚至更为关键。==

  

As you might recall, fork() creates an exact copy of the address space of the caller;

==你可能还记得，fork() 创建调用者地址空间的精确副本；==

  

with a large address space, making such a copy is slow and data intensive.

==对于大型地址空间，制作这样的副本既慢又需要大量数据。==

  

Even worse, most of the address space is immediately over-written by a subsequent call to exec(), which overlays the calling process's address space with that of the soon-to-be-exec'd program.

==更糟糕的是，大部分地址空间会立即被随后的 exec() 调用覆盖，该调用用即将执行的程序的地址空间覆盖调用进程的地址空间。==

  

By instead performing a copy-on-write fork(), the OS avoids much of the needless copying and thus retains the correct semantics while improving performance.

==通过执行写时复制 fork()，操作系统避免了大部分不必要的复制，从而在保持正确语义的同时提高了性能。==

  

23.2 The Linux Virtual Memory System

==23.2 Linux 虚拟内存系统==

  

We'll now discuss some of the more interesting aspects of the Linux VM system.

==我们现在将讨论 Linux 虚拟机系统的一些更有趣的方面。==

  

Linux development has been driven forward by real engineers solving real problems encountered in production, and thus a large number of features have slowly been incorporated into what is now a fully functional, feature-filled virtual memory system.

==Linux 的开发一直由解决生产中遇到的实际问题的真正工程师推动，因此大量功能已慢慢整合到现在功能齐全、特性丰富的虚拟内存系统中。==

  

While we won't be able to discuss every aspect of Linux VM, we'll touch on the most important ones, especially where it has gone beyond what is found in classic VM systems such as VAX/VMS.

==虽然我们无法讨论 Linux 虚拟机的方方面面，但我们将触及最重要的方面，特别是它超越了经典虚拟机系统（如 VAX/VMS）的地方。==

  

We'll also try to highlight commonalities between Linux and older systems.

==我们还将尝试强调 Linux 与旧系统之间的共性。==

  

For this discussion, we'll focus on Linux for Intel x86.

==为了便于讨论，我们将重点关注基于 Intel x86 的 Linux。==

  

While Linux can and does run on many different processor architectures, Linux on x86 is its most dominant and important deployment, and thus the focus of our attention.

==虽然 Linux 可以并且确实在许多不同的处理器架构上运行，但 x86 上的 Linux 是其最主导和最重要的部署，因此是我们关注的焦点。==

  

The Linux Address Space

==Linux 地址空间==

  

Much like other modern operating systems, and also like VAX/VMS, a Linux virtual address space consists of a user portion (where user program code, stack, heap, and other parts reside) and a kernel portion (where kernel code, stacks, heap, and other parts reside).

==这就跟其他现代操作系统一样，也像 VAX/VMS 一样，Linux 虚拟地址空间由用户部分（用户程序代码、栈、堆和其他部分驻留在此）和内核部分（内核代码、栈、堆和其他部分驻留在此）组成。==

  

Like those other systems, upon a context switch, the user portion of the currently-running address space changes;

==像其他系统一样，在上下文切换时，当前运行的地址空间的用户部分会发生变化；==

  

the kernel portion is the same across processes.

==内核部分在进程间是相同的。==

  

Like those other systems, a program running in user mode cannot access kernel virtual pages;

==像其他系统一样，在用户模式下运行的程序无法访问内核虚拟页面；==

  

only by trapping into the kernel and transitioning to privileged mode can such memory be accessed.

==只有通过陷入内核并转换为特权模式，才能访问此类内存。==

  

In classic 32-bit Linux (i.e., Linux with a 32-bit virtual address space), the split between user and kernel portions of the address space takes place at address 0xC0000000, or three-quarters of the way through the address space.

==在经典的 32 位 Linux（即具有 32 位虚拟地址空间的 Linux）中，地址空间的用户部分和内核部分之间的分割发生在地址 0xC0000000 处，即地址空间的四分之三处。==

  

Thus, virtual addresses through 0xBFFFFFFF are user virtual addresses; the remaining virtual addresses (0xC0000000 through 0xFFFFFFFF) are in the kernel's virtual address space.

==因此，直到 0xBFFFFFFF 的虚拟地址是用户虚拟地址；剩余的虚拟地址（0xC0000000 到 0xFFFFFFFF）位于内核的虚拟地址空间中。==

  

64-bit Linux has a similar split but at slightly different points.

==64 位 Linux 有类似的分割，但在稍微不同的点上。==

  

Figure 23.2 shows a depiction of a typical (simplified) address space.

==图 23.2 显示了一个典型（简化）地址空间的描绘。==

  

One slightly interesting aspect of Linux is that it contains two types of kernel virtual addresses.

==Linux 一个稍微有趣的方面是它包含两种类型的内核虚拟地址。==

  

The first are known as kernel logical addresses.

==第一种称为内核逻辑地址。==

  

This is what you would consider the normal virtual address space of the kernel;

==这就是你会认为是内核的正常虚拟地址空间；==

  

to get more memory of this type, kernel code merely needs to call kmalloc.

==要获得更多这种类型的内存，内核代码只需调用 kmalloc。==

  

Most kernel data structures live here, such as page tables, per-process kernel stacks, and so forth.

==大多数内核数据结构都驻留在这里，例如页表、每进程内核栈等。==

  

Unlike most other memory in the system, kernel logical memory cannot be swapped to disk.

==与系统中的大多数其他内存不同，内核逻辑内存不能交换到磁盘。==

  

Figure 23.2: The Linux Address Space

==图 23.2：Linux 地址空间==

  

The most interesting aspect of kernel logical addresses is their connection to physical memory.

==内核逻辑地址最有趣的方面是它们与物理内存的连接。==

  

Specifically, there is a direct mapping between kernel logical addresses and the first portion of physical memory.

==具体来说，内核逻辑地址与物理内存的第一部分之间存在直接映射。==

  

Thus, kernel logical address 0xC0000000 translates to physical address 0x00000000, 0xC0000FFF to 0x00000FFF, and so forth.

==因此，内核逻辑地址 0xC0000000 转换为物理地址 0x00000000，0xC0000FFF 转换为 0x00000FFF，依此类推。==

  

This direct mapping has two implications.

==这种直接映射有两个含义。==

  

The first is that it is simple to translate back and forth between kernel logical addresses and physical addresses;

==首先，在内核逻辑地址和物理地址之间来回转换很简单；==

  

as a result, these addresses are often treated as if they are indeed physical.

==结果，这些地址通常被视为它们确实是物理地址。==

  

The second is that if a chunk of memory is contiguous in kernel logical address space, it is also contiguous in physical memory.

==其次，如果一块内存在内核逻辑地址空间中是连续的，那么它在物理内存中也是连续的。==

  

This makes memory allocated in this part of the kernel's address space suitable for operations which need contiguous physical memory to work correctly, such as I/O transfers to and from devices via direct memory access (DMA).

==这使得在内核地址空间的这一部分分配的内存适合需要连续物理内存才能正常工作的操作，例如通过直接内存访问 (DMA) 与设备进行的 I/O 传输。==

  

The other type of kernel address is a kernel virtual address.

==另一种类型的内核地址是内核虚拟地址。==

  

To get memory of this type, kernel code calls a different allocator, vmalloc, which returns a pointer to a virtually contiguous region of the desired size.

==要获得这种类型的内存，内核代码调用不同的分配器 vmalloc，它返回指向所需大小的虚拟连续区域的指针。==

  

Unlike kernel logical memory, kernel virtual memory is usually not contiguous;

==与内核逻辑内存不同，内核虚拟内存通常不是连续的；==

  

each kernel virtual page may map to non-contiguous physical pages (and is thus not suitable for DMA).

==每个内核虚拟页面可能映射到非连续的物理页面（因此不适合 DMA）。==

  

However, such memory is easier to allocate as a result, and thus used for large buffers where finding a contiguous large chunk of physical memory would be challenging.

==然而，这种内存因此更容易分配，因此用于大型缓冲区，在这种情况下，找到连续的大块物理内存将具有挑战性。==

  

In 32-bit Linux, one other reason for the existence of kernel virtual addresses is that they enable the kernel to address more than (roughly) 1 GB of memory.

==在 32 位 Linux 中，存在内核虚拟地址的另一个原因是它们使内核能够寻址超过（大约）1 GB 的内存。==

  

Years ago, machines had much less memory than this, and enabling access to more than 1 GB was not an issue.

==几年前，机器的内存比这少得多，启用对超过 1 GB 的访问不是问题。==

  

However, technology progressed, and soon there was a need to enable the kernel to use larger amounts of memory.

==然而，技术进步了，很快就需要让内核使用更大量的内存。==

  

Kernel virtual addresses, and their disconnection from a strict one-to-one mapping to physical memory, make this possible.

==内核虚拟地址及其与物理内存严格一对一映射的脱节，使得这成为可能。==

  

However, with the move to 64-bit Linux, the need is less urgent, because the kernel is not confined to only the last 1 GB of the virtual address space.

==然而，随着向 64 位 Linux 的迁移，这种需求不再那么紧迫，因为内核不再局限于虚拟地址空间的最后 1 GB。==

  

Page Table Structure

==页表结构==

  

Because we are focused on Linux for x86, our discussion will center on the type of page-table structure provided by x86, as it determines what Linux can and cannot do.

==因为我们专注于 x86 的 Linux，所以我们的讨论将集中在 x86 提供的页表结构类型上，因为它决定了 Linux 能做什么和不能做什么。==

  

As mentioned before, x86 provides a hardware-managed, multi-level page table structure, with one page table per process;

==如前所述，x86 提供了一种硬件管理的多级页表结构，每个进程一个页表；==

  

the OS simply sets up mappings in its memory, points a privileged register at the start of the page directory, and the hardware handles the rest.

==操作系统只需在其内存中设置映射，将特权寄存器指向页目录的开始，硬件就会处理剩下的工作。==

  

The OS gets involved, as expected, at process creation, deletion, and upon context switches, making sure in each case that the correct page table is being used by the hardware MMU to perform translations.

==正如预期的那样，操作系统会在进程创建、删除和上下文切换时介入，确保在每种情况下硬件 MMU 都使用正确的页表来执行转换。==

  

Probably the biggest change in recent years is the move from 32-bit x86 to 64-bit x86, as briefly mentioned above.

==近年来最大的变化可能是从 32 位 x86 向 64 位 x86 的迁移，如上所述。==

  

As seen in the VAX/VMS system, 32-bit address spaces have been around for a long time, and as technology changed, they were finally starting to become a real limit for programs.

==正如在 VAX/VMS 系统中看到的那样，32 位地址空间已经存在了很长时间，随着技术的变化，它们终于开始成为程序的真正限制。==

  

Virtual memory makes it easy to program systems, but with modern systems containing many GB of memory, 32 bits were no longer enough to refer to each of them.

==虚拟内存使系统编程变得容易，但随着现代系统包含数 GB 的内存，32 位不再足以引用其中的每一个。==

  

Thus, the next leap became necessary.

==因此，下一次飞跃变得必要。==

COMPLETE VIRTUAL MEMORY SYSTEMS
==完整的虚拟内存系统==

Moving to a 64-bit address affects page table structure in x86 in the expected manner.
==转向 64 位地址会以预期的方式影响 x86 中的页表结构。==

Because x86 uses a multi-level page table, current 64-bit systems use a four-level table.
==由于 x86 使用多级页表，当前的 64 位系统使用四级页表。==

The full 64-bit nature of the virtual address space is not yet in use, however, rather only the bottom 48 bits.
==然而，虚拟地址空间的完整 64 位特性尚未被使用，目前仅使用了低 48 位。==

Thus, a virtual address can be viewed as follows:
==因此，虚拟地址可以看作如下结构：==

As you can see in the picture, the top 16 bits of a virtual address are unused (and thus play no role in translation).
==如图所示，虚拟地址的高 16 位未被使用（因此不参与地址转换）。==

The bottom 12 bits (due to the 4-KB page size) are used as the offset (and hence just used directly, and not translated).
==低 12 位（由于 4-KB 的页面大小）被用作偏移量（因此直接使用，不进行转换）。==

Leaving the middle 36 bits of virtual address to take part in the translation.
==剩下的中间 36 位虚拟地址参与转换。==

The P1 portion of the address is used to index into the topmost page directory.
==地址的 P1 部分用于索引顶层页目录。==

The translation proceeds from there, one level at a time, until the actual page of the page table is indexed by P4, yielding the desired page table entry.
==转换从那里开始，一次一级，直到 P4 索引到页表的实际页面，从而生成所需的页表项。==

As system memories grow even larger, more parts of this voluminous address space will become enabled, leading to five-level and eventually six-level page-table tree structures.
==随着系统内存变得更大，这个庞大地址空间的更多部分将被启用，从而导致五级甚至最终六级的页表树结构。==

Imagine that: a simple page table lookup requiring six levels of translation, just to figure out where in memory a certain piece of data resides.
==想象一下：一个简单的页表查找需要六级转换，仅仅为了弄清楚某块数据驻留在内存的何处。==

Large Page Support
==大页支持==

Intel x86 allows for the use of multiple page sizes, not just the standard 4-KB page.
==Intel x86 允许使用多种页面大小，而不仅仅是标准的 4-KB 页面。==

Specifically, recent designs support 2-MB and even 1-GB pages in hardware.
==具体来说，最近的设计在硬件上支持 2-MB 甚至 1-GB 的页面。==

Thus, over time, Linux has evolved to allow applications to utilize these huge pages (as they are called in the world of Linux).
==因此，随着时间的推移，Linux 已经演变为允许应用程序利用这些“巨型页”（在 Linux 世界中这样称呼）。==

Using huge pages, as hinted at earlier, leads to numerous benefits.
==如前所述，使用巨型页会带来许多好处。==

As seen in VAX/VMS, doing so reduces the number of mappings that are needed in the page table.
==正如在 VAX/VMS 中所见，这样做减少了页表中所需的映射数量。==

The larger the pages, the fewer the mappings.
==页面越大，映射越少。==

However, fewer page-table entries is not the driving force behind huge pages.
==然而，较少的页表项并不是巨型页背后的驱动力。==

Rather, it's better TLB behavior and related performance gains.
==相反，更好的 TLB 行为和相关的性能增益才是关键。==

When a process actively uses a large amount of memory, it quickly fills up the TLB with translations.
==当一个进程频繁使用大量内存时，它会迅速用转换填满 TLB。==

If those translations are for 4-KB pages, only a small amount of total memory can be accessed without inducing TLB misses.
==如果这些转换是针对 4-KB 页面的，那么只有少量总内存可以在不引发 TLB 未命中的情况下被访问。==

The result, for modern "big memory" workloads running on machines with many GBs of memory, is a noticeable performance cost.
==结果是，对于在拥有数 GB 内存的机器上运行的现代“大内存”工作负载，会有显著的性能成本。==

Recent research shows that some applications spend 10% of their cycles servicing TLB misses [B+13].
==最近的研究表明，一些应用程序花费 10% 的周期来处理 TLB 未命中 [B+13]。==

Huge pages allow a process to access a large tract of memory without TLB misses, by using fewer slots in the TLB, and thus is the main advantage.
==巨型页允许进程在不发生 TLB 未命中的情况下访问大片内存，因为它在 TLB 中使用的槽位更少，这是其主要优势。==

However, there are other benefits to huge pages: there is a shorter TLB-miss path, meaning that when a TLB miss does occur, it is serviced more quickly.
==然而，巨型页还有其他好处：TLB 未命中路径更短，这意味着当 TLB 未命中确实发生时，它的处理速度更快。==

In addition, allocation can be quite fast (in certain scenarios), a small but sometimes important benefit.
==此外，分配速度可能非常快（在某些场景下），这是一个虽小但有时很重要的好处。==

TIP: CONSIDER INCREMENTALISM
==提示：考虑渐进主义==

Many times in life, you are encouraged to be a revolutionary.
==生活中很多时候，你被鼓励成为一名革命者。==

"Think big!", they say.
==“想大点！”，他们说。==

"Change the world!", they scream.
==“改变世界！”，他们尖叫。==

And you can see why it is appealing; in some cases, big changes are needed, and thus pushing hard for them makes a lot of sense.
==你可以明白为什么这很吸引人；在某些情况下，确实需要巨大的改变，因此努力推动它们非常有意义。==

And, if you try it this way, at least they might stop yelling at you.
==而且，如果你尝试这样做，至少他们可能会停止对你大喊大叫。==

However, in many cases, a slower, more incremental approach might be the right thing to do.
==然而，在许多情况下，较慢、更渐进的方法可能是正确的做法。==

The Linux huge page example in this chapter is an example of engineering incrementalism.
==本章中的 Linux 巨型页示例就是工程渐进主义的一个例子。==

Instead of taking the stance of a fundamentalist and insisting large pages were the way of the future, developers took the measured approach.
==开发者没有采取原教旨主义者的立场，坚持认为大页是未来的方向，而是采取了审慎的方法。==

First introducing specialized support for it, learning more about its upsides and downsides.
==首先为其引入专门的支持，以此更多地了解其优缺点。==

And, only when there was real reason for it, adding more generic support for all applications.
==并且，只有当确实有理由时，才为所有应用程序添加更通用的支持。==

Incrementalism, while sometimes scorned, often leads to slow, thoughtful, and sensible progress.
==渐进主义虽然有时被轻视，但往往能带来缓慢、深思熟虑和理性的进步。==

When building systems, such an approach might just be the thing you need.
==在构建系统时，这种方法可能正是你所需要的。==

Indeed, this may be true in life as well.
==事实上，这在生活中可能也是如此。==

One interesting aspect of Linux support for huge pages is how it was done incrementally.
==Linux 对巨型页支持的一个有趣方面是它是如何渐进地完成的。==

At first, Linux developers knew such support was only important for a few applications, such as large databases with stringent performance demands.
==起初，Linux 开发者知道这种支持仅对少数应用程序重要，例如具有严格性能要求的大型数据库。==

Thus, the decision was made to allow applications to explicitly request memory allocations with large pages (either through the mmap() or shmget() calls).
==因此，决定允许应用程序显式请求大页内存分配（通过 mmap() 或 shmget() 调用）。==

In this way, most applications would be unaffected (and continue to use only 4-KB pages).
==这样，大多数应用程序将不受影响（并继续仅使用 4-KB 页面）。==

A few demanding applications would have to be changed to use these interfaces, but for them it would be worth the pain.
==少数要求苛刻的应用程序必须更改以使用这些接口，但对它们来说，这是值得的。==

More recently, as the need for better TLB behavior is more common among many applications, Linux developers have added transparent huge page support.
==最近，随着许多应用程序对更好 TLB 行为的需求变得更加普遍，Linux 开发者添加了透明巨型页支持。==

When this feature is enabled, the operating system automatically looks for opportunities to allocate huge pages (usually 2 MB, but on some systems, 1 GB) without requiring application modification.
==启用此功能后，操作系统会自动寻找分配巨型页（通常为 2 MB，但在某些系统上为 1 GB）的机会，而无需修改应用程序。==

Huge pages are not without their costs.
==巨型页并非没有成本。==

The biggest potential cost is internal fragmentation, i.e., a page that is large but sparsely used.
==最大的潜在成本是内部碎片，即一个很大但使用稀疏的页面。==

This form of waste can fill memory with large but little used pages.
==这种形式的浪费会用很大但很少使用的页面填满内存。==

Swapping, if enabled, also does not work well with huge pages, sometimes greatly amplifying the amount of I/O a system does.
==如果启用了交换，它在巨型页上的工作效果也不好，有时会极大地放大系统的 I/O 量。==

Overhead of allocation can also be bad (in some other cases).
==分配的开销也可能很糟糕（在某些其他情况下）。==

Overall, one thing is clear: the 4-KB page size which served systems so well for so many years is not the universal solution it once was.
==总的来说，有一点很清楚：服务于系统多年的 4-KB 页面大小已不再是曾经的通用解决方案。==

Growing memory sizes demand that we consider large pages and other solutions as part of a necessary evolution of VM systems.
==不断增长的内存大小要求我们将大页和其他解决方案视为虚拟机系统必要演进的一部分。==

Linux's slow adoption of this hardware-based technology is evidence of the coming change.
==Linux 对这种基于硬件的技术的缓慢采用证明了即将到来的变化。==

The Page Cache
==页缓存==

To reduce costs of accessing persistent storage (the focus of the third part of this book), most systems use aggressive caching subsystems to keep popular data items in memory.
==为了降低访问持久存储（本书第三部分的重点）的成本，大多数系统使用积极的缓存子系统将热门数据项保存在内存中。==

Linux, in this regard, is no different than traditional operating systems.
==在这方面，Linux 与传统操作系统没有什么不同。==

The Linux page cache is unified, keeping pages in memory from three primary sources: memory-mapped files, file data and metadata from devices, and heap and stack pages that comprise each process.
==Linux 页缓存是统一的，将来自三个主要来源的页面保存在内存中：内存映射文件、来自设备的文件数据和元数据，以及构成每个进程的堆和栈页面。==

Sometimes called anonymous memory, because there is no named file underneath of it, but rather swap space.
==堆和栈页面有时被称为匿名内存，因为它们下面没有命名的文件，而是交换空间。==

These entities are kept in a page cache hash table, allowing for quick lookup when said data is needed.
==这些实体保存在页缓存哈希表中，以便在需要所述数据时进行快速查找。==

The page cache tracks if entries are clean (read but not updated) or dirty (a.k.a., modified).
==页缓存跟踪条目是干净的（已读但未更新）还是脏的（即已修改）。==

Dirty data is periodically written to the backing store (i.e., to a specific file for file data, or to swap space for anonymous regions) by background threads (called pdflush).
==脏数据由后台线程（称为 pdflush）定期写入后备存储（即文件数据写入特定文件，或匿名区域写入交换空间）。==

Thus ensuring that modified data eventually is written back to persistent storage.
==从而确保修改后的数据最终被写回持久存储。==

This background activity either takes place after a certain time period or if too many pages are considered dirty (both configurable parameters).
==这种后台活动要么在特定时间段后发生，要么在被视为脏页的页面过多时发生（两者都是可配置参数）。==

In some cases, a system runs low on memory, and Linux has to decide which pages to kick out of memory to free up space.
==在某些情况下，系统内存不足，Linux 必须决定将哪些页面踢出内存以释放空间。==

To do so, Linux uses a modified form of 2Q replacement [JS94], which we describe here.
==为此，Linux 使用了 2Q 替换算法 [JS94] 的修改版本，我们在这里进行描述。==

The basic idea is simple: standard LRU replacement is effective, but can be subverted by certain common access patterns.
==基本思想很简单：标准的 LRU（最近最少使用）替换是有效的，但可能会被某些常见的访问模式破坏。==

For example, if a process repeatedly accesses a large file (especially one that is nearly the size of memory, or larger), LRU will kick every other file out of memory.
==例如，如果一个进程重复访问一个大文件（特别是大小接近内存或更大的文件），LRU 将把所有其他文件踢出内存。==

Even worse: retaining portions of this file in memory isn't useful, as they are never re-referenced before getting kicked out of memory.
==更糟糕的是：将该文件的部分保留在内存中并没有用，因为它们在被踢出内存之前从未被重新引用。==

The Linux version of the 2Q replacement algorithm solves this problem by keeping two lists, and dividing memory between them.
==Linux 版本的 2Q 替换算法通过保留两个列表并在它们之间划分内存来解决这个问题。==

When accessed for the first time, a page is placed on one queue (called A1 in the original paper, but the inactive list in Linux).
==当第一次被访问时，页面被放置在一个队列中（原论文中称为 A1，但在 Linux 中称为非活动列表）。==

When it is re-referenced, the page is promoted to the other queue (called Aq in the original, but the active list in Linux).
==当它被重新引用时，页面被提升到另一个队列（原论文中称为 Aq，但在 Linux 中称为活动列表）。==

When replacement needs to take place, the candidate for replacement is taken from the inactive list.
==当需要进行替换时，替换候选者从非活动列表中选取。==

Linux also periodically moves pages from the bottom of the active list to the inactive list, keeping the active list to about two-thirds of the total page cache size [G04].
==Linux 还会定期将页面从活动列表的底部移动到非活动列表，将活动列表保持在总页缓存大小的约三分之二 [G04]。==

Linux would ideally manage these lists in perfect LRU order, but, as discussed in earlier chapters, doing so is costly.
==Linux 理想情况下会以完美的 LRU 顺序管理这些列表，但正如前面章节所讨论的，这样做成本很高。==

Thus, as with many OSes, an approximation of LRU (similar to clock replacement) is used.
==因此，与许多操作系统一样，使用了 LRU 的近似算法（类似于时钟替换）。==

This 2Q approach generally behaves quite a bit like LRU, but notably handles the case where a cyclic large-file access occurs by confining the pages of that cyclic access to the inactive list.
==这种 2Q 方法通常表现得非常像 LRU，但值得注意的是，它通过将循环访问的页面限制在非活动列表中，从而处理了发生循环大文件访问的情况。==

Because said pages are never re-referenced before getting kicked out of memory, they do not flush out other useful pages found in the active list.
==因为所述页面在被踢出内存之前从未被重新引用，所以它们不会冲掉活动列表中的其他有用页面。==

ASIDE: THE UBIQUITY OF MEMORY-MAPPING
==旁白：内存映射的普遍性==

Memory mapping predates Linux by some years, and is used in many places within Linux and other modern systems.
==内存映射早于 Linux 多年，并且在 Linux 和其他现代系统中的许多地方使用。==

The idea is simple: by calling mmap() on an already opened file descriptor, a process is returned a pointer to the beginning of a region of virtual memory where the contents of the file seem to be located.
==这个想法很简单：通过在已经打开的文件描述符上调用 mmap()，进程会返回一个指向虚拟内存区域开头的指针，文件的内容似乎就位于该区域。==

By then using that pointer, a process can access any part of the file with a simple pointer dereference.
==然后通过使用该指针，进程可以通过简单的指针解引用访问文件的任何部分。==

Accesses to parts of a memory-mapped file that have not yet been brought into memory trigger page faults.
==访问尚未调入内存的内存映射文件的部分会触发页面错误。==

At which point the OS will page in the relevant data and make it accessible by updating the page table of the process accordingly (i.e., demand paging).
==此时，操作系统将调入相关数据，并通过相应地更新进程的页表使其可访问（即按需分页）。==

Every regular Linux process uses memory-mapped files, even though the code in main() does not call mmap() directly.
==每个常规 Linux 进程都使用内存映射文件，即使 main() 中的代码没有直接调用 mmap()。==

Because of how Linux loads code from the executable and shared library code into memory.
==这是因为 Linux 将代码从可执行文件和共享库代码加载到内存中的方式。==

Below is the (highly abbreviated) output of the pmap command line tool, which shows what different mapping comprise the virtual address space of a running program (the shell, in this example, tcsh).
==下面是 pmap 命令行工具的（高度缩写的）输出，它显示了哪些不同的映射构成了正在运行的程序的虚拟地址空间（在此示例中为 shell，即 tcsh）。==

The output shows four columns: the virtual address of the mapping, its size, the protection bits of the region, and the source of the mapping.
==输出显示四列：映射的虚拟地址、其大小、该区域的保护位以及映射的来源。==

As you can see from this output, the code from the tcsh binary, as well as code from libc, libcrypt, libtinfo, and code from the dynamic linker itself (ld.so) are all mapped into the address space.
==从输出中可以看出，来自 tcsh 二进制文件的代码，以及来自 libc、libcrypt、libtinfo 的代码，还有动态链接器本身 (ld.so) 的代码都映射到了地址空间中。==

Also present are two anonymous regions, the heap (the second entry, labeled anon) and the stack (labeled stack).
==同样存在的还有两个匿名区域，堆（第二个条目，标记为 anon）和栈（标记为 stack）。==

Memory-mapped files provide a straightforward and efficient way for the OS to construct a modern address space.
==内存映射文件为操作系统构建现代地址空间提供了一种直接而有效的方法。==

Security And Buffer Overflows
==安全与缓冲区溢出==

Probably the biggest difference between modern VM systems (Linux, Solaris, or one of the BSD variants) and ancient ones (VAX/VMS) is the emphasis on security in the modern era.
==现代虚拟机系统（Linux、Solaris 或 BSD 变体之一）与古代系统（VAX/VMS）之间最大的区别可能在于现代时代对安全的重视。==

Protection has always been a serious concern for operating systems.
==保护一直是操作系统的一个严重关切。==

But with machines more interconnected than ever, it is no surprise that developers have implemented a variety of defensive countermeasures to halt those wily hackers from gaining control of systems.
==但是，随着机器之间的互联比以往任何时候都更加紧密，开发人员实施各种防御对策以阻止那些狡猾的黑客控制系统也就不足为奇了。==

One major threat is found in buffer overflow attacks, which can be used against normal user programs and even the kernel itself.
==缓冲区溢出攻击是一个主要威胁，它可用于攻击普通用户程序甚至内核本身。==

The idea of these attacks is to find a bug in the target system which lets the attacker inject arbitrary data into the target's address space.
==这些攻击的想法是在目标系统中找到一个错误，让攻击者可以将任意数据注入目标的地址空间。==

Such vulnerabilities sometime arise because the developer assumes (erroneously) that an input will not be overly long, and thus (trustingly) copies the input into a buffer.
==这种漏洞有时会出现，是因为开发人员（错误地）假设输入不会太长，因此（轻信地）将输入复制到缓冲区中。==

Because the input is in fact too long, it overflows the buffer, thus overwriting memory of the target.
==因为输入实际上太长，它会溢出缓冲区，从而覆盖目标的内存。==

Code as innocent as the below can be the source of the problem:
==像下面这样无辜的代码可能是问题的根源：==

```c
int some_function(char *input) {
    char dest_buffer[100];
    strcpy(dest_buffer, input); // oops, unbounded copy!
}

```

==（代码片段）==

In many cases, such an overflow is not catastrophic, e.g., bad input innocently given to a user program or even the OS will probably cause it to crash, but no worse.
==在许多情况下，这种溢出不是灾难性的，例如，无意中给予用户程序甚至操作系统的错误输入可能会导致其崩溃，但不会更糟。==

However, malicious programmers can carefully craft the input that overflows the buffer so as to inject their own code into the targeted system.
==然而，恶意程序员可以精心制作溢出缓冲区的输入，以便将他们自己的代码注入目标系统。==

Essentially allowing them to take it over and do their own bidding.
==本质上允许他们接管系统并执行他们的命令。==

If successful upon a network-connected user program, attackers can run arbitrary computations or even rent out cycles on the compromised system.
==如果对联网的用户程序攻击成功，攻击者可以运行任意计算，甚至出租受损系统的计算周期。==

If successful upon the operating system itself, the attack can access even more resources, and is a form of what is called privilege escalation (i.e., user code gaining kernel access rights).
==如果对操作系统本身的攻击成功，该攻击可以访问更多资源，这是一种所谓的权限提升（即用户代码获得内核访问权限）。==

If you can't guess, these are all Bad Things.
==如果你猜不到，这些都是坏事。==

The first and most simple defense against buffer overflow is to prevent execution of any code found within certain regions of an address space (e.g., within the stack).
==针对缓冲区溢出的第一个也是最简单的防御措施是防止执行在地址空间特定区域（例如栈内）发现的任何代码。==

The NX bit (for No-eXecute), introduced by AMD into their version of x86 (a similar XD bit is now available on Intel's), is one such defense.
==AMD 在其 x86 版本中引入的 NX 位（代表不执行，Intel 现在也有类似的 XD 位）就是这样一种防御措施。==

It just prevents execution from any page which has this bit set in its corresponding page table entry.
==它只是防止从任何在其对应页表项中设置了此位的页面执行。==

The approach prevents code, injected by an attacker into the target's stack, from being executed, and thus mitigates the problem.
==这种方法可以防止攻击者注入目标栈的代码被执行，从而缓解了问题。==

However, clever attackers are ... clever, and even when injected code cannot be added explicitly by the attacker, arbitrary code sequences can be executed by malicious code.
==然而，聪明的攻击者毕竟是……聪明的，即使攻击者无法显式添加注入代码，恶意代码也可以执行任意代码序列。==

The idea is known, in its most general form, as return-oriented programming (ROP) [S07], and really it is quite brilliant.
==这个想法最普遍的形式被称为面向返回编程 (ROP) [S07]，而且这确实非常绝妙。==

The observation behind ROP is that there are lots of bits of code (gadgets, in ROP terminology) within any program's address space, especially C programs that link with the voluminous C library.
==ROP 背后的观察结果是，在任何程序的地址空间内都有大量的代码片段（在 ROP 术语中称为 gadget），特别是链接了庞大 C 库的 C 程序。==

Thus, an attacker can overwrite the stack such that the return address in the currently executing function points to a desired malicious instruction (or series of instructions), followed by a return instruction.
==因此，攻击者可以覆盖栈，使得当前执行函数的返回地址指向所需的恶意指令（或指令系列），其后是一个返回指令。==

By stringing together a large number of gadgets (i.e., ensuring each return jumps to the next gadget), the attacker can execute arbitrary code.
==通过串联大量的 gadget（即确保每次返回都跳转到下一个 gadget），攻击者可以执行任意代码。==

Amazing!
==太惊人了！==

To defend against ROP (including its earlier form, the return-to-libc attack [S+04]), Linux (and other systems) add another defense, known as address space layout randomization (ASLR).
==为了防御 ROP（包括其早期形式，return-to-libc 攻击 [S+04]），Linux（和其他系统）增加了另一种防御，称为地址空间布局随机化 (ASLR)。==

Instead of placing code, stack, and the heap at fixed locations within the virtual address space, the OS randomizes their placement.
==操作系统不再将代码、栈和堆放置在虚拟地址空间内的固定位置，而是随机化它们的放置位置。==

Thus making it quite challenging to craft the intricate code sequence required to implement this class of attacks.
==从而使得构建实施此类攻击所需的复杂代码序列变得相当具有挑战性。==

Most attacks on vulnerable user programs will thus cause crashes, but not be able to gain control of the running program.
==因此，针对易受攻击的用户程序的大多数攻击将导致崩溃，但无法获得对正在运行程序的控制权。==

Interestingly, you can observe this randomness in practice rather easily.
==有趣的是，你可以很容易地在实践中观察到这种随机性。==

Here's a piece of code that demonstrates it on a modern Linux system:
==这是一段在现代 Linux 系统上演示它的代码：==

```c
int main(int argc, char *argv[]) {
    int stack = 0;
    printf("%p\n", &stack);
    return 0;
}

```

==（代码片段）==

This code just prints out the (virtual) address of a variable on the stack.
==这段代码只是打印栈上一个变量的（虚拟）地址。==

In older non-ASLR systems, this value would be the same each time.
==在旧的非 ASLR 系统中，此值每次都是相同的。==

But, as you can see below, the value changes with each run:
==但是，如下所示，该值随每次运行而变化：==

```
prompt> ./random
0x7ffd3e55d2b4
prompt> ./random
0x7ffe1033b8f4
prompt> ./random
0x7ffe45522e94

```

==（命令行输出）==

ASLR is such a useful defense for user-level programs that it has also been incorporated into the kernel, in a feature unimaginatively called kernel address space layout randomization (KASLR).
==ASLR 对用户级程序来说是一种非常有用的防御措施，以至于它也被纳入内核，这一功能被缺乏想象力地称为内核地址空间布局随机化 (KASLR)。==

However, it turns out the kernel may have even bigger problems to handle, as we discuss next.
==然而，事实证明内核可能有更大的问题要处理，正如我们接下来讨论的那样。==

Other Security Problems: Meltdown And Spectre
==其他安全问题：Meltdown 和 Spectre==

As we write these words (August, 2018), the world of systems security has been turned upside down by two new and related attacks.
==当我们写下这些文字时（2018 年 8 月），系统安全领域被两个新的相关攻击颠覆了。==

The first is called Meltdown, and the second Spectre.
==第一个叫做 Meltdown（熔断），第二个叫做 Spectre（幽灵）。==

They were discovered at about the same time by four different groups of researchers/engineers.
==它们大约在同一时间被四个不同的研究人员/工程师小组发现。==

And have led to deep questioning of the fundamental protections offered by computer hardware and the OS above.
==并导致了对计算机硬件和上层操作系统提供的基本保护的深刻质疑。==

See spectreattack.com for papers describing each attack in detail; Spectre is considered the more problematic of the two.
==请访问 spectreattack.com 查看详细描述每种攻击的论文；Spectre 被认为是两者中问题更严重的一个。==

The general weakness exploited in each of these attacks is that the CPUs found in modern systems perform all sorts of crazy behind-the-scenes tricks to improve performance.
==这两种攻击利用的普遍弱点是，现代系统中的 CPU 执行各种疯狂的幕后技巧来提高性能。==

One class of technique that lies at the core of the problem is called speculative execution.
==处于问题核心的一类技术称为推测执行。==

In which the CPU guesses which instructions will soon be executed in the future, and starts executing them ahead of time.
==在这种技术中，CPU 猜测哪些指令很快将在未来执行，并提前开始执行它们。==

If the guesses are correct, the program runs faster.
==如果猜测正确，程序运行得更快。==

If not, the CPU undoes their effects on architectural state (e.g., registers) and tries again, this time going down the right path.
==如果不对，CPU 会撤消它们对架构状态（例如寄存器）的影响并重试，这次沿着正确的路径进行。==

The problem with speculation is that it tends to leave traces of its execution in various parts of the system, such as processor caches, branch predictors, etc.
==推测的问题在于它往往会在系统的各个部分留下执行痕迹，例如处理器缓存、分支预测器等。==

And thus the problem: as the authors of the attacks show, such state can make vulnerable the contents of memory, even memory that we thought was protected by the MMU.
==因此出现了问题：正如攻击的作者所展示的那样，这种状态会使内存内容变得脆弱，甚至是那些我们认为受 MMU 保护的内存。==

One avenue to increasing kernel protection was thus to remove as much of the kernel address space from each user process and instead have a separate kernel page table for most kernel data (called kernel page-table isolation, or KPTI) [G+17].
==因此，增加内核保护的一个途径是从每个用户进程中移除尽可能多的内核地址空间，取而代之的是为大多数内核数据使用单独的内核页表（称为内核页表隔离，或 KPTI）[G+17]。==

Thus, instead of mapping the kernel's code and data structures into each process, only the barest minimum is kept therein.
==因此，不再将内核的代码和数据结构映射到每个进程中，而是仅在其中保留最少的部分。==

When switching into the kernel, then, a switch to the kernel page table is now needed.
==那么，当切换到内核时，现在需要切换到内核页表。==

Doing so improves security and avoids some attack vectors, but at a cost: performance. Switching page tables is costly.
==这样做提高了安全性并避免了一些攻击向量，但有代价：性能。切换页表是昂贵的。==

Ah, the costs of security: convenience and performance.
==啊，安全的代价：便利和性能。==

Unfortunately, KPTI doesn't solve all of the security problems laid out above, just some of them.
==不幸的是，KPTI 并没有解决上述所有的安全问题，只解决了部分问题。==

And simple solutions, such as turning off speculation, would make little sense, because systems would run thousands of times slower.
==而简单的解决方案，如关闭推测，没有什么意义，因为系统运行速度会慢上数千倍。==

Thus, it is an interesting time to be alive, if systems security is your thing.
==因此，如果系统安全是你感兴趣的领域，这真是一个有趣的时代。==

To truly understand these attacks, you'll (likely) have to learn a lot more first.
==要真正理解这些攻击，你（可能）必须先学很多东西。==

Begin by understanding modern computer architecture, as found in advanced books on the topic, focusing on speculation and all the mechanisms needed to implement it.
==从理解现代计算机体系结构开始，就像在该主题的高级书籍中看到的那样，重点关注推测以及实现它所需的所有机制。==

Definitely read about the Meltdown and Spectre attacks, at the websites mentioned above.
==一定要阅读上述网站上关于 Meltdown 和 Spectre 攻击的内容。==

They actually also include a useful primer on speculation, so perhaps are not a bad place to start.
==它们实际上也包含了关于推测的有用的入门知识，所以也许是一个不错的起点。==

And study the operating system for further vulnerabilities. Who knows what problems remain?
==并研究操作系统以寻找更多的漏洞。谁知道还遗留着什么问题呢？==

23.3 Summary
==23.3 总结==

You have now seen a top-to-bottom review of two virtual memory systems.
==你现在已经看到了两个虚拟内存系统的自上而下的回顾。==

Hopefully, most of the details were easy to follow, as you should have already had a good understanding of the basic mechanisms and policies.
==希望大多数细节都很容易理解，因为你应该已经对基本机制和策略有了很好的理解。==

More detail on VAX/VMS is available in the excellent (and short) paper by Levy and Lipman [LL82].
==关于 VAX/VMS 的更多细节可以在 Levy 和 Lipman 撰写的优秀（且简短）的论文 [LL82] 中找到。==

We encourage you to read it, as it is a great way to see what the source material behind these chapters is like.
==我们鼓励你阅读它，因为这是一个很好的方式来看看这些章节背后的原始材料是什么样的。==

You have also learned a bit about Linux.
==你也学到了一些关于 Linux 的知识。==

While a large and complex system, it inherits many good ideas from the past, many of which we have not had room to discuss in detail.
==虽然是一个庞大而复杂的系统，但它继承了许多过去的优秀理念，其中许多我们没有空间详细讨论。==

For example, Linux performs lazy copy-on-write copying of pages upon fork(), thus lowering overheads by avoiding unnecessary copying.
==例如，Linux 在 fork() 时执行页面的延迟写时复制，从而通过避免不必要的复制来降低开销。==

Linux also demand zeroes pages (using memory-mapping of the /dev/zero device).
==Linux 同样按需清零页面（使用 /dev/zero 设备的内存映射）。==

And has a background swap daemon (swapd) that swaps pages to disk to reduce memory pressure.
==并有一个后台交换守护进程 (swapd)，将页面交换到磁盘以减少内存压力。==

Indeed, the VM is filled with good ideas taken from the past, and also includes many of its own innovations.
==确实，虚拟机充满了从过去借鉴的好主意，也包含许多自己的创新。==

To learn more, check out these reasonable (but, alas, outdated) books [BC05,G04].
==要了解更多信息，请查阅这些尚可（但，唉，已过时）的书籍 [BC05,G04]。==

We encourage you to read them on your own, as we can only provide the merest drop from what is an ocean of complexity.
==我们鼓励你自学阅读它们，因为在一片复杂的海洋中，我们只能提供沧海一粟。==

But, you've got to start somewhere. What is any ocean, but a multitude of drops? [M04]
==但是，你总得从某个地方开始。任何海洋不都是由无数的水滴组成的吗？[M04]==

Summary Dialogue on Memory Virtualization
==关于内存虚拟化的总结对话==

Student: (Gulps) Wow, that was a lot of material.
==学生：（吞口水）哇，材料真多。==

Professor: Yes, and?
==教授：是的，然后呢？==

Student: Well, how am I supposed to remember it all? You know, for the exam?
==学生：嗯，我该怎么记住这一切呢？你知道，为了考试？==

Professor: Goodness, I hope that's not why you are trying to remember it.
==教授：天哪，我希望那不是你试图记住它的原因。==

Student: Why should I then?
==学生：那我为什么要记住呢？==

Professor: Come on, I thought you knew better.
==教授：拜托，我以为你更懂事。==

You're trying to learn something here, so that when you go off into the world, you'll understand how systems actually work.
==你是在这里学习一些东西，这样当你步入社会时，你会理解系统实际上是如何工作的。==

Student: Hmm... can you give an example?
==学生：嗯……你能举个例子吗？==

Professor: Sure! One time back in graduate school, my friends and I were measuring how long memory accesses took, and once in a while the numbers were way higher than we expected.
==教授：当然！有一次在读研究生的时候，我和朋友们正在测量内存访问需要多长时间，偶尔这些数字会比我们预期的要高得多。==

We thought all the data was fitting nicely into the second-level hardware cache, you see, and thus should have been really fast to access.
==我们以为所有数据都能很好地放入二级硬件缓存中，你看，因此访问应该非常快。==

Student: (nods)
==学生：（点头）==

Professor: We couldn't figure out what was going on. So what do you do in such a case?
==教授：我们搞不清楚发生了什么。所以在这种情况下你会怎么做？==

Easy, ask a professor! So we went and asked one of our professors, who looked at the graph we had produced, and simply said "TLB".
==简单，问教授！所以我们去问了一位教授，他看了看我们生成的图表，只是简单地说了句“TLB”。==

Aha! Of course, TLB misses! Why didn't we think of that?
==啊哈！当然，TLB 未命中！我们怎么没想到呢？==

Having a good model of how virtual memory works helps diagnose all sorts of interesting performance problems.
==拥有一个良好的虚拟内存工作模型有助于诊断各种有趣的性能问题。==

Student: I think I see.
==学生：我想我明白了。==

I'm trying to build these mental models of how things work, so that when I'm out there working on my own, I won't be surprised when a system doesn't quite behave as expected.
==我正试图建立这些事物如何工作的心理模型，这样当我独自在外面工作时，如果系统表现不符合预期，我就不会感到惊讶。==

I should even be able to anticipate how the system will work just by thinking about it.
==我甚至应该能够仅仅通过思考来预测系统将如何工作。==

Professor: Exactly. So what have you learned? What's in your mental model of how virtual memory works?
==教授：正是。那你学到了什么？你关于虚拟内存如何工作的心理模型里有什么？==

Student: Well, I think I now have a pretty good idea of what happens when memory is referenced by a process.
==学生：嗯，我想我现在很清楚当进程引用内存时会发生什么了。==

Which, as you've said many times, happens on each instruction fetch as well as explicit loads and stores.
==正如你多次说过的，这发生在每次指令获取以及显式的加载和存储时。==

Professor: Sounds good - tell me more.
==教授：听起来不错——多跟我说说。==

Student: Well, one thing I'll always remember is that the addresses we see in a user program, written in C for example...
==学生：嗯，有一件事我会永远记住，那就是我们在用户程序中看到的地址，例如用 C 写的……==

Professor: What other language is there?
==教授：还有什么其他语言吗？==

Student: (continuing)... Yes, I know you like C. So do I!
==学生：（继续）……是的，我知道你喜欢 C。我也喜欢！==

Anyhow, as I was saying, I now really know that all addresses that we can observe within a program are virtual addresses.
==无论如何，正如我所说，我现在真的知道我们在程序中能观察到的所有地址都是虚拟地址。==

That I, as a programmer, am just given this illusion of where data and code are in memory.
==作为一名程序员，我只是得到了数据和代码在内存中位置的这种幻觉。==

I used to think it was cool that I could print the address of a pointer, but now I find it frustrating - it's just a virtual address!
==我曾经觉得能打印指针的地址很酷，但现在我觉得这很令人沮丧——它只是一个虚拟地址！==

I can't see the real physical address where the data lives.
==我看不到数据驻留的真实物理地址。==

Professor: Nope, the OS definitely hides that from you. What else?
==教授：没错，操作系统绝对向你隐瞒了这一点。还有什么？==

Student: Well, I think the TLB is a really key piece, providing the system with a small hardware cache of address translations.
==学生：嗯，我认为 TLB 是非常关键的部分，它为系统提供了一个小的地址转换硬件缓存。==

Page tables are usually quite large and hence live in big and slow memories.
==页表通常很大，因此存在于大而慢的内存中。==

Without that TLB, programs would certainly run a great deal more slowly.
==没有那个 TLB，程序肯定会运行得慢很多。==

Seems like the TLB truly makes virtualizing memory possible.
==看起来 TLB 确实使虚拟化内存成为可能。==

I couldn't imagine building a system without one!
==我无法想象构建一个没有它的系统！==

And I shudder at the thought of a program with a working set that exceeds the coverage of the TLB: with all those TLB misses, it would be hard to watch.
==一想到程序的工作集超过 TLB 的覆盖范围，我就不寒而栗：带着所有那些 TLB 未命中，那将惨不忍睹。==

Professor: Yes, cover the eyes of the children! Beyond the TLB, what did you learn?
==教授：是的，遮住孩子们的眼睛！除了 TLB，你还学到了什么？==

Student: I also now understand that the page table is one of those data structures you need to know about.
==学生：我现在也明白了页表是你需要了解的数据结构之一。==

It's just a data structure, though, and that means almost any structure could be used.
==不过它只是一个数据结构，这意味着几乎可以使用任何结构。==

We started with simple structures, like arrays (a.k.a. linear page tables), and advanced all the way up to multi-level tables (which look like trees).
==我们从简单的结构开始，如数组（即线性页表），一直进阶到多级表（看起来像树）。==

And even crazier things like pageable page tables in kernel virtual memory.
==甚至更疯狂的东西，比如内核虚拟内存中的可分页页表。==

All to save a little space in memory!
==所有这一切都是为了节省一点内存空间！==

Professor: Indeed.
==教授：确实。==

Student: And here's one more important thing: I learned that the address translation structures need to be flexible enough to support what programmers want to do with their address spaces.
==学生：还有一件重要的事：我了解到地址转换结构需要足够灵活，以支持程序员想要对其地址空间进行的操作。==

Structures like the multi-level table are perfect in this sense.
==像多级表这样的结构在这个意义上是完美的。==

They only create table space when the user needs a portion of the address space, and thus there is little waste.
==它们仅在用户需要一部分地址空间时才创建表空间，因此几乎没有浪费。==

Earlier attempts, like the simple base and bounds register, just weren't flexible enough.
==早期的尝试，如简单的基址和界限寄存器，就不够灵活。==

The structures need to match what users expect and want out of their virtual memory system.
==结构需要匹配用户对虚拟内存系统的期望和需求。==

Professor: That's a nice perspective. What about all of the stuff we learned about swapping to disk?
==教授：这个视角不错。那我们学到的关于交换到磁盘的所有内容呢？==

Student: Well, it's certainly fun to study, and good to know how page replacement works.
==学生：嗯，学习它当然很有趣，了解页面替换如何工作也很好。==

Some of the basic policies are kind of obvious (like LRU, for example), but building a real virtual memory system seems more interesting, like we saw in the VMS case study.
==一些基本策略显而易见（例如 LRU），但构建一个真正的虚拟内存系统似乎更有趣，就像我们在 VMS 案例研究中看到的那样。==

But somehow, I found the mechanisms more interesting, and the policies less so.
==但不知何故，我发现机制更有趣，而策略则没那么有趣。==

Professor: Oh, why is that?
==教授：哦，那是为什么？==

Student: Well, as you said, in the end the best solution to policy problems is simple: buy more memory.
==学生：嗯，正如你所说，最终解决策略问题的最佳方案很简单：购买更多内存。==

But the mechanisms you need to understand to know how stuff really works. Speaking of which...
==但你需要理解机制才能知道东西实际上是如何工作的。说到这个……==

Professor: Yes?
==教授：什么？==

Student: Well, my machine is running a little slowly these days... and memory certainly doesn't cost that much...
==学生：嗯，我的机器这几天运行得有点慢……而且内存确实不贵……==

Professor: Oh fine, fine! Here's a few bucks. Go and get yourself some DRAM, cheapskate.
==教授：哦，好吧，好吧！这有几块钱。去给自己买点 DRAM 吧，小气鬼。==

Student: Thanks professor! I'll never swap to disk again or, if I do, at least I'll know what's actually going on!
==学生：谢谢教授！我再也不会交换到磁盘了，或者，如果我这样做，至少我知道实际上发生了什么！==

Part II Concurrency
==第二部分 并发==

A Dialogue on Concurrency
==关于并发的对话==

Professor: And thus we reach the second of our three pillars of operating systems: concurrency.
==教授：至此，我们到达了操作系统的三大支柱中的第二个：并发。==

Student: I thought there were four pillars...?
==学生：我以为有四大支柱……？==

Professor: Nope, that was in an older version of the book.
==教授：不，那是旧版书里的。==

Student: Umm... OK. So what is concurrency, oh wonderful professor?
==学生：嗯……好吧。那么什么是并发呢，哦，很棒的教授？==

Professor: Well, imagine we have a peach
==教授：嗯，想象一下我们有一个桃子==

Student: (interrupting) Peaches again! What is it with you and peaches?
==学生：（打断）又是桃子！你和桃子到底怎么回事？==

Professor: Ever read T.S. Eliot? The Love Song of J. Alfred Prufrock, "Do I dare to eat a peach", and all that fun stuff?
==教授：读过 T.S. 艾略特吗？《J. 阿尔弗雷德·普鲁弗洛克的情歌》，“我敢吃桃子吗”，以及所有那些有趣的东西？==

Student: Oh yes! In English class in high school. Great stuff! I really liked the part where
==学生：哦，是的！在高中英语课上。很棒的东西！我真的很喜欢那部分==

Professor: (interrupting) This has nothing to do with that I just like peaches.
==教授：（打断）这与那无关，我只是喜欢桃子。==

Anyhow, imagine there are a lot of peaches on a table, and a lot of people who wish to eat them.
==无论如何，想象一下桌子上有很多桃子，还有很多人想吃它们。==

Let's say we did it this way: each eater first identifies a peach visually, and then tries to grab it and eat it.
==假设我们这样做：每个食客首先用视觉识别一个桃子，然后试图抓住并吃掉它。==

What is wrong with this approach?
==这种方法有什么问题？==

Student: Hmmm... seems like you might see a peach that somebody else also sees.
==学生：嗯……好像你可能会看到别人也看到的桃子。==

If they get there first, when you reach out, no peach for you!
==如果他们先到了，当你伸手去拿时，你就没桃子吃了！==

Professor: Exactly! So what should we do about it?
==教授：没错！那我们该怎么办？==

Student: Well, probably develop a better way of going about this.
==学生：嗯，可能需要开发一种更好的方法来解决这个问题。==

Maybe form a line, and when you get to the front, grab a peach and get on with it.
==也许排个队，当你排到前面时，拿一个桃子然后继续。==

Professor: Good! But what's wrong with your approach?
==教授：很好！但是你的方法有什么问题？==

Student: Sheesh, do I have to do all the work?
==学生：哎，非得我做所有的工作吗？==

Professor: Yes.
==教授：是的。==

Student: OK, let me think. Well, we used to have many people grabbing for peaches all at once, which is faster.
==学生：好吧，让我想想。以前我们有多人同时抢桃子，这比较快。==

But in my way, we just go one at a time, which is correct, but quite a bit slower.
==但用我的方法，我们一次只去一个人，这是正确的，但要慢很多。==

The best kind of approach would be fast and correct, probably.
==最好的方法可能是既快速又正确。==

Professor: You are really starting to impress. In fact, you just told us everything we need to know about concurrency!
==教授：你真的开始令人印象深刻了。事实上，你刚刚告诉了我们需要知道的关于并发的一切！==

Well done.
==做得好。==

Student: I did? I thought we were just talking about peaches.
==学生：是吗？我以为我们只是在谈论桃子。==

Remember, this is usually the part where you make it about computers again.
==记得，这通常是你把它再次扯回计算机的部分。==

Professor: Indeed. My apologies! One must never forget the concrete.
==教授：确实。我的错！人永远不能忘记具体事物。==

Well, as it turns out, there are certain types of programs that we call multi-threaded applications.
==嗯，事实证明，有一种特定类型的程序，我们称为多线程应用程序。==

Each thread is kind of like an independent agent running around in this program, doing things on the program's behalf.
==每个线程都有点像在该程序中运行的独立代理，代表程序做事。==

But these threads access memory, and for them, each spot of memory is kind of like one of those peaches.
==但是这些线程访问内存，对它们来说，内存的每个位置都有点像那些桃子之一。==

If we don't coordinate access to memory between threads, the program won't work as expected. Make sense?
==如果我们不协调线程之间的内存访问，程序就不会按预期工作。有道理吗？==

Student: Kind of. But why do we talk about this in an OS class? Isn't that just application programming?
==学生：有点道理。但为什么我们在操作系统课上讨论这个？那不只是应用程序编程吗？==

Professor: Good question! A few reasons, actually.
==教授：好问题！事实上，有几个原因。==

First, the OS must support multi-threaded applications with primitives such as locks and condition variables, which we'll talk about soon.
==首先，操作系统必须通过原语（如锁和条件变量，我们很快会讨论）来支持多线程应用程序。==

Second, the OS itself was the first concurrent program it must access its own memory very carefully or many strange and terrible things will happen.
==其次，操作系统本身就是第一个并发程序——它必须非常小心地访问自己的内存，否则会发生许多奇怪而可怕的事情。==

Really, it can get quite grisly.
==真的，那可能会非常可怕。==

Student: I see. Sounds interesting. There are more details, I imagine?
==学生：我明白了。听起来很有趣。我想还有更多细节吧？==

Professor: Indeed there are...
==教授：确实有……==

Concurrency: An Introduction
==并发：简介==

Thus far, we have seen the development of the basic abstractions that the OS performs.
==到目前为止，我们已经看到了操作系统执行的基本抽象的发展。==

We have seen how to take a single physical CPU and turn it into multiple virtual CPUs, thus enabling the illusion of multiple programs running at the same time.
==我们已经看到了如何利用单个物理 CPU 并将其转换为多个虚拟 CPU，从而实现多个程序同时运行的错觉。==

We have also seen how to create the illusion of a large, private virtual memory for each process.
==我们还看到了如何为每个进程创建一个巨大的、私有的虚拟内存的错觉。==

This abstraction of the address space enables each program to behave as if it has its own memory when indeed the OS is secretly multiplexing address spaces across physical memory (and sometimes, disk).
==这种地址空间的抽象使每个程序表现得好像拥有自己的内存，而实际上操作系统正在物理内存（有时是磁盘）上秘密地复用地址空间。==

In this chapter, we introduce a new abstraction for a single running process: that of a thread.
==在本章中，我们为单个正在运行的进程引入一个新的抽象：线程。==

Instead of our classic view of a single point of execution within a program (i.e., a single PC where instructions are being fetched from and executed).
==代替我们在程序中看到的单点执行的经典视图（即，一个单一的 PC，指令从中被获取并执行）。==

A multi-threaded program has more than one point of execution (i.e., multiple PCs, each of which is being fetched and executed from).
==多线程程序有不止一个执行点（即，多个 PC，每一个都在被获取和执行）。==

Perhaps another way to think of this is that each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data.
==也许另一种思考方式是，每个线程都非常像一个单独的进程，除了一点不同：它们共享相同的地址空间，因此可以访问相同的数据。==

The state of a single thread is thus very similar to that of a process.
==因此，单个线程的状态与进程的状态非常相似。==

It has a program counter (PC) that tracks where the program is fetching instructions from.
==它有一个程序计数器 (PC) 来跟踪程序从哪里获取指令。==

Each thread has its own private set of registers it uses for computation.
==每个线程都有自己用于计算的私有寄存器组。==

Thus, if there are two threads that are running on a single processor, when switching from running one (T1) to running the other (T2), a context switch must take place.
==因此，如果有两个线程在单个处理器上运行，当从运行一个 (T1) 切换到运行另一个 (T2) 时，必须进行上下文切换。==

The context switch between threads is quite similar to the context switch between processes.
==线程之间的上下文切换与进程之间的上下文切换非常相似。==

As the register state of T1 must be saved and the register state of T2 restored before running T2.
==因为在运行 T2 之前，必须保存 T1 的寄存器状态并恢复 T2 的寄存器状态。==

With processes, we saved state to a process control block (PCB).
==对于进程，我们将状态保存到进程控制块 (PCB) 中。==

Now, we'll need one or more thread control blocks (TCBs) to store the state of each thread of a process.
==现在，我们需要一个或多个线程控制块 (TCB) 来存储进程的每个线程的状态。==

There is one major difference, though, in the context switch we perform between threads as compared to processes: the address space remains the same (i.e., there is no need to switch which page table we are using).
==不过，与进程相比，我们在线程之间执行的上下文切换有一个主要区别：地址空间保持不变（即，无需切换我们正在使用的页表）。==

One other major difference between threads and processes concerns the stack.
==线程和进程之间的另一个主要区别涉及栈。==

In our simple model of the address space of a classic process (which we can now call a single-threaded process), there is a single stack, usually residing at the bottom of the address space.
==在我们对经典进程（我们现在可以称之为单线程进程）地址空间的简单模型中，只有一个栈，通常位于地址空间的底部。==

However, in a multi-threaded process, each thread runs independently and of course may call into various routines to do whatever work it is doing.
==然而，在多线程进程中，每个线程独立运行，当然也可以调用各种例程来做它正在做的任何工作。==

Instead of a single stack in the address space, there will be one per thread.
==地址空间中不再只有一个栈，而是每个线程一个。==

Let's say we have a multi-threaded process that has two threads in it; the resulting address space looks different.
==假设我们有一个包含两个线程的多线程进程；结果地址空间看起来会有所不同。==

In this figure, you can see two stacks spread throughout the address space of the process.
==在这个图中，你可以看到两个栈分布在进程的地址空间中。==

Thus, any stack-allocated variables, parameters, return values, and other things that we put on the stack will be placed in what is sometimes called thread-local storage, i.e., the stack of the relevant thread.
==因此，我们在栈上放置的任何栈分配变量、参数、返回值和其他东西都将放置在有时被称为线程本地存储的地方，即相关线程的栈中。==

You might also notice how this ruins our beautiful address space layout.
==你可能还会注意到这如何破坏了我们漂亮的地址空间布局。==

Before, the stack and heap could grow independently and trouble only arose when you ran out of room in the address space.
==以前，栈和堆可以独立增长，只有当地址空间空间不足时才会出现问题。==

Here, we no longer have such a nice situation.
==在这里，我们不再有这么好的情况。==

Fortunately, this is usually OK, as stacks do not generally have to be very large (the exception being in programs that make heavy use of recursion).
==幸运的是，这通常没问题，因为栈通常不需要很大（大量使用递归的程序除外）。==

26.1 Why Use Threads?
==26.1 为什么要使用线程？==

Before getting into the details of threads and some of the problems you might have in writing multi-threaded programs, let's first answer a more simple question.
==在深入探讨线程的细节以及在编写多线程程序时可能遇到的一些问题之前，让我们先回答一个更简单的问题。==

Why should you use threads at all?
==你为什么要使用线程？==

As it turns out, there are at least two major reasons you should use threads. The first is simple: parallelism.
==事实证明，至少有两个主要原因你应该使用线程。第一个很简单：并行性。==

Imagine you are writing a program that performs operations on very large arrays, for example, adding two large arrays together, or incrementing the value of each element in the array by some amount.
==想象一下，你正在编写一个对非常大的数组执行操作的程序，例如，将两个大数组相加，或者将数组中每个元素的值增加一定的量。==

If you are running on just a single processor, the task is straightforward: just perform each operation and be done.
==如果你只是在单个处理器上运行，任务很简单：只需执行每个操作即可完成。==

However, if you are executing the program on a system with multiple processors, you have the potential of speeding up this process considerably by using the processors to each perform a portion of the work.
==但是，如果你在具有多个处理器的系统上执行程序，你可以通过使用处理器分别执行一部分工作来大大加快此过程。==

The task of transforming your standard single-threaded program into a program that does this sort of work on multiple CPUs is called parallelization.
==将标准单线程程序转换为在多个 CPU 上执行此类工作的程序的任务称为并行化。==

And using a thread per CPU to do this work is a natural and typical way to make programs run faster on modern hardware.
==使用每个 CPU 一个线程来完成这项工作是在现代硬件上使程序运行得更快的自然且典型的方式。==

The second reason is a bit more subtle: to avoid blocking program progress due to slow I/O.
==第二个原因稍微微妙一些：为了避免由于缓慢的 I/O 而阻塞程序进度。==

Imagine that you are writing a program that performs different types of I/O: either waiting to send or receive a message, for an explicit disk I/O to complete, or even (implicitly) for a page fault to finish.
==想象一下，你正在编写一个执行不同类型 I/O 的程序：等待发送或接收消息，等待显式磁盘 I/O 完成，甚至（隐式）等待页面错误完成。==

Instead of waiting, your program may wish to do something else, including utilizing the CPU to perform computation, or even issuing further I/O requests.
==你的程序可能不想等待，而是希望做其他事情，包括利用 CPU 执行计算，甚至发出进一步的 I/O 请求。==

Using threads is a natural way to avoid getting stuck.
==使用线程是避免卡住的自然方式。==

While one thread in your program waits (i.e., is blocked waiting for I/O), the CPU scheduler can switch to other threads, which are ready to run and do something useful.
==当你程序中的一个线程等待（即，因等待 I/O 而被阻塞）时，CPU 调度程序可以切换到其他准备好运行并做一些有用事情的线程。==

Threading enables overlap of I/O with other activities within a single program, much like multiprogramming did for processes across programs.
==线程化使得单个程序内的 I/O 与其他活动重叠成为可能，就像多道程序设计对跨程序的进程所做的那样。==

As a result, many modern server-based applications (web servers, database management systems, and the like) make use of threads in their implementations.
==因此，许多现代基于服务器的应用程序（Web 服务器、数据库管理系统等）在其实现中使用了线程。==

Of course, in either of the cases mentioned above, you could use multiple processes instead of threads.
==当然，在上述任一情况下，你都可以使用多个进程来代替线程。==

However, threads share an address space and thus make it easy to share data, and hence are a natural choice when constructing these types of programs.
==然而，线程共享地址空间，因此很容易共享数据，因此在构建这些类型的程序时是自然的选择。==

Processes are a more sound choice for logically separate tasks where little sharing of data structures in memory is needed.
==对于逻辑上独立的、几乎不需要在内存中共享数据结构的任务，进程是更合理的选择。==

26.2 An Example: Thread Creation
==26.2 一个例子：线程创建==

Let's get into some of the details.
==让我们深入了解一些细节。==

Say we wanted to run a program that creates two threads, each of which does some independent work, in this case printing "A" or "B".
==假设我们想运行一个程序，该程序创建两个线程，每个线程做一些独立的工作，在这种情况下打印“A”或“B”。==

The code is shown in Figure 26.2.
==代码如图 26.2 所示。==

The main program creates two threads, each of which will run the function mythread(), though with different arguments (the string A or B).
==主程序创建两个线程，每个线程都将运行函数 mythread()，但使用不同的参数（字符串 A 或 B）。==

Once a thread is created, it may start running right away (depending on the whims of the scheduler).
==一旦线程被创建，它可能会立即开始运行（取决于调度程序的意愿）。==

Alternately, it may be put in a "ready" but not "running" state and thus not run yet.
==或者，它可能会被置于“就绪”但非“运行”状态，因此尚未运行。==

Of course, on a multiprocessor, the threads could even be running at the same time, but let's not worry about this possibility quite yet.
==当然，在多处理器上，线程甚至可以同时运行，但我们先不要担心这种可能性。==

After creating the two threads (let's call them T1 and T2), the main thread calls pthread_join(), which waits for a particular thread to complete.
==创建两个线程（我们称之为 T1 和 T2）后，主线程调用 pthread_join()，等待特定线程完成。==

It does so twice, thus ensuring T1 and T2 will run and complete before finally allowing the main thread to run again.
==它这样做两次，从而确保 T1 和 T2 在最终允许主线程再次运行之前运行并完成。==

When it does, it will print "main: end" and exit.
==当它这样做时，它将打印“main: end”并退出。==

Overall, three threads were employed during this run: the main thread, T1, and T2.
==总的来说，在这次运行中使用了三个线程：主线程、T1 和 T2。==

Let us examine the possible execution ordering of this little program.
==让我们检查一下这个小程序的可能执行顺序。==

In the execution diagram (Figure 26.3), time increases in the downwards direction, and each column shows when a different thread (the main one, or Thread 1, or Thread 2) is running.
==在执行图（图 26.3）中，时间向下增加，每一列显示不同的线程（主线程，或线程 1，或线程 2）何时运行。==

Note, however, that this ordering is not the only possible ordering.
==但是请注意，此顺序并非唯一可能的顺序。==

In fact, given a sequence of instructions, there are quite a few, depending on which thread the scheduler decides to run at a given point.
==事实上，给定一系列指令，有相当多的顺序，这取决于调度程序决定在给定点运行哪个线程。==

For example, once a thread is created, it may run immediately, which would lead to the execution shown in Figure 26.4.
==例如，一旦创建了一个线程，它可能会立即运行，这将导致图 26.4 所示的执行。==

We also could even see "B" printed before "A", if, say, the scheduler decided to run Thread 2 first even though Thread 1 was created earlier.
==如果不巧，比如说，调度程序决定先运行线程 2，即使线程 1 创建得更早，我们甚至可以看到“B”在“A”之前打印。==

There is no reason to assume that a thread that is created first will run first.
==没有理由假设先创建的线程会先运行。==

Figure 26.5 shows this final execution ordering, with Thread 2 getting to strut its stuff before Thread 1.
==图 26.5 显示了这个最终的执行顺序，线程 2 在线程 1 之前得以大显身手。==

As you might be able to see, one way to think about thread creation is that it is a bit like making a function call.
==正如你可能看到的那样，思考线程创建的一种方式是它有点像进行函数调用。==

However, instead of first executing the function and then returning to the caller, the system instead creates a new thread of execution for the routine that is being called.
==然而，系统不是先执行函数然后返回给调用者，而是为被调用的例程创建一个新的执行线程。==

And it runs independently of the caller, perhaps before returning from the create, but perhaps much later.
==并且它独立于调用者运行，可能在从创建返回之前，但也可能在很久以后。==

What runs next is determined by the OS scheduler, and although the scheduler likely implements some sensible algorithm, it is hard to know what will run at any given moment in time.
==接下来运行什么由操作系统调度程序决定，虽然调度程序可能实施了一些合理的算法，但很难知道在任何给定的时刻会运行什么。==

As you also might be able to tell from this example, threads make life complicated: it is already hard to tell what will run when!
==正如你可能从这个例子中看出的那样，线程使生活变得复杂：已经很难分辨什么将在何时运行！==

Computers are hard enough to understand without concurrency.
==没有并发，计算机已经够难理解的了。==

Unfortunately, with concurrency, it simply gets worse. Much worse.
==不幸的是，有了并发，情况只会变得更糟。糟得多。==

26.3 Why It Gets Worse: Shared Data
==26.3 为什么会变得更糟：共享数据==

The simple thread example we showed above was useful in showing how threads are created and how they can run in different orders depending on how the scheduler decides to run them.
==我们在上面展示的简单线程示例对于展示如何创建线程以及它们如何根据调度程序决定运行它们的顺序以不同的顺序运行很有用。==

What it doesn't show you, though, is how threads interact when they access shared data.
==但是，它没有向你展示线程在访问共享数据时如何交互。==

Let us imagine a simple example where two threads wish to update a global shared variable.
==让我们想象一个简单的例子，其中两个线程希望更新一个全局共享变量。==

The code we'll study is in Figure 26.6.
==我们将研究的代码在图 26.6 中。==

Here are a few notes about the code.
==这里有关于代码的几点说明。==

First, as Stevens suggests [SR05], we wrap the thread creation and join routines to simply exit on failure.
==首先，正如 Stevens 建议的那样 [SR05]，我们将线程创建和加入例程包装起来，以便在失败时简单地退出。==

For a program as simple as this one, we want to at least notice an error occurred (if it did), but not do anything very smart about it (e.g., just exit).
==对于像这样简单的程序，我们希望至少注意到发生了错误（如果有的话），但不对其做任何非常聪明的事情（例如，只是退出）。==

Thus, Pthread_create() simply calls pthread_create() and makes sure the return code is 0.
==因此，Pthread_create() 只是调用 pthread_create() 并确保返回代码为 0。==

If it isn't, Pthread_create() just prints a message and exits.
==如果不是，Pthread_create() 只是打印一条消息并退出。==

Second, instead of using two separate function bodies for the worker threads, we just use a single piece of code, and pass the thread an argument (in this case, a string) so we can have each thread print a different letter before its messages.
==其次，我们不为工作线程使用两个单独的函数体，而是只使用一段代码，并向线程传递一个参数（在本例中为一个字符串），以便我们可以让每个线程在其消息之前打印不同的字母。==

Finally, and most importantly, we can now look at what each worker is trying to do: add a number to the shared variable counter, and do so 10 million times (1e7) in a loop.
==最后，也是最重要的一点，我们现在可以看看每个工作者试图做什么：将一个数字加到共享变量 counter 上，并在循环中执行 1000 万次 (1e7)。==

Thus, the desired final result is: 20,000,000.
==因此，期望的最终结果是：20,000,000。==

We now compile and run the program, to see how it behaves. Sometimes, everything works how we might expect:
==我们现在编译并运行程序，看看它的表现如何。有时，一切都像我们预期的那样工作：==

```
prompt> gcc -o main main.c -Wall -pthread; ./main
main: begin (counter = 0)
A: begin
B: begin
A: done
B: done
main: done with both (counter = 20000000)

```

==（命令行输出）==

Unfortunately, when we run this code, even on a single processor, we don't necessarily get the desired result.
==不幸的是，当我们运行这段代码时，即使在单处理器上，也不一定能得到预期的结果。==

Sometimes, we get:
==有时，我们会得到：==

```
main: done with both (counter = 19345221)

```

==（部分输出）==

Let's try it one more time, just to see if we've gone crazy.
==让我们再试一次，看看我们是不是疯了。==

After all, aren't computers supposed to produce deterministic results, as you have been taught?!
==毕竟，正如你被教导的那样，计算机不应该产生确定性的结果吗？！==

Perhaps your professors have been lying to you? (gasp)
==也许你的教授一直在对你撒谎？（倒吸一口凉气）==

```
main: done with both (counter = 19221041)

```

==（部分输出）==

Not only is each run wrong, but also yields a different result!
==不仅每次运行都错了，而且还产生了不同的结果！==

A big question remains: why does this happen?
==一个大问题仍然存在：为什么会发生这种情况？==

TIP: KNOW AND USE YOUR TOOLS
==提示：了解并使用你的工具==

You should always learn new tools that help you write, debug, and understand computer systems.
==你应该始终学习新工具，以帮助你编写、调试和理解计算机系统。==

Here, we use a neat tool called a disassembler.
==在这里，我们使用一个叫做反汇编器的灵巧工具。==

When you run a disassembler on an executable, it shows you what assembly instructions make up the program.
==当你在可执行文件上运行反汇编器时，它会显示组成程序的汇编指令。==

For example, if we wish to understand the low-level code to update a counter (as in our example), we run objdump (Linux) to see the assembly code.
==例如，如果我们想了解更新计数器的底层代码（如我们的示例中所示），我们运行 objdump (Linux) 来查看汇编代码。==

Doing so produces a long listing of all the instructions in the program, neatly labeled (particularly if you compiled with the -g flag), which includes symbol information in the program.
==这样做会生成程序中所有指令的长列表，标记整齐（特别是如果你使用 -g 标志编译），其中包括程序中的符号信息。==

The objdump program is just one of many tools you should learn how to use.
==objdump 程序只是你应该学习如何使用的众多工具之一。==

A debugger like gdb, memory profilers like valgrind or purify, and of course the compiler itself are others that you should spend time to learn more about.
==像 gdb 这样的调试器，像 valgrind 或 purify 这样的内存分析器，当然还有编译器本身，都是你应该花时间了解更多的其他工具。==

The better you are at using your tools, the better systems you'll be able to build.
==你越擅长使用工具，你就能构建出越好的系统。==

26.4 The Heart Of The Problem: Uncontrolled Scheduling
==26.4 问题的核心：不受控制的调度==

To understand why this happens, we must understand the code sequence that the compiler generates for the update to counter.
==要理解为什么会发生这种情况，我们必须理解编译器为更新 counter 生成的代码序列。==

In this case, we wish to simply add a number (1) to counter.
==在这种情况下，我们希望简单地将一个数字 (1) 加到 counter 上。==

Thus, the code sequence for doing so might look something like this (in x86);
==因此，执行此操作的代码序列可能看起来像这样（在 x86 中）；==

```assembly
mov 0x8049a1c, %eax
add $0x1, %eax
mov %eax, 0x8049a1c

```

==（汇编代码）==

This example assumes that the variable counter is located at address 0x8049a1c.
==此示例假设变量 counter 位于地址 0x8049a1c。==

In this three-instruction sequence, the x86 mov instruction is used first to get the memory value at the address and put it into register eax.
==在这个三条指令的序列中，首先使用 x86 mov 指令获取该地址的内存值并将其放入寄存器 eax 中。==

Then, the add is performed, adding 1 (0x1) to the contents of the eax register, and finally, the contents of eax are stored back into memory at the same address.
==然后，执行 add 操作，将 1 (0x1) 加到 eax 寄存器的内容中，最后，将 eax 的内容存回同一地址的内存中。==

Let us imagine one of our two threads (Thread 1) enters this region of code, and is thus about to increment counter by one.
==让我们想象我们的两个线程之一（线程 1）进入该代码区域，因此即将把 counter 增加 1。==

It loads the value of counter (let's say it's 50 to begin with) into its register eax.
==它将 counter 的值（假设一开始是 50）加载到其寄存器 eax 中。==

Thus, eax = 50 for Thread 1.
==因此，对于线程 1，eax = 50。==

Then it adds one to the register; thus eax = 51.
==然后它将寄存器加 1；因此 eax = 51。==

Now, something unfortunate happens: a timer interrupt goes off; thus, the OS saves the state of the currently running thread (its PC, its registers including eax, etc.) to the thread's TCB.
==现在，不幸的事情发生了：定时器中断触发；因此，操作系统将当前运行线程的状态（其 PC、包括 eax 在内的寄存器等）保存到线程的 TCB 中。==

Now something worse happens: Thread 2 is chosen to run, and it enters this same piece of code.
==现在发生了更糟糕的事情：线程 2 被选中运行，并且它进入了同一段代码。==

It also executes the first instruction, getting the value of counter and putting it into its eax (remember: each thread when running has its own private registers; the registers are virtualized by the context-switch code that saves and restores them).
==它也执行第一条指令，获取 counter 的值并将其放入它的 eax 中（记住：每个线程运行时都有自己的私有寄存器；寄存器由保存和恢复它们的上下文切换代码进行虚拟化）。==

The value of counter is still 50 at this point, and thus Thread 2 has eax = 50.
==此时 counter 的值仍然是 50，因此线程 2 的 eax = 50。==

Let's then assume that Thread 2 executes the next two instructions, incrementing eax by 1 (thus eax = 51), and then saving the contents of eax into counter (address 0x8049a1c).
==然后假设线程 2 执行接下来的两条指令，将 eax 增加 1（因此 eax = 51），然后将 eax 的内容保存到 counter（地址 0x8049a1c）中。==

Thus, the global variable counter now has the value 51.
==因此，全局变量 counter 现在的值为 51。==

Finally, another context switch occurs, and Thread 1 resumes running.
==最后，发生另一次上下文切换，线程 1 恢复运行。==

Recall that it had just executed the mov and add, and is now about to perform the final mov instruction.
==回想一下，它刚刚执行了 mov 和 add，现在即将执行最后的 mov 指令。==

Recall also that eax = 51.
==还要回想一下，eax = 51。==

Thus, the final mov instruction executes, and saves the value to memory; the counter is set to 51 again.
==因此，最后的 mov 指令执行，并将值保存到内存；counter 再次被设置为 51。==

Put simply, what has happened is this: the code to increment counter has been run twice, but counter, which started at 50, is now only equal to 51.
==简单来说，发生的事情是这样的：增加 counter 的代码已经运行了两次，但是 counter 从 50 开始，现在只等于 51。==

A "correct" version of this program should have resulted in the variable counter equal to 52.
==该程序的“正确”版本应该导致变量 counter 等于 52。==

Let's look at a detailed execution trace to understand the problem better.
==让我们看一个详细的执行跟踪以更好地理解这个问题。==

Assume, for this example, that the above code is loaded at address 100 in memory.
==在这个例子中，假设上面的代码加载在内存地址 100 处。==

With these assumptions, what happens is shown in Figure 26.7.
==在这些假设下，发生的情况如图 26.7 所示。==

Assume the counter starts at value 50, and trace through this example to make sure you understand what is going on.
==假设 counter 从值 50 开始，并跟踪此示例以确保你了解正在发生的事情。==

What we have demonstrated here is called a race condition (or, more specifically, a data race): the results depend on the timing of the code's execution.
==我们在那里演示的称为竞态条件（或更具体地说，数据竞争）：结果取决于代码执行的时机。==

With some bad luck (i.e., context switches that occur at untimely points in the execution), we get the wrong result.
==运气不好（即，上下文切换发生在执行的不合时宜的点上），我们会得到错误的结果。==

In fact, we may get a different result each time.
==事实上，我们每次可能会得到不同的结果。==

Thus, instead of a nice deterministic computation (which we are used to from computers), we call this result indeterminate, where it is not known what the output will be and it is indeed likely to be different across runs.
==因此，我们称这个结果是不确定的，而不是（我们习惯于从计算机获得的）良好的确定性计算，其中不知道输出将是什么，并且确实可能在运行之间有所不同。==

Because multiple threads executing this code can result in a race condition, we call this code a critical section.
==因为执行此代码的多个线程可能导致竞态条件，所以我们将此代码称为临界区。==

A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread.
==临界区是一段访问共享变量（或更一般地说，共享资源）的代码，并且不得由多个线程并发执行。==

What we really want for this code is what we call mutual exclusion.
==我们真正想要的是所谓的互斥。==

This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so.
==此属性保证如果一个线程正在临界区内执行，其他线程将被阻止这样做。==

Virtually all of these terms, by the way, were coined by Edsger Dijkstra, who was a pioneer in the field and indeed won the Turing Award because of this and other work.
==顺便说一句，几乎所有这些术语都是由 Edsger Dijkstra 创造的，他是该领域的先驱，并确实因此和其他工作而获得了图灵奖。==

See his 1968 paper on "Cooperating Sequential Processes" [D68] for an amazingly clear description of the problem.
==请参阅他 1968 年关于“协作顺序进程”的论文 [D68]，了解对该问题的惊人清晰描述。==

We'll be hearing more about Dijkstra in this section of the book.
==我们将在本书的这一部分听到更多关于 Dijkstra 的内容。==

TIP: USE ATOMIC OPERATIONS
==提示：使用原子操作==

Atomic operations are one of the most powerful underlying techniques in building computer systems.
==原子操作是构建计算机系统中最强大的底层技术之一。==

From the computer architecture, to concurrent code (what we are studying here), to file systems (which we'll study soon enough), database management systems, and even distributed systems [L+93].
==从计算机体系结构，到并发代码（我们正在研究的内容），到文件系统（我们很快就会研究），数据库管理系统，甚至分布式系统 [L+93]。==

The idea behind making a series of actions atomic is simply expressed with the phrase "all or nothing".
==使一系列动作原子化的想法可以用“全有或全无”这个短语简单地表达。==

It should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible.
==它应该看起来要么是你希望组合在一起的所有动作都发生了，要么是它们都没有发生，没有可见的中间状态。==

Sometimes, the grouping of many actions into a single atomic action is called a transaction, an idea developed in great detail in the world of databases and transaction processing [GR92].
==有时，将许多动作组合成单个原子动作称为事务，这是在数据库和事务处理领域中非常详细地发展的概念 [GR92]。==

In our theme of exploring concurrency, we'll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution, but the idea of atomicity is much bigger than that, as we will see.
==在我们探索并发的主题中，我们将使用同步原语将短指令序列转换为原子执行块，但正如我们将看到的，原子性的概念远不止于此。==

For example, file systems use techniques such as journaling or copy-on-write in order to atomically transition their on-disk state, critical for operating correctly in the face of system failures.
==例如，文件系统使用诸如日志记录或写时复制之类的技术来原子地转换其磁盘状态，这对于在系统故障面前正确运行至关重要。==

If that doesn't make sense, don't worry - it will, in some future chapter.
==如果这没有意义，别担心——在未来的某一章中会有的。==

26.5 The Wish For Atomicity
==26.5 对原子性的渴望==

One way to solve this problem would be to have more powerful instructions that, in a single step, did exactly whatever we needed done and thus removed the possibility of an untimely interrupt.
==解决此问题的一种方法是拥有更强大的指令，这些指令在一个步骤中精确地完成我们需要完成的任何事情，从而消除不合时宜的中断的可能性。==

For example, what if we had a super instruction that looked like this:
==例如，如果我们有一个看起来像这样的超级指令会怎样：==

```assembly
memory-add 0x8049a1c, $0x1

```

==（伪代码）==

Assume this instruction adds a value to a memory location, and the hardware guarantees that it executes atomically.
==假设此指令将一个值添加到内存位置，并且硬件保证它原子地执行。==

When the instruction executed, it would perform the update as desired.
==当指令执行时，它将按预期执行更新。==

It could not be interrupted mid-instruction, because that is precisely the guarantee we receive from the hardware.
==它不能在指令中途被中断，因为这正是我们从硬件获得的保证。==

When an interrupt occurs, either the instruction has not run at all, or it has run to completion.
==当中断发生时，要么指令根本没有运行，要么它已经运行完成。==

There is no in-between state. Hardware can be a beautiful thing, no?
==没有中间状态。硬件可以是一件美好的事情，不是吗？==

Atomically, in this context, means "as a unit", which sometimes we take as "all or none."
==在这种情况下，原子地意味着“作为一个单元”，有时我们将其理解为“全有或全无”。==

What we'd like is to execute the three instruction sequence atomically.
==我们想要的是原子地执行这三条指令序列。==

As we said, if we had a single instruction to do this, we could just issue that instruction and be done.
==正如我们所说，如果我们有一条指令来执行此操作，我们只需发出该指令即可完成。==

But in the general case, we won't have such an instruction.
==但在一般情况下，我们不会有这样的指令。==

Imagine we were building a concurrent B-tree, and wished to update it.
==想象一下，我们要构建一个并发 B 树，并希望更新它。==

Would we really want the hardware to support an "atomic update of B-tree" instruction?
==我们真的希望硬件支持“B 树的原子更新”指令吗？==

Probably not, at least in a sane instruction set.
==可能不会，至少在理智的指令集中不会。==

Thus, what we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call synchronization primitives.
==因此，我们要做的就是向硬件请求一些有用的指令，在此基础上我们可以构建一组通用的所谓的同步原语。==

By using this hardware support, in combination with some help from the operating system, we will be able to build multi-threaded code that accesses critical sections in a synchronized and controlled manner.
==通过使用此硬件支持，再加上操作系统的帮助，我们将能够构建以同步和受控方式访问临界区的多线程代码。==

And thus reliably produces the correct result despite the challenging nature of concurrent execution.
==因此，尽管并发执行具有挑战性，但仍能可靠地产生正确的结果。==

Pretty awesome, right?
==太棒了，对吧？==

This is the problem we will study in this section of the book.
==这就是我们将在本书这一部分研究的问题。==

It is a wonderful and hard problem, and should make your mind hurt (a bit).
==这是一个美妙而艰难的问题，应该会让你的头脑（有点）痛。==

If it doesn't, then you don't understand! Keep working until your head hurts.
==如果不痛，那你就没懂！继续努力直到你的头痛。==

You then know you're headed in the right direction.
==那时你就知道你正朝着正确的方向前进。==

At that point, take a break; we don't want your head hurting too much.
==在那时，休息一下；我们不希望你的头太痛。==

THE CRUX: HOW TO SUPPORT SYNCHRONIZATION
==关键点：如何支持同步==

What support do we need from the hardware in order to build useful synchronization primitives?
==我们需要硬件什么样的支持才能构建有用的同步原语？==

What support do we need from the OS?
==我们需要操作系统什么样的支持？==

How can we build these primitives correctly and efficiently?
==我们如何正确高效地构建这些原语？==

How can programs use them to get the desired results?
==程序如何使用它们来获得预期的结果？==

26.6 One More Problem: Waiting For Another
==26.6 还有一个问题：等待另一个==

This chapter has set up the problem of concurrency as if only one type of interaction occurs between threads, that of accessing shared variables and the need to support atomicity for critical sections.
==本章将并发问题设定为好像线程之间只发生一种类型的交互，即访问共享变量以及支持临界区原子性的需要。==

As it turns out, there is another common interaction that arises, where one thread must wait for another to complete some action before it continues.
==事实证明，还有另一种常见的交互出现，即一个线程必须等待另一个线程完成某些操作才能继续。==

This interaction arises, for example, when a process performs a disk I/O and is put to sleep.
==这种交互例如在一个进程执行磁盘 I/O 并被置于睡眠状态时出现。==

When the I/O completes, the process needs to be roused from its slumber so it can continue.
==当 I/O 完成时，需要将进程从睡眠中唤醒以便它可以继续。==

Thus, in the coming chapters, we'll be not only studying how to build support for synchronization primitives to support atomicity but also for mechanisms to support this type of sleeping/waking interaction that is common in multi-threaded programs.
==因此，在接下来的章节中，我们将不仅研究如何构建对同步原语的支持以支持原子性，还将研究支持多线程程序中常见的这种睡眠/唤醒交互类型的机制。==

If this doesn't make sense right now, that is OK!
==如果现在这没有意义，没关系！==

It will soon enough, when you read the chapter on condition variables.
==当你阅读关于条件变量的章节时，很快就会明白。==

If it doesn't by then, well, then it is less OK, and you should read that chapter again (and again) until it does make sense.
==如果那时还不明白，好吧，那就有点不太好了，你应该一遍又一遍地阅读那一章，直到明白为止。==

ASIDE: KEY CONCURRENCY TERMS
==旁白：关键并发术语==

CRITICAL SECTION, RACE CONDITION, INDETERMINATE, MUTUAL EXCLUSION
==临界区，竞态条件，不确定性，互斥==

These four terms are so central to concurrent code that we thought it worth while to call them out explicitly.
==这四个术语对并发代码如此核心，以至于我们认为值得明确地指出它们。==

See some of Dijkstra's early work [D65,D68] for more details.
==有关更多详细信息，请参阅 Dijkstra 的一些早期作品 [D65,D68]。==

Critical section is a piece of code that accesses a shared resource, usually a variable or data structure.
==临界区是一段访问共享资源（通常是变量或数据结构）的代码。==

Race condition (or data race [NM92]) arises if multiple threads of execution enter the critical section at roughly the same time.
==如果多个执行线程大约在同一时间进入临界区，就会出现竞态条件（或数据竞争 [NM92]）。==

Both attempt to update the shared data structure, leading to a surprising (and perhaps undesirable) outcome.
==两者都试图更新共享数据结构，导致令人惊讶的（也许是不希望的）结果。==

An indeterminate program consists of one or more race conditions.
==不确定程序包含一个或多个竞态条件。==

The output of the program varies from run to run, depending on which threads ran when.
==程序的输出因运行而异，具体取决于哪些线程在何时运行。==

The outcome is thus not deterministic, something we usually expect from computer systems.
==因此结果不是确定性的，而这通常是我们对计算机系统的期望。==

To avoid these problems, threads should use some kind of mutual exclusion primitives.
==为了避免这些问题，线程应该使用某种互斥原语。==

Doing so guarantees that only a single thread ever enters a critical section, thus avoiding races, and resulting in deterministic program outputs.
==这样做可以保证只有一个线程进入临界区，从而避免竞争，并产生确定性的程序输出。==

26.7 Summary: Why in OS Class?
==26.7 总结：为什么要上操作系统课？==

Before wrapping up, one question that you might have is: why are we studying this in OS class?
==在结束之前，你可能有一个问题：为什么我们在操作系统课上学习这个？==

"History" is the one-word answer; the OS was the first concurrent program, and many techniques were created for use within the OS.
==“历史”是一个词的答案；操作系统是第一个并发程序，许多技术都是为了在操作系统内使用而创建的。==

Later, with multi-threaded processes, application programmers also had to consider such things.
==后来，随着多线程进程的出现，应用程序程序员也必须考虑这些事情。==

For example, imagine the case where there are two processes running.
==例如，想象一下有两个进程正在运行的情况。==

Assume they both call write() to write to the file, and both wish to append the data to the file (i.e., add the data to the end of the file, thus increasing its length).
==假设它们都调用 write() 写入文件，并且都希望将数据追加到文件中（即，将数据添加到文件末尾，从而增加其长度）。==

To do so, both must allocate a new block, record in the inode of the file where this block lives, and change the size of the file to reflect the new larger size (among other things; we'll learn more about files in the third part of the book).
==为此，两者都必须分配一个新块，在文件的 inode 中记录该块的位置，并更改文件的大小以反映新的较大尺寸（除其他外；我们将在本书的第三部分了解更多关于文件的信息）。==

Because an interrupt may occur at any time, the code that updates these shared structures (e.g., a bitmap for allocation, or the file's inode) are critical sections.
==因为中断可能随时发生，所以更新这些共享结构（例如，用于分配的位图或文件的 inode）的代码是临界区。==

Thus, OS designers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures.
==因此，操作系统设计者从引入中断的一开始，就不得不担心操作系统如何更新内部结构。==

An untimely interrupt causes all of the problems described above.
==不合时宜的中断会导致上述所有问题。==

Not surprisingly, page tables, process lists, file system structures, and virtually every kernel data structure has to be carefully accessed, with the proper synchronization primitives, to work correctly.
==毫不奇怪，页表、进程列表、文件系统结构以及几乎所有内核数据结构都必须使用适当的同步原语仔细访问，才能正常工作。==

CONCURRENCY: AN INTRODUCTION
==并发：简介==

Homework (Simulation)
==作业（模拟）==

This program, x86.py, allows you to see how different thread interleavings either cause or avoid race conditions.
==这个程序 x86.py 允许你查看不同的线程交错执行是如何导致或避免竞争条件的。==

See the README for details on how the program works, then answer the questions below.
==请参阅 README 了解程序如何工作的详细信息，然后回答下面的问题。==

Questions
==问题==

1. Let's examine a simple program, "loop.s".
==2. 让我们检查一个简单的程序 "loop.s"。==

First, just read and understand it.
==首先，阅读并理解它。==

Then, run it with these arguments: ./x86.py -t 1 -p loop.s -i 100 -R dx
==然后，使用以下参数运行它：./x86.py -t 1 -p loop.s -i 100 -R dx==

This specifies a single thread, an interrupt every 100 instructions, and tracing of register dx.
==这指定了单个线程，每 100 条指令发生一次中断，并追踪寄存器 dx。==

What will dx be during the run?
==运行期间 dx 的值会是多少？==

Use the -c flag to check your answers; the answers, on the left, show the value of the register (or memory value) after the instruction on the right has run.
==使用 -c 标志来检查你的答案；左侧的答案显示了右侧指令运行后的寄存器值（或内存值）。==

2. Same code, different flags: ./x86.py -p loop.s -t 2 -i 10 -a dx=3,dx=3 -R dx
==3. 相同的代码，不同的标志：./x86.py -p loop.s -t 2 -i 10 -a dx=3,dx=3 -R dx==

This specifies two threads, and initializes each %dx to 3.
==这指定了两个线程，并将每个 %dx 初始化为 3。==

What values will %dx see?
==%dx 会看到什么值？==

Run with -c to check.
==运行 -c 来检查。==

Does the presence of multiple threads affect your calculations?
==多个线程的存在会影响你的计算吗？==

Is there a race in this code?
==这段代码中存在竞争吗？==

3. Run this: ./x86.py -p loop.s -t 2 -i 3 -r -R dx -a dx=3,dx=3
==4. 运行这个：./x86.py -p loop.s -t 2 -i 3 -r -R dx -a dx=3,dx=3==

This makes the interrupt interval small/random; use different seeds (-s) to see different interleavings.
==这使得中断间隔变得很小/随机；使用不同的种子 (-s) 来查看不同的交错执行。==

Does the interrupt frequency change anything?
==中断频率会改变什么吗？==

4. Now, a different program, looping-race-nolock.s, which accesses a shared variable located at address 2000; we'll call this variable value.
==5. 现在，看一个不同的程序 looping-race-nolock.s，它访问位于地址 2000 的共享变量；我们将这个变量称为 value。==

Run it with a single thread to confirm your understanding: ./x86.py -p looping-race-nolock.s -t 1 -M 2000
==使用单个线程运行它以确认你的理解：./x86.py -p looping-race-nolock.s -t 1 -M 2000==

What is value (i.e., at memory address 2000) throughout the run?
==在整个运行过程中，value（即内存地址 2000 处）是多少？==

Use -c to check.
==使用 -c 来检查。==

5. Run with multiple iterations/threads: ./x86.py -p looping-race-nolock.s -t 2 -a bx=3 -M 2000
==6. 使用多次迭代/线程运行：./x86.py -p looping-race-nolock.s -t 2 -a bx=3 -M 2000==

Why does each thread loop three times?
==为什么每个线程循环三次？==

What is final value of value?
==value 的最终值是多少？==

6. Run with random interrupt intervals: ./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0 with different seeds (-s 1, -s 2, etc.)
==7. 使用随机中断间隔运行：./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0 以及不同的种子（-s 1, -s 2 等）。==

Can you tell by looking at the thread interleaving what the final value of value will be?
==你能通过观察线程交错来判断 value 的最终值吗？==

Does the timing of the interrupt matter?
==中断的时机重要吗？==

Where can it safely occur?
==它可以在哪里安全地发生？==

Where not?
==哪里不安全？==

In other words, where is the critical section exactly?
==换句话说，临界区究竟在哪里？==

7. Now examine fixed interrupt intervals: ./x86.py -p looping-race-nolock.s -a bx=1 -t 2 -M 2000 -i 1
==8. 现在检查固定的中断间隔：./x86.py -p looping-race-nolock.s -a bx=1 -t 2 -M 2000 -i 1==

What will the final value of the shared variable value be?
==共享变量 value 的最终值将是多少？==

What about when you change -i 2, -i 3, etc.?
==当你将其改为 -i 2, -i 3 等时会怎样？==

For which interrupt intervals does the program give the "correct" answer?
==对于哪些中断间隔，程序会给出“正确”的答案？==

8. Run the same for more loops (e.g., set -a bx=100).
==9. 针对更多循环运行相同的操作（例如，设置 -a bx=100）。==

What interrupt intervals (-i) lead to a correct outcome?
==哪些中断间隔 (-i) 会导致正确的结果？==

Which intervals are surprising?
==哪些间隔令人惊讶？==

9. One last program: wait-for-me.s.
==10. 最后一个程序：wait-for-me.s。==

Run: ./x86.py -p wait-for-me.s -a ax=1,ax=0 -R ax -M 2000
==运行：./x86.py -p wait-for-me.s -a ax=1,ax=0 -R ax -M 2000==

This sets the %ax register to 1 for thread 0, and 0 for thread 1, and watches %ax and memory location 2000.
==这将线程 0 的 %ax 寄存器设置为 1，线程 1 的设置为 0，并监视 %ax 和内存位置 2000。==

How should the code behave?
==代码应该如何表现？==

How is the value at location 2000 being used by the threads?
==线程是如何使用位置 2000 处的值的？==

What will its final value be?
==它的最终值会是多少？==

10. Now switch the inputs: ./x86.py -p wait-for-me.s -a ax=0,ax=1 -R ax -M 2000
==11. 现在切换输入：./x86.py -p wait-for-me.s -a ax=0,ax=1 -R ax -M 2000==

How do the threads behave?
==线程如何表现？==

What is thread 0 doing?
==线程 0 在做什么？==

How would changing the interrupt interval (e.g., -i 1000, or perhaps to use random intervals) change the trace outcome?
==改变中断间隔（例如 -i 1000，或者使用随机间隔）会如何改变跟踪结果？==

Is the program efficiently using the CPU?
==程序是否有效地利用了 CPU？==

Interlude: Thread API
==插曲：线程 API==

This chapter briefly covers the main portions of the thread API.
==本章简要介绍线程 API 的主要部分。==

Each part will be explained further in the subsequent chapters, as we show how to use the API.
==随着我们展示如何使用这些 API，每个部分将在后续章节中得到进一步解释。==

More details can be found in various books and online sources.
==更多详细信息可以在各种书籍和在线资源中找到。==

We should note that the subsequent chapters introduce the concepts of locks and condition variables more slowly, with many examples; this chapter is thus better used as a reference.
==我们需要指出，后续章节会通过许多示例更缓慢地介绍锁和条件变量的概念；因此，本章最好作为参考使用。==

CRUX: HOW TO CREATE AND CONTROL THREADS
==关键问题：如何创建和控制线程==

What interfaces should the OS present for thread creation and control?
==操作系统应该为线程创建和控制提供什么接口？==

How should these interfaces be designed to enable ease of use as well as utility?
==这些接口应该如何设计，以兼顾易用性和实用性？==

27.1 Thread Creation
==27.1 线程创建==

The first thing you have to be able to do to write a multi-threaded program is to create new threads, and thus some kind of thread creation interface must exist.
==编写多线程程序首先要做的是创建新线程，因此必须存在某种线程创建接口。==

In POSIX, it is easy:
==在 POSIX 中，这很容易：==

This declaration might look a little complex (particularly if you haven't used function pointers in C), but actually it's not too bad.
==这个声明可能看起来有点复杂（特别是如果你没有在 C 中使用过函数指针），但实际上并不太糟糕。==

There are four arguments: thread, attr, start_routine, and arg.
==有四个参数：thread、attr、start_routine 和 arg。==

The first, thread, is a pointer to a structure of type pthread_t; we'll use this structure to interact with this thread, and thus we need to pass it to pthread_create() in order to initialize it.
==第一个参数 thread 是一个指向 pthread_t 类型结构的指针；我们将使用这个结构与该线程交互，因此我们需要将其传递给 pthread_create() 以对其进行初始化。==

The second argument, attr, is used to specify any attributes this thread might have.
==第二个参数 attr 用于指定该线程可能具有的任何属性。==

Some examples include setting the stack size or perhaps information about the scheduling priority of the thread.
==一些例子包括设置栈大小或关于线程调度优先级的信息。==

An attribute is initialized with a separate call to pthread_attr_init(); see the manual page for details.
==属性通过单独调用 pthread_attr_init() 进行初始化；详情请参阅手册页。==

However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in.
==然而，在大多数情况下，默认值就可以了；在这种情况下，我们只需传入 NULL 值。==

The third argument is the most complex, but is really just asking: which function should this thread start running in?
==第三个参数是最复杂的，但实际上只是在问：这个线程应该从哪个函数开始运行？==

In C, we call this a function pointer, and this one tells us the following is expected: a function name (start_routine), which is passed a single argument of type void* (as indicated in the parentheses after start_routine), and which returns a value of type void* (i.e., a void pointer).
==在 C 语言中，我们称之为函数指针，它告诉我们需要以下内容：一个函数名 (start_routine)，它接收一个 void* 类型的参数（如 start_routine 后的括号所示），并返回一个 void* 类型的值（即 void 指针）。==

If this routine instead required an integer argument, instead of a void pointer, the declaration would look like this:
==如果这个例程需要一个整数参数而不是 void 指针，声明将如下所示：==

If instead the routine took a void pointer as an argument, but returned an integer, it would look like this:
==如果该例程接受一个 void 指针作为参数，但返回一个整数，它将如下所示：==

Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution.
==最后，第四个参数 arg 正是要传递给线程开始执行的函数的参数。==

You might ask: why do we need these void pointers?
==你可能会问：为什么我们需要这些 void 指针？==

Well, the answer is quite simple: having a void pointer as an argument to the function start_routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result.
==嗯，答案很简单：将 void 指针作为函数 start_routine 的参数允许我们传入任何类型的参数；将其作为返回值允许线程返回任何类型的结果。==

Let's look at an example in Figure 27.1.
==让我们看一个图 27.1 中的例子。==

Here we just create a thread that is passed two arguments, packaged into a single type we define ourselves (myarg_t).
==在这里，我们只是创建了一个线程，传入两个参数，这些参数被打包到我们自己定义的单一类型 (myarg_t) 中。==

The thread, once created, can simply cast its argument to the type it expects and thus unpack the arguments as desired.
==线程一旦创建，就可以简单地将其参数转换为它期望的类型，从而按需解包参数。==

And there it is!
==就是这样！==

Once you create a thread, you really have another live executing entity, complete with its own call stack, running within the same address space as all the currently existing threads in the program.
==一旦你创建了一个线程，你就真正拥有了另一个活动的执行实体，它拥有自己的调用栈，并且与程序中所有当前存在的线程在同一个地址空间内运行。==

The fun thus begins!
==乐趣由此开始！==

27.2 Thread Completion
==27.2 线程完成==

The example above shows how to create a thread.
==上面的例子展示了如何创建一个线程。==

However, what happens if you want to wait for a thread to complete?
==但是，如果你想等待一个线程完成会发生什么？==

You need to do something special in order to wait for completion; in particular, you must call the routine pthread_join().
==你需要做一些特别的事情来等待完成；具体来说，你必须调用 pthread_join() 例程。==

Figure 27.1: Creating a Thread
==图 27.1：创建一个线程==

This routine takes two arguments.
==这个例程接受两个参数。==

The first is of type pthread_t, and is used to specify which thread to wait for.
==第一个参数是 pthread_t 类型，用于指定要等待哪个线程。==

This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread_create()); if you keep it around, you can use it to wait for that thread to terminate.
==这个变量由线程创建例程初始化（当你将指向它的指针作为参数传递给 pthread_create() 时）；如果你保留它，你可以用它来等待该线程终止。==

The second argument is a pointer to the return value you expect to get back.
==第二个参数是一个指向你期望取回的返回值的指针。==

Because the routine can return anything, it is defined to return a pointer to void; because the pthread_join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself.
==因为该例程可以返回任何东西，它被定义为返回一个 void 指针；因为 pthread_join() 例程会改变传入参数的值，你需要传入指向该值的指针，而不仅仅是值本身。==

Let's look at another example (Figure 27.2).
==让我们看另一个例子（图 27.2）。==

In the code, a single thread is again created, and passed a couple of arguments via the myarg_t structure.
==在代码中，再次创建了一个单独的线程，并通过 myarg_t 结构传递了几个参数。==

To return values, the myret_t type is used.
==为了返回值，使用了 myret_t 类型。==

Once the thread is finished running, the main thread, which has been waiting inside of the pthread_join() routine, then returns, and we can access the values returned from the thread, namely whatever is in myret_t.
==一旦线程运行结束，一直在 pthread_join() 例程中等待的主线程随后返回，我们就可以访问从该线程返回的值，即 myret_t 中的任何内容。==

A few things to note about this example.
==关于这个例子有几点需要注意。==

First, often times we don't have to do all of this painful packing and unpacking of arguments.
==首先，很多时候我们不需要做所有这些痛苦的参数打包和解包工作。==

For example, if we just create a thread with no arguments, we can pass NULL in as an argument when the thread is created.
==例如，如果我们只是创建一个没有参数的线程，我们可以在创建线程时传入 NULL 作为参数。==

Similarly, we can pass NULL into pthread_join() if we don't care about the return value.
==同样，如果我们不关心返回值，我们可以将 NULL 传入 pthread_join()。==

Note we use wrapper functions here; specifically, we call Malloc(), Pthread_join(), and Pthread_create(), which just call their similarly-named lower-case versions and make sure the routines did not return anything unexpected.
==注意我们在这里使用了包装函数；具体来说，我们调用 Malloc()、Pthread_join() 和 Pthread_create()，它们只是调用同名的小写版本，并确保例程没有返回任何意外的内容。==

Figure 27.2: Waiting for Thread Completion
==图 27.2：等待线程完成==

Second, if we are just passing in a single value (e.g., a long long int), we don't have to package it up as an argument.
==其次，如果我们只是传入一个单独的值（例如，一个 long long int），我们不必将其打包为参数。==

Figure 27.3 shows an example.
==图 27.3 展示了一个例子。==

In this case, life is a bit simpler, as we don't have to package arguments and return values inside of structures.
==在这种情况下，生活变得简单了一些，因为我们不必将参数和返回值打包在结构体中。==

Third, we should note that one has to be extremely careful with how values are returned from a thread.
==第三，我们应该注意，必须非常小心地处理从线程返回值的方式。==

Specifically, never return a pointer which refers to something allocated on the thread's call stack.
==具体来说，永远不要返回一个指向分配在线程调用栈上的内容的指针。==

If you do, what do you think will happen? (think about it!)
==如果你这样做了，你认为会发生什么？（想一想！）==

Here is an example of a dangerous piece of code, modified from the example in Figure 27.2.
==这里有一段危险代码的示例，修改自图 27.2 中的例子。==

In this case, the variable oops is allocated on the stack of mythread.
==在这种情况下，变量 oops 分配在 mythread 的栈上。==

However, when it returns, the value is automatically deallocated (that's why the stack is so easy to use, after all!), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results.
==然而，当它返回时，该值会自动被释放（毕竟这就是栈如此易用的原因！），因此，传回一个指向现已释放的变量的指针将导致各种糟糕的结果。==

Certainly, when you print out the values you think you returned, you'll probably (but not necessarily!) be surprised.
==当然，当你打印出你认为已经返回的值时，你可能会（但不一定！）感到惊讶。==

Try it and find out for yourself!
==试一试，自己去发现！==

Figure 27.3: Simpler Argument Passing to a Thread
==图 27.3：向线程传递更简单的参数==

Finally, you might notice that the use of pthread_create() to create a thread, followed by an immediate call to pthread_join(), is a pretty strange way to create a thread.
==最后，你可能会注意到，使用 pthread_create() 创建一个线程，紧接着调用 pthread_join()，是一种相当奇怪的创建线程的方式。==

In fact, there is an easier way to accomplish this exact task; it's called a procedure call.
==事实上，有一种更简单的方法来完成这个确切的任务；它被称为过程调用。==

Clearly, we'll usually be creating more than just one thread and waiting for it to complete, otherwise there is not much purpose to using threads at all.
==显然，我们通常会创建不止一个线程并等待其完成，否则使用线程就没什么意义了。==

We should note that not all code that is multi-threaded uses the join routine.
==我们应该注意，并非所有多线程代码都使用 join 例程。==

For example, a multi-threaded web server might create a number of worker threads, and then use the main thread to accept requests and pass them to the workers, indefinitely.
==例如，多线程 Web 服务器可能会创建许多工作线程，然后使用主线程接受请求并将它们传递给工作线程，无限期地运行。==

Such long-lived programs thus may not need to join.
==因此，这种长寿命的程序可能不需要 join。==

However, a parallel program that creates threads to execute a particular task (in parallel) will likely use join to make sure all such work completes before exiting or moving onto the next stage of computation.
==然而，创建一个并行程序来执行特定任务（并行执行）很可能会使用 join，以确保所有此类工作在退出或进入下一阶段计算之前完成。==

27.3 Locks
==27.3 锁==

Beyond thread creation and join, probably the next most useful set of functions provided by the POSIX threads library are those for providing mutual exclusion to a critical section via locks.
==除了线程创建和 join 之外，POSIX 线程库提供的下一组最有用的函数可能是那些通过锁为临界区提供互斥的函数。==

The most basic pair of routines to use for this purpose is provided by the following:
==为此目的使用的最基本的一对例程如下：==

The routines should be easy to understand and use.
==这些例程应该很容易理解和使用。==

When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful.
==当你有一段代码是临界区，因此需要保护以确保正确操作时，锁非常有用。==

You can probably imagine what the code looks like:
==你大概可以想象代码是什么样子的：==

The intent of the code is as follows: if no other thread holds the lock when pthread_mutex_lock() is called, the thread will acquire the lock and enter the critical section.
==代码的意图如下：如果在调用 pthread_mutex_lock() 时没有其他线程持有锁，该线程将获取锁并进入临界区。==

If another thread does indeed hold the lock, the thread trying to grab the lock will not return from the call until it has acquired the lock (implying that the thread holding the lock has released it via the unlock call).
==如果另一个线程确实持有锁，试图获取锁的线程将不会从调用中返回，直到它获取了锁（这意味着持有锁的线程已经通过 unlock 调用释放了它）。==

Of course, many threads may be stuck waiting inside the lock acquisition function at a given time; only the thread with the lock acquired, however, should call unlock.
==当然，许多线程可能在给定时间卡在锁获取函数中等待；然而，只有获取了锁的线程才应该调用 unlock。==

Unfortunately, this code is broken, in two important ways.
==不幸的是，这段代码在两个重要方面是破损的。==

The first problem is a lack of proper initialization.
==第一个问题是缺乏正确的初始化。==

All locks must be properly initialized in order to guarantee that they have the correct values to begin with and thus work as desired when lock and unlock are called.
==所有锁都必须正确初始化，以保证它们一开始就具有正确的值，从而在调用 lock 和 unlock 时按预期工作。==

With POSIX threads, there are two ways to initialize locks.
==在 POSIX 线程中，有两种初始化锁的方法。==

One way to do this is to use PTHREAD_MUTEX_INITIALIZER, as follows:
==一种方法是使用 PTHREAD_MUTEX_INITIALIZER，如下所示：==

Doing so sets the lock to the default values and thus makes the lock usable.
==这样做将锁设置为默认值，从而使锁可用。==

The dynamic way to do it (i.e., at run time) is to make a call to pthread_mutex_init(), as follows:
==动态的方法（即在运行时）是调用 pthread_mutex_init()，如下所示：==

The first argument to this routine is the address of the lock itself, whereas the second is an optional set of attributes.
==该例程的第一个参数是锁本身的地址，而第二个参数是一组可选的属性。==

Read more about the attributes yourself; passing NULL in simply uses the defaults.
==你自己去阅读更多关于属性的内容；传入 NULL 只是使用默认值。==

Either way works, but we usually use the dynamic (latter) method.
==两种方法都可以，但我们通常使用动态（后一种）方法。==

Note that a corresponding call to pthread_mutex_destroy() should also be made, when you are done with the lock; see the manual page for all of the details.
==请注意，当你使用完锁时，也应该相应地调用 pthread_mutex_destroy()；详情请参阅手册页。==

The second problem with the code above is that it fails to check error codes when calling lock and unlock.
==上述代码的第二个问题是，在调用 lock 和 unlock 时未能检查错误代码。==

Just like virtually any library routine you call in a UNIX system, these routines can also fail!
==就像你在 UNIX 系统中调用的几乎任何库例程一样，这些例程也可能失败！==

If your code doesn't properly check error codes, the failure will happen silently, which in this case could allow multiple threads into a critical section.
==如果你的代码没有正确检查错误代码，失败将悄无声息地发生，在这种情况下，这可能允许连个线程进入临界区。==

Minimally, use wrappers, which assert that the routine succeeded, as shown in Figure 27.4; more sophisticated (non-toy) programs, which can't simply exit when something goes wrong, should check for failure and do something appropriate when a call does not succeed.
==至少要使用包装器，断言例程已成功，如图 27.4 所示；更复杂的（非玩具）程序不能在出错时简单地退出，应该检查失败并在调用不成功时采取适当的措施。==

Figure 27.4: An Example Wrapper
==图 27.4：一个包装器示例==

The lock and unlock routines are not the only routines within the pthreads library to interact with locks.
==锁和解锁例程并不是 pthreads 库中唯一与锁交互的例程。==

Two other routines of interest:
==另外两个感兴趣的例程：==

These two calls are used in lock acquisition.
==这两个调用用于获取锁。==

The trylock version returns failure if the lock is already held; the timedlock version of acquiring a lock returns after a timeout or after acquiring the lock, whichever happens first.
==trylock 版本如果锁已被持有则返回失败；timedlock 版本的锁获取会在超时后或获取锁后返回，取决于哪个先发生。==

Thus, the timedlock with a timeout of zero degenerates to the trylock case.
==因此，超时时间为零的 timedlock 退化为 trylock 情况。==

Both of these versions should generally be avoided; however, there are a few cases where avoiding getting stuck (perhaps indefinitely) in a lock acquisition routine can be useful, as we'll see in future chapters (e.g., when we study deadlock).
==这两个版本通常应该避免使用；然而，在少数情况下，避免在锁获取例程中卡住（可能是无限期地）是有用的，我们将在后续章节中看到（例如，当我们研究死锁时）。==

27.4 Condition Variables
==27.4 条件变量==

The other major component of any threads library, and certainly the case with POSIX threads, is the presence of a condition variable.
==任何线程库的另一个主要组成部分，当然 POSIX 线程也是如此，就是条件变量的存在。==

Condition variables are useful when some kind of signaling must take place between threads, if one thread is waiting for another to do something before it can continue.
==当线程之间必须进行某种信号传递，即一个线程在继续之前正在等待另一个线程做某事时，条件变量非常有用。==

Two primary routines are used by programs wishing to interact in this way:
==希望以这种方式交互的程序主要使用两个例程：==

To use a condition variable, one has to in addition have a lock that is associated with this condition.
==要使用条件变量，还必须有一个与该条件关联的锁。==

When calling either of the above routines, this lock should be held.
==当调用上述任何一个例程时，应该持有这个锁。==

The first routine, pthread_cond_wait(), puts the calling thread to sleep, and thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thread might care about.
==第一个例程 pthread_cond_wait() 使调用线程进入睡眠状态，从而等待其他线程向其发出信号，通常是在程序中发生了当前睡眠线程可能关心的某些变化时。==

A typical usage looks like this:
==典型的用法如下所示：==

In this code, after initialization of the relevant lock and condition, a thread checks to see if the variable ready has yet been set to something other than zero.
==在这段代码中，在初始化相关的锁和条件后，线程检查变量 ready 是否已被设置为非零值。==

If not, the thread simply calls the wait routine in order to sleep until some other thread wakes it.
==如果没有，线程只需调用等待例程即可睡眠，直到其他线程将其唤醒。==

The code to wake a thread, which would run in some other thread, looks like this:
==唤醒线程的代码（将在其他线程中运行）如下所示：==

A few things to note about this code sequence.
==关于这段代码序列有几点需要注意。==

First, when signaling (as well as when modifying the global variable ready), we always make sure to have the lock held.
==首先，在发出信号时（以及修改全局变量 ready 时），我们要始终确保持有锁。==

This ensures that we don't accidentally introduce a race condition into our code.
==这确保了我们不会意外地将竞争条件引入代码中。==

Second, you might notice that the wait call takes a lock as its second parameter, whereas the signal call only takes a condition.
==其次，你可能会注意到 wait 调用将锁作为其第二个参数，而 signal 调用只接受条件。==

The reason for this difference is that the wait call, in addition to putting the calling thread to sleep, releases the lock when putting said caller to sleep.
==这种差异的原因在于，wait 调用除了使调用线程进入睡眠状态外，还在使该调用者睡眠时释放锁。==

Imagine if it did not: how could the other thread acquire the lock and signal it to wake up?
==试想如果它不这样做：其他线程如何获取锁并向其发出唤醒信号？==

However, before returning after being woken, the pthread_cond_wait() re-acquires the lock, thus ensuring that any time the waiting thread is running between the lock acquire at the beginning of the wait sequence, and the lock release at the end, it holds the lock.
==然而，在被唤醒后返回之前，pthread_cond_wait() 会重新获取锁，从而确保等待线程在等待序列开始时的锁获取和结束时的锁释放之间运行的任何时候，都持有该锁。==

One last oddity: the waiting thread re-checks the condition in a while loop, instead of a simple if statement.
==最后一个奇怪之处：等待线程在一个 while 循环中重新检查条件，而不是一个简单的 if 语句。==

We'll discuss this issue in detail when we study condition variables in a future chapter, but in general, using a while loop is the simple and safe thing to do.
==当我们未来的章节中研究条件变量时，我们将详细讨论这个问题，但总的来说，使用 while 循环是简单且安全的做法。==

Although it rechecks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread; in such a case, without rechecking, the waiting thread will continue thinking that the condition has changed even though it has not.
==虽然它重新检查了条件（可能增加了一点开销），但有些 pthread 实现可能会虚假地唤醒等待线程；在这种情况下，如果不重新检查，等待线程将继续认为条件已更改，即使实际上并未更改。==

It is safer thus to view waking up as a hint that something might have changed, rather than an absolute fact.
==因此，将唤醒视为某种事情可能已经改变的提示，而不是绝对的事实，会更安全。==

Note that sometimes it is tempting to use a simple flag to signal between two threads, instead of a condition variable and associated lock.
==请注意，有时人们会忍不住使用一个简单的标志在两个线程之间传递信号，而不是使用条件变量和关联的锁。==

For example, we could rewrite the waiting code above to look more like this in the waiting code:
==例如，我们可以重写上面的等待代码，使其在等待代码中看起来更像这样：==

The associated signaling code would look like this:
==关联的信号代码将如下所示：==

Don't ever do this, for the following reasons.
==永远不要这样做，原因如下。==

First, it performs poorly in many cases (spinning for a long time just wastes CPU cycles).
==首先，它在许多情况下性能很差（长时间自旋只会浪费 CPU 周期）。==

Second, it is error prone.
==其次，它容易出错。==

As recent research shows, it is surprisingly easy to make mistakes when using flags (as above) to synchronize between threads; in that study, roughly half the uses of these ad hoc synchronizations were buggy!
==正如最近的研究表明的那样，在使用标志（如上所述）在线程之间进行同步时，犯错是出奇地容易；在那项研究中，大约一半的这种临时同步的使用是有错误的！==

Don't be lazy; use condition variables even when you think you can get away without doing so.
==不要懒惰；即使你认为可以不这样做，也要使用条件变量。==

If condition variables sound confusing, don't worry too much (yet) - we'll be covering them in great detail in a subsequent chapter.
==如果条件变量听起来令人困惑，不要太担心（目前）——我们将在随后的章节中非常详细地介绍它们。==

Until then, it should suffice to know that they exist and to have some idea how and why they are used.
==在那之前，知道它们的存在并对其如何以及为何使用有一些了解就足够了。==

27.5 Compiling and Running
==27.5 编译和运行==

All of the code examples in this chapter are relatively easy to get up and running.
==本章中的所有代码示例都相对容易启动和运行。==

To compile them, you must include the header pthread.h in your code.
==要编译它们，必须在代码中包含头文件 pthread.h。==

On the link line, you must also explicitly link with the pthreads library, by adding the -pthread flag.
==在链接行上，你还必须通过添加 -pthread 标志显式链接 pthreads 库。==

For example, to compile a simple multi-threaded program, all you have to do is the following:
==例如，要编译一个简单的多线程程序，你要做的就是执行以下操作：==

As long as main.c includes the pthreads header, you have now successfully compiled a concurrent program.
==只要 main.c 包含 pthreads 头文件，你就已经成功编译了一个并发程序。==

Whether it works or not, as usual, is a different matter entirely.
==像往常一样，它是否工作完全是另一回事。==

27.6 Summary
==27.6 总结==

We have introduced the basics of the pthread library, including thread creation, building mutual exclusion via locks, and signaling and waiting via condition variables.
==我们介绍了 pthread 库的基础知识，包括线程创建、通过锁建立互斥以及通过条件变量进行信号传递和等待。==

You don't need much else to write robust and efficient multi-threaded code, except patience and a great deal of care!
==要编写健壮且高效的多线程代码，除了耐心和极大的细心之外，你不需要太多其他东西！==

We now end the chapter with a set of tips that might be useful to you when you write multi-threaded code (see the aside on the following page for details).
==我们现在以一组提示结束本章，当你编写多线程代码时，这些提示可能会对你有所帮助（详见下页的旁注）。==

There are other aspects of the API that are interesting; if you want more information, type man -k pthread on a Linux system to see over one hundred APIs that make up the entire interface.
==API 还有其他有趣的方面；如果你想要更多信息，请在 Linux 系统上输入 man -k pthread 以查看构成整个接口的一百多个 API。==

However, the basics discussed herein should enable you to build sophisticated (and hopefully, correct and performant) multi-threaded programs.
==然而，本文讨论的基础知识应该使你能够构建复杂的（并希望是正确和高性能的）多线程程序。==

The hard part with threads is not the APIs, but rather the tricky logic of how you build concurrent programs.
==线程的难点不在于 API，而在于如何构建并发程序的棘手逻辑。==

Read on to learn more.
==继续阅读以了解更多信息。==

ASIDE: THREAD API GUIDELINES
==旁注：线程 API 指南==

There are a number of small but important things to remember when you use the POSIX thread library (or really, any thread library) to build a multi-threaded program.
==在使用 POSIX 线程库（或者实际上是任何线程库）构建多线程程序时，需要记住许多小而重要的事情。==

They are:
==它们是：==

• Keep it simple.
==• 保持简单。==

Above all else, any code to lock or signal between threads should be as simple as possible.
==最重要的是，任何用于在线程之间锁定或发送信号的代码都应尽可能简单。==

Tricky thread interactions lead to bugs.
==棘手的线程交互会导致错误。==

• Minimize thread interactions.
==• 最小化线程交互。==

Try to keep the number of ways in which threads interact to a minimum.
==尽量将线程交互的方式数量保持在最低限度。==

Each interaction should be carefully thought out and constructed with tried and true approaches (many of which we will learn about in the coming chapters).
==每一次交互都应该经过仔细思考，并使用经过验证的方法（我们将在接下来的章节中学习其中的许多方法）来构建。==

• Initialize locks and condition variables.
==• 初始化锁和条件变量。==

Failure to do so will lead to code that sometimes works and sometimes fails in very strange ways.
==如果不这样做，将导致代码有时工作，有时以非常奇怪的方式失败。==

• Check your return codes.
==• 检查你的返回码。==

Of course, in any C and UNIX programming you do, you should be checking each and every return code, and it's true here as well.
==当然，在你进行的任何 C 和 UNIX 编程中，你应该检查每一个返回码，这里也是如此。==

Failure to do so will lead to bizarre and hard to understand behavior, making you likely to (a) scream, (b) pull some of your hair out, or (c) both.
==如果不这样做，将导致离奇且难以理解的行为，使你可能 (a) 尖叫，(b) 拔掉一些头发，或者 (c) 两者兼而有之。==

• Be careful with how you pass arguments to, and return values from, threads.
==• 小心向线程传递参数和从线程返回值的方式。==

In particular, any time you are passing a reference to a variable allocated on the stack, you are probably doing something wrong.
==特别是，任何时候如果你传递一个对栈上分配的变量的引用，你可能就做错了。==

• Each thread has its own stack.
==• 每个线程都有自己的栈。==

As related to the point above, please remember that each thread has its own stack.
==与上面的一点相关，请记住每个线程都有自己的栈。==

Thus, if you have a locally-allocated variable inside of some function a thread is executing, it is essentially private to that thread; no other thread can (easily) access it.
==因此，如果在线程执行的某个函数内部有一个本地分配的变量，它本质上是该线程私有的；没有其他线程可以（容易地）访问它。==

To share data between threads, the values must be in the heap or otherwise some locale that is globally accessible.
==要在线程之间共享数据，这些值必须位于堆中，或者其他全局可访问的区域。==

• Always use condition variables to signal between threads.
==• 始终使用条件变量在线程之间发送信号。==

While it is often tempting to use a simple flag, don't do it.
==虽然经常有人想使用简单的标志，但不要这样做。==

• Use the manual pages.
==• 使用手册页。==

On Linux, in particular, the pthread man pages are highly informative and discuss many of the nuances presented here, often in even more detail.
==特别是在 Linux 上，pthread 手册页信息量很大，讨论了这里介绍的许多细微差别，通常更为详细。==

Read them carefully!
==仔细阅读它们！==

Homework (Code)
==作业（代码）==

In this section, we'll write some simple multi-threaded programs and use a specific tool, called helgrind, to find problems in these programs.
==在本节中，我们将编写一些简单的多线程程序，并使用一个名为 helgrind 的特定工具来查找这些程序中的问题。==

Read the README in the homework download for details on how to build the programs and run helgrind.
==请阅读作业下载中的 README，了解有关如何构建程序和运行 helgrind 的详细信息。==

Questions
==问题==

1. First build main-race.c.
==2. 首先构建 main-race.c。==

Examine the code so you can see the (hopefully obvious) data race in the code.
==检查代码，以便你能看到代码中（希望是显而易见的）数据竞争。==

Now run helgrind (by typing valgrind --tool=helgrind main-race) to see how it reports the race.
==现在运行 helgrind（通过输入 valgrind --tool=helgrind main-race）看看它是如何报告竞争的。==

Does it point to the right lines of code?
==它是否指向了正确的代码行？==

What other information does it give to you?
==它还为你提供了什么其他信息？==

2. What happens when you remove one of the offending lines of code?
==3. 当你删除其中一行有问题的代码时会发生什么？==

Now add a lock around one of the updates to the shared variable, and then around both.
==现在在对共享变量的其中一个更新周围添加锁，然后在两个更新周围都添加锁。==

What does helgrind report in each of these cases?
==helgrind 在每种情况下报告什么？==

3. Now let's look at main-deadlock.c.
==4. 现在让我们看看 main-deadlock.c。==

Examine the code.
==检查代码。==

This code has a problem known as deadlock (which we discuss in much more depth in a forthcoming chapter).
==这段代码有一个称为死锁的问题（我们将在即将到来的章节中更深入地讨论这个问题）。==

Can you see what problem it might have?
==你能看出它可能有什么问题吗？==

4. Now run helgrind on this code.
==5. 现在在这段代码上运行 helgrind。==

What does helgrind report?
==helgrind 报告了什么？==

5. Now run helgrind on main-deadlock-global.c.
==6. 现在在 main-deadlock-global.c 上运行 helgrind。==

Examine the code; does it have the same problem that main-deadlock.c has?
==检查代码；它是否具有与 main-deadlock.c 相同的问题？==

Should helgrind be reporting the same error?
==helgrind 应该报告相同的错误吗？==

What does this tell you about tools like helgrind?
==这告诉你关于像 helgrind 这样的工具有什么特点？==

6. Let's next look at main-signal.c.
==7. 接下让我们看看 main-signal.c。==

This code uses a variable (done) to signal that the child is done and that the parent can now continue.
==这段代码使用一个变量 (done) 来发信号表示子线程已完成，父线程现在可以继续。==

Why is this code inefficient?
==为什么这段代码效率低下？==

(what does the parent end up spending its time doing, particularly if the child thread takes a long time to complete?)
==（父线程最终把时间花在做什么上，特别是如果子线程需要很长时间才能完成？）==

7. Now run helgrind on this program.
==8. 现在在这个程序上运行 helgrind。==

What does it report?
==它报告了什么？==

Is the code correct?
==代码正确吗？==

8. Now look at a slightly modified version of the code, which is found in main-signal-cv.c.
==9. 现在看看代码的一个稍作修改的版本，可以在 main-signal-cv.c 中找到。==

This version uses a condition variable to do the signaling (and associated lock).
==这个版本使用条件变量来进行信号传递（以及相关的锁）。==

Why is this code preferred to the previous version?
==为什么这段代码比前一个版本更好？==

Is it correctness, or performance, or both?
==是因为正确性，还是性能，或者两者兼而有之？==

9. Once again run helgrind on main-signal-cv.
==10. 再次在 main-signal-cv 上运行 helgrind。==

Does it report any errors?
==它是否报告任何错误？==

Locks
==锁==

From the introduction to concurrency, we saw one of the fundamental problems in concurrent programming: we would like to execute a series of instructions atomically, but due to the presence of interrupts on a single processor (or multiple threads executing on multiple processors concurrently), we couldn't.
==从并发介绍中，我们看到了并发编程中的一个基本问题：我们希望原子地执行一系列指令，但由于单处理器上中断的存在（或多个线程在多个处理器上并发执行），我们无法做到。==

In this chapter, we thus attack this problem directly, with the introduction of something referred to as a lock.
==因此，在本章中，我们将通过引入一种称为锁的东西来直接解决这个问题。==

Programmers annotate source code with locks, putting them around critical sections, and thus ensure that any such critical section executes as if it were a single atomic instruction.
==程序员用锁来注释源代码，将它们放在临界区周围，从而确保任何此类临界区就像一条原子指令一样执行。==

28.1 Locks: The Basic Idea
==28.1 锁：基本思想==

As an example, assume our critical section looks like this, the canonical update of a shared variable:
==作为一个例子，假设我们的临界区看起来像这样，即对共享变量的规范更新：==

Of course, other critical sections are possible, such as adding an element to a linked list or other more complex updates to shared structures, but we'll just keep to this simple example for now.
==当然，其他的临界区也是可能的，例如向链表添加元素或对共享结构进行其他更复杂的更新，但我们目前仅保留这个简单的例子。==

To use a lock, we add some code around the critical section like this:
==要使用锁，我们在临界区周围添加一些代码，如下所示：==

A lock is just a variable, and thus to use one, you must declare a lock variable of some kind (such as mutex above).
==锁只是一个变量，因此要使用它，你必须声明某种锁变量（如上面的 mutex）。==

This lock variable (or just "lock" for short) holds the state of the lock at any instant in time.
==这个锁变量（或简称为“锁”）保存了锁在任何时刻的状态。==

It is either available (or unlocked or free) and thus no thread holds the lock, or acquired (or locked or held), and thus exactly one thread holds the lock and presumably is in a critical section.
==它要么是可用的（或未锁定的或空闲的），因此没有线程持有该锁；要么是被获取的（或锁定的或持有的），因此正好有一个线程持有该锁，并且大概正处于临界区中。==

We could store other information in the data type as well, such as which thread holds the lock, or a queue for ordering lock acquisition, but information like that is hidden from the user of the lock.
==我们也可以在数据类型中存储其他信息，例如哪个线程持有锁，或者用于排序锁获取的队列，但像那样的信息对锁的用户是隐藏的。==

The semantics of the lock() and unlock() routines are simple.
==lock() 和 unlock() 例程的语义很简单。==

Calling the routine lock() tries to acquire the lock; if no other thread holds the lock (i.e., it is free), the thread will acquire the lock and enter the critical section; this thread is sometimes said to be the owner of the lock.
==调用例程 lock() 尝试获取锁；如果没有其他线程持有该锁（即它是空闲的），该线程将获取锁并进入临界区；该线程有时被称为锁的所有者。==

If another thread then calls lock() on that same lock variable (mutex in this example), it will not return while the lock is held by another thread; in this way, other threads are prevented from entering the critical section while the first thread that holds the lock is in there.
==如果另一个线程随后在同一个锁变量（本例中的 mutex）上调用 lock()，只要锁被另一个线程持有，它就不会返回；通过这种方式，当第一个持有锁的线程在临界区内时，其他线程被阻止进入临界区。==

Once the owner of the lock calls unlock(), the lock is now available (free) again.
==一旦锁的所有者调用 unlock()，锁现在再次变为可用（空闲）。==

If no other threads are waiting for the lock (i.e., no other thread has called lock() and is stuck therein), the state of the lock is simply changed to free.
==如果没有其他线程在等待锁（即没有其他线程调用了 lock() 并卡在其中），锁的状态只是简单地更改为空闲。==

If there are waiting threads (stuck in lock()), one of them will (eventually) notice (or be informed of) this change of the lock's state, acquire the lock, and enter the critical section.
==如果有等待的线程（卡在 lock() 中），其中一个将（最终）注意到（或被告知）锁状态的这种变化，获取锁，并进入临界区。==

Locks provide some minimal amount of control over scheduling to programmers.
==锁为程序员提供了一些对调度的最小控制。==

In general, we view threads as entities created by the programmer but scheduled by the OS, in any fashion that the OS chooses.
==一般来说，我们将线程视为由程序员创建但由操作系统调度的实体，操作系统以其选择的任何方式进行调度。==

Locks yield some of that control back to the programmer; by putting a lock around a section of code, the programmer can guarantee that no more than a single thread can ever be active within that code.
==锁将部分控制权交还给程序员；通过在一段代码周围加上锁，程序员可以保证在这段代码中永远不超过一个线程处于活动状态。==

Thus locks help transform the chaos that is traditional OS scheduling into a more controlled activity.
==因此，锁有助于将传统操作系统调度的混乱转化为一种更受控制的活动。==

28.2 Pthread Locks
==28.2 Pthread 锁==

The name that the POSIX library uses for a lock is a mutex, as it is used to provide mutual exclusion between threads, i.e., if one thread is in the critical section, it excludes the others from entering until it has completed the section.
==POSIX 库用于锁的名称是互斥锁 (mutex)，因为它用于在线程之间提供互斥，即如果一个线程在临界区中，它会排除其他线程进入，直到它完成该部分。==

Thus, when you see the following POSIX threads code, you should understand that it is doing the same thing as above (we again use our wrappers that check for errors upon lock and unlock):
==因此，当你看到下面的 POSIX 线程代码时，你应该明白它在做与上面相同的事情（我们再次使用在锁定和解锁时检查错误的包装器）：==

You might also notice here that the POSIX version passes a variable to lock and unlock, as we may be using different locks to protect different variables.
==你可能还会注意到，POSIX 版本将一个变量传递给 lock 和 unlock，因为我们要使用不同的锁来保护不同的变量。==

Doing so can increase concurrency: instead of one big lock that is used any time any critical section is accessed (a coarse-grained locking strategy), one will often protect different data and data structures with different locks, thus allowing more threads to be in locked code at once (a more fine-grained approach).
==这样做可以增加并发性：我们通常会用不同的锁保护不同的数据和数据结构，而不是使用一个在任何时候访问任何临界区时都使用的大锁（粗粒度锁定策略），从而允许更多线程同时处于锁定代码中（一种更细粒度的方法）。==

28.3 Building A Lock
==28.3 构建一个锁==

By now, you should have some understanding of how a lock works, from the perspective of a programmer.
==到现在为止，从程序员的角度来看，你应该对锁是如何工作的有了一定的了解。==

But how should we build a lock?
==但是我们要如何构建一个锁呢？==

What hardware support is needed?
==需要什么硬件支持？==

What OS support?
==什么操作系统支持？==

It is this set of questions we address in the rest of this chapter.
==这正是我们在本章剩余部分要解决的一系列问题。==

THE CRUX: HOW TO BUILD A LOCK
==关键问题：如何构建一个锁==

How can we build an efficient lock?
==我们如何构建一个高效的锁？==

Efficient locks provide mutual exclusion at low cost, and also might attain a few other properties we discuss below.
==高效的锁以低成本提供互斥，并且可能获得我们下面讨论的其他一些属性。==

What hardware support is needed?
==需要什么硬件支持？==

What OS support?
==什么操作系统支持？==

To build a working lock, we will need some help from our old friend, the hardware, as well as our good pal, the OS.
==为了构建一个工作的锁，我们将需要我们的老朋友——硬件的帮助，以及我们的好伙伴——操作系统的帮助。==

Over the years, a number of different hardware primitives have been added to the instruction sets of various computer architectures; while we won't study how these instructions are implemented (that, after all, is the topic of a computer architecture class), we will study how to use them in order to build a mutual exclusion primitive like a lock.
==多年来，许多不同的硬件原语已被添加到各种计算机架构的指令集中；虽然我们不会研究这些指令是如何实现的（毕竟，那是计算机体系结构课程的主题），但我们将研究如何使用它们来构建像锁这样的互斥原语。==

We will also study how the OS gets involved to complete the picture and enable us to build a sophisticated locking library.
==我们还将研究操作系统如何参与其中以完善这幅图景，并使我们能够构建复杂的锁定库。==

28.4 Evaluating Locks
==28.4 评估锁==

Before building any locks, we should first understand what our goals are, and thus we ask how to evaluate the efficacy of a particular lock implementation.
==在构建任何锁之前，我们应该首先了解我们的目标是什么，因此我们要问如何评估特定锁实现的功效。==

To evaluate whether a lock works (and works well), we should establish some basic criteria.
==为了评估锁是否有效（以及效果如何），我们应该建立一些基本标准。==

The first is whether the lock does its basic task, which is to provide mutual exclusion.
==首先是锁是否完成了它的基本任务，即提供互斥。==

Basically, does the lock work, preventing multiple threads from entering a critical section?
==基本上，锁是否起作用，防止多个线程进入临界区？==

The second is fairness.
==第二是公平性。==

Does each thread contending for the lock get a fair shot at acquiring it once it is free?
==每个争夺锁的线程在锁空闲时是否有公平的机会获取它？==

Another way to look at this is by examining the more extreme case: does any thread contending for the lock starve while doing so, thus never obtaining it?
==另一种看待这个问题的方式是检查更极端的情况：是否有任何争夺锁的线程在争夺过程中饿死，从而从未获得锁？==

The final criterion is performance, specifically the time overheads added by using the lock.
==最后一个标准是性能，具体来说是使用锁增加的时间开销。==

There are a few different cases that are worth considering here.
==这里有几种不同的情况值得考虑。==

One is the case of no contention; when a single thread is running and grabs and releases the lock, what is the overhead of doing so?
==一种是无竞争的情况；当单个线程运行并获取和释放锁时，这样做的开销是多少？==

Another is the case where multiple threads are contending for the lock on a single CPU; in this case, are there performance concerns?
==另一种情况是多个线程在单个 CPU 上争夺锁；在这种情况下，是否有性能方面的顾虑？==

Finally, how does the lock perform when there are multiple CPUs involved, and threads on each contending for the lock?
==最后，当涉及多个 CPU，并且每个 CPU 上的线程都在争夺锁时，锁的性能如何？==

By comparing these different scenarios, we can better understand the performance impact of using various locking techniques, as described below.
==通过比较这些不同的场景，我们可以更好地理解使用各种锁定技术对性能的影响，如下所述。==

28.5 Controlling Interrupts
==28.5 控制中断==

One of the earliest solutions used to provide mutual exclusion was to disable interrupts for critical sections; this solution was invented for single-processor systems.
==用于提供互斥的最早解决方案之一是禁用临界区的中断；该解决方案是为单处理器系统发明的。==

The code would look like this:
==代码如下所示：==

Assume we are running on such a single-processor system.
==假设我们在这样一个单处理器系统上运行。==

By turning off interrupts (using some kind of special hardware instruction) before entering a critical section, we ensure that the code inside the critical section will not be interrupted, and thus will execute as if it were atomic.
==通过在进入临界区之前关闭中断（使用某种特殊的硬件指令），我们要确保临界区内的代码不会被中断，从而像原子操作一样执行。==

When we are finished, we re-enable interrupts (again, via a hardware instruction) and thus the program proceeds as usual.
==完成后，我们重新启用中断（再次通过硬件指令），程序照常进行。==

The main positive of this approach is its simplicity.
==这种方法的主要优点是简单。==

You certainly don't have to scratch your head too hard to figure out why this works.
==你当然不需要绞尽脑汁去弄清楚为什么这是有效的。==

Without interruption, a thread can be sure that the code it executes will execute and that no other thread will interfere with it.
==没有中断，线程可以确信它执行的代码将被执行，并且没有其他线程会干扰它。==

The negatives, unfortunately, are many.
==不幸的是，缺点也很多。==

First, this approach requires us to allow any calling thread to perform a privileged operation (turning interrupts on and off), and thus trust that this facility is not abused.
==首先，这种方法要求我们允许任何调用线程执行特权操作（打开和关闭中断），从而信任该设施不会被滥用。==

As you already know, any time we are required to trust an arbitrary program, we are probably in trouble.
==如你所知，任何时候我们需要信任任意程序时，我们可能就有麻烦了。==

Here, the trouble manifests in numerous ways: a greedy program could call lock() at the beginning of its execution and thus monopolize the processor; worse, an errant or malicious program could call lock() and go into an endless loop.
==在这里，麻烦以多种方式表现出来：一个贪婪的程序可能在其执行开始时调用 lock()，从而独占处理器；更糟糕的是，一个错误或恶意的程序可能会调用 lock() 并进入死循环。==

In this latter case, the OS never regains control of the system, and there is only one recourse: restart the system.
==在后一种情况下，操作系统永远无法重新获得对系统的控制权，并且只有一种补救措施：重新启动系统。==

Using interrupt disabling as a general-purpose synchronization solution requires too much trust in applications.
==使用中断禁用作为通用同步解决方案需要对应用程序过多的信任。==

Second, the approach does not work on multiprocessors.
==其次，该方法不适用于多处理器。==

If multiple threads are running on different CPUs, and each try to enter the same critical section, it does not matter whether interrupts are disabled; threads will be able to run on other processors, and thus could enter the critical section.
==如果多个线程在不同的 CPU 上运行，并且每个线程都试图进入同一个临界区，那么是否禁用中断并不重要；线程将能够在其他处理器上运行，从而可能进入临界区。==

As multiprocessors are now commonplace, our general solution will have to do better than this.
==由于多处理器现在很常见，我们的通用解决方案必须做得比这更好。==

Third, turning off interrupts for extended periods of time can lead to interrupts becoming lost, which can lead to serious systems problems.
==第三，长时间关闭中断可能会导致中断丢失，这可能导致严重的系统问题。==

Imagine, for example, if the CPU missed the fact that a disk device has finished a read request.
==例如，试想一下如果 CPU 错过了磁盘设备已完成读取请求的事实。==

How will the OS know to wake the process waiting for said read?
==操作系统如何知道要唤醒等待该读取的进程？==

Figure 28.1: First Attempt: A Simple Flag
==图 28.1：第一次尝试：一个简单的标志==

For these reasons, turning off interrupts is only used in limited contexts as a mutual-exclusion primitive.
==由于这些原因，关闭中断仅在有限的上下文中用作互斥原语。==

For example, in some cases an operating system itself will use interrupt masking to guarantee atomicity when accessing its own data structures, or at least to prevent certain messy interrupt handling situations from arising.
==例如，在某些情况下，操作系统本身在访问其自己的数据结构时会使用中断屏蔽来保证原子性，或者至少防止出现某些混乱的中断处理情况。==

This usage makes sense, as the trust issue disappears inside the OS, which always trusts itself to perform privileged operations anyhow.
==这种用法是有道理的，因为信任问题在操作系统内部消失了，无论如何，操作系统总是信任自己执行特权操作。==

28.6 A Failed Attempt: Just Using Loads/Stores
==28.6 一次失败的尝试：仅使用加载/存储==

To move beyond interrupt-based techniques, we will have to rely on CPU hardware and the instructions it provides us to build a proper lock.
==为了超越基于中断的技术，我们将不得不依靠 CPU 硬件及其提供的指令来构建合适的锁。==

Let's first try to build a simple lock by using a single flag variable.
==让我们首先尝试使用单个标志变量来构建一个简单的锁。==

In this failed attempt, we'll see some of the basic ideas needed to build a lock, and (hopefully) see why just using a single variable and accessing it via normal loads and stores is insufficient.
==在这个失败的尝试中，我们将看到构建锁所需的一些基本思想，并（希望）了解为什么仅使用单个变量并通过正常的加载和存储访问它是不足够的。==

In this first attempt (Figure 28.1), the idea is quite simple: use a simple variable (flag) to indicate whether some thread has possession of a lock.
==在第一次尝试（图 28.1）中，这个想法很简单：使用一个简单的变量 (flag) 来指示某个线程是否拥有锁。==

The first thread that enters the critical section will call lock(), which tests whether the flag is equal to 1 (in this case, it is not), and then sets the flag to 1 to indicate that the thread now holds the lock.
==第一个进入临界区的线程将调用 lock()，测试标志是否等于 1（在本例中，它不是），然后将标志设置为 1 以指示该线程现在持有锁。==

When finished with the critical section, the thread calls unlock() and clears the flag, thus indicating that the lock is no longer held.
==当临界区结束时，线程调用 unlock() 并清除标志，从而指示不再持有锁。==

If another thread happens to call lock() while that first thread is in the critical section, it will simply spin-wait in the while loop for that thread to call unlock() and clear the flag.
==如果另一个线程恰好在第一个线程处于临界区时调用 lock()，它将只是在 while 循环中自旋等待该线程调用 unlock() 并清除标志。==

Once that first thread does so, the waiting thread will fall out of the while loop, set the flag to 1 for itself, and proceed into the critical section.
==一旦第一个线程这样做，等待线程将跳出 while 循环，将自己的标志设置为 1，并进入临界区。==

Unfortunately, the code has two problems: one of correctness, and another of performance.
==不幸的是，代码有两个问题：一个是正确性问题，另一个是性能问题。==

Figure 28.2: Trace: No Mutual Exclusion
==图 28.2：跟踪：无互斥==

The correctness problem is simple to see once you get used to thinking about concurrent programming.
==一旦习惯了思考并发编程，正确性问题就很容易看出来。==

Imagine the code interleaving in Figure 28.2; assume flag=0 to begin.
==想象一下图 28.2 中的代码交错；假设开始时 flag=0。==

As you can see from this interleaving, with timely (untimely?) interrupts, we can easily produce a case where both threads set the flag to 1 and both threads are thus able to enter the critical section.
==正如你从这种交错中看到的那样，通过及时的（不合时宜的？）中断，我们可以很容易地产生这样一种情况：两个线程都将标志设置为 1，因此两个线程都能够进入临界区。==

This behavior is what professionals call "bad" - we have obviously failed to provide the most basic requirement: providing mutual exclusion.
==这种行为就是专业人士所说的“糟糕”——我们显然未能提供最基本的要求：提供互斥。==

The performance problem, which we will address more later on, is the fact that the way a thread waits to acquire a lock that is already held: it endlessly checks the value of flag, a technique known as spin-waiting.
==我们稍后将更多地讨论性能问题，即线程等待获取已被持有的锁的方式：它无休止地检查 flag 的值，这种技术称为自旋等待。==

Spin-waiting wastes time waiting for another thread to release a lock.
==自旋等待浪费了等待另一个线程释放锁的时间。==

The waste is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run (at least, until a context switch occurs)!
==这种浪费在单处理器上非常高，因为等待者正在等待的线程甚至无法运行（至少，直到发生上下文切换）！==

Thus, as we move forward and develop more sophisticated solutions, we should also consider ways to avoid this kind of waste.
==因此，随着我们继续前进并开发更复杂的解决方案，我们也应该考虑避免这种浪费的方法。==

28.7 Building Working Spin Locks with Test-And-Set
==28.7 使用测试并设置构建可工作的自旋锁==

Because disabling interrupts does not work on multiple processors, and because simple approaches using loads and stores (as shown above) don't work, system designers started to invent hardware support for locking.
==由于禁用中断在多处理器上不起作用，并且因为使用加载和存储的简单方法（如上所示）也不起作用，系统设计者开始发明对锁定的硬件支持。==

The earliest multiprocessor systems, such as the Burroughs B5000 in the early 1960's, had such support; today all systems provide this type of support, even for single CPU systems.
==最早的多处理器系统，如 20 世纪 60 年代初的 Burroughs B5000，就拥有这种支持；今天的所有系统都提供这种类型的支持，即使是单 CPU 系统。==

The simplest bit of hardware support to understand is known as a test-and-set (or atomic exchange) instruction.
==最容易理解的硬件支持被称为测试并设置 (test-and-set)（或原子交换）指令。==

We define what the test-and-set instruction does via the following C code snippet:
==我们通过以下 C 代码片段定义测试并设置指令的作用：==

Each architecture that supports test-and-set calls it by a different name.
==每个支持测试并设置的架构都用不同的名称来称呼它。==

On SPARC it is called the load/store unsigned byte instruction (ldstub); on x86 it is the locked version of the atomic exchange (xchg).
==在 SPARC 上，它被称为加载/存储无符号字节指令 (ldstub)；在 x86 上，它是原子交换 (xchg) 的锁定版本。==

ASIDE: DEKKER'S AND PETERSON'S ALGORITHMS
==旁注：DEKKER 和 PETERSON 算法==

In the 1960's, Dijkstra posed the concurrency problem to his friends, and one of them, a mathematician named Theodorus Jozef Dekker, came up with a solution.
==在 20 世纪 60 年代，Dijkstra 向他的朋友们提出了并发问题，其中一位名叫 Theodorus Jozef Dekker 的数学家提出了一个解决方案。==

Unlike the solutions we discuss here, which use special hardware instructions and even OS support, Dekker's algorithm uses just loads and stores (assuming they are atomic with respect to each other, which was true on early hardware).
==与我们在这里讨论的使用特殊硬件指令甚至操作系统支持的解决方案不同，Dekker 算法仅使用加载和存储（假设它们相对于彼此是原子的，这在早期硬件上是正确的）。==

Dekker's approach was later refined by Peterson.
==Dekker 的方法后来由 Peterson 改进。==

Once again, just loads and stores are used, and the idea is to ensure that two threads never enter a critical section at the same time.
==再一次，仅使用加载和存储，其想法是确保两个线程永远不会同时进入临界区。==

Here is Peterson's algorithm (for two threads); see if you can understand the code.
==这是 Peterson 算法（针对两个线程）；看看你是否能理解代码。==

What are the flag and turn variables used for?
==flag 和 turn 变量是用来做什么的？==

For some reason, developing locks that work without special hardware support became all the rage for a while, giving theory-types a lot of problems to work on.
==出于某种原因，开发无需特殊硬件支持即可工作的锁一度风靡一时，给理论型人士带来了很多可以研究的问题。==

Of course, this line of work became quite useless when people realized it is much easier to assume a little hardware support (and indeed that support had been around from the earliest days of multiprocessing).
==当然，当人们意识到假设一点硬件支持要容易得多（而且确实这种支持从多处理的最早时期就已经存在）时，这一行工作就变得毫无用处了。==

Further, algorithms like the ones above don't work on modern hardware (due to relaxed memory consistency models), thus making them even less useful than they were before.
==此外，像上面的算法在现代硬件上不起作用（由于宽松的内存一致性模型），从而使它们比以前更没用。==

Yet more research relegated to the dustbin of history...
==然而，更多的研究被扔进了历史的垃圾桶......==

Figure 28.3: A Simple Spin Lock Using Test-and-set
==图 28.3：使用测试并设置的简单自旋锁==

What the test-and-set instruction does is as follows.
==测试并设置指令的作用如下。==

It returns the old value pointed to by the old_ptr, and simultaneously updates said value to new.
==它返回 old_ptr 指向的旧值，同时将该值更新为 new。==

The key, of course, is that this sequence of operations is performed atomically.
==当然，关键在于这一系列操作是原子执行的。==

The reason it is called "test and set" is that it enables you to "test" the old value (which is what is returned) while simultaneously "setting" the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple spin lock, as we now examine in Figure 28.3.
==之所以称为“测试并设置”，是因为它使你能够“测试”旧值（即返回的值），同时将内存位置“设置”为新值；事实证明，这条稍微强大一点的指令足以构建一个简单的自旋锁，正如我们要图 28.3 中看到的那样。==

Or better yet: figure it out first yourself!
==或者更好的是：先自己弄清楚！==

Let's make sure we understand why this lock works.
==让我们确保我们理解为什么这个锁是有效的。==

Imagine first the case where a thread calls lock() and no other thread currently holds the lock; thus, flag should be 0.
==首先想象一种情况，线程调用 lock() 且当前没有其他线程持有该锁；因此，flag 应该为 0。==

When the thread calls TestAndSet(flag, 1), the routine will return the old value of flag, which is 0; thus, the calling thread, which is testing the value of flag, will not get caught spinning in the while loop and will acquire the lock.
==当线程调用 TestAndSet(flag, 1) 时，该例程将返回 flag 的旧值，即 0；因此，测试 flag 值的调用线程将不会在 while 循环中自旋，并会获取锁。==

The thread will also atomically set the value to 1, thus indicating that the lock is now held.
==该线程还将原子地将值设置为 1，从而指示现在持有锁。==

When the thread is finished with its critical section, it calls unlock() to set the flag back to zero.
==当线程完成其临界区时，它调用 unlock() 将标志设回零。==

The second case we can imagine arises when one thread already has the lock held (i.e., flag is 1).
==我们可以想象的第二种情况发生在已经有一个线程持有锁时（即 flag 为 1）。==

In this case, this thread will call lock() and then call TestAndSet(flag, 1) as well.
==在这种情况下，该线程将调用 lock()，然后也调用 TestAndSet(flag, 1)。==

This time, TestAndSet() will return the old value at flag, which is 1 (because the lock is held), while simultaneously setting it to 1 again.
==这次，TestAndSet() 将返回 flag 处的旧值，即 1（因为锁被持有），同时再次将其设置为 1。==

As long as the lock is held by another thread, TestAndSet() will repeatedly return 1, and thus this thread will spin and spin until the lock is finally released.
==只要锁被另一个线程持有，TestAndSet() 就会重复返回 1，因此该线程将一直自旋，直到锁最终被释放。==

When the flag is finally set to 0 by some other thread, this thread will call TestAndSet() again, which will now return 0 while atomically setting the value to 1 and thus acquire the lock and enter the critical section.
==当 flag 最终被其他线程设置为 0 时，该线程将再次调用 TestAndSet()，现在将返回 0，同时原子地将值设置为 1，从而获取锁并进入临界区。==

By making both the test (of the old lock value) and set (of the new value) a single atomic operation, we ensure that only one thread acquires the lock.
==通过将测试（旧锁值）和设置（新值）都作为一个原子操作，我们确保只有一个线程获取锁。==

And that's how to build a working mutual exclusion primitive!
==这就是如何构建一个有效的互斥原语！==

You may also now understand why this type of lock is usually referred to as a spin lock.
==你现在也可能明白为什么这种类型的锁通常被称为自旋锁。==

It is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available.
==它是构建起来最简单的锁类型，只是利用 CPU 周期进行自旋，直到锁变得可用。==

To work correctly on a single processor, it requires a preemptive scheduler (i.e., one that will interrupt a thread via a timer, in order to run a different thread, from time to time).
==为了在单处理器上正确工作，它需要一个抢占式调度程序（即一个会通过定时器中断线程，以便不时运行不同线程的调度程序）。==

Without preemption, spin locks don't make much sense on a single CPU, as a thread spinning on a CPU will never relinquish it.
==如果没有抢占，自旋锁在单 CPU 上就没有多大意义，因为在 CPU 上自旋的线程永远不会放弃它。==

TIP: THINK ABOUT CONCURRENCY AS A MALICIOUS SCHEDULER
==提示：将并发视为恶意调度程序==

From this example, you might get a sense of the approach you need to take to understand concurrent execution.
==从这个例子中，你可能会感觉到你需要采取什么方法来理解并发执行。==

What you should try to do is to pretend you are a malicious scheduler, one that interrupts threads at the most inopportune of times in order to foil their feeble attempts at building synchronization primitives.
==你应该尝试做的是假装你是一个恶意的调度程序，一个会在最不合适的时机中断线程，以挫败它们构建同步原语的微弱尝试的调度程序。==

What a mean scheduler you are!
==你是一个多么刻薄的调度程序啊！==

Although the exact sequence of interrupts may be improbable, it is possible, and that is all we need to demonstrate that a particular approach does not work.
==虽然确切的中断序列可能不太可能发生，但它是可能的，这就足以证明特定方法行不通。==

It can be useful to think maliciously! (at least, sometimes)
==恶意思考可能很有用！（至少有时是这样）==

28.8 Evaluating Spin Locks
==28.8 评估自旋锁==

Given our basic spin lock, we can now evaluate how effective it is along our previously described axes.
==鉴于我们的基本自旋锁，我们现在可以沿着我们之前描述的轴来评估它的有效性。==

The most important aspect of a lock is correctness: does it provide mutual exclusion?
==锁最重要的方面是正确性：它是否提供互斥？==

The answer here is yes: the spin lock only allows a single thread to enter the critical section at a time.
==这里的答案是肯定的：自旋锁一次只允许一个线程进入临界区。==

Thus, we have a correct lock.
==因此，我们有一个正确的锁。==

The next axis is fairness.
==下一个轴是公平性。==

How fair is a spin lock to a waiting thread?
==自旋锁对等待线程的公平程度如何？==

Can you guarantee that a waiting thread will ever enter the critical section?
==你能保证等待线程会进入临界区吗？==

The answer here, unfortunately, is bad news: spin locks don't provide any fairness guarantees.
==不幸的是，这里的答案是坏消息：自旋锁不提供任何公平性保证。==

Indeed, a thread spinning may spin forever, under contention.
==事实上，在竞争下，自旋的线程可能会永远自旋。==

Simple spin locks (as discussed thus far) are not fair and may lead to starvation.
==简单的自旋锁（如目前讨论的）是不公平的，可能会导致饥饿。==

The final axis is performance.
==最后一个轴是性能。==

What are the costs of using a spin lock?
==使用自旋锁的成本是多少？==

To analyze this more carefully, we suggest thinking about a few different cases.
==为了更仔细地分析这一点，我们建议考虑几种不同的情况。==

In the first, imagine threads competing for the lock on a single processor; in the second, consider threads spread out across many CPUs.
==第一种情况，想象线程在单处理器上争夺锁；第二种情况，考虑线程分布在许多 CPU 上。==

For spin locks, in the single CPU case, performance overheads can be quite painful; imagine the case where the thread holding the lock is preempted within a critical section.
==对于自旋锁，在单 CPU 情况下，性能开销可能会非常痛苦；想象一下持有锁的线程在临界区内被抢占的情况。==

The scheduler might then run every other thread (imagine there are N-1 others), each of which tries to acquire the lock.
==然后调度程序可能会运行所有其他线程（假设还有 N-1 个），每个线程都试图获取锁。==

In this case, each of those threads will spin for the duration of a time slice before giving up the CPU, a waste of CPU cycles.
==在这种情况下，这些线程中的每一个都将在一个时间片的时间内自旋，然后放弃 CPU，这是对 CPU 周期的浪费。==

However, on multiple CPUs, spin locks work reasonably well (if the number of threads roughly equals the number of CPUs).
==然而，在多 CPU 上，自旋锁工作得相当好（如果线程数大致等于 CPU 数）。==

The thinking goes as follows: imagine Thread A on CPU 1 and Thread B on CPU 2, both contending for a lock.
==思路如下：想象一下 CPU 1 上的线程 A 和 CPU 2 上的线程 B，都在争夺锁。==

If Thread A (CPU 1) grabs the lock, and then Thread B tries to, B will spin (on CPU 2).
==如果线程 A (CPU 1) 抓住了锁，然后线程 B 试图获取，B 将（在 CPU 2 上）自旋。==

However, presumably the critical section is short, and thus soon the lock becomes available, and is acquired by Thread B.
==然而，据推测临界区很短，因此锁很快就变得可用，并由线程 B 获取。==

Spinning to wait for a lock held on another processor doesn't waste many cycles in this case, and thus can be effective.
==在这种情况下，自旋等待另一个处理器上持有的锁不会浪费很多周期，因此可能是有效的。==

28.9 Compare-And-Swap
==28.9 比较并交换==

Figure 28.4: Compare-and-swap
==图 28.4：比较并交换==

Another hardware primitive that some systems provide is known as the compare-and-swap instruction (as it is called on SPARC, for example), or compare-and-exchange (as it called on x86).
==某些系统提供的另一种硬件原语称为比较并交换 (compare-and-swap) 指令（例如在 SPARC 上），或比较并交换 (compare-and-exchange)（在 x86 上）。==

The C pseudocode for this single instruction is found in Figure 28.4.
==这条单指令的 C 伪代码见图 28.4。==

The basic idea is for compare-and-swap to test whether the value at the address specified by ptr is equal to expected; if so, update the memory location pointed to by ptr with the new value.
==基本思想是让比较并交换测试 ptr 指定地址处的值是否等于 expected；如果是，用 new 值更新 ptr 指向的内存位置。==

If not, do nothing.
==如果不是，什么也不做。==

In either case, return the original value at that memory location, thus allowing the code calling compare-and-swap to know whether it succeeded or not.
==在任何一种情况下，返回该内存位置的原始值，从而允许调用比较并交换的代码知道它是否成功。==

With the compare-and-swap instruction, we can build a lock in a manner quite similar to that with test-and-set.
==使用比较并交换指令，我们可以以与测试并设置非常相似的方式构建锁。==

For example, we could just replace the lock() routine above with the following:
==例如，我们可以将上面的 lock() 例程替换为以下内容：==

The rest of the code is the same as the test-and-set example above.
==其余代码与上面的测试并设置示例相同。==

This code works quite similarly; it simply checks if the flag is 0 and if so, atomically swaps in a 1 thus acquiring the lock.
==这段代码的工作原理非常相似；它只是检查标志是否为 0，如果是，则原子地交换为 1，从而获取锁。==

Threads that try to acquire the lock while it is held will get stuck spinning until the lock is finally released.
==试图在锁被持有时获取锁的线程将被卡在自旋中，直到锁最终被释放。==

If you want to see how to really make a C-callable x86-version of compare-and-swap, the code sequence (from [505]) might be useful.
==如果你想看看如何真正制作一个 C 可调用的 x86 版本的比较并交换，代码序列（来自 [505]）可能会有用。==

Finally, as you may have sensed, compare-and-swap is a more powerful instruction than test-and-set.
==最后，正如你可能已经感觉到的那样，比较并交换是比测试并设置更强大的指令。==

We will make some use of this power in the future when we briefly delve into topics such as lock-free synchronization.
==当我们未来简要深入探讨无锁同步等主题时，我们将利用这种力量。==

However, if we just build a simple spin lock with it, its behavior is identical to the spin lock we analyzed above.
==然而，如果我们只是用它构建一个简单的自旋锁，它的行为与我们上面分析的自旋锁完全相同。==

28.10 Load-Linked and Store-Conditional
==28.10 链接加载和条件存储==

Some platforms provide a pair of instructions that work in concert to help build critical sections.
==某些平台提供一对协同工作的指令来帮助构建临界区。==

On the MIPS architecture, for example, the load-linked and store-conditional instructions can be used in tandem to build locks and other concurrent structures.
==例如，在 MIPS 架构上，链接加载 (load-linked) 和条件存储 (store-conditional) 指令可以串联使用以构建锁和其他并发结构。==

The C pseudocode for these instructions is as found in Figure 28.5.
==这些指令的 C 伪代码如图 28.5 所示。==

Alpha, PowerPC, and ARM provide similar instructions.
==Alpha、PowerPC 和 ARM 提供类似的指令。==

The load-linked operates much like a typical load instruction, and simply fetches a value from memory and places it in a register.
==链接加载的操作很像典型的加载指令，只是简单地从内存中获取一个值并将其放入寄存器中。==

The key difference comes with the store-conditional, which only succeeds (and updates the value stored at the address just load-linked from) if no intervening store to the address has taken place.
==关键区别在于条件存储，只有在没有发生对该地址的中间存储时，它才会成功（并更新刚刚链接加载的地址处存储的值）。==

In the case of success, the store-conditional returns 1 and updates the value at ptr to value; if it fails, the value at ptr is not updated and 0 is returned.
==如果成功，条件存储返回 1 并将 ptr 处的值更新为 value；如果失败，ptr 处的值不会更新，并返回 0。==

As a challenge to yourself, try thinking about how to build a lock using load-linked and store-conditional.
==作为对自己的挑战，试着思考如何使用链接加载和条件存储来构建锁。==

Then, when you are finished, look at the code below which provides one simple solution.
==然后，当你完成后，看看下面的代码，它提供了一个简单的解决方案。==

Do it!
==做吧！==

The solution is in Figure 28.6.
==解决方案在图 28.6 中。==

Figure 28.5: Load-linked And Store-conditional
==图 28.5：链接加载和条件存储==

The lock() code is the only interesting piece.
==lock() 代码是唯一有趣的部分。==

First, a thread spins waiting for the flag to be set to 0 (and thus indicate the lock is not held).
==首先，线程自旋等待标志设置为 0（从而指示锁未被持有）。==

Once so, the thread tries to acquire the lock via the store-conditional; if it succeeds, the thread has atomically changed the flag's value to 1 and thus can proceed into the critical section.
==一旦如此，线程尝试通过条件存储获取锁；如果成功，线程已原子地将标志值更改为 1，从而可以进入临界区。==

Note how failure of the store-conditional might arise.
==注意条件存储的失败是如何发生的。==

One thread calls lock() and executes the load-linked, returning 0 as the lock is not held.
==一个线程调用 lock() 并执行链接加载，返回 0，因为锁未被持有。==

Before it can attempt the store-conditional, it is interrupted and another thread enters the lock code, also executing the load-linked instruction, and also getting a 0 and continuing.
==在它尝试条件存储之前，它被中断，另一个线程进入锁定代码，也执行链接加载指令，同样得到 0 并继续。==

At this point, two threads have each executed the load-linked and each are about to attempt the store-conditional.
==此时，两个线程都已执行了链接加载，并且都准备尝试条件存储。==

The key feature of these instructions is that only one of these threads will succeed in updating the flag to 1 and thus acquire the lock; the second thread to attempt the store-conditional will fail (because the other thread updated the value of flag between its load-linked and store-conditional) and thus have to try to acquire the lock again.
==这些指令的关键特性是，这些线程中只有一个能成功将标志更新为 1 并从而获取锁；第二个尝试条件存储的线程将失败（因为另一个线程在其链接加载和条件存储之间更新了 flag 的值），因此必须再次尝试获取锁。==

Figure 28.6: Using LL/SC To Build A Lock
==图 28.6：使用 LL/SC 构建锁==

In class a few years ago, undergraduate student David Capel suggested a more concise form of the above, for those of you who enjoy short-circuiting boolean conditionals.
==几年前在课堂上，本科生 David Capel 提出了上述代码的一种更简洁的形式，供那些喜欢短路布尔条件的人参考。==

See if you can figure out why it is equivalent.
==看看你能不能弄清楚为什么它是等价的。==

It certainly is shorter!
==它确实更短！==

28.11 Fetch-And-Add
==28.11 获取并增加==

One final hardware primitive is the fetch-and-add instruction, which atomically increments a value while returning the old value at a particular address.
==最后一个硬件原语是获取并增加 (fetch-and-add) 指令，它原子地递增一个值，同时返回特定地址处的旧值。==

The C pseudocode for the fetch-and-add instruction looks like this:
==获取并增加指令的 C 伪代码如下所示：==

TIP: LESS CODE IS BETTER CODE (LAUER'S LAW)
==提示：代码越少越好（LAUER 定律）==

Programmers tend to brag about how much code they wrote to do something.
==程序员倾向于吹嘘他们为做某事写了多少代码。==

Doing so is fundamentally broken.
==这样做从根本上是错误的。==

What one should brag about, rather, is how little code one wrote to accomplish a given task.
==相反，一个人应该吹嘘的是，他为完成给定任务写了多少代码。==

Short, concise code is always preferred; it is likely easier to understand and has fewer bugs.
==简短、简洁的代码总是首选；它可能更容易理解，错误也更少。==

As Hugh Lauer said, when discussing the construction of the Pilot operating system: "If the same people had twice as much time, they could produce as good of a system in half the code."
==正如 Hugh Lauer 在讨论 Pilot 操作系统的构建时所说的那样：“如果同样的人有两倍的时间，他们可以用一半的代码通过生产出同样好的系统。”==

We'll call this Lauer's Law, and it is well worth remembering.
==我们称之为 Lauer 定律，这很值得记住。==

So next time you're bragging about how much code you wrote to finish the assignment, think again, or better yet, go back, rewrite, and make the code as clear and concise as possible.
==所以下次当你吹嘘你写了多少代码来完成作业时，再想一想，或者更好的是，回去重写，让代码尽可能清晰简洁。==

In this example, we'll use fetch-and-add to build a more interesting ticket lock, as introduced by Mellor-Crummey and Scott.
==在这个例子中，我们将使用获取并增加来构建一个更有趣的票据锁 (ticket lock)，正如 Mellor-Crummey 和 Scott 所介绍的那样。==

The lock and unlock code is found in Figure 28.7.
==锁定和解锁代码见图 28.7。==

Instead of a single value, this solution uses a ticket and turn variable in combination to build a lock.
==这个解决方案使用 ticket 和 turn 变量组合来构建锁，而不是单个值。==

The basic operation is pretty simple: when a thread wishes to acquire a lock, it first does an atomic fetch-and-add on the ticket value; that value is now considered this thread's "turn" (myturn).
==基本操作非常简单：当一个线程希望获取锁时，它首先对 ticket 值进行原子获取并增加；该值现在被视为该线程的“回合” (myturn)。==

The globally shared lock->turn is then used to determine which thread's turn it is; when (myturn == turn) for a given thread, it is that thread's turn to enter the critical section.
==然后使用全局共享的 lock->turn 来确定轮到哪个线程；当给定线程的 (myturn == turn) 时，轮到该线程进入临界区。==

Unlock is accomplished simply by incrementing the turn such that the next waiting thread (if there is one) can now enter the critical section.
==解锁只需通过递增 turn 来完成，这样下一个等待线程（如果有的话）现在就可以进入临界区。==

Note one important difference with this solution versus our previous attempts: it ensures progress for all threads.
==请注意此解决方案与我们之前的尝试的一个重要区别：它确保了所有线程的进度。==

Once a thread is assigned its ticket value, it will be scheduled at some point in the future (once those in front of it have passed through the critical section and released the lock).
==一旦线程被分配了票据值，它将在未来的某个时刻被调度（一旦它前面的线程通过了临界区并释放了锁）。==

In our previous attempts, no such guarantee existed; a thread spinning on test-and-set (for example) could spin forever even as other threads acquire and release the lock.
==在我们之前的尝试中，不存在这样的保证；在测试并设置上自旋的线程（例如）可能会永远自旋，即使其他线程获取并释放了锁。==

28.12 Too Much Spinning: What Now?
==28.12 自旋太多：现在怎么办？==

Our hardware-based locks are simple (only a few lines of code) and they work (you could even prove that if you'd like to, by writing some code), which are two excellent properties of any system or code.
==我们要基于硬件的锁很简单（只有几行代码）并且它们有效（如果你愿意，你甚至可以通过编写一些代码来证明这一点），这是任何系统或代码的两个极好的属性。==

However, in some cases, these solutions can be quite inefficient.
==然而，在某些情况下，这些解决方案可能效率极低。==

Imagine you are running two threads on a single processor.
==想象一下你在单个处理器上运行两个线程。==

Now imagine that one thread (thread 0) is in a critical section and thus has a lock held, and unfortunately gets interrupted.
==现在想象一下，一个线程（线程 0）处于临界区，因此持有一个锁，但不幸被中断了。==

The second thread (thread 1) now tries to acquire the lock, but finds that it is held.
==第二个线程（线程 1）现在试图获取锁，但发现它被持有了。==

Thus, it begins to spin.
==因此，它开始自旋。==

And spin.
==并且自旋。==

Then it spins some more.
==然后它再自旋一会儿。==

And finally, a timer interrupt goes off, thread 0 is run again, which releases the lock, and finally (the next time it runs, say), thread 1 won't have to spin so much and will be able to acquire the lock.
==最后，定时器中断触发，线程 0 再次运行，释放锁，最终（比如说，下次运行时），线程 1 就不必自旋那么多了，并且能够获取锁。==

Figure 28.7: Ticket Locks
==图 28.7：票据锁==

Thus, any time a thread gets caught spinning in a situation like this, it wastes an entire time slice doing nothing but checking a value that isn't going to change!
==因此，任何时候线程在这种情况下被卡住自旋，它都会浪费整个时间片，除了检查一个不会改变的值之外什么都不做！==

The problem gets worse with N threads contending for a lock; N-1 time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock.
==当 N 个线程争夺锁时，问题会变得更糟；N-1 个时间片可能会以类似的方式被浪费，仅仅是自旋并等待单个线程释放锁。==

And thus, our next problem:
==因此，我们的下一个问题：==

THE CRUX: HOW TO AVOID SPINNING
==关键问题：如何避免自旋==

How can we develop a lock that doesn't needlessly waste time spinning on the CPU?
==我们如何开发一个不会在 CPU 上无谓地浪费时间自旋的锁？==

Hardware support alone cannot solve the problem.
==仅靠硬件支持无法解决问题。==

We'll need OS support too!
==我们也需要操作系统支持！==

Let's now figure out just how that might work.
==现在让我们弄清楚那可能是如何工作的。==

28.13 A Simple Approach: Just Yield, Baby
==28.13 一个简单的方法：让出就好，宝贝==

Hardware support got us pretty far: working locks, and even (as with the case of the ticket lock) fairness in lock acquisition.
==硬件支持让我们走了很远：有效的锁，甚至（就像票据锁的情况一样）锁获取的公平性。==

However, we still have a problem: what to do when a context switch occurs in a critical section, and threads start to spin endlessly, waiting for the interrupted (lock-holding) thread to be run again?
==然而，我们仍然有一个问题：当临界区发生上下文切换，线程开始无休止地自旋，等待被中断的（持有锁的）线程再次运行时，该怎么办？==

Our first try is a simple and friendly approach: when you are going to spin, instead give up the CPU to another thread.
==我们的第一次尝试是一个简单而友好的方法：当你要自旋时，将 CPU 让给另一个线程。==

As Al Davis might say, "just yield, baby!"
==正如 Al Davis 可能说的那样，“让出就好，宝贝！”==

Figure 28.8 shows the approach.
==图 28.8 展示了这种方法。==

Figure 28.8: Lock With Test-and-set And Yield
==图 28.8：带有测试并设置和 Yield 的锁==

In this approach, we assume an operating system primitive yield() which a thread can call when it wants to give up the CPU and let another thread run.
==在这种方法中，我们假设有一个操作系统原语 yield()，线程可以在想要放弃 CPU 并让另一个线程运行时调用它。==

A thread can be in one of three states (running, ready, or blocked); yield is simply a system call that moves the caller from the running state to the ready state, and thus promotes another thread to running.
==线程可以处于三种状态之一（运行、就绪或阻塞）；yield 只是一个系统调用，它将调用者从运行状态移动到就绪状态，从而提升另一个线程来运行。==

Thus, the yielding thread essentially deschedules itself.
==因此，让出的线程本质上是取消了自己的调度。==

Think about the example with two threads on one CPU; in this case, our yield-based approach works quite well.
==考虑一个 CPU 上有两个线程的例子；在这种情况下，我们基于 yield 的方法效果很好。==

If a thread happens to call lock() and find a lock held, it will simply yield the CPU, and thus the other thread will run and finish its critical section.
==如果一个线程恰好调用 lock() 并发现锁被持有，它将简单地让出 CPU，从而另一个线程将运行并完成其临界区。==

In this simple case, the yielding approach works well.
==在这个简单的例子中，yield 方法效果很好。==

Let us now consider the case where there are many threads (say 100) contending for a lock repeatedly.
==现在让我们考虑有许多线程（比如 100 个）反复争夺锁的情况。==

In this case, if one thread acquires the lock and is preempted before releasing it, the other 99 will each call lock(), find the lock held, and yield the CPU.
==在这种情况下，如果一个线程获取了锁并在释放之前被抢占，其他 99 个线程将各自调用 lock()，发现锁被持有，并让出 CPU。==

Assuming some kind of round-robin scheduler, each of the 99 will execute this run-and-yield pattern before the thread holding the lock gets to run again.
==假设某种轮询调度程序，在持有锁的线程再次运行之前，这 99 个线程中的每一个都将执行这种运行并让出的模式。==

While better than our spinning approach (which would waste 99 time slices spinning), this approach is still costly; the cost of a context switch can be substantial, and there is thus plenty of waste.
==虽然比我们的自旋方法（浪费 99 个时间片自旋）要好，但这种方法仍然昂贵；上下文切换的成本可能很大，因此存在大量浪费。==

Worse, this approach does not address starvation.
==更糟糕的是，这种方法不能解决饥饿问题。==

A thread may get caught in an endless yield loop while other threads repeatedly enter and exit the critical section.
==一个线程可能会陷入无休止的 yield 循环，而其他线程则反复进入和退出临界区。==

We clearly will need an approach that addresses starvation directly.
==我们显然需要一种直接解决饥饿的方法。==

28.14 Using Queues: Sleeping Instead Of Spinning
==28.14 使用队列：睡眠代替自旋==

The real problem with some previous approaches (other than the ticket lock) is that they leave too much to chance.
==以前的一些方法（除了票据锁）的真正问题在于它们留给运气的成分太多了。==

The scheduler determines which thread runs next; if the scheduler makes a bad choice, a thread that runs must either spin waiting for the lock (our first approach), or yield the CPU immediately (our second approach).
==调度程序决定下一个运行哪个线程；如果调度程序做出了糟糕的选择，运行的线程要么必须自旋等待锁（我们的第一种方法），要么立即让出 CPU（我们的第二种方法）。==

Either way, there is potential for waste and no prevention of starvation.
==无论哪种方式，都存在浪费的潜力，并且无法防止饥饿。==

Figure 28.9: Lock With Queues, Test-and-set, Yield, And Wakeup
==图 28.9：带有队列、测试并设置、Yield 和唤醒的锁==

Thus, we must explicitly exert some control over which thread next gets to acquire the lock after the current holder releases it.
==因此，我们必须显式地控制在当前持有者释放锁之后，下一个由哪个线程获取锁。==

To do this, we will need a little more OS support, as well as a queue to keep track of which threads are waiting to acquire the lock.
==为此，我们需要更多的操作系统支持，以及一个队列来跟踪哪些线程正在等待获取锁。==

For simplicity, we will use the support provided by Solaris, in terms of two calls: park() to put a calling thread to sleep, and unpark(threadID) to wake a particular thread as designated by threadID.
==为了简单起见，我们将使用 Solaris 提供的支持，即两个调用：park() 使调用线程进入睡眠状态，以及 unpark(threadID) 唤醒由 threadID 指定的特定线程。==

These two routines can be used in tandem to build a lock that puts a caller to sleep if it tries to acquire a held lock and wakes it when the lock is free.
==这两个例程可以串联使用来构建锁，如果调用者试图获取被持有的锁，它将使调用者进入睡眠状态，并在锁空闲时将其唤醒。==

Let's look at the code in Figure 28.9 to understand one possible use of such primitives.
==让我们看看图 28.9 中的代码，以了解此类原语的一种可能用法。==

LOCKS

==锁==

  

ASIDE: MORE REASON TO AVOID SPINNING: PRIORITY INVERSION

==旁注：避免自旋的更多理由：优先级反转==

  

One good reason to avoid spin locks is performance: as described in the main text, if a thread is interrupted while holding a lock, other threads that use spin locks will spend a large amount of CPU time just waiting for the lock to become available.

==避免使用自旋锁的一个很好的理由是性能：正如正文中所述，如果一个线程在持有锁时被中断，其他使用自旋锁的线程将花费大量的 CPU 时间仅仅为了等待锁变得可用。==

  

However, it turns out there is another interesting reason to avoid spin locks on some systems: correctness.

==然而，事实证明，在某些系统上避免使用自旋锁还有另一个有趣的原因：正确性。==

  

The problem to be wary of is known as priority inversion, which unfortunately is an intergalactic scourge, occurring on Earth [M15] and Mars [R97]!

==需要警惕的问题被称为优先级反转，不幸的是，这是一个星际祸害，在地球 [M15] 和火星 [R97] 上都发生过！==

  

Let's assume there are two threads in a system.

==让我们假设系统中有两个线程。==

  

Thread 2 (T2) has a high scheduling priority, and Thread 1 (T1) has lower priority.

==线程 2 (T2) 具有较高的调度优先级，而线程 1 (T1) 具有较低的优先级。==

  

In this example, let's assume that the CPU scheduler will always run T2 over T1, if indeed both are runnable.

==在这个例子中，让我们假设如果两个线程都可以运行，CPU 调度器总是会优先运行 T2 而不是 T1。==

  

T1 only runs when T2 is not able to do so (e.g., when T2 is blocked on I/O).

==只有当 T2 无法运行时（例如，当 T2 因 I/O 而阻塞时），T1 才会运行。==

  

Now, the problem.

==现在，问题来了。==

  

Assume T2 is blocked for some reason.

==假设 T2 因为某种原因被阻塞了。==

  

So T1 runs, grabs a spin lock, and enters a critical section.

==于是 T1 运行，获取了一个自旋锁，并进入了临界区。==

  

T2 now becomes unblocked (perhaps because an I/O completed), and the CPU scheduler immediately schedules it (thus descheduling T1).

==现在 T2 解除阻塞（可能是因为 I/O 完成了），CPU 调度器立即调度它（从而取消了 T1 的调度）。==

  

T2 now tries to acquire the lock, and because it can't (T1 holds the lock), it just keeps spinning.

==T2 现在尝试获取锁，但因为无法获取（T1 持有该锁），它只能一直自旋。==

  

Because the lock is a spin lock, T2 spins forever, and the system is hung.

==因为该锁是自旋锁，T2 会永远自旋下去，导致系统挂起。==

  

Just avoiding the use of spin locks, unfortunately, does not avoid the problem of inversion (alas).

==遗憾的是，仅仅避免使用自旋锁并不能避免反转问题（唉）。==

  

Imagine three threads, T1, T2, and T3, with T3 at the highest priority, and T1 the lowest.

==想象有三个线程，T1、T2 和 T3，其中 T3 优先级最高，T1 最低。==

  

Imagine now that T1 grabs a lock.

==现在想象 T1 获取了一个锁。==

  

T3 then starts, and because it is higher priority than T1, runs immediately (preempting T1).

==然后 T3 启动，因为它的优先级高于 T1，所以立即运行（抢占了 T1）。==

  

T3 tries to acquire the lock that T1 holds, but gets stuck waiting, because T1 still holds it.

==T3 试图获取 T1 持有的锁，但因 T1 仍然持有该锁而被卡住等待。==

  

If T2 starts to run, it will have higher priority than T1, and thus it will run.

==如果 T2 开始运行，它的优先级高于 T1，因此它会运行。==

  

T3, which is higher priority than T2, is stuck waiting for T1, which may never run now that T2 is running.

==T3 的优先级虽然高于 T2，但它被卡住等待 T1，而由于 T2 正在运行，T1 可能永远无法运行。==

  

Isn't it sad that the mighty T3 can't run, while lowly T2 controls the CPU?

==强大的 T3 无法运行，而低微的 T2 却控制着 CPU，这难道不令人悲哀吗？==

  

Having high priority just ain't what it used to be.

==拥有高优先级已经今非昔比了。==

  

You can address the priority inversion problem in a number of ways.

==你可以通过多种方式解决优先级反转问题。==

  

In the specific case where spin locks cause the problem, you can avoid using spin locks (described more below).

==在自旋锁导致该问题的特定情况下，你可以避免使用自旋锁（下文将详细介绍）。==

  

More generally, a higher-priority thread waiting for a lower-priority thread can temporarily boost the lower thread's priority, thus enabling it to run and overcoming the inversion, a technique known as priority inheritance.

==更一般地说，等待低优先级线程的高优先级线程可以暂时提升低优先级线程的优先级，从而使其能够运行并克服反转，这种技术被称为优先级继承。==

  

A last solution is simplest: ensure all threads have the same priority.

==最后一个解决方案是最简单的：确保所有线程具有相同的优先级。==

  

We do a couple of interesting things in this example.

==在这个例子中，我们做了一些有趣的事情。==

  

First, we combine the old test-and-set idea with an explicit queue of lock waiters to make a more efficient lock.

==首先，我们将旧的测试并设置（test-and-set）思想与显式的锁等待队列相结合，以制作一个更高效的锁。==

  

Second, we use a queue to help control who gets the lock next and thus avoid starvation.

==其次，我们使用队列来帮助控制谁下一个获得锁，从而避免饥饿。==

  

You might notice how the guard is used (Figure 28.9, page 16), basically as a spin-lock around the flag and queue manipulations the lock is using.

==你可能会注意到 guard 是如何使用的（第 16 页图 28.9），它基本上是作为围绕锁所使用的标志和队列操作的自旋锁。==

  

This approach thus doesn't avoid spin-waiting entirely; a thread might be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again.

==因此，这种方法并没有完全避免自旋等待；一个线程可能在获取或释放锁时被中断，从而导致其他线程自旋等待该线程再次运行。==

  

However, the time spent spinning is quite limited (just a few instructions inside the lock and unlock code, instead of the user-defined critical section), and thus this approach may be reasonable.

==然而，自旋花费的时间非常有限（只是锁和解锁代码中的几条指令，而不是用户定义的临界区），因此这种方法可能是合理的。==

  

You might also observe that in lock(), when a thread can not acquire the lock (it is already held), we are careful to add ourselves to a queue (by calling the gettid() function to get the thread ID of the current thread), set guard to 0, and yield the CPU.

==你可能还会观察到，在 lock() 中，当一个线程无法获取锁（锁已被持有）时，我们会小心地将自己添加到队列中（通过调用 gettid() 函数获取当前线程的 ID），将 guard 设置为 0，并让出 CPU。==

  

A question for the reader: What would happen if the release of the guard lock came after the park(), and not before?

==给读者提个问题：如果 guard 锁的释放在 park() 之后而不是之前，会发生什么？==

  

Hint: something bad.

==提示：会发生坏事。==

  

You might further detect that the flag does not get set back to 0 when another thread gets woken up.

==你可能进一步发现，当另一个线程被唤醒时，标志并没有被重新设置为 0。==

  

Why is this?

==这是为什么呢？==

  

Well, it is not an error, but rather a necessity!

==嗯，这不是错误，而是必须的！==

  

When a thread is woken up, it will be as if it is returning from park(); however, it does not hold the guard at that point in the code and thus cannot even try to set the flag to 1.

==当一个线程被唤醒时，它就像是从 park() 返回一样；然而，在代码的那个点它并不持有 guard，因此甚至不能尝试将标志设置为 1。==

  

Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it; flag is not set to 0 in-between.

==因此，我们要么直接将锁从释放锁的线程传递给下一个获取它的线程；在此期间 flag 不会被设置为 0。==

  

Finally, you might notice the perceived race condition in the solution, just before the call to park().

==最后，你可能会注意到解决方案中存在的感知竞态条件，就在调用 park() 之前。==

  

With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is no longer held.

==如果时机恰好不对，一个线程将准备 park（停泊），因为它假设自己应该休眠直到锁不再被持有。==

  

A switch at that time to another thread (say, a thread holding the lock) could lead to trouble, for example, if that thread then released the lock.

==如果在那个时间切换到另一个线程（比如，持有锁的线程），可能会导致麻烦，例如，如果那个线程随后释放了锁。==

  

The subsequent park by the first thread would then sleep forever (potentially), a problem sometimes called the wakeup/waiting race.

==第一个线程随后的 park 操作将导致它永远休眠（可能），这个问题有时被称为唤醒/等待竞争。==

  

Solaris solves this problem by adding a third system call: setpark().

==Solaris 通过添加第三个系统调用来解决这个问题：setpark()。==

  

By calling this routine, a thread can indicate it is about to park.

==通过调用这个例程，线程可以表明它即将 park。==

  

If it then happens to be interrupted and another thread calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping.

==如果它随后恰好被中断，并且另一个线程在 park 实际被调用之前调用了 unpark，那么随后的 park 将立即返回而不是休眠。==

  

The code modification, inside of lock(), is quite small.

==lock() 内部的代码修改非常小。==

  

A different solution could pass the guard into the kernel.

==另一种解决方案是将 guard 传递给内核。==

  

In that case, the kernel could take precautions to atomically release the lock and dequeue the running thread.

==在这种情况下，内核可以采取预防措施来原子地释放锁并将运行线程出队。==

  

28.15 Different OS, Different Support

==28.15 不同的操作系统，不同的支持==

  

We have thus far seen one type of support that an OS can provide in order to build a more efficient lock in a thread library.

==到目前为止，我们已经看到了一种操作系统可以提供的支持，以便在线程库中构建更高效的锁。==

  

Other OS's provide similar support; the details vary.

==其他操作系统提供类似的支持；细节各不相同。==

  

For example, Linux provides a futex which is similar to the Solaris interface but provides more in-kernel functionality.

==例如，Linux 提供了 futex，它类似于 Solaris 接口，但提供了更多的内核功能。==

  

Specifically, each futex has associated with it a specific physical memory location, as well as a per-futex in-kernel queue.

==具体来说，每个 futex 都有一个特定的物理内存位置与之关联，以及一个每个 futex 独有的内核队列。==

  

Callers can use futex calls (described below) to sleep and wake as need be.

==调用者可以使用 futex 调用（如下所述）根据需要进行休眠和唤醒。==

  

Specifically, two calls are available.

==具体来说，有两个调用可用。==

  

The call to futex_wait(address, expected) puts the calling thread to sleep, assuming the value at the address address is equal to expected.

==调用 futex_wait(address, expected) 会使调用线程休眠，前提是地址 address 处的值等于 expected。==

  

If it is not equal, the call returns immediately.

==如果不相等，调用将立即返回。==

  

The call to the routine futex_wake(address) wakes one thread that is waiting on the queue.

==调用例程 futex_wake(address) 会唤醒队列中等待的一个线程。==

  

The usage of these calls in a Linux mutex is shown in Figure 28.10 (page 19).

==图 28.10（第 19 页）展示了 Linux 互斥锁中这些调用的用法。==

  

This code snippet from lowlevellock.h in the nptl library (part of the gnu libc library) [L09] is interesting for a few reasons.

==这段来自 nptl 库（gnu libc 库的一部分）[L09] 中 lowlevellock.h 的代码片段之所以有趣，有几个原因。==

  

First, it uses a single integer to track both whether the lock is held or not (the high bit of the integer) and the number of waiters on the lock (all the other bits).

==首先，它使用一个整数来同时跟踪锁是否被持有（整数的高位）以及锁上的等待者数量（所有其他位）。==

  

Thus, if the lock is negative, it is held (because the high bit is set and that bit determines the sign of the integer).

==因此，如果锁是负数，则表示它被持有（因为高位被设置，而该位决定了整数的符号）。==

  

Second, the code snippet shows how to optimize for the common case, specifically when there is no contention for the lock; with only one thread acquiring and releasing a lock, very little work is done (the atomic bit test-and-set to lock and an atomic add to release the lock).

==其次，该代码片段展示了如何针对常见情况进行优化，特别是在没有锁争用的情况下；当只有一个线程获取和释放锁时，所做的工作非常少（用于加锁的原子位测试并设置，以及用于释放锁的原子加法）。==

  

See if you can puzzle through the rest of this "real-world" lock to understand how it works.

==看看你能不能弄清楚这个“现实世界”锁的其余部分，以理解它是如何工作的。==

  

Do it and become a master of Linux locking, or at least somebody who listens when a book tells you to do something.

==去做吧，成为 Linux 锁的大师，或者至少成为一个听从书本建议的人。==

  

28.16 Two-Phase Locks

==28.16 两阶段锁==

  

One final note: the Linux approach has the flavor of an old approach that has been used on and off for years, going at least as far back to Dahm Locks in the early 1960's [M82], and is now referred to as a two-phase lock.

==最后一点：Linux 的方法具有一种古老方法的韵味，这种方法多年来一直被断断续续地使用，至少可以追溯到 20 世纪 60 年代初的 Dahm 锁 [M82]，现在被称为两阶段锁。==

  

A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released.

==两阶段锁意识到自旋可能是有用的，特别是如果锁即将被释放。==

  

So in the first phase, the lock spins for a while, hoping that it can acquire the lock.

==因此在第一阶段，锁会自旋一段时间，希望能够获取锁。==

  

However, if the lock is not acquired during the first spin phase, a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later.

==然而，如果在第一个自旋阶段没有获取到锁，就会进入第二阶段，调用者被置于休眠状态，只有当锁稍后变为空闲时才会被唤醒。==

  

The Linux lock above is a form of such a lock, but it only spins once; a generalization of this could spin in a loop for a fixed amount of time before using futex support to sleep.

==上面的 Linux 锁就是这种锁的一种形式，但它只自旋一次；对此的一般化做法可能是先在一个循环中自旋固定的时间，然后再使用 futex 支持进行休眠。==

  

Two-phase locks are yet another instance of a hybrid approach, where combining two good ideas may indeed yield a better one.

==两阶段锁是混合方法的又一个实例，结合两个好主意确实可能会产生一个更好的主意。==

  

Of course, whether it does depends strongly on many things, including the hardware environment, number of threads, and other workload details.

==当然，是否如此在很大程度上取决于许多事情，包括硬件环境、线程数量和其他工作负载细节。==

  

As always, making a single general-purpose lock, good for all possible use cases, is quite a challenge.

==一如既往，制造一个适用于所有可能用例的单一通用锁是一个巨大的挑战。==

  

28.17 Summary

==28.17 总结==

  

The above approach shows how real locks are built these days: some hardware support (in the form of a more powerful instruction) plus some operating system support (e.g., in the form of park() and unpark() primitives on Solaris, or futex on Linux).

==上述方法展示了如今真正的锁是如何构建的：一些硬件支持（以更强大的指令形式）加上一些操作系统支持（例如，Solaris 上的 park() 和 unpark() 原语，或 Linux 上的 futex）。==

  

Of course, the details differ, and the exact code to perform such locking is usually highly tuned.

==当然，细节各不相同，执行这种锁定的具体代码通常经过高度调优。==

  

Check out the Solaris or Linux code bases if you want to see more details; they are a fascinating read [L09, S09].

==如果你想了解更多细节，请查看 Solaris 或 Linux 代码库；它们读起来非常有趣 [L09, S09]。==

  

Also see David et al.'s excellent work for a comparison of locking strategies on modern multiprocessors [D+13].

==另外，请参阅 David 等人的出色工作，了解现代多处理器上锁定策略的比较 [D+13]。==

  

Like buy a print copy of OSTEP!

==比如买一本 OSTEP 的纸质版！==

  

Even though the book is available for free online, wouldn't you just love a hard cover for your desk?

==尽管这本书可以在线免费获取，难道你不想在桌上放一本精装书吗？==

  

Or, better yet, ten copies to share with friends and family?

==或者，更好的是，买十本与朋友和家人分享？==

  

And maybe one extra copy to throw at an enemy?

==也许还可以多买一本扔向敌人？==

  

(the book is heavy, and thus chucking it is surprisingly effective)

==（这本书很重，所以扔起来效果出奇地好）==

  

References

==参考文献==

  

[D91] "Just Win, Baby: Al Davis and His Raiders" by Glenn Dickey. Harcourt, 1991.

==[D91] "Just Win, Baby: Al Davis and His Raiders" 作者 Glenn Dickey。Harcourt, 1991。==

  

The book about Al Davis and his famous quote.

==关于 Al Davis 和他那句名言的书。==

  

Or, we suppose, the book is more about Al Davis and the Raiders, and not so much the quote.

==或者，我们认为，这本书更多的是关于 Al Davis 和 Raiders 队，而不完全是关于那句名言。==

  

To be clear: we are not recommending this book, we just needed a citation.

==澄清一下：我们并不推荐这本书，我们只是需要一个引文。==

  

[D+13] "Everything You Always Wanted to Know about Synchronization but Were Afraid to Ask" by Tudor David, Rachid Guerraoui, Vasileios Trigonakis. SOSP '13.

==[D+13] "Everything You Always Wanted to Know about Synchronization but Were Afraid to Ask" 作者 Tudor David, Rachid Guerraoui, Vasileios Trigonakis。SOSP '13。==

  

An excellent paper comparing many different ways to build locks using hardware primitives.

==一篇比较使用硬件原语构建锁的多种不同方法的优秀论文。==

  

Great to see how many ideas work on modern hardware.

==很高兴看到有多少想法在现代硬件上行之有效。==

  

[D68] "Cooperating sequential processes" by Edsger W. Dijkstra. 1968.

==[D68] "Cooperating sequential processes" 作者 Edsger W. Dijkstra。1968。==

  

One of the early seminal papers.

==早期的开创性论文之一。==

  

Discusses how Dijkstra posed the original concurrency problem, and Dekker's solution.

==讨论了 Dijkstra 如何提出最初的并发问题，以及 Dekker 的解决方案。==

  

[H93] "MIPS R4000 Microprocessor User's Manual" by Joe Heinrich. Prentice-Hall, June 1993.

==[H93] "MIPS R4000 Microprocessor User's Manual" 作者 Joe Heinrich。Prentice-Hall, 1993 年 6 月。==

  

The old MIPS user's manual.

==旧的 MIPS 用户手册。==

  

Download it while it still exists.

==趁它还在的时候下载它。==

  

[H91] "Wait-free Synchronization" by Maurice Herlihy. ACM TOPLAS, Volume 13: 1, January 1991.

==[H91] "Wait-free Synchronization" 作者 Maurice Herlihy。ACM TOPLAS, 第 13 卷：1, 1991 年 1 月。==

  

A landmark paper introducing a different approach to building concurrent data structures.

==一篇介绍构建并发数据结构不同方法的里程碑式论文。==

  

Because of the complexity involved, some of these ideas have been slow to gain acceptance in deployment.

==由于涉及的复杂性，其中一些想法在实际部署中被接受得很慢。==

  

[L81] "Observations on the Development of an Operating System" by Hugh Lauer. SOSP '81.

==[L81] "Observations on the Development of an Operating System" 作者 Hugh Lauer。SOSP '81。==

  

A must-read retrospective about the development of the Pilot OS, an early PC operating system.

==关于 Pilot OS（早期的 PC 操作系统）开发过程的必读回顾。==

  

Fun and full of insights.

==有趣且充满见解。==

  

[L09] "glibc 2.9 (include Linux pthreads implementation)" by Many authors.

==[L09] "glibc 2.9 (include Linux pthreads implementation)" 作者多位。==

  

In particular, take a look at the nptl subdirectory where you will find most of the pthread support in Linux today.

==特别是，看看 nptl 子目录，你会发现当今 Linux 中大部分的 pthread 支持都在那里。==

  

[M82] "The Architecture of the Burroughs B5000: 20 Years Later and Still Ahead of the Times?" by A. Mayer. 1982.

==[M82] "The Architecture of the Burroughs B5000: 20 Years Later and Still Ahead of the Times?" 作者 A. Mayer。1982。==

  

"It (RDLK) is an indivisible operation which reads from and writes into a memory location."

==“它 (RDLK) 是一个不可分割的操作，它读取并写入一个内存位置。”==

  

RDLK is thus test-and-set!

==RDLK 因此就是 test-and-set（测试并设置）！==

  

Dave Dahm created spin locks ("Buzz Locks") and a two-phase lock called "Dahm Locks."

==Dave Dahm 创造了自旋锁（“Buzz Locks”）和一种称为“Dahm Locks”的两阶段锁。==

  

[M15] "OSSpinLock Is Unsafe" by J. McCall.

==[M15] "OSSpinLock Is Unsafe" 作者 J. McCall。==

  

Calling OSSpinLock on a Mac is unsafe when using threads of different priorities - you might spin forever!

==在 Mac 上使用不同优先级的线程时调用 OSSpinLock 是不安全的——你可能会永远自旋！==

  

So be careful, Mac fanatics, even your mighty system can be less than perfect.

==所以要小心，Mac 的狂热粉丝们，即使是你们强大的系统也可能并不完美。==

  

[MS91] "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors" by John M. Mellor-Crummey and M. L. Scott. ACM TOCS, 1991.

==[MS91] "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors" 作者 John M. Mellor-Crummey 和 M. L. Scott。ACM TOCS, 1991。==

  

An excellent and thorough survey on different locking algorithms.

==关于不同锁定算法的优秀而彻底的调查。==

  

However, no operating systems support is used, just fancy hardware instructions.

==然而，没有使用操作系统支持，只是使用了花哨的硬件指令。==

  

[P81] "Myths About the Mutual Exclusion Problem" by G.L. Peterson. 1981.

==[P81] "Myths About the Mutual Exclusion Problem" 作者 G.L. Peterson。1981。==

  

Peterson's algorithm introduced here.

==这里介绍了皮特森算法。==

  

[R97] "What Really Happened on Mars?" by Glenn E. Reeves.

==[R97] "What Really Happened on Mars?" 作者 Glenn E. Reeves。==

  

A description of priority inversion on Mars Pathfinder.

==关于火星探路者号上优先级反转的描述。==

  

Concurrent code correctness matters, especially in space!

==并发代码的正确性很重要，尤其是在太空中！==

  

[S05] "Guide to porting from Solaris to Linux on x86" by Ajay Sood, April 29, 2005.

==[S05] "Guide to porting from Solaris to Linux on x86" 作者 Ajay Sood，2005 年 4 月 29 日。==

  

[S09] "OpenSolaris Thread Library" by Sun.

==[S09] "OpenSolaris Thread Library" 作者 Sun。==

  

Pretty interesting, although who knows what will happen now that Oracle owns Sun.

==非常有趣，虽然谁知道现在 Oracle 拥有了 Sun 会发生什么。==

  

Thanks to Mike Swift for the pointer.

==感谢 Mike Swift 的指点。==

  

[W09] "Load-Link, Store-Conditional" by Many authors. Wikipedia.

==[W09] "Load-Link, Store-Conditional" 作者多位。维基百科。==

  

Can you believe we referenced Wikipedia?

==你能相信我们引用了维基百科吗？==

  

But, we found the information there and it felt wrong not to.

==但是，我们在那里找到了信息，如果不引用感觉不对。==

  

Further, it was useful, listing the instructions for the different architectures.

==此外，它很有用，列出了不同架构的指令。==

  

Actually Wikipedia is pretty amazing, so don't be so harsh, OK?

==实际上维基百科非常了不起，所以别那么苛刻，好吗？==

  

[WG00] "The SPARC Architecture Manual: Version 9" by D. Weaver, T. Germond. SPARC International, 2000.

==[WG00] "The SPARC Architecture Manual: Version 9" 作者 D. Weaver, T. Germond。SPARC International, 2000。==

  

Homework (Simulation)

==作业（模拟）==

  

This program, x86.py, allows you to see how different thread interleavings either cause or avoid race conditions.

==这个程序，x86.py，允许你查看不同的线程交错是如何导致或避免竞态条件的。==

  

See the README for details on how the program works and answer the questions below.

==请参阅 README 了解程序如何工作的详细信息，并回答以下问题。==

  

Questions

==问题==

  

1. Examine flag.s.

==2. 检查 flag.s。==

  

This code "implements" locking with a single memory flag.

==这段代码用单个内存标志“实现”了锁定。==

  

Can you understand the assembly?

==你能看懂汇编代码吗？==

  

2. When you run with the defaults, does flag.s work?

==3. 当你使用默认值运行时，flag.s 能工作吗？==

  

Use the -M and -R flags to trace variables and registers (and turn on to see their values).

==使用 -M 和 -R 标志来跟踪变量和寄存器（并开启以查看它们的值）。==

  

Can you predict what value will end up in flag?

==你能预测 flag 最终会是什么值吗？==

  

3. Change the value of the register bx with the a flag (e.g., -a bx=2, bx=2 if you are running just two threads).

==4. 使用 a 标志更改寄存器 bx 的值（例如，如果你只运行两个线程，则 -a bx=2, bx=2）。==

  

What does the code do?

==代码是做什么的？==

  

How does it change your answer for the question above?

==它如何改变你对上述问题的回答？==

  

4. Set bx to a high value for each thread, and then use the i flag to generate different interrupt frequencies; what values lead to a bad outcomes?

==5. 为每个线程将 bx 设置为一个高值，然后使用 i 标志生成不同的中断频率；什么值会导致糟糕的结果？==

  

Which lead to good outcomes?

==哪些会导致好的结果？==

  

5. Now let's look at the program test-and-set.s.

==6. 现在让我们看看程序 test-and-set.s。==

  

First, try to understand the code, which uses the xchg instruction to build a simple locking primitive.

==首先，尝试理解代码，它使用 xchg 指令构建了一个简单的锁定原语。==

  

How is the lock acquire written?

==获取锁是如何编写的？==

  

How about lock release?

==释放锁呢？==

  

6. Now run the code, changing the value of the interrupt interval (-i) again, and making sure to loop for a number of times.

==7. 现在运行代码，再次更改中断间隔 (-i) 的值，并确保循环多次。==

  

Does the code always work as expected?

==代码总是按预期工作吗？==

  

Does it sometimes lead to an inefficient use of the CPU?

==它有时会导致 CPU 使用效率低下吗？==

  

How could you quantify that?

==你如何量化它？==

  

7. Use the P flag to generate specific tests of the locking code.

==8. 使用 P 标志生成锁定代码的特定测试。==

  

For example, run a schedule that grabs the lock in the first thread, but then tries to acquire it in the second.

==例如，运行一个调度，在第一个线程中获取锁，然后尝试在第二个线程中获取它。==

  

Does the right thing happen?

==发生了正确的事情吗？==

  

What else should you test?

==你还应该测试什么？==

  

8. Now let's look at the code in peterson.s, which implements Peterson's algorithm (mentioned in a sidebar in the text).

==9. 现在让我们看看 peterson.s 中的代码，它实现了 Peterson 算法（在正文的侧边栏中提到）。==

  

Study the code and see if you can make sense of it.

==研究代码，看看你是否能理解它。==

  

9. Now run the code with different values of i.

==10. 现在使用不同的 i 值运行代码。==

  

What kinds of different behavior do you see?

==你看到了什么样的不同行为？==

  

Make sure to set the thread IDs appropriately (using -a bx=0, bx=1 for example) as the code assumes it.

==确保适当地设置线程 ID（例如使用 -a bx=0, bx=1），因为代码假设了这一点。==

  

10. Can you control the scheduling (with the -P flag) to "prove" that the code works?

==11. 你能控制调度（使用 -P 标志）来“证明”代码有效吗？==

  

What are the different cases you should show hold?

==你应该展示哪些不同的情况成立？==

  

Think about mutual exclusion and deadlock avoidance.

==思考一下互斥和死锁避免。==

  

11. Now study the code for the ticket lock in ticket.s.

==12. 现在研究 ticket.s 中票据锁（ticket lock）的代码。==

  

Does it match the code in the chapter?

==它与本章中的代码匹配吗？==

  

Then run with the following flags: -a bx=1000, bx=1000 (causing each thread to loop through the critical section 1000 times).

==然后使用以下标志运行：-a bx=1000, bx=1000（导致每个线程循环通过临界区 1000 次）。==

  

Watch what happens; do the threads spend much time spin-waiting for the lock?

==观察发生了什么；线程是否花费大量时间自旋等待锁？==

  

12. How does the code behave as you add more threads?

==13. 当你添加更多线程时，代码表现如何？==

14. Now examine yield.s, in which a yield instruction enables one thread to yield control of the CPU (realistically, this would be an OS primitive, but for the simplicity, we assume an instruction does the task).

==15. 现在检查 yield.s，其中 yield 指令使一个线程能够让出 CPU 的控制权（实际上，这将是一个 OS 原语，但为了简单起见，我们假设一条指令完成该任务）。==

  

Find a scenario where test-and-set.s wastes cycles spinning, but yield.s does not.

==找到一个场景，test-and-set.s 浪费周期进行自旋，但 yield.s 没有。==

  

How many instructions are saved?

==节省了多少条指令？==

  

In what scenarios do these savings arise?

==这些节省出现在什么场景中？==

  

14. Finally, examine test-and-test-and-set.s.

==15. 最后，检查 test-and-test-and-set.s。==

  

What does this lock do?

==这个锁是做什么的？==

  

What kind of savings does it introduce as compared to test-and-set.s?

==与 test-and-set.s 相比，它引入了什么样的节省？==

  

Lock-based Concurrent Data Structures

==基于锁的并发数据结构==

  

Before moving beyond locks, we'll first describe how to use locks in some common data structures.

==在超越锁之前，我们将首先描述如何在一些常见数据结构中使用锁。==

  

Adding locks to a data structure to make it usable by threads makes the structure thread safe.

==向数据结构添加锁以使其可被线程使用，会使该结构变得线程安全。==

  

Of course, exactly how such locks are added determines both the correctness and performance of the data structure.

==当然，究竟如何添加这些锁决定了数据结构的正确性和性能。==

  

And thus, our challenge:

==因此，我们的挑战是：==

  

CRUX: HOW TO ADD LOCKS TO DATA STRUCTURES

==关键：如何向数据结构添加锁==

  

When given a particular data structure, how should we add locks to it, in order to make it work correctly?

==当给定一个特定的数据结构时，我们应该如何向其添加锁，以使其正确工作？==

  

Further, how do we add locks such that the data structure yields high performance, enabling many threads to access the structure at once, i.e., concurrently?

==此外，我们如何添加锁，使数据结构产生高性能，允许许多线程同时访问该结构，即并发访问？==

  

Of course, we will be hard pressed to cover all data structures or all methods for adding concurrency, as this is a topic that has been studied for years, with (literally) thousands of research papers published about it.

==当然，我们很难涵盖所有数据结构或所有增加并发性的方法，因为这是一个已经被研究多年的主题，发表了（字面上）数千篇相关研究论文。==

  

Thus, we hope to provide a sufficient introduction to the type of thinking required, and refer you to some good sources of material for further inquiry on your own.

==因此，我们希望对所需的思维方式提供充分的介绍，并为你推荐一些好的资料来源，以便你自己进行进一步的探究。==

  

We found Moir and Shavit's survey to be a great source of information [MS04].

==我们发现 Moir 和 Shavit 的调查是一个很好的信息来源 [MS04]。==

  

29.1 Concurrent Counters

==29.1 并发计数器==

  

One of the simplest data structures is a counter.

==最简单的数据结构之一是计数器。==

  

It is a structure that is commonly used and has a simple interface.

==它是一种常用的结构，并且具有简单的接口。==

  

We define a simple non-concurrent counter in Figure 29.1.

==我们在图 29.1 中定义了一个简单的非并发计数器。==

  

Simple But Not Scalable

==简单但不可扩展==

  

As you can see, the non-synchronized counter is a trivial data structure, requiring a tiny amount of code to implement.

==正如你所见，非同步计数器是一个微不足道的数据结构，只需要极少量的代码即可实现。==

  

We now have our next challenge: how can we make this code thread safe?

==我们现在面临下一个挑战：我们如何使这段代码线程安全？==

  

Figure 29.2 shows how we do so.

==图 29.2 展示了我们是如何做到的。==

  

LOCK-BASED CONCURRENT DATA STRUCTURES

==基于锁的并发数据结构==

  

Figure 29.1: A Counter Without Locks

==图 29.1：一个无锁计数器==

  

This concurrent counter is simple and works correctly.

==这个并发计数器很简单，并且工作正确。==

  

In fact, it follows a design pattern common to the simplest and most basic concurrent data structures: it simply adds a single lock, which is acquired when calling a routine that manipulates the data structure, and is released when returning from the call.

==事实上，它遵循了最简单和最基本的并发数据结构常见的设计模式：它只是简单地添加了一个锁，当调用操作数据结构的例程时获取该锁，并在从调用返回时释放该锁。==

  

In this manner, it is similar to a data structure built with monitors [BH73], where locks are acquired and released automatically as you call and return from object methods.

==这种方式类似于使用监视器（monitors）构建的数据结构 [BH73]，在监视器中，当你调用对象方法和从对象方法返回时，锁会自动获取和释放。==

  

At this point, you have a working concurrent data structure.

==至此，你已经拥有了一个可工作的并发数据结构。==

  

The problem you might have is performance.

==你可能遇到的问题是性能。==

  

If your data structure is too slow, you'll have to do more than just add a single lock; such optimizations, if needed, are thus the topic of the rest of the chapter.

==如果你的数据结构太慢，你就不仅仅是添加一个锁那么简单了；因此，如果需要的话，此类优化将是本章其余部分的主题。==

  

Note that if the data structure is not too slow, you are done!

==请注意，如果数据结构不太慢，你就大功告成了！==

  

No need to do something fancy if something simple will work.

==如果简单的东西管用，就不需要做花哨的事情。==

  

To understand the performance costs of the simple approach, we run a benchmark in which each thread updates a single shared counter a fixed number of times; we then vary the number of threads.

==为了理解简单方法的性能成本，我们运行一个基准测试，其中每个 thread 将单个共享计数器更新固定次数；然后我们改变线程的数量。==

  

Figure 29.5 shows the total time taken, with one to four threads active; each thread updates the counter one million times.

==图 29.5 显示了总耗时，有 1 到 4 个活跃线程；每个线程将计数器更新 100 万次。==

  

This experiment was run upon an iMac with four Intel 2.7 GHz i5 CPUs; with more CPUs active, we hope to get more total work done per unit time.

==这个实验是在一台配备四个 Intel 2.7 GHz i5 CPU 的 iMac 上运行的；随着更多 CPU 处于活动状态，我们希望单位时间内完成的总工作量更多。==

  

From the top line in the figure (labeled 'Precise'), you can see that the performance of the synchronized counter scales poorly.

==从图中的顶线（标记为“Precise”，精确）可以看出，同步计数器的性能扩展性很差。==

  

Whereas a single thread can complete the million counter updates in a tiny amount of time (roughly 0.03 seconds), having two threads each update the counter one million times concurrently leads to a massive slowdown (taking over 5 seconds!).

==虽然单个线程可以在极短的时间内（大约 0.03 秒）完成 100 万次计数器更新，但让两个线程并发地各更新计数器 100 万次会导致巨大的减速（耗时超过 5 秒！）。==

  

It only gets worse with more threads.

==随着线程增多，情况只会变得更糟。==

  

Figure 29.2: A Counter With Locks

==图 29.2：带锁的计数器==

  

Ideally, you'd like to see the threads complete just as quickly on multiple processors as the single thread does on one.

==理想情况下，你希望看到线程在多处理器上完成任务的速度与单个线程在一个处理器上完成任务的速度一样快。==

  

Achieving this end is called perfect scaling; even though more work is done, it is done in parallel, and hence the time taken to complete the task is not increased.

==达到这个目标被称为完美扩展（perfect scaling）；即使完成了更多的工作，但因为是并行完成的，所以完成任务所需的时间并没有增加。==

  

Scalable Counting

==可扩展计数==

  

Amazingly, researchers have studied how to build more scalable counters for years [MS04].

==令人惊讶的是，研究人员多年来一直在研究如何构建更可扩展的计数器 [MS04]。==

  

Even more amazing is the fact that scalable counters matter, as recent work in operating system performance analysis has shown [B+10]; without scalable counting, some workloads running on Linux suffer from serious scalability problems on multicore machines.

==更令人惊讶的是，可扩展计数器确实很重要，正如最近在操作系统性能分析方面的工作所表明的那样 [B+10]；如果没有可扩展计数，Linux 上运行的某些工作负载在多核机器上会遭受严重的扩展性问题。==

  

Many techniques have been developed to attack this problem.

==已经开发了许多技术来解决这个问题。==

  

We'll describe one approach known as an approximate counter [C06].

==我们将描述一种称为近似计数器（approximate counter）的方法 [C06]。==

  

The approximate counter works by representing a single logical counter via numerous local physical counters, one per CPU core, as well as a single global counter.

==近似计数器通过使用许多本地物理计数器（每个 CPU 核心一个）以及一个全局计数器来表示单个逻辑计数器。==

  

Specifically, on a machine with four CPUs, there are four local counters and one global one.

==具体来说，在一台有四个 CPU 的机器上，有四个本地计数器和一个全局计数器。==

  

In addition to these counters, there are also locks: one for each local counter, and one for the global counter.

==除了这些计数器外，还有锁：每个本地计数器一个，全局计数器一个。==

  

The basic idea of approximate counting is as follows.

==近似计数的基本思想如下。==

  

When a thread running on a given core wishes to increment the counter, it increments its local counter; access to this local counter is synchronized via the corresponding local lock.

==当运行在给定核心上的线程希望增加计数器时，它会增加其本地计数器；对该本地计数器的访问通过相应的本地锁进行同步。==

  

Because each CPU has its own local counter, threads across CPUs can update local counters without contention, and thus updates to the counter are scalable.

==因为每个 CPU 都有自己的本地计数器，跨 CPU 的线程可以无争用地更新本地计数器，因此计数器的更新是可扩展的。==

  

However, to keep the global counter up to date (in case a thread wishes to read its value), the local values are periodically transferred to the global counter, by acquiring the global lock and incrementing it by the local counter's value; the local counter is then reset to zero.

==然而，为了保持全局计数器最新（以防线程希望读取其值），本地值会定期传输到全局计数器，方法是获取全局锁并将其增加本地计数器的值；然后将本地计数器重置为零。==

  

How often this local-to-global transfer occurs is determined by a threshold S.

==这种从本地到全局的传输发生的频率由阈值 S 决定。==

  

The smaller S is, the more the counter behaves like the non-scalable counter above; the bigger S is, the more scalable the counter, but the further off the global value might be from the actual count.

==S 越小，计数器的行为就越像上面的不可扩展计数器；S 越大，计数器的扩展性越好，但全局值与实际计数的偏差可能就越大。==

  

One could simply acquire all the local locks and the global lock (in a specified order, to avoid deadlock) to get an exact value, but that is not scalable.

==人们可以简单地获取所有本地锁和全局锁（按照指定顺序，以避免死锁）来获得精确值，但这不可扩展。==

  

To make this clear, let's look at an example (Figure 29.3).

==为了清楚起见，让我们看一个例子（图 29.3）。==

  

In this example, the threshold S is set to 5, and there are threads on each of four CPUs updating their local counters L1...L4.

==在这个例子中，阈值 S 设置为 5，并且在四个 CPU 的每一个上都有线程更新其本地计数器 L1...L4。==

  

The global counter value (G) is also shown in the trace, with time increasing downward.

==全局计数器值 (G) 也显示在跟踪中，时间向下增加。==

  

At each time step, a local counter may be incremented; if the local value reaches the threshold S, the local value is transferred to the global counter and the local counter is reset.

==在每个时间步，本地计数器可能会增加；如果本地值达到阈值 S，本地值将被传输到全局计数器，并且本地计数器被重置。==

  

The lower line in Figure 29.5 (labeled 'Approximate', on page 6) shows the performance of approximate counters with a threshold S of 1024.

==图 29.5 中的下线（标记为“Approximate”，近似，在第 6 页）显示了阈值 S 为 1024 的近似计数器的性能。==

  

Performance is excellent; the time taken to update the counter four million times on four processors is hardly higher than the time taken to update it one million times on one processor.

==性能非常出色；在四个处理器上更新计数器 400 万次所花费的时间几乎不高于在一个处理器上更新它 100 万次所花费的时间。==

  

We need the local locks because we assume there may be more than one thread on each core.

==我们需要本地锁，因为我们假设每个核心上可能有多个线程。==

  

If, instead, only one thread ran on each core, no local lock would be needed.

==如果每个核心上只有一个线程运行，则不需要本地锁。==

  

Figure 29.4: Approximate Counter Implementation

==图 29.4：近似计数器实现==

  

Figure 29.5: Performance of Traditional vs. Approximate Counters

==图 29.5：传统计数器与近似计数器的性能对比==

  

Figure 29.6: Scaling Approximate Counters

==图 29.6：近似计数器的扩展性==

  

Figure 29.6 shows the importance of the threshold value S, with four threads each incrementing the counter 1 million times on four CPUs.

==图 29.6 显示了阈值 S 的重要性，四个线程在四个 CPU 上各将计数器增加 100 万次。==

  

If S is low, performance is poor (but the global count is always quite accurate); if S is high, performance is excellent, but the global count lags (by at most the number of CPUs multiplied by S).

==如果 S 较低，性能较差（但全局计数总是相当准确）；如果 S 较高，性能极佳，但全局计数会滞后（最多滞后 CPU 数量乘以 S）。==

  

This accuracy/performance trade-off is what approximate counters enable.

==这种准确性与性能的权衡正是近似计数器所能实现的。==

  

A rough version of an approximate counter is found in Figure 29.4 (page 5).

==近似计数器的一个粗略版本见图 29.4（第 5 页）。==

  

Read it, or better yet, run it yourself in some experiments to better understand how it works.

==阅读它，或者更好的是，自己在一些实验中运行它，以更好地理解它是如何工作的。==

  

TIP: MORE CONCURRENCY ISN'T NECESSARILY FASTER

==提示：更多的并发并不一定更快==

  

If the scheme you design adds a lot of overhead (for example, by acquiring and releasing locks frequently, instead of once), the fact that it is more concurrent may not be important.

==如果你设计的方案增加了大量的开销（例如，通过频繁地获取和释放锁，而不是一次性完成），那么它更具并发性这一事实可能并不重要。==

  

Simple schemes tend to work well, especially if they use costly routines rarely.

==简单的方案往往效果很好，特别是如果它们很少使用昂贵的例程。==

  

Adding more locks and complexity can be your downfall.

==增加更多的锁和复杂性可能会导致你的失败。==

  

All of that said, there is one way to really know: build both alternatives (simple but less concurrent, and complex but more concurrent) and measure how they do.

==综上所述，只有一种方法可以真正知道结果：构建两种方案（简单但并发性较低，以及复杂但并发性较高）并测量它们的表现。==

  

In the end, you can't cheat on performance; your idea is either faster, or it isn't.

==归根结底，你无法在性能上作弊；你的想法要么更快，要么不快。==

  

29.2 Concurrent Linked Lists

==29.2 并发链表==

  

We next examine a more complicated structure, the linked list.

==接下来我们检查一个更复杂的结构，链表。==

  

Let's start with a basic approach once again.

==让我们再次从基本方法开始。==

  

For simplicity, we'll omit some of the obvious routines that such a list would have and just focus on concurrent insert and lookup; we'll leave it to the reader to think about delete, etc.

==为了简单起见，我们将省略此类列表可能具有的一些显而易见的例程，仅关注并发插入和查找；我们将删除等操作留给读者思考。==

  

Figure 29.7 shows the code for this rudimentary data structure.

==图 29.7 显示了这个基本数据结构的代码。==

  

As you can see in the code, the code simply acquires a lock in the insert routine upon entry, and releases it upon exit.

==正如你在代码中看到的，代码只是简单地在进入插入例程时获取锁，并在退出时释放它。==

  

One small tricky issue arises if malloc() happens to fail (a rare case); in this case, the code must also release the lock before failing the insert.

==如果 malloc() 恰好失败（一种罕见情况），会出现一个小而棘手的问题；在这种情况下，代码还必须在插入失败之前释放锁。==

  

This kind of exceptional control flow has been shown to be quite error prone; a recent study of Linux kernel patches found that a huge fraction of bugs (nearly 40%) are found on such rarely-taken code paths (indeed, this observation sparked some of our own research, in which we removed all memory-failing paths from a Linux file system, resulting in a more robust system [S+11]).

==这种异常控制流已被证明非常容易出错；最近对 Linux 内核补丁的一项研究发现，很大一部分错误（近 40%）是在这种很少执行的代码路径上发现的（确实，这一观察引发了我们自己的一些研究，我们在研究中从 Linux 文件系统中移除了所有内存失败路径，从而产生了一个更健壮的系统 [S+11]）。==

  

Thus, a challenge: can we rewrite the insert and lookup routines to remain correct under concurrent insert but avoid the case where the failure path also requires us to add the call to unlock?

==因此，一个挑战是：我们能否重写插入和查找例程，以便在并发插入下保持正确，但避免失败路径也要求我们添加解锁调用的情况？==

  

The answer, in this case, is yes.

==在这种情况下，答案是肯定的。==

  

Specifically, we can rearrange the code a bit so that the lock and release only surround the actual critical section in the insert code, and that a common exit path is used in the lookup code.

==具体来说，我们可以稍微重新排列代码，使得锁和释放仅包围插入代码中的实际临界区，并且在查找代码中使用通用的退出路径。==

  

The former works because part of the insert actually need not be locked; assuming that malloc() itself is thread-safe, each thread can call into it without worry of race conditions or other concurrency bugs.

==前者之所以有效，是因为插入的一部分实际上不需要锁定；假设 malloc() 本身是线程安全的，每个 thread 都可以调用它而无需担心竞态条件或其他并发错误。==

  

Only when updating the shared list does a lock need to be held.

==只有在更新共享列表时才需要持有锁。==

  

See Figure 29.8 for the details of these modifications.

==有关这些修改的详细信息，请参见图 29.8。==

  

As for the lookup routine, it is a simple code transformation to jump out of the main search loop to a single return path.

==至于查找例程，它是一个简单的代码转换，从主搜索循环跳出到单个返回路径。==

  

Doing so again reduces the number of lock acquire/release points in the code, and thus decreases the chances of accidentally introducing bugs (such as forgetting to unlock before returning) into the code.

==这样做再次减少了代码中锁获取/释放点的数量，从而降低了意外引入错误（例如在返回之前忘记解锁）到代码中的机会。==

  

Figure 29.7: Concurrent Linked List

==图 29.7：并发链表==

  

Figure 29.8: Concurrent Linked List: Rewritten

==图 29.8：并发链表：重写版==

  

Scaling Linked Lists

==扩展链表==

  

Though we again have a basic concurrent linked list, once again we are in a situation where it does not scale particularly well.

==虽然我们再次拥有了一个基本的并发链表，但我们再次处于扩展性不是特别好的境地。==

  

One technique that researchers have explored to enable more concurrency within a list is something called hand-over-hand locking (a.k.a. lock coupling) [MS04].

==研究人员为在列表中启用更多并发性而探索的一种技术称为交替锁定（hand-over-hand locking，又名锁耦合）[MS04]。==

  

The idea is pretty simple.

==这个想法很简单。==

  

Instead of having a single lock for the entire list, you instead add a lock per node of the list.

==不再为整个列表设置单个锁，而是为列表的每个节点添加一个锁。==

  

When traversing the list, the code first grabs the next node's lock and then releases the current node's lock (which inspires the name hand-over-hand).

==遍历列表时，代码首先获取下一个节点的锁，然后释放当前节点的锁（这激发了“交替”这个名字的灵感）。==

  

TIP: BE WARY OF LOCKS AND CONTROL FLOW

==提示：警惕锁和控制流==

  

A general design tip, which is useful in concurrent code as well as elsewhere, is to be wary of control flow changes that lead to function returns, exits, or other similar error conditions that halt the execution of a function.

==一个通用的设计提示（在并发代码以及其他地方都很有用）是警惕导致函数返回、退出或停止函数执行的其他类似错误条件的控制流更改。==

  

Because many functions will begin by acquiring a lock, allocating some memory, or doing other similar stateful operations, when errors arise, the code has to undo all of the state before returning, which is error-prone.

==因为许多函数将以获取锁、分配一些内存或执行其他类似的有状态操作开始，所以当出现错误时，代码必须在返回之前撤消所有状态，这很容易出错。==

  

Thus, it is best to structure code to minimize this pattern.

==因此，最好构建代码以最大限度地减少这种模式。==

  

Conceptually, a hand-over-hand linked list makes some sense; it enables a high degree of concurrency in list operations.

==从概念上讲，交替锁定链表是有一定道理的；它在列表操作中实现了高度的并发性。==

  

However, in practice, it is hard to make such a structure faster than the simple single lock approach, as the overheads of acquiring and releasing locks for each node of a list traversal is prohibitive.

==然而，在实践中，很难使这种结构比简单的单锁方法更快，因为为列表遍历的每个节点获取和释放锁的开销过高。==

  

Even with very large lists, and a large number of threads, the concurrency enabled by allowing multiple ongoing traversals is unlikely to be faster than simply grabbing a single lock, performing an operation, and releasing it.

==即使对于非常大的列表和大量的线程，允许进行多个正在进行的遍历所带来的并发性也不太可能比简单地获取单个锁、执行操作并释放它更快。==

  

Perhaps some kind of hybrid (where you grab a new lock every so many nodes) would be worth investigating.

==也许某种混合方法（每隔多少个节点获取一个新锁）值得研究。==

  

29.3 Concurrent Queues

==29.3 并发队列==

  

As you know by now, there is always a standard method to make a concurrent data structure: add a big lock.

==正如你现在所知道的，总有一种标准方法来制作并发数据结构：添加一个大锁。==

  

For a queue, we'll skip that approach, assuming you can figure it out.

==对于队列，我们将跳过这种方法，假设你能弄明白。==

  

Instead, we'll take a look at a slightly more concurrent queue designed by Michael and Scott [MS98].

==相反，我们将看一看由 Michael 和 Scott 设计的并发性稍高的队列 [MS98]。==

  

The data structures and code used for this queue are found in Figure 29.9 (page 11).

==用于此队列的数据结构和代码见图 29.9（第 11 页）。==

  

If you study this code carefully, you'll notice that there are two locks, one for the head of the queue, and one for the tail.

==如果你仔细研究这段代码，你会注意到有两个锁，一个用于队列头部，一个用于尾部。==

  

The goal of these two locks is to enable concurrency of enqueue and dequeue operations.

==这两个锁的目标是实现入队和出队操作的并发。==

  

In the common case, the enqueue routine will only access the tail lock, and dequeue only the head lock.

==在常见情况下，入队例程将只访问尾部锁，而出队只访问头部锁。==

  

One trick used by Michael and Scott is to add a dummy node (allocated in the queue initialization code); this dummy enables the separation of head and tail operations.

==Michael 和 Scott 使用的一个技巧是添加一个虚拟节点（在队列初始化代码中分配）；这个虚拟节点实现了头尾操作的分离。==

  

Study the code, or better yet, type it in, run it, and measure it, to understand how it works deeply.

==研究代码，或者更好的是，把它输入进去，运行它，并测量它，以深入了解它是如何工作的。==

  

Queues are commonly used in multi-threaded applications.

==队列常用于多线程应用程序。==

  

However, the type of queue used here (with just locks) often does not completely meet the needs of such programs.

==然而，这里使用的队列类型（仅带锁）通常不能完全满足此类程序的需求。==

  

A more fully developed bounded queue, that enables a thread to wait if the queue is either empty or overly full, is the subject of our intense study in the next chapter on condition variables.

==一个更完善的有界队列（bounded queue），能够使线程在队列为空或过满时等待，是我们下一章关于条件变量的重点研究主题。==

  

Watch for it!

==敬请期待！==

  

Figure 29.9: Michael and Scott Concurrent Queue

==图 29.9：Michael 和 Scott 并发队列==

  

Figure 29.10: A Concurrent Hash Table

==图 29.10：并发哈希表==

  

29.4 Concurrent Hash Table

==29.4 并发哈希表==

  

We end our discussion with a simple and widely applicable concurrent data structure, the hash table.

==我们以一个简单且广泛适用的并发数据结构——哈希表，来结束我们的讨论。==

  

We'll focus on a simple hash table that does not resize; a little more work is required to handle resizing, which we leave as an exercise for the reader (sorry!).

==我们将专注于一个不调整大小的简单哈希表；处理调整大小需要更多的工作，我们将留给读者作为练习（抱歉！）。==

  

This concurrent hash table (Figure 29.10) is straightforward, is built using the concurrent lists we developed earlier, and works incredibly well.

==这个并发哈希表（图 29.10）很简单，是使用我们之前开发的并发列表构建的，并且工作效果非常好。==

  

The reason for its good performance is that instead of having a single lock for the entire structure, it uses a lock per hash bucket (each of which is represented by a list).

==其性能良好的原因在于，它没有为整个结构使用单个锁，而是每个哈希桶（每个桶由一个列表表示）使用一个锁。==

  

Doing so enables many concurrent operations to take place.

==这样做可以进行许多并发操作。==

  

Figure 29.11 (page 13) shows the performance of the hash table under concurrent updates (from 10,000 to 50,000 concurrent updates from each of four threads, on the same iMac with four CPUs).

==图 29.11（第 13 页）显示了哈希表在并发更新下的性能（四个线程中的每一个进行 10,000 到 50,000 次并发更新，在同一台配备四个 CPU 的 iMac 上）。==

  

Also shown, for the sake of comparison, is the performance of a linked list (with a single lock).

==为了进行比较，还显示了链表（具有单个锁）的性能。==

  

As you can see from the graph, this simple concurrent hash table scales magnificently; the linked list, in contrast, does not.

==正如你从图表中看到的那样，这个简单的并发哈希表扩展性极佳；相比之下，链表则不然。==

  

29.5 Summary

==29.5 总结==

  

We have introduced a sampling of concurrent data structures, from counters, to lists and queues, and finally to the ubiquitous and heavily-used hash table.

==我们介绍了一些并发数据结构的示例，从计数器到列表和队列，最后是无处不在且大量使用的哈希表。==

  

We have learned a few important lessons along the way: to be careful with acquisition and release of locks around control flow changes; that enabling more concurrency does not necessarily increase performance; that performance problems should only be remedied once they exist.

==一路上我们学到了一些重要的教训：在控制流变化周围获取和释放锁时要小心；启用更多并发性并不一定会提高性能；性能问题只有在存在时才应予以补救。==

  

This last point, of avoiding premature optimization, is central to any performance-minded developer; there is no value in making something faster if doing so will not improve the overall performance of the application.

==最后一点，避免过早优化，对于任何关注性能的开发人员来说都是核心；如果这样做不会提高应用程序的整体性能，那么让某些东西变得更快就没有价值。==

  

Of course, we have just scratched the surface of high performance structures.

==当然，我们只是触及了高性能结构的表面。==

  

See Moir and Shavit's excellent survey for more information, as well as links to other sources [MS04].

==有关更多信息以及其他来源的链接，请参阅 Moir 和 Shavit 的精彩调查 [MS04]。==

  

In particular, you might be interested in other structures (such as B-trees); for this knowledge, a database class is your best bet.

==特别是，你可能对其他结构（如 B 树）感兴趣；对于这些知识，数据库课程是你最好的选择。==

  

You also might be curious about techniques that don't use traditional locks at all; such non-blocking data structures are something we'll get a taste of in the chapter on common concurrency bugs, but frankly this topic is an entire area of knowledge requiring more study than is possible in this humble book.

==你也可能对完全不使用传统锁的技术感到好奇；这种非阻塞数据结构我们将在关于常见并发错误的一章中尝鲜，但坦率地说，这个话题是一个完整的知识领域，需要的研究比这本拙劣的书所能提供的要多得多。==

  

Find out more on your own if you desire (as always!).

==如果你愿意，可以自己去了解更多（一如既往！）。==

  

Figure 29.11: Scaling Hash Tables

==图 29.11：扩展哈希表==

  

TIP: AVOID PREMATURE OPTIMIZATION (KNUTH'S LAW)

==提示：避免过早优化（高德纳定律）==

  

When building a concurrent data structure, start with the most basic approach, which is to add a single big lock to provide synchronized access.

==构建并发数据结构时，从最基本的方法开始，即添加一个大锁以提供同步访问。==

  

By doing so, you are likely to build a correct lock; if you then find that it suffers from performance problems, you can refine it, thus only making it fast if need be.

==这样做，你很可能会构建一个正确的锁；如果你随后发现它存在性能问题，你可以对其进行改进，从而仅在需要时使其变快。==

  

As Knuth famously stated, "Premature optimization is the root of all evil."

==正如高德纳（Knuth）那句名言所说：“过早优化是万恶之源。”==

  

Many operating systems utilized a single lock when first transitioning to multiprocessors, including Sun OS and Linux.

==许多操作系统在首次过渡到多处理器时都使用了单个锁，包括 Sun OS 和 Linux。==

  

In the latter, this lock even had a name, the big kernel lock (BKL).

==在后者中，这个锁甚至有一个名字，大内核锁（Big Kernel Lock，BKL）。==

  

For many years, this simple approach was a good one, but when multi-CPU systems became the norm, only allowing a single active thread in the kernel at a time became a performance bottleneck.

==多年来，这种简单的方法是很好的，但是当多 CPU 系统成为常态时，一次只允许一个活动线程进入内核就成了性能瓶颈。==

  

Thus, it was finally time to add the optimization of improved concurrency to these systems.

==因此，终于到了向这些系统添加改进并发性的优化的时候了。==

  

Within Linux, the more straightforward approach was taken: replace one lock with many.

==在 Linux 内部，采用了更直接的方法：用多个锁替换一个锁。==

  

Within Sun, a more radical decision was made: build a brand new operating system, known as Solaris, that incorporates concurrency more fundamentally from day one.

==在 Sun 内部，做出了一个更激进的决定：构建一个名为 Solaris 的全新操作系统，从第一天起就更根本地整合了并发性。==

  

Read the Linux and Solaris kernel books for more information about these fascinating systems [BC05, MM00].

==阅读 Linux 和 Solaris 内核书籍，了解有关这些迷人系统的更多信息 [BC05, MM00]。==


LOCK-BASED CONCURRENT DATA STRUCTURES

==基于锁的并发数据结构==

  

References

==参考文献==

  

[B+10] "An Analysis of Linux Scalability to Many Cores" by Silas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Morris, Nickolai Zeldovich.

==[B+10] 《Linux 在多核上的可扩展性分析》，作者 Silas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Morris, Nickolai Zeldovich。==

  

OSDI '10, Vancouver, Canada, October 2010.

==OSDI '10，加拿大温哥华，2010 年 10 月。==

  

A great study of how Linux performs on multicore machines, as well as some simple solutions.

==这是一项关于 Linux 在多核机器上表现的精彩研究，同时也提供了一些简单的解决方案。==

  

Includes a neat sloppy counter to solve one form of the scalable counting problem.

==其中包括一个巧妙的“懒惰计数器”（sloppy counter），用于解决一种形式的可扩展计数问题。==

  

[BH73] "Operating System Principles" by Per Brinch Hansen.

==[BH73] 《操作系统原理》，作者 Per Brinch Hansen。==

  

Prentice-Hall, 1973.

==Prentice-Hall 出版社，1973 年。==

  

One of the first books on operating systems; certainly ahead of its time.

==这是最早的操作系统书籍之一；无疑是超前于时代的。==

  

Introduced monitors as a concurrency primitive.

==书中引入了管程（monitors）作为一种并发原语。==

  

[BC05] "Understanding the Linux Kernel (Third Edition)" by Daniel P. Bovet and Marco Cesati.

==[BC05] 《深入理解 Linux 内核（第三版）》，作者 Daniel P. Bovet 和 Marco Cesati。==

  

O'Reilly Media, November 2005.

==O'Reilly Media 出版社，2005 年 11 月。==

  

The classic book on the Linux kernel.

==这是一本关于 Linux 内核的经典著作。==

  

You should read it.

==你应该读一读。==

  

[C06] "The Search For Fast, Scalable Counters" by Jonathan Corbet.

==[C06] 《寻找快速、可扩展的计数器》，作者 Jonathan Corbet。==

  

February 1, 2006.

==2006 年 2 月 1 日。==

  

Available: [https://lwn.net/Articles/170003](https://lwn.net/Articles/170003).

==网址：[https://lwn.net/Articles/170003](https://lwn.net/Articles/170003)。==

  

LWN has many wonderful articles about the latest in Linux.

==LWN 上有许多关于 Linux 最新动态的精彩文章。==

  

This article is a short description of scalable approximate counting;

==本文简要介绍了可扩展的近似计数；==

  

read it, and others, to learn more about the latest in Linux.

==阅读这篇文章以及其他文章，以了解更多关于 Linux 的最新信息。==

  

[L+13] "A Study of Linux File System Evolution" by Lanyue Lu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Shan Lu.

==[L+13] 《Linux 文件系统演变研究》，作者 Lanyue Lu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Shan Lu。==

  

FAST '13, San Jose, CA, February 2013.

==FAST '13，加利福尼亚州圣何塞，2013 年 2 月。==

  

Our paper that studies every patch to Linux file systems over nearly a decade.

==这是我们的论文，研究了近十年来 Linux 文件系统的每一个补丁。==

  

Lots of fun findings in there; read it to see!

==里面有很多有趣的发现；读读看吧！==

  

The work was painful to do though;

==不过这项工作做起来很痛苦；==

  

the poor graduate student, Lanyue Lu, had to look through every single patch by hand in order to understand what they did.

==可怜的研究生 Lanyue Lu 必须手工查看每一个补丁，以理解它们的作用。==

  

[MS98] "Nonblocking Algorithms and Preemption-safe Locking on Multiprogrammed Shared-memory Multiprocessors" by M. Michael, M. Scott.

==[MS98] 《多道程序共享内存多处理器上的非阻塞算法与抢占安全锁》，作者 M. Michael, M. Scott。==

  

Journal of Parallel and Distributed Computing, Vol. 51, No. 1, 1998.

==《并行与分布式计算期刊》，第 51 卷，第 1 期，1998 年。==

  

Professor Scott and his students have been at the forefront of concurrent algorithms and data structures for many years;

==Scott 教授和他的学生多年来一直处于并发算法和数据结构领域的前沿；==

  

check out his web page, numerous papers, or books to find out more.

==查看他的网页、大量论文或书籍以了解更多信息。==

  

[MS04] "Concurrent Data Structures" by Mark Moir and Nir Shavit.

==[MS04] 《并发数据结构》，作者 Mark Moir 和 Nir Shavit。==

  

In Handbook of Data Structures and Applications (Editors D. Metha and S.Sahni).

==收录于《数据结构与应用手册》（编辑 D. Metha 和 S.Sahni）。==

  

Chapman and Hall/CRC Press, 2004.

==Chapman and Hall/CRC Press 出版社，2004 年。==

  

A short but relatively comprehensive reference on concurrent data structures.

==这是一份关于并发数据结构的简短但相对全面的参考资料。==

  

Though it is missing some of the latest works in the area (due to its age), it remains an incredibly useful reference.

==虽然由于年代久远，它缺少了该领域的一些最新成果，但它仍然是一份非常有用的参考资料。==

  

[MM00] "Solaris Internals: Core Kernel Architecture" by Jim Mauro and Richard McDougall.

==[MM00] 《Solaris 内核结构》，作者 Jim Mauro 和 Richard McDougall。==

  

Prentice Hall, October 2000.

==Prentice Hall 出版社，2000 年 10 月。==

  

The Solaris book.

==这是关于 Solaris 的权威书籍。==

  

You should also read this, if you want to learn about something other than Linux.

==如果你想学习 Linux 以外的知识，你也应该读读这本书。==

  

[S+11] "Making the Common Case the Only Case with Anticipatory Memory Allocation" by Swaminathan Sundararaman, Yupu Zhang, Sriram Subramanian, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau.

==[S+11] 《通过预期内存分配使常见情况成为唯一情况》，作者 Swaminathan Sundararaman 等。==

  

FAST '11, San Jose, CA, February 2011.

==FAST '11，加利福尼亚州圣何塞，2011 年 2 月。==

  

Our work on removing possibly-failing allocation calls from kernel code paths.

==我们关于从内核代码路径中移除可能失败的分配调用的工作。==

  

By allocating all potentially needed memory before doing any work, we avoid failure deep down in the storage stack.

==通过在进行任何工作之前分配所有可能需要的内存，我们避免了存储栈深处的故障。==

  

Homework (Code)

==作业（代码）==

  

In this homework, you'll gain some experience with writing concurrent code and measuring its performance.

==在这个作业中，你将获得一些编写并发代码并测量其性能的经验。==

  

Learning to build code that performs well is a critical skill and thus gaining a little experience here with it is quite worthwhile.

==学习构建性能良好的代码是一项关键技能，因此在这里获得一些相关经验是非常值得的。==

  

Questions

==问题==

  

1. We'll start by redoing the measurements within this chapter.

==2. 我们将从重做本章中的测量开始。==

  

Use the call gettimeofday() to measure time within your program.

==使用 `gettimeofday()` 调用来测量程序中的时间。==

  

How accurate is this timer?

==这个计时器有多准确？==

  

What is the smallest interval it can measure?

==它能测量的最小间隔是多少？==

  

Gain confidence in its workings, as we will need it in all subsequent questions.

==对它的工作原理建立信心，因为我们在随后的所有问题中都需要用到它。==

  

You can also look into other timers, such as the cycle counter available on x86 via the rdtsc instruction.

==你也可以研究一下其他计时器，例如 x86 上通过 `rdtsc` 指令可用的周期计数器。==

  

2. Now, build a simple concurrent counter and measure how long it takes to increment the counter many times as the number of threads increases.

==3. 现在，构建一个简单的并发计数器，并测量随着线程数量的增加，多次递增计数器所需的时间。==

  

How many CPUs are available on the system you are using?

==你使用的系统有多少个 CPU 可用？==

  

Does this number impact your measurements at all?

==这个数字对你的测量结果有影响吗？==

  

3. Next, build a version of the approximate counter.

==4. 接下来，构建一个近似计数器的版本。==

  

Once again, measure its performance as the number of threads varies, as well as the threshold.

==再次测量其性能随线程数量以及阈值变化的情况。==

  

Do the numbers match what you see in the chapter?

==这些数字与你在本章中看到的相符吗？==

  

4. Build a version of a linked list that uses hand-over-hand locking [MS04], as cited in the chapter.

==5. 构建一个链表版本，使用本章引用的 [MS04] 中提到的交替锁（hand-over-hand locking）。==

  

You should read the paper first to understand how it works, and then implement it.

==你应该先阅读论文以了解其工作原理，然后再实现它。==

  

Measure its performance.

==测量它的性能。==

  

When does a hand-over-hand list work better than a standard list as shown in the chapter?

==交替锁链表在什么情况下比本章展示的标准链表效果更好？==

  

5. Pick your favorite data structure, such as a B-tree or other slightly more interesting structure.

==6. 挑选一个你最喜欢的数据结构，例如 B 树或其他稍微有趣一点的结构。==

  

Implement it, and start with a simple locking strategy such as a single lock.

==实现它，并从一个简单的锁策略（如单一大锁）开始。==

  

Measure its performance as the number of concurrent threads increases.

==测量随着并发线程数量增加，它的性能表现。==

  

6. Finally, think of a more interesting locking strategy for this favorite data structure of yours.

==7. 最后，为你这个最喜欢的数据结构想一个更有趣的锁策略。==

  

Implement it, and measure its performance.

==实现它，并测量其性能。==

  

How does it compare to the straightforward locking approach?

==它与简单的锁方法相比如何？==

  

Condition Variables

==条件变量==

  

Thus far we have developed the notion of a lock and seen how one can be properly built with the right combination of hardware and OS support.

==到目前为止，我们已经建立了锁的概念，并看到了如何通过硬件和操作系统支持的正确结合来构建锁。==

  

Unfortunately, locks are not the only primitives that are needed to build concurrent programs.

==不幸的是，锁并不是构建并发程序所需的唯一原语。==

  

In particular, there are many cases where a thread wishes to check whether a condition is true before continuing its execution.

==特别是，在许多情况下，线程希望在继续执行之前检查某个条件是否为真。==

  

For example, a parent thread might wish to check whether a child thread has completed before continuing (this is often called a join());

==例如，父线程可能希望在继续之前检查子线程是否已完成（这通常称为 `join()`）；==

  

how should such a wait be implemented?

==这样的等待应该如何实现？==

  

Let's look at Figure 30.1.

==让我们看看图 30.1。==

  

```c

void child(void *arg) {

    printf("child\n");

    // XXX how to indicate we are done?

    return NULL;

}

  

int main(int argc, char *argv[]) {

    printf("parent: begin\n");

    pthread_t c;

    Pthread_create(&c, NULL, child, NULL); // child

    // XXX how to wait for child?

    printf("parent: end\n");

    return 0;

}

  

```

  

==（代码图示 30.1）==

==（代码图示 30.1）==

  

Figure 30.1: A Parent Waiting For Its Child

==图 30.1：父线程等待子线程==

  

What we would like to see here is the following output:

==我们希望在这里看到以下输出：==

  

parent: begin

parent: begin

  

child

child

  

parent: end

parent: end

  

We could try using a shared variable, as you see in Figure 30.2.

==我们可以尝试使用共享变量，如图 30.2 所示。==

  

This solution will generally work, but it is hugely inefficient as the parent spins and wastes CPU time.

==这种解决方案通常是可行的，但效率极低，因为父线程会自旋（spin）并浪费 CPU 时间。==

  

```c

volatile int done = 0;

  

void *child(void *arg) {

    printf("child\n");

    done = 1;

    return NULL;

}

  

int main(int argc, char *argv[]) {

    printf("parent: begin\n");

    pthread_t c;

    Pthread_create(&c, NULL, child, NULL); // child

    while (done == 0)

        ; // spin

    printf("parent: end\n");

    return 0;

}

  

```

  

==（代码图示 30.2）==

==（代码图示 30.2）==

  

Figure 30.2: Parent Waiting For Child: Spin-based Approach

==图 30.2：父线程等待子线程：基于自旋的方法==

  

What we would like here instead is some way to put the parent to sleep until the condition we are waiting for (e.g., the child is done executing) comes true.

==相反，我们希望有一种方法可以让父线程休眠，直到我们等待的条件（例如，子线程执行完毕）成真。==

  

THE CRUX: HOW TO WAIT FOR A CONDITION

==关键问题：如何等待一个条件==

  

In multi-threaded programs, it is often useful for a thread to wait for some condition to become true before proceeding.

==在多线程程序中，线程在继续之前等待某个条件变为真通常很有用。==

  

The simple approach, of just spinning until the condition becomes true, is grossly inefficient and wastes CPU cycles, and in some cases, can be incorrect.

==简单的自旋直到条件变真的方法效率极低，浪费 CPU 周期，而且在某些情况下可能是不正确的。==

  

Thus, how should a thread wait for a condition?

==那么，线程应该如何等待一个条件呢？==

  

30.1 Definition and Routines

==30.1 定义与例程==

  

To wait for a condition to become true, a thread can make use of what is known as a condition variable.

==为了等待条件变为真，线程可以使用所谓的条件变量（condition variable）。==

  

A condition variable is an explicit queue that threads can put themselves on when some state of execution (i.e., some condition) is not as desired (by waiting on the condition);

==条件变量是一个显式的队列，当某些执行状态（即某些条件）不符合预期时，线程可以将自己放入该队列中（通过在条件上等待）；==

  

some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by signaling on the condition).

==当另一个线程改变了上述状态时，它可以唤醒一个（或多个）正在等待的线程，从而允许它们继续执行（通过在条件上发送信号）。==

  

The idea goes back to Dijkstra's use of "private semaphores" [D68];

==这个想法可以追溯到 Dijkstra 对“私有信号量”的使用 [D68]；==

  

a similar idea was later named a "condition variable" by Hoare in his work on monitors [H74].

==类似的想法后来被 Hoare 在他关于管程的研究中命名为“条件变量” [H74]。==

  

To declare such a condition variable, one simply writes something like this: pthread_cond_t c;, which declares c as a condition variable (note: proper initialization is also required).

==要声明这样一个条件变量，只需编写类似这样的代码：`pthread_cond_t c;`，这将声明 `c` 为一个条件变量（注意：还需要进行适当的初始化）。==

  

A condition variable has two operations associated with it: wait() and signal().

==条件变量有两个相关的操作：`wait()` 和 `signal()`。==

  

The wait() call is executed when a thread wishes to put itself to sleep;

==当线程希望让自己休眠时，执行 `wait()` 调用；==

  

the signal() call is executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting on this condition.

==当线程改变了程序中的某些内容，从而想要唤醒在这个条件上等待的休眠线程时，执行 `signal()` 调用。==

  

Specifically, the POSIX calls look like this:

==具体来说，POSIX 调用如下所示：==

  

`pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);`

`pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);`

  

`pthread_cond_signal(pthread_cond_t *c);`

`pthread_cond_signal(pthread_cond_t *c);`

  

We will often refer to these as wait() and signal() for simplicity.

==为了简单起见，我们通常将它们称为 `wait()` 和 `signal()`。==

  

One thing you might notice about the wait() call is that it also takes a mutex as a parameter;

==关于 `wait()` 调用，你可能会注意到的一点是它还接受一个互斥锁（mutex）作为参数；==

  

it assumes that this mutex is locked when wait() is called.

==它假设在调用 `wait()` 时该互斥锁已被锁定。==

  

The responsibility of wait() is to release the lock and put the calling thread to sleep (atomically);

==`wait()` 的职责是释放锁并将调用线程置于休眠状态（原子地）；==

  

when the thread wakes up (after some other thread has signaled it), it must re-acquire the lock before returning to the caller.

==当线程醒来时（在其他线程向其发送信号后），它必须在返回给调用者之前重新获取锁。==

  

This complexity stems from the desire to prevent certain race conditions from occurring when a thread is trying to put itself to sleep.

==这种复杂性源于希望防止线程尝试进入休眠状态时发生某些竞态条件。==

  

Let's take a look at the solution to the join problem (Figure 30.3) to understand this better.

==让我们看看 join 问题的解决方案（图 30.3）以更好地理解这一点。==

  

```c

int done = 0;

pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;

pthread_cond_t c = PTHREAD_COND_INITIALIZER;

  

void thr_exit() {

    Pthread_mutex_lock(&m);

    done = 1;

    Pthread_cond_signal(&c);

    Pthread_mutex_unlock(&m);

}

  

void *child(void *arg) {

    printf("child\n");

    thr_exit();

    return NULL;

}

  

void thr_join() {

    Pthread_mutex_lock(&m);

    while (done == 0)

        Pthread_cond_wait(&c, &m);

    Pthread_mutex_unlock(&m);

}

  

int main(int argc, char *argv[]) {

    printf("parent: begin\n");

    pthread_t p;

    Pthread_create(&p, NULL, child, NULL);

    thr_join();

    printf("parent: end\n");

    return 0;

}

  

```

  

==（代码图示 30.3）==

==（代码图示 30.3）==

  

Figure 30.3: Parent Waiting For Child: Use A Condition Variable

==图 30.3：父线程等待子线程：使用条件变量==

  

There are two cases to consider.

==有两种情况需要考虑。==

  

In the first, the parent creates the child thread but continues running itself (assume we have only a single processor) and thus immediately calls into thr_join() to wait for the child thread to complete.

==在第一种情况下，父线程创建子线程但自己继续运行（假设我们只有一个处理器），因此立即调用 `thr_join()` 以等待子线程完成。==

  

In this case, it will acquire the lock, check if the child is done (it is not), and put itself to sleep by calling wait() (hence releasing the lock).

==在这种情况下，它将获取锁，检查子线程是否完成（尚未完成），并通过调用 `wait()` 使自己进入休眠状态（从而释放锁）。==

  

The child will eventually run, print the message "child", and call thr_exit() to wake the parent thread;

==子线程最终会运行，打印消息 "child"，并调用 `thr_exit()` 唤醒父线程；==

  

this code just grabs the lock, sets the state variable done, and signals the parent thus waking it.

==这段代码只是获取锁，设置状态变量 `done`，并向父线程发送信号从而唤醒它。==

  

Finally, the parent will run (returning from wait() with the lock held), unlock the lock, and print the final message "parent: end".

==最后，父线程将运行（持有锁从 `wait()` 返回），解锁，并打印最后一条消息 "parent: end"。==

  

In the second case, the child runs immediately upon creation, sets done to 1, calls signal to wake a sleeping thread (but there is none, so it just returns), and is done.

==在第二种情况下，子线程在创建后立即运行，将 `done` 设置为 1，调用 `signal` 唤醒休眠线程（但没有线程在休眠，所以它直接返回），然后结束。==

  

The parent then runs, calls thr_join(), sees that done is 1, and thus does not wait and returns.

==然后父线程运行，调用 `thr_join()`，看到 `done` 为 1，因此不等待并直接返回。==

  

One last note: you might observe the parent uses a while loop instead of just an if statement when deciding whether to wait on the condition.

==最后一点说明：你可能会注意到，在决定是否等待条件时，父线程使用了 `while` 循环而不是简单的 `if` 语句。==

  

While this does not seem strictly necessary per the logic of the program, it is always a good idea, as we will see below.

==虽然根据程序的逻辑这似乎不是绝对必要的，但这总是一个好主意，我们将在下面看到原因。==

  

To make sure you understand the importance of each piece of the thr_exit() and thr_join() code, let's try a few alternate implementations.

==为了确保你理解 `thr_exit()` 和 `thr_join()` 代码每一部分的重要性，让我们尝试几种替代实现。==

  

First, you might be wondering if we need the state variable done.

==首先，你可能想知道我们是否需要状态变量 `done`。==

  

What if the code looked like the example below? (Figure 30.4)

==如果代码像下面的例子那样会怎样？（图 30.4）==

  

```c

void thr_exit() {

    Pthread_mutex_lock(&m);

    Pthread_cond_signal(&c);

    Pthread_mutex_unlock(&m);

}

  

void thr_join() {

    Pthread_mutex_lock(&m);

    Pthread_cond_wait(&c, &m);

    Pthread_mutex_unlock(&m);

}

  

```

  

==（代码图示 30.4）==

==（代码图示 30.4）==

  

Figure 30.4: Parent Waiting: No State Variable

==图 30.4：父线程等待：无状态变量==

  

Unfortunately this approach is broken.

==不幸的是，这种方法是错误的。==

  

Imagine the case where the child runs immediately and calls thr_exit() immediately;

==设想一下子线程立即运行并立即调用 `thr_exit()` 的情况；==

  

in this case, the child will signal, but there is no thread asleep on the condition.

==在这种情况下，子线程会发送信号，但没有线程在条件上休眠。==

  

When the parent runs, it will simply call wait and be stuck; no thread will ever wake it.

==当父线程运行时，它只会调用 `wait` 并被卡住；没有任何线程会唤醒它。==

  

From this example, you should appreciate the importance of the state variable done;

==通过这个例子，你应该意识到状态变量 `done` 的重要性；==

  

it records the value the threads are interested in knowing.

==它记录了线程有兴趣知道的值。==

  

The sleeping, waking, and locking all are built around it.

==休眠、唤醒和锁定都是围绕它构建的。==

  

```c

void thr_exit() {

    done = 1;

    Pthread_cond_signal(&c);

}

  

void thr_join() {

    if (done == 0)

        Pthread_cond_wait(&c);

}

  

```

  

==（代码图示 30.5）==

==（代码图示 30.5）==

  

Figure 30.5: Parent Waiting: No Lock

==图 30.5：父线程等待：无锁==

  

Here (Figure 30.5) is another poor implementation.

==这里（图 30.5）是另一个糟糕的实现。==

  

In this example, we imagine that one does not need to hold a lock in order to signal and wait.

==在这个例子中，我们设想不需要持有锁就可以发送信号和等待。==

  

What problem could occur here? Think about it!

==这里会发生什么问题？思考一下！==

  

The issue here is a subtle race condition.

==这里的问题是一个微妙的竞态条件。==

  

Specifically, if the parent calls thr_join() and then checks the value of done, it will see that it is 0 and thus try to go to sleep.

==具体来说，如果父线程调用 `thr_join()` 然后检查 `done` 的值，它会看到它是 0，因此尝试进入休眠状态。==

  

But just before it calls wait to go to sleep, the parent is interrupted, and the child runs.

==但就在它调用 `wait` 进入休眠之前，父线程被中断，子线程开始运行。==

  

The child changes the state variable done to 1 and signals, but no thread is waiting and thus no thread is woken.

==子线程将状态变量 `done` 更改为 1 并发送信号，但没有线程在等待，因此没有线程被唤醒。==

  

When the parent runs again, it sleeps forever, which is sad.

==当父线程再次运行时，它会永远休眠，这很可悲。==

  

Hopefully, from this simple join example, you can see some of the basic requirements of using condition variables properly.

==希望通过这个简单的 join 示例，你能看到正确使用条件变量的一些基本要求。==

  

To make sure you understand, we now go through a more complicated example: the producer/consumer or bounded-buffer problem.

==为了确保你理解，我们现在来看一个更复杂的例子：生产者/消费者或有界缓冲区问题。==

  

TIP: ALWAYS HOLD THE LOCK WHILE SIGNALING

==提示：发送信号时始终持有锁==

  

Although it is strictly not necessary in all cases, it is likely simplest and best to hold the lock while signaling when using condition variables.

==尽管在所有情况下这并非严格必要，但在使用条件变量发送信号时持有锁可能是最简单且最好的做法。==

  

The example above shows a case where you must hold the lock for correctness;

==上面的例子展示了一个必须持有锁才能保证正确性的情况；==

  

however, there are some other cases where it is likely OK not to, but probably is something you should avoid.

==然而，在其他一些情况下，不持有锁可能也没问题，但这可能是你应该避免的事情。==

  

Thus, for simplicity, hold the lock when calling signal.

==因此，为了简单起见，调用 `signal` 时请持有锁。==

  

The converse of this tip, i.e., hold the lock when calling wait, is not just a tip, but rather mandated by the semantics of wait, because wait always (a) assumes the lock is held when you call it, (b) releases said lock when putting the caller to sleep, and (c) re-acquires the lock just before returning.

==这个提示的反面，即“调用 `wait` 时持有锁”，不仅仅是一个提示，而是 `wait` 语义所强制要求的，因为 `wait` 总是 (a) 假设你在调用它时持有锁，(b) 在将调用者置于休眠状态时释放该锁，并且 (c) 在返回之前重新获取锁。==

  

Thus, the generalization of this tip is correct: hold the lock when calling signal or wait, and you will always be in good shape.

==因此，这个提示的概括是正确的：调用 `signal` 或 `wait` 时持有锁，你的代码状态就会一直保持良好。==

  

Note that this example is not "real" code, because the call to pthread_cond_wait() always requires a mutex as well as a condition variable;

==请注意，这个例子不是“真实”的代码，因为调用 `pthread_cond_wait()` 总是需要一个互斥锁和一个条件变量；==

  

here, we just pretend that the interface does not do so for the sake of the negative example.

==在这里，为了反面示例的缘故，我们只是假装接口不这样做。==

  

30.2 The Producer/Consumer (Bounded Buffer) Problem

==30.2 生产者/消费者（有界缓冲区）问题==

  

The next synchronization problem we will confront in this chapter is known as the producer/consumer problem, or sometimes as the bounded buffer problem, which was first posed by Dijkstra [D72].

==我们在本章中面临的下一个同步问题被称为生产者/消费者问题，有时也称为有界缓冲区问题，它最初由 Dijkstra 提出 [D72]。==

  

Indeed, it was this very producer/consumer problem that led Dijkstra and his co-workers to invent the generalized semaphore (which can be used as either a lock or a condition variable) [D01];

==事实上，正是这个生产者/消费者问题促使 Dijkstra 和他的同事们发明了通用信号量（既可用作锁，也可用作条件变量）[D01]；==

  

we will learn more about semaphores later.

==我们稍后将学习更多关于信号量的知识。==

  

Imagine one or more producer threads and one or more consumer threads.

==想象有一个或多个生产者线程以及一个或多个消费者线程。==

  

Producers generate data items and place them in a buffer;

==生产者生成数据项并将它们放入缓冲区；==

  

consumers grab said items from the buffer and consume them in some way.

==消费者从缓冲区中抓取这些数据项并以某种方式消费它们。==

  

This arrangement occurs in many real systems.

==这种安排出现在许多实际系统中。==

  

For example, in a multi-threaded web server, a producer puts HTTP requests into a work queue (i.e., the bounded buffer);

==例如，在多线程 Web 服务器中，生产者将 HTTP 请求放入工作队列（即有界缓冲区）；==

  

consumer threads take requests out of this queue and process them.

==消费者线程从该队列中取出请求并进行处理。==

  

A bounded buffer is also used when you pipe the output of one program into another, e.g., grep foo file.txt | wc -l.

==当你将一个程序的输出管道传输到另一个程序时，也会使用有界缓冲区，例如 `grep foo file.txt | wc -l`。==

  

This example runs two processes concurrently;

==这个例子并发运行两个进程；==

  

grep writes lines from file.txt with the string foo in them to what it thinks is standard output;

==`grep` 将 `file.txt` 中包含字符串 `foo` 的行写入它认为是标准输出的地方；==

  

the UNIX shell redirects the output to what is called a UNIX pipe (created by the pipe system call).

==UNIX shell 将输出重定向到所谓的 UNIX 管道（由 `pipe` 系统调用创建）。==

  

The other end of this pipe is connected to the standard input of the process wc, which simply counts the number of lines in the input stream and prints out the result.

==管道的另一端连接到进程 `wc` 的标准输入，该进程只是统计输入流中的行数并打印结果。==

  

Thus, the grep process is the producer; the wc process is the consumer; between them is an in-kernel bounded buffer;

==因此，`grep` 进程是生产者；`wc` 进程是消费者；它们之间是一个内核内的有界缓冲区；==

  

you, in this example, are just the happy user.

==而在这个例子中，你只是那个快乐的用户。==

  

Because the bounded buffer is a shared resource, we must of course require synchronized access to it, lest a race condition arise.

==因为有界缓冲区是一个共享资源，我们当然必须要求对它的同步访问，以免出现竞态条件。==

  

To begin to understand this problem better, let us examine some actual code.

==为了开始更好地理解这个问题，让我们检查一些实际的代码。==

  

The first thing we need is a shared buffer, into which a producer puts data, and out of which a consumer takes data.

==我们需要的第一样东西是一个共享缓冲区，生产者将数据放入其中，消费者从中取出数据。==

  

Let's just use a single integer for simplicity (you can certainly imagine placing a pointer to a data structure into this slot instead), and the two inner routines to put a value into the shared buffer, and to get a value out of the buffer.

==为了简单起见，我们只使用一个整数（你当然可以想象将指向数据结构的指针放入此槽中），以及两个内部例程，用于将值放入共享缓冲区和从缓冲区取出值。==

  

See Figure 30.6 (page 6) for details.

==详情请见图 30.6（第 6 页）。==

  

```c

int buffer;

int count = 0; // initially, empty

  

void put(int value) {

    assert(count == 0);

    count = 1;

    buffer = value;

}

  

int get() {

    assert(count == 1);

    count = 0;

    return buffer;

}

  

```

  

==（代码图示 30.6）==

==（代码图示 30.6）==

  

Figure 30.6: The Put And Get Routines (v1)

==图 30.6：Put 和 Get 例程 (v1)==

  

Pretty simple, no?

==很简单，不是吗？==

  

The put() routine assumes the buffer is empty (and checks this with an assertion), and then simply puts a value into the shared buffer and marks it full by setting count to 1.

==`put()` 例程假设缓冲区为空（并通过断言检查这一点），然后简单地将一个值放入共享缓冲区，并通过将 `count` 设置为 1 将其标记为满。==

  

The get() routine does the opposite, setting the buffer to empty (i.e., setting count to 0) and returning the value.

==`get()` 例程执行相反的操作，将缓冲区设置为空（即，将 `count` 设置为 0）并返回值。==

  

Don't worry that this shared buffer has just a single entry;

==不用担心这个共享缓冲区只有一个条目；==

  

later, we'll generalize it to a queue that can hold multiple entries, which will be even more fun than it sounds.

==稍后，我们会将其推广到一个可以容纳多个条目的队列，这比听起来还要有趣。==

  

Now we need to write some routines that know when it is OK to access the buffer to either put data into it or get data out of it.

==现在我们需要编写一些例程，这些例程知道何时可以访问缓冲区以放入数据或取出数据。==

  

The conditions for this should be obvious: only put data into the buffer when count is zero (i.e., when the buffer is empty), and only get data from the buffer when count is one (i.e., when the buffer is full).

==条件应该是显而易见的：只有当 `count` 为零时（即缓冲区为空时）才将数据放入缓冲区，并且只有当 `count` 为 1 时（即缓冲区为满时）才从缓冲区获取数据。==

  

If we write the synchronization code such that a producer puts data into a full buffer, or a consumer gets data from an empty one, we have done something wrong (and in this code, an assertion will fire).

==如果我们编写的同步代码导致生产者将数据放入已满的缓冲区，或者消费者从空缓冲区获取数据，那么我们就做错了（在这段代码中，断言会被触发）。==

  

This work is going to be done by two types of threads, one set of which we'll call the producer threads, and the other set which we'll call consumer threads.

==这项工作将由两类线程完成，一组我们称为生产者线程，另一组我们称为消费者线程。==

  

Figure 30.7 shows the code for a producer that puts an integer into the shared buffer loops number of times, and a consumer that gets the data out of that shared buffer (forever), each time printing out the data item it pulled from the shared buffer.

==图 30.7 显示了生产者和消费者的代码，生产者将整数放入共享缓冲区 `loops` 次，消费者（永远）从共享缓冲区中取出数据，每次都打印出它从共享缓冲区中取出的数据项。==

  

```c

void *producer(void *arg) {

    int i;

    int loops = (int) arg;

    for (i = 0; i < loops; i++) {

        put(i);

    }

}

  

void *consumer(void *arg) {

    while (1) {

        int tmp = get();

        printf("%d\n", tmp);

    }

}

  

```

  

==（代码图示 30.7）==

==（代码图示 30.7）==

  

Figure 30.7: Producer/Consumer Threads (v1)

==图 30.7：生产者/消费者线程 (v1)==

  

A Broken Solution

==一个有问题的解决方案==

  

Now imagine that we have just a single producer and a single consumer.

==现在想象一下我们只有一个生产者和一个消费者。==

  

Obviously the put() and get() routines have critical sections within them, as put() updates the buffer, and get() reads from it.

==显然 `put()` 和 `get()` 例程内部有临界区，因为 `put()` 更新缓冲区，而 `get()` 从中读取。==

  

However, putting a lock around the code doesn't work; we need something more.

==然而，在代码周围加锁是不起作用的；我们需要更多的东西。==

  

Not surprisingly, that something more is some condition variables.

==不足为奇的是，这“更多的东西”就是条件变量。==

  

In this (broken) first try (Figure 30.8), we have a single condition variable cond and associated lock mutex.

==在这个（有问题的）第一次尝试中（图 30.8），我们有一个条件变量 `cond` 和相关的锁 `mutex`。==

  

```c

int loops; // must initialize somewhere...

cond_t cond;

mutex_t mutex;

  

void *producer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex); // p1

        if (count == 1) // p2

            Pthread_cond_wait(&cond, &mutex); // p3

        put(i); // p4

        Pthread_cond_signal(&cond); // p5

        Pthread_mutex_unlock(&mutex); // p6

    }

}

  

void *consumer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex); // c1

        if (count == 0) // c2

            Pthread_cond_wait(&cond, &mutex); // c3

        int tmp = get(); // c4

        Pthread_cond_signal(&cond); // c5

        Pthread_mutex_unlock(&mutex); // c6

        printf("%d\n", tmp);

    }

}

  

```

  

==（代码图示 30.8）==

==（代码图示 30.8）==

  

Figure 30.8: Producer/Consumer: Single CV And If Statement

==图 30.8：生产者/消费者：单个条件变量和 If 语句==

  

Let's examine the signaling logic between producers and consumers.

==让我们检查一下生产者和消费者之间的信号逻辑。==

  

When a producer wants to fill the buffer, it waits for it to be empty (p1-p3).

==当生产者想要填充缓冲区时，它等待缓冲区为空（p1-p3）。==

  

The consumer has the exact same logic, but waits for a different condition: fullness (c1-c3).

==消费者具有完全相同的逻辑，但等待不同的条件：满（c1-c3）。==

  

With just a single producer and a single consumer, the code in Figure 30.8 works.

==仅使用一个生产者和一个消费者时，图 30.8 中的代码是有效的。==

  

However, if we have more than one of these threads (e.g., two consumers), the solution has two critical problems.

==然而，如果我们有不止一个这样的线程（例如，两个消费者），该解决方案有两个关键问题。==

  

What are they?

==它们是什么？==

  

(pause here to think)...

==（在这里停下来思考一下）...==

  

Let's understand the first problem, which has to do with the if statement before the wait.

==让我们了解第一个问题，它与等待之前的 `if` 语句有关。==

  

Assume there are two consumers (Tc1 and Tc2) and one producer (Tp).

==假设有两个消费者（Tc1 和 Tc2）和一个生产者（Tp）。==

  

First, a consumer (Tc1) runs;

==首先，一个消费者（Tc1）运行；==

  

it acquires the lock (c1), checks if any buffers are ready for consumption (c2), and finding that none are, waits (c3) (which releases the lock).

==它获取锁（c1），检查是否有缓冲区可供消费（c2），发现没有，于是等待（c3）（这释放了锁）。==

  

Then the producer (Tp) runs.

==然后生产者（Tp）运行。==

  

It acquires the lock (p1), checks if all buffers are full (p2), and finding that not to be the case, goes ahead and fills the buffer (p4).

==它获取锁（p1），检查所有缓冲区是否已满（p2），发现未满，于是继续并填充缓冲区（p4）。==

  

The producer then signals that a buffer has been filled (p5).

==然后生产者发出信号，表示缓冲区已被填充（p5）。==

  

Critically, this moves the first consumer (Tc1) from sleeping on a condition variable to the ready queue;

==关键是，这将第一个消费者（Tc1）从在条件变量上休眠移动到就绪队列；==

  

Tc1 is now able to run (but not yet running).

==Tc1 现在可以运行（但尚未运行）。==

  

The producer then continues until realizing the buffer is full, at which point it sleeps (p6, p1-p3).

==然后生产者继续执行，直到意识到缓冲区已满，此时它休眠（p6, p1-p3）。==

  

Here is where the problem occurs: another consumer (Tc2) sneaks in and consumes the one existing value in the buffer (c1, c2, c4, c5, c6, skipping the wait at c3 because the buffer is full).

==这就是问题发生的地方：另一个消费者（Tc2）潜入并消耗了缓冲区中现有的唯一值（c1, c2, c4, c5, c6，跳过了 c3 处的等待，因为缓冲区已满）。==

  

Now assume Tc1 runs; just before returning from the wait, it re-acquires the lock and then returns.

==现在假设 Tc1 运行；就在从等待返回之前，它重新获取锁然后返回。==

  

It then calls get() (c4), but there are no buffers to consume!

==然后它调用 `get()`（c4），但没有缓冲区可供消费！==

  

An assertion triggers, and the code has not functioned as desired.

==断言触发，代码未按预期运行。==

  

Clearly, we should have somehow prevented Tc1 from trying to consume because Tc2 snuck in and consumed the one value in the buffer that had been produced.

==显然，我们应该以某种方式阻止 Tc1 尝试消费，因为 Tc2 潜入并消耗了缓冲区中已生成的唯一值。==

  

Figure 30.9 shows the action each thread takes, as well as its scheduler state (Ready, Running, or Sleeping) over time.

==图 30.9 显示了每个线程采取的操作，以及随时间推移的调度程序状态（就绪、运行或休眠）。==

  

[Figure 30.9 Table omitted for brevity, but describes the race condition sequence]

==[图 30.9 表格省略，描述了竞态条件序列]==

  

Figure 30.9: Thread Trace: Broken Solution (v1)

==图 30.9：线程跟踪：有问题的解决方案 (v1)==

  

The problem arises for a simple reason: after the producer woke Tc1, but before Tc1 ever ran, the state of the bounded buffer changed (thanks to Tc2).

==问题的出现原因很简单：在生产者唤醒 Tc1 之后，但在 Tc1 运行之前，有界缓冲区的状态发生了变化（多亏了 Tc2）。==

  

Signaling a thread only wakes them up; it is thus a hint that the state of the world has changed (in this case, that a value has been placed in the buffer), but there is no guarantee that when the woken thread runs, the state will still be as desired.

==向线程发送信号只是唤醒它们；因此这只是一个暗示，表明世界的状态发生了变化（在这种情况下，是值已被放入缓冲区），但不保证当被唤醒的线程运行时，状态仍然符合预期。==

  

This interpretation of what a signal means is often referred to as Mesa semantics, after the first research that built a condition variable in such a manner [LR80];

==这种对信号含义的解释通常被称为 Mesa 语义，源自第一个以这种方式构建条件变量的研究 [LR80]；==

  

the contrast, referred to as Hoare semantics, is harder to build but provides a stronger guarantee that the woken thread will run immediately upon being woken [H74].

==相反的概念称为 Hoare 语义，它更难构建，但提供了更强的保证，即被唤醒的线程将在被唤醒后立即运行 [H74]。==

  

Virtually every system ever built employs Mesa semantics.

==实际上，所有构建过的系统都采用 Mesa 语义。==

  

Better, But Still Broken: While, Not If

==好一点，但仍然有问题：While，而不是 If==

  

Fortunately, this fix is easy (Figure 30.10): change the if to a while.

==幸运的是，这个修复很简单（图 30.10）：将 `if` 更改为 `while`。==

  

Think about why this works;

==想一想为什么这行得通；==

  

now consumer Tc1 wakes up and (with the lock held) immediately re-checks the state of the shared variable (c2).

==现在消费者 Tc1 醒来并（持有锁）立即重新检查共享变量的状态（c2）。==

  

If the buffer is empty at that point, the consumer simply goes back to sleep (c3).

==如果此时缓冲区为空，消费者只需回去继续休眠（c3）。==

  

The corollary if is also changed to a while in the producer (p2).

==同理，生产者中的 `if` 也更改为 `while`（p2）。==

  

Thanks to Mesa semantics, a simple rule to remember with condition variables is to always use while loops.

==由于 Mesa 语义，使用条件变量时要记住的一个简单规则是始终使用 `while` 循环。==

  

Sometimes you don't have to re-check the condition, but it is always safe to do so;

==有时你不必重新检查条件，但这样做总是安全的；==

  

just do it and be happy.

==照做并保持快乐就好。==

  

```c

// ... [Variable Declarations] ...

void *producer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex); // p1

        while (count == 1) // p2

            Pthread_cond_wait(&cond, &mutex); // p3

        put(i); // p4

        Pthread_cond_signal(&cond); // p5

        Pthread_mutex_unlock(&mutex); // p6

    }

}

  

void *consumer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex); // c1

        while (count == 0) // c2

            Pthread_cond_wait(&cond, &mutex); // c3

        int tmp = get(); // c4

        Pthread_cond_signal(&cond); // c5

        Pthread_mutex_unlock(&mutex); // c6

        printf("%d\n", tmp);

    }

}

  

```

  

==（代码图示 30.10）==

==（代码图示 30.10）==

  

Figure 30.10: Producer/Consumer: Single CV And While

==图 30.10：生产者/消费者：单个条件变量和 While==

  

However, this code still has a bug, the second of two problems mentioned above.

==然而，这段代码仍然有一个错误，即上面提到的两个问题中的第二个。==

  

Can you see it?

==你能看出来吗？==

  

It has something to do with the fact that there is only one condition variable.

==这与只有一个条件变量有关。==

  

Try to figure out what the problem is, before reading ahead. DO IT!

==在继续阅读之前，试着找出问题所在。动手吧！==

  

(pause for you to think, or close your eyes...)

==（停下来让你思考，或者闭上眼睛……）==

  

Let's confirm you figured it out correctly, or perhaps let's confirm that you are now awake and reading this part of the book.

==让我们确认你是否正确指出了问题，或者确认你现在是清醒的并在阅读本书的这一部分。==

  

The problem occurs when two consumers run first (Tc1 and Tc2) and both go to sleep (c3).

==当两个消费者先运行（Tc1 和 Tc2）并且都进入休眠状态（c3）时，就会出现问题。==

  

Then, the producer runs, puts a value in the buffer, and wakes one of the consumers (say Tc1).

==然后，生产者运行，将一个值放入缓冲区，并唤醒其中一个消费者（比如 Tc1）。==

  

The producer then loops back (releasing and reacquiring the lock along the way) and tries to put more data in the buffer;

==然后生产者循环回来（沿途释放并重新获取锁），并尝试将更多数据放入缓冲区；==

  

because the buffer is full, the producer instead waits on the condition (thus sleeping).

==因为缓冲区已满，生产者转而在条件上等待（从而休眠）。==

  

Now, one consumer is ready to run (Tc1), and two threads are sleeping on a condition (Tc2 and Tp).

==现在，一个消费者准备好运行（Tc1），两个线程在条件上休眠（Tc2 和 Tp）。==

  

We are about to cause a problem: things are getting exciting!

==我们即将引发一个问题：事情变得刺激起来了！==

  

The consumer Tc1 then wakes by returning from wait() (c3), re-checks the condition (c2), and finding the buffer full, consumes the value (c4).

==消费者 Tc1 随后通过从 `wait()` 返回而醒来（c3），重新检查条件（c2），发现缓冲区已满，便消耗该值（c4）。==

  

This consumer then, critically, signals on the condition (c5), waking only one thread that is sleeping.

==然后，这个消费者关键性地在条件上发送信号（c5），仅唤醒一个正在休眠的线程。==

  

However, which thread should it wake?

==但是，它应该唤醒哪个线程呢？==

  

Because the consumer has emptied the buffer, it clearly should wake the producer.

==因为消费者已经清空了缓冲区，显然它应该唤醒生产者。==

  

However, if it wakes the consumer Tc2 (which is definitely possible, depending on how the wait queue is managed), we have a problem.

==然而，如果它唤醒了消费者 Tc2（这绝对是可能的，取决于等待队列的管理方式），我们就有了问题。==

  

Specifically, the consumer Tc2 will wake up and find the buffer empty (c2), and go back to sleep (c3).

==具体来说，消费者 Tc2 将醒来并发现缓冲区为空（c2），然后回去继续休眠（c3）。==

  

The producer Tp, which has a value to put into the buffer, is left sleeping.

==有值要放入缓冲区的生产者 Tp 被留下来继续休眠。==

  

The other consumer thread, Tc1 also goes back to sleep.

==另一个消费者线程 Tc1 也回去继续休眠。==

  

All three threads are left sleeping, a clear bug; see Figure 30.11 for the brutal step-by-step of this terrible calamity.

==所有三个线程都处于休眠状态，这是一个明显的错误；请参见图 30.11，了解这场可怕灾难的残酷步骤。==

  

Figure 30.11: Thread Trace: Broken Solution (v2)

==图 30.11：线程跟踪：有问题的解决方案 (v2)==

  

Signaling is clearly needed, but must be more directed.

==显然需要发送信号，但必须更有针对性。==

  

A consumer should not wake other consumers, only producers, and vice-versa.

==消费者不应该唤醒其他消费者，只应该唤醒生产者，反之亦然。==

  

The Single Buffer Producer/Consumer Solution

==单缓冲区生产者/消费者解决方案==

  

The solution here is once again a small one: use two condition variables, instead of one, in order to properly signal which type of thread should wake up when the state of the system changes.

==这里的解决方案再一次很简单：使用两个条件变量而不是一个，以便在系统状态发生变化时正确地向应该唤醒的线程类型发送信号。==

  

Figure 30.12 shows the resulting code.

==图 30.12 显示了生成的代码。==

  

In the code, producer threads wait on the condition empty, and signals fill.

==在代码中，生产者线程在条件 `empty` 上等待，并发送 `fill` 信号。==

  

Conversely, consumer threads wait on fill and signal empty.

==相反，消费者线程在 `fill` 上等待，并发送 `empty` 信号。==

  

By doing so, the second problem above is avoided by design: a consumer can never accidentally wake a consumer, and a producer can never accidentally wake a producer.

==通过这样做，设计上避免了上面的第二个问题：消费者永远不会意外唤醒消费者，生产者也永远不会意外唤醒生产者。==

  

```c

cond_t empty, fill;

mutex_t mutex;

  

void *producer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex);

        while (count == 1)

            Pthread_cond_wait(&empty, &mutex);

        put(i);

        Pthread_cond_signal(&fill);

        Pthread_mutex_unlock(&mutex);

    }

}

  

void *consumer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex);

        while (count == 0)

            Pthread_cond_wait(&fill, &mutex);

        int tmp = get();

        Pthread_cond_signal(&empty);

        Pthread_mutex_unlock(&mutex);

        printf("%d\n", tmp);

    }

}

  

```

  

==（代码图示 30.12）==

==（代码图示 30.12）==

  

Figure 30.12: Producer/Consumer: Two CVs And While

==图 30.12：生产者/消费者：两个条件变量和 While==

  

TIP: USE WHILE (NOT IF) FOR CONDITIONS

==提示：对条件使用 WHILE（而非 IF）==

  

When checking for a condition in a multi-threaded program, using a while loop is always correct;

==在多线程程序中检查条件时，使用 `while` 循环总是正确的；==

  

using an if statement only might be, depending on the semantics of signaling.

==仅使用 `if` 语句是否正确，取决于信号的语义。==

  

Thus, always use while and your code will behave as expected.

==因此，始终使用 `while`，你的代码就会按预期运行。==

  

Using while loops around conditional checks also handles the case where spurious wakeups occur.

==在条件检查周围使用 `while` 循环还可以处理发生虚假唤醒的情况。==

  

In some thread packages, due to details of the implementation, it is possible that two threads get woken up though just a single signal has taken place [L11].

==在某些线程包中，由于实现的细节，可能发生仅发送了一个信号却唤醒了两个线程的情况 [L11]。==

  

Spurious wakeups are further reason to re-check the condition a thread is waiting on.

==虚假唤醒是重新检查线程正在等待的条件的另一个理由。==

  

The Correct Producer/Consumer Solution

==正确的生产者/消费者解决方案==

  

We now have a working producer/consumer solution, albeit not a fully general one.

==我们现在有了一个可行的生产者/消费者解决方案，尽管还不是完全通用的。==

  

The last change we make is to enable more concurrency and efficiency;

==我们要做的最后一个改变是启用更高的并发性和效率；==

  

specifically, we add more buffer slots, so that multiple values can be produced before sleeping, and similarly multiple values can be consumed before sleeping.

==具体来说，我们添加更多的缓冲区槽位，以便在休眠前可以生成多个值，同样在休眠前可以消费多个值。==

  

With just a single producer and consumer, this approach is more efficient as it reduces context switches;

==仅对于单个生产者和消费者，这种方法更有效，因为它减少了上下文切换；==

  

with multiple producers or consumers (or both), it even allows concurrent producing or consuming to take place, thus increasing concurrency.

==对于多个生产者或消费者（或两者兼有），它甚至允许并发生产或消费发生，从而提高并发性。==

  

Fortunately, it is a small change from our current solution.

==幸运的是，这只需对我们当前的解决方案稍作修改。==

  

The first change for this correct solution is within the buffer structure itself and the corresponding put() and get() (Figure 30.13).

==这个正确解决方案的第一个变化在于缓冲区结构本身以及相应的 `put()` 和 `get()`（图 30.13）。==

  

We also slightly change the conditions that producers and consumers check in order to determine whether to sleep or not.

==我们还略微改变了生产者和消费者为了确定是否休眠而检查的条件。==

  

We also show the correct waiting and signaling logic (Figure 30.14).

==我们还展示了正确的等待和信号逻辑（图 30.14）。==

  

A producer only sleeps if all buffers are currently filled (p2);

==生产者仅在所有缓冲区当前都已满时才休眠（p2）；==

  

similarly, a consumer only sleeps if all buffers are currently empty (c2).

==同样，消费者仅在所有缓冲区当前都为空时才休眠（c2）。==

  

And thus we solve the producer/consumer problem;

==至此我们解决了生产者/消费者问题；==

  

time to sit back and drink a cold one.

==是时候坐下来喝杯冷饮了。==

  

```c

int buffer[MAX];

int fill_ptr = 0;

int use_ptr = 0;

int count = 0;

  

void put(int value) {

    buffer[fill_ptr] = value;

    fill_ptr = (fill_ptr + 1) % MAX;

    count++;

}

  

int get() {

    int tmp = buffer[use_ptr];

    use_ptr = (use_ptr + 1) % MAX;

    count--;

    return tmp;

}

  

```

  

==（代码图示 30.13）==

==（代码图示 30.13）==

  

Figure 30.13: The Correct Put And Get Routines

==图 30.13：正确的 Put 和 Get 例程==

  

```c

cond_t empty, fill;

mutex_t mutex;

  

void *producer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex); // p1

        while (count == MAX) // p2

            Pthread_cond_wait(&empty, &mutex); // p3

        put(i); // p4

        Pthread_cond_signal(&fill); // p5

        Pthread_mutex_unlock(&mutex); // p6

    }

}

  

void *consumer(void *arg) {

    int i;

    for (i = 0; i < loops; i++) {

        Pthread_mutex_lock(&mutex); // c1

        while (count == 0) // c2

            Pthread_cond_wait(&fill, &mutex); // c3

        int tmp = get(); // c4

        Pthread_cond_signal(&empty); // c5

        Pthread_mutex_unlock(&mutex); // c6

        printf("%d\n", tmp);

    }

}

  

```

  

==（代码图示 30.14）==

==（代码图示 30.14）==

  

Figure 30.14: The Correct Producer/Consumer Synchronization

==图 30.14：正确的生产者/消费者同步==

  

30.3 Covering Conditions

==30.3 覆盖条件==

  

We'll now look at one more example of how condition variables can be used.

==我们现在来看另一个如何使用条件变量的例子。==

  

This code study is drawn from Lampson and Redell's paper on Pilot [LR80], the same group who first implemented the Mesa semantics described above (the language they used was Mesa, hence the name).

==这个代码研究取自 Lampson 和 Redell 关于 Pilot 的论文 [LR80]，正是该小组首先实现了上述 Mesa 语义（他们使用的语言是 Mesa，因此得名）。==

  

The problem they ran into is best shown via simple example, in this case in a simple multi-threaded memory allocation library.

==他们遇到的问题最好通过一个简单的例子来展示，在这个例子中是一个简单的多线程内存分配库。==

  

Figure 30.15 shows a code snippet which demonstrates the issue.

==图 30.15 显示了演示该问题的代码片段。==

  

As you might see in the code, when a thread calls into the memory allocation code, it might have to wait in order for more memory to become free.

==正如你在代码中看到的，当线程调用内存分配代码时，它可能必须等待以便有更多内存可用。==

  

Conversely, when a thread frees memory, it signals that more memory is free.

==相反，当线程释放内存时，它会发出信号表示有更多内存可用。==

  

However, our code above has a problem: which waiting thread (there can be more than one) should be woken up?

==然而，我们上面的代码有一个问题：应该唤醒哪个正在等待的线程（可能不止一个）？==

  

```c

int bytesLeft = MAX_HEAP_SIZE;

cond_t c;

mutex_t m;

  

void *allocate(int size) {

    Pthread_mutex_lock(&m);

    while (bytesLeft < size)

        Pthread_cond_wait(&c, &m);

    void *ptr = ...; // get mem from heap

    bytesLeft -= size;

    Pthread_mutex_unlock(&m);

    return ptr;

}

  

void free(void *ptr, int size) {

    Pthread_mutex_lock(&m);

    bytesLeft += size;

    Pthread_cond_signal(&c); // whom to signal??

    Pthread_mutex_unlock(&m);

}

  

```

  

==（代码图示 30.15）==

==（代码图示 30.15）==

  

Figure 30.15: Covering Conditions: An Example

==图 30.15：覆盖条件：一个例子==

  

Consider the following scenario. Assume there are zero bytes free;

==考虑以下场景。假设有零字节可用；==

  

thread Ta calls allocate(100), followed by thread Tb which asks for less memory by calling allocate(10).

==线程 Ta 调用 `allocate(100)`，随后线程 Tb 通过调用 `allocate(10)` 请求较少的内存。==

  

Both Ta and Tb thus wait on the condition and go to sleep;

==Ta 和 Tb 因此都在条件上等待并进入休眠状态；==

  

there aren't enough free bytes to satisfy either of these requests.

==没有足够的可用字节来满足其中任何一个请求。==

  

At that point, assume a third thread, Tc calls free(50).

==此时，假设第三个线程 Tc 调用 `free(50)`。==

  

Unfortunately, when it calls signal to wake a waiting thread, it might not wake the correct waiting thread, Tb, which is waiting for only 10 bytes to be freed;

==不幸的是，当它调用信号唤醒一个等待线程时，它可能不会唤醒正确的等待线程 Tb，Tb 仅等待 10 个字节被释放；==

  

Ta should remain waiting, as not enough memory is yet free.

==Ta 应该继续等待，因为还没有足够的内存可用。==

  

Thus, the code in the figure does not work, as the thread waking other threads does not know which thread (or threads) to wake up.

==因此，图中的代码不起作用，因为唤醒其他线程的线程不知道该唤醒哪个（或哪些）线程。==

  

The solution suggested by Lampson and Redell is straightforward: replace the pthread_cond_signal() call in the code above with a call to pthread_cond_broadcast(), which wakes up all waiting threads.

==Lampson 和 Redell 建议的解决方案很简单：将上述代码中的 `pthread_cond_signal()` 调用替换为 `pthread_cond_broadcast()` 调用，后者会唤醒所有等待的线程。==

  

By doing so, we guarantee that any threads that should be woken are.

==通过这样做，我们保证了任何应该被唤醒的线程都会被唤醒。==

  

The downside, of course, can be a negative performance impact, as we might needlessly wake up many other waiting threads that shouldn't (yet) be awake.

==当然，缺点可能是负面的性能影响，因为我们可能会不必要地唤醒许多其他不应该（尚未）被唤醒的等待线程。==

  

Those threads will simply wake up, re-check the condition, and then go immediately back to sleep.

==这些线程只会醒来，重新检查条件，然后立即回去继续休眠。==

  

Lampson and Redell call such a condition a covering condition, as it covers all the cases where a thread needs to wake up (conservatively);

==Lampson 和 Redell 将这种情况称为覆盖条件（covering condition），因为它覆盖了线程需要唤醒的所有情况（保守地）；==

  

the cost, as we've discussed, is that too many threads might be woken.

==正如我们所讨论的，代价是可能会唤醒过多的线程。==

  

The astute reader might also have noticed we could have used this approach earlier (see the producer/consumer problem with only a single condition variable).

==敏锐的读者可能也注意到了，我们早些时候可以使用这种方法（参见仅使用单个条件变量的生产者/消费者问题）。==

  

However, in that case, a better solution was available to us, and thus we used it.

==然而，在那种情况下，我们有一个更好的解决方案，因此我们使用了它。==

  

In general, if you find that your program only works when you change your signals to broadcasts (but you don't think it should need to), you probably have a bug;

==一般来说，如果你发现你的程序只有在你将信号更改为广播时才能工作（但你认为本不需要这样），那么你可能有一个 bug；==

  

fix it!

==修复它！==

  

But in cases like the memory allocator above, broadcast may be the most straightforward solution available.

==但在像上面的内存分配器这样的情况下，广播可能是最直接的解决方案。==

  

30.4 Summary

==30.4 总结==

  

We have seen the introduction of another important synchronization primitive beyond locks: condition variables.

==我们已经看到了除锁之外的另一个重要同步原语的引入：条件变量。==

  

By allowing threads to sleep when some program state is not as desired, CVs enable us to neatly solve a number of important synchronization problems, including the famous (and still important) producer/consumer problem, as well as covering conditions.

==通过允许线程在某些程序状态不符合预期时休眠，条件变量使我们能够巧妙地解决许多重要的同步问题，包括著名的（且仍然重要的）生产者/消费者问题，以及覆盖条件。==

  

A more dramatic concluding sentence would go here, such as "He loved Big Brother" [O49].

==这里应该有一个更具戏剧性的结束语，比如“他热爱老大哥” [O49]。==

  

References

==参考文献==

  

[D68] "Cooperating sequential processes" by Edsger W. Dijkstra.

==[D68] 《协作顺序进程》，作者 Edsger W. Dijkstra。==

  

1968.

==1968 年。==

  

Another classic from Dijkstra;

==Dijkstra 的另一部经典著作；==

  

reading his early works on concurrency will teach you much of what you need to know.

==阅读他关于并发的早期著作将教会你许多你需要知道的知识。==

  

[D72] "Information Streams Sharing a Finite Buffer" by E.W. Dijkstra.

==[D72] 《共享有限缓冲区的信息流》，作者 E.W. Dijkstra。==

  

Information Processing Letters 1: 179-180, 1972.

==《信息处理快报》1: 179-180，1972 年。==

  

The famous paper that introduced the producer/consumer problem.

==这就是引入生产者/消费者问题的著名论文。==

  

[D01] "My recollections of operating system design" by E.W. Dijkstra.

==[D01] 《我对操作系统设计的回忆》，作者 E.W. Dijkstra。==

  

April, 2001.

==2001 年 4 月。==

  

A fascinating read for those of you interested in how the pioneers of our field came up with some very basic and fundamental concepts, including ideas like "interrupts" and even "a stack"!

==对于那些有兴趣了解我们领域的先驱如何提出一些非常基本和基础概念（包括“中断”甚至“栈”等想法）的人来说，这是一本引人入胜的读物！==

  

[H74] "Monitors: An Operating System Structuring Concept" by C.A.R. Hoare.

==[H74] 《管程：一种操作系统结构概念》，作者 C.A.R. Hoare。==

  

Communications of the ACM, 17:10, pages 549-557, October 1974.

==《ACM 通讯》，17:10，第 549-557 页，1974 年 10 月。==

  

Hoare did a fair amount of theoretical work in concurrency.

==Hoare 在并发领域做了大量的理论工作。==

  

However, he is still probably most known for his work on Quicksort, the coolest sorting algorithm in the world, at least according to these authors.

==然而，他最著名的可能还是他在快速排序方面的工作，这是世界上最酷的排序算法，至少根据这些作者的说法是这样。==

  

[L11] "Pthread_cond_signal Man Page" by Mysterious author.

==[L11] 《Pthread_cond_signal 手册页》，作者 神秘作者。==

  

March, 2011.

==2011 年 3 月。==

  

The Linux man page shows a nice simple example of why a thread might get a spurious wakeup, due to race conditions within the signal/wakeup code.

==Linux 手册页展示了一个很好的简单例子，说明了由于信号/唤醒代码中的竞态条件，线程为什么可能会收到虚假唤醒。==

  

[LR80] "Experience with Processes and Monitors in Mesa" by B.W. Lampson, D.R. Redell.

==[LR80] 《Mesa 中进程与管程的经验》，作者 B.W. Lampson, D.R. Redell。==

  

Communications of the ACM. 23:2, pages 105-117, February 1980.

==《ACM 通讯》，23:2，第 105-117 页，1980 年 2 月。==

  

A classic paper about how to actually implement signaling and condition variables in a real system, leading to the term "Mesa" semantics for what it means to be woken up;

==这是一篇关于如何在真实系统中实际实现信号和条件变量的经典论文，从而产生了“Mesa”语义这一术语，用来描述被唤醒的含义；==

  

the older semantics, developed by Tony Hoare [H74], then became known as "Hoare" semantics, which is a bit unfortunate of a name.

==由 Tony Hoare [H74] 开发的旧语义随后被称为“Hoare”语义，这个名字有点不幸。==

  

[O49] "1984" by George Orwell.

==[O49] 《1984》，作者 George Orwell。==

  

Secker and Warburg, 1949.

==Secker and Warburg 出版社，1949 年。==

  

A little heavy-handed, but of course a must read.

==有点笨拙，但当然是必读之作。==

  

That said, we kind of gave away the ending by quoting the last sentence. Sorry!

==话虽如此，我们引用了最后一句话，有点剧透了结局。抱歉！==

  

And if the government is reading this, let us just say that we think that the government is "double plus good".

==如果政府正在读这句话，我们只想说，我们认为政府是“加倍好”（double plus good）。==

  

Hear that, our pals at the NSA?

==听到了吗，我们在 NSA 的朋友们？==

  

Homework (Code)

==作业（代码）==

  

This homework lets you explore some real code that uses locks and condition variables to implement various forms of the producer/consumer queue discussed in the chapter.

==这份作业让你探索一些真实的代码，这些代码使用锁和条件变量来实现本章讨论的各种形式的生产者/消费者队列。==

  

You'll look at the real code, run it in various configurations, and use it to learn about what works and what doesn't, as well as other intricacies.

==你将查看真实的代码，在各种配置下运行它，并利用它来学习什么是有效的，什么是无效的，以及其他复杂细节。==

  

Read the README for details.

==阅读 README 了解详情。==

  

Questions

==问题==

  

1. Our first question focuses on main-two-cvs-while.c (the working solution).

==2. 我们的第一个问题关注 `main-two-cvs-while.c`（可行的解决方案）。==

  

First, study the code.

==首先，研究代码。==

  

Do you think you have an understanding of what should happen when you run the program?

==你认为你已经理解运行程序时应该发生什么了吗？==

  

2. Run with one producer and one consumer, and have the producer produce a few values.

==3. 运行一个生产者和一个消费者，并让生产者生成一些值。==

  

Start with a buffer (size 1), and then increase it.

==从缓冲区（大小为 1）开始，然后增加它。==

  

How does the behavior of the code change with larger buffers?

==随着缓冲区变大，代码的行为会发生怎样的变化？==

  

(or does it?) What would you predict num_full to be with different buffer sizes (e.g., -m 10) and different numbers of produced items (e.g., -l 100), when you change the consumer sleep string from default (no sleep) to -C 0,0,0,0,0,0,1?

==（或者它会变吗？）当你将消费者休眠字符串从默认值（无休眠）更改为 `-C 0,0,0,0,0,0,1` 时，对于不同的缓冲区大小（例如 `-m 10`）和不同的生成项目数量（例如 `-l 100`），你预测 `num_full` 会是多少？==

  

3. If possible, run the code on different systems (e.g., a Mac and Linux).

==4. 如果可能，请在不同的系统上运行代码（例如 Mac 和 Linux）。==

  

Do you see different behavior across these systems?

==你在这些系统之间看到不同的行为了吗？==

  

4. Let's look at some timings.

==5. 让我们看一些计时。==

  

How long do you think the following execution, with one producer, three consumers, a single-entry shared buffer, and each consumer pausing at point c3 for a second, will take?

==你认为以下执行需要多长时间：一个生产者，三个消费者，单条目共享缓冲区，并且每个消费者在 c3 点暂停一秒？==

  

`./main-two-cvs-while -p 1 -c 3 -m 1 -C 0,0,0,1,0,0,0:0,0,0,1,0,0,0:0,0,0,1,0,0,0 -l 10 -v -t`

`./main-two-cvs-while -p 1 -c 3 -m 1 -C 0,0,0,1,0,0,0:0,0,0,1,0,0,0:0,0,0,1,0,0,0 -l 10 -v -t`

  

5. Now change the size of the shared buffer to 3 (-m 3).

==6. 现在将共享缓冲区的大小更改为 3 (`-m 3`)。==

  

Will this make any difference in the total time?

==这会对总时间产生影响吗？==

  

6. Now change the location of the sleep to c6 (this models a consumer taking something off the queue and then doing something with it), again using a single-entry buffer.

==7. 现在将休眠的位置更改为 c6（这模拟了消费者从队列中取出一个东西然后用它做一些事情），同样使用单条目缓冲区。==

  

What time do you predict in this case?

==在这种情况下，你预测的时间是多少？==

  

`./main-two-cvs-while -p 1 -c 3 -m 1 -C 0,0,0,0,0,0,1:0,0,0,0,0,0,1:0,0,0,0,0,0,1 -l 10 -v -t`

`./main-two-cvs-while -p 1 -c 3 -m 1 -C 0,0,0,0,0,0,1:0,0,0,0,0,0,1:0,0,0,0,0,0,1 -l 10 -v -t`

  

7. Finally, change the buffer size to 3 again (-m 3).

==8. 最后，再次将缓冲区大小更改为 3 (`-m 3`)。==

  

What time do you predict now?

==你现在预测的时间是多少？==

  

8. Now let's look at main-one-cv-while.c.

==9. 现在让我们看看 `main-one-cv-while.c`。==

  

Can you configure a sleep string, assuming a single producer, one consumer, and a buffer of size 1, to cause a problem with this code?

==假设只有一个生产者、一个消费者和一个大小为 1 的缓冲区，你能配置一个休眠字符串来导致此代码出现问题吗？==


CONDITION VARIABLES
==条件变量==

9. Now change the number of consumers to two.
==10. 现在将消费者的数量改为两个。==

Can you construct sleep strings for the producer and the consumers so as to cause a problem in the code?
==你能否为生产者和消费者构建休眠字符串（sleep strings），从而导致代码出现问题？==

10. Now examine main-two-cvs-if.c.
==11. 现在检查 main-two-cvs-if.c。==

Can you cause a problem to happen in this code?
==你能让这段代码出现问题吗？==

Again consider the case where there is only one consumer, and then the case where there is more than one.
==再次考虑只有一个消费者的情况，然后再考虑有一个以上消费者的情况。==

11. Finally, examine main-two-cvs-while-extra-unlock.c.
==12. 最后，检查 main-two-cvs-while-extra-unlock.c。==

What problem arises when you release the lock before doing a put or a get?
==当你执行 put 或 get 操作之前释放锁时，会出现什么问题？==

Can you reliably cause such a problem to happen, given the sleep strings?
==给定休眠字符串，你能可靠地重现这个问题吗？==

What bad thing can happen?
==会发生什么糟糕的事情？==

Semaphores
==信号量==

As we know now, one needs both locks and condition variables to solve a broad range of relevant and interesting concurrency problems.
==正如我们现在所知，人们需要锁和条件变量来解决广泛的相关且有趣的并发问题。==

One of the first people to realize this years ago was Edsger Dijkstra (though it is hard to know the exact history), known among other things for his famous "shortest paths" algorithm in graph theory, an early polemic on structured programming entitled "Goto Statements Considered Harmful" (what a great title!), and, in the case we will study here, the introduction of a synchronization primitive called the semaphore.
==早在几年前，Edsger Dijkstra 是最早意识到这一点的人之一（尽管确切的历史很难考证），他因图论中著名的“最短路径”算法、早期关于结构化编程的檄文《Goto 语句被认为是有害的》（多棒的标题！），以及我们这里要学习的引入一种称为信号量的同步原语而闻名。==

Indeed, Dijkstra and colleagues invented the semaphore as a single primitive for all things related to synchronization;
==实际上，Dijkstra 和他的同事发明了信号量，作为一个与同步相关的所有事情的单一原语；==

as you will see, one can use semaphores as both locks and condition variables.
==正如你将看到的，我们可以把信号量既当作锁来用，也当作条件变量来用。==

**THE CRUX: HOW TO USE SEMAPHORES**
==**关键点：如何使用信号量**==

How can we use semaphores instead of locks and condition variables?
==我们如何使用信号量来代替锁和条件变量？==

What is the definition of a semaphore?
==信号量的定义是什么？==

What is a binary semaphore?
==什么是二值信号量？==

Is it straightforward to build a semaphore out of locks and condition variables?
==用锁和条件变量构建信号量容易吗？==

To build locks and condition variables out of semaphores?
==用信号量构建锁和条件变量呢？==

31.1 Semaphores: A Definition
==31.1 信号量：定义==

A semaphore is an object with an integer value that we can manipulate with two routines;
==信号量是一个具有整数值的对象，我们可以通过两个例程来操作它；==

in the POSIX standard, these routines are `sem_wait()` and `sem_post()`.
==在 POSIX 标准中，这两个例程是 `sem_wait()` 和 `sem_post()`。==

Because the initial value of the semaphore determines its behavior, before calling any other routine to interact with the semaphore, we must first initialize it to some value, as the code in Figure 31.1 does.
==因为信号量的初始值决定了它的行为，所以在调用任何其他例程与信号量交互之前，我们必须首先将其初始化为某个值，如图 31.1 中的代码所示。==

Historically, `sem_wait()` was called P() by Dijkstra and `sem_post()` called V().
==历史上，Dijkstra 将 `sem_wait()` 称为 P()，将 `sem_post()` 称为 V()。==

These shortened forms come from Dutch words; interestingly, which Dutch words they supposedly derive from has changed over time.
==这些缩写形式来自荷兰语单词；有趣的是，它们据称源自哪个荷兰语单词随着时间的推移而发生了变化。==

Originally, P() came from "passering" (to pass) and V() from "vrijgave" (release);
==最初，P() 来自 “passering”（通过），V() 来自 “vrijgave”（释放）；==

later, Dijkstra wrote P() was from "prolaag", a contraction of "probeer" (Dutch for "try") and "verlaag" ("decrease"), and V() from "verhoog" which means "increase".
==后来，Dijkstra 写道 P() 来自 “prolaag”，是 “probeer”（荷兰语“尝试”）和 “verlaag”（“减少”）的缩写，而 V() 来自 “verhoog”，意为“增加”。==

Sometimes, people call them down and up.
==有时，人们称它们为 down 和 up。==

Use the Dutch versions to impress your friends, or confuse them, or both.
==使用荷兰语版本来给你的朋友留下深刻印象，或者迷惑他们，或者兼而有之。==

Figure 31.1: Initializing A Semaphore
==图 31.1：初始化信号量==

In the figure, we declare a semaphore s and initialize it to the value 1 by passing 1 in as the third argument.
==在图中，我们声明了一个信号量 s，并通过传入 1 作为第三个参数将其初始化为值 1。==

The second argument to `sem_init()` will be set to 0 in all of the examples we'll see;
==在我们看到的所有示例中，`sem_init()` 的第二个参数都将被设置为 0；==

this indicates that the semaphore is shared between threads in the same process.
==这表示信号量在同一进程的线程之间共享。==

See the man page for details on other usages of semaphores (namely, how they can be used to synchronize access across different processes), which require a different value for that second argument.
==有关信号量其他用法的详细信息（即如何使用它们来同步跨不同进程的访问），请参阅手册页，这需要为第二个参数设置不同的值。==

After a semaphore is initialized, we can call one of two functions to interact with it, `sem_wait()` or `sem_post()`.
==信号量初始化后，我们可以调用两个函数之一与其交互，即 `sem_wait()` 或 `sem_post()`。==

The behavior of these two functions is seen in Figure 31.2.
==这两个函数的行为如图 31.2 所示。==

For now, we are not concerned with the implementation of these routines, which clearly requires some care;
==目前，我们不关心这些例程的实现，这显然需要一些小心；==

with multiple threads calling into `sem_wait()` and `sem_post()`, there is the obvious need for managing these critical sections.
==当多个线程调用 `sem_wait()` 和 `sem_post()` 时，显然需要管理这些临界区。==

We will now focus on how to use these primitives; later we may discuss how they are built.
==我们要现在关注如何使用这些原语；稍后我们可能会讨论它们是如何构建的。==

We should discuss a few salient aspects of the interfaces here.
==我们应该在这里讨论接口的几个显著方面。==

First, we can see that `sem_wait()` will either return right away (because the value of the semaphore was one or higher when we called `sem_wait()`), or it will cause the caller to suspend execution waiting for a subsequent post.
==首先，我们可以看到 `sem_wait()` 要么立即返回（因为调用 `sem_wait()` 时信号量的值为 1 或更高），要么导致调用者挂起执行，等待随后的 post 操作。==

Of course, multiple calling threads may call into `sem_wait()`, and thus all be queued waiting to be woken.
==当然，多个调用线程可能会调用 `sem_wait()`，因此它们都会排队等待被唤醒。==

Second, we can see that `sem_post()` does not wait for some particular condition to hold like `sem_wait()` does.
==其次，我们可以看到 `sem_post()` 不像 `sem_wait()` 那样等待某个特定条件成立。==

Rather, it simply increments the value of the semaphore and then, if there is a thread waiting to be woken, wakes one of them up.
==相反，它只是增加信号量的值，然后如果有线程在等待被唤醒，就唤醒其中一个。==

Third, the value of the semaphore, when negative, is equal to the number of waiting threads.
==第三，当信号量的值为负数时，其绝对值等于等待线程的数量。==

Though the value generally isn't seen by users of the semaphores, this invariant is worth knowing and perhaps can help you remember how a semaphore functions.
==虽然信号量的用户通常看不到该值，但了解这个不变量是值得的，也许可以帮助你记住信号量是如何工作的。==

Figure 31.2: Semaphore: Definitions Of Wait And Post
==图 31.2：信号量：Wait 和 Post 的定义==

Figure 31.3: A Binary Semaphore (That Is, A Lock)
==图 31.3：二值信号量（即，锁）==

Don't worry (yet) about the seeming race conditions possible within the semaphore;
==（暂时）不要担心信号量内部可能出现的表面上的竞态条件；==

assume that the actions they make are performed atomically.
==假设它们执行的操作是原子性的。==

We will soon use locks and condition variables to do just this.
==我们很快就会使用锁和条件变量来做到这一点。==

31.2 Binary Semaphores (Locks)
==31.2 二值信号量（锁）==

We are now ready to use a semaphore.
==我们现在准备好使用信号量了。==

Our first use will be one with which we are already familiar: using a semaphore as a lock.
==我们的第一个用途是我们已经熟悉的：将信号量用作锁。==

See Figure 31.3 for a code snippet; therein, you'll see that we simply surround the critical section of interest with a `sem_wait()`/`sem_post()` pair.
==参见图 31.3 的代码片段；在那里，你会看到我们只是简单地用一对 `sem_wait()`/`sem_post()` 包围了感兴趣的临界区。==

Critical to making this work, though, is the initial value of the semaphore m (initialized to X in the figure).
==然而，使其工作的关键是信号量 m 的初始值（在图中初始化为 X）。==

What should X be?
==X 应该是多少？==

(Try thinking about it before going on)...
==（在继续之前试着思考一下）...==

Looking back at definition of the `sem_wait()` and `sem_post()` routines above, we can see that the initial value should be 1.
==回顾上面 `sem_wait()` 和 `sem_post()` 例程的定义，我们可以看到初始值应该是 1。==

To make this clear, let's imagine a scenario with two threads.
==为了清楚起见，让我们想象一个有两个线程的场景。==

The first thread (Thread 0) calls `sem_wait()`;
==第一个线程（线程 0）调用 `sem_wait()`；==

it will first decrement the value of the semaphore, changing it to 0.
==它将首先减少信号量的值，将其变为 0。==

Then, it will wait only if the value is not greater than or equal to 0.
==然后，只有当值不大于或等于 0 时，它才会等待。==

Because the value is 0, `sem_wait()` will simply return and the calling thread will continue;
==因为值是 0，`sem_wait()` 将直接返回，调用线程将继续执行；==

Thread 0 is now free to enter the critical section.
==线程 0 现在可以自由进入临界区了。==

If no other thread tries to acquire the lock while Thread 0 is inside the critical section, when it calls `sem_post()`, it will simply restore the value of the semaphore to 1 (and not wake a waiting thread, because there are none).
==如果在线程 0 处于临界区内时没有其他线程尝试获取锁，当它调用 `sem_post()` 时，它只会将信号量的值恢复为 1（并且不会唤醒等待的线程，因为没有等待的线程）。==

Figure 31.4 shows a trace of this scenario.
==图 31.4 显示了这个场景的追踪过程。==

A more interesting case arises when Thread 0 "holds the lock" (i.e., it has called `sem_wait()` but not yet called `sem_post()`), and another thread (Thread 1) tries to enter the critical section by calling `sem_wait()`.
==当线程 0 “持有锁”（即它已经调用了 `sem_wait()` 但尚未调用 `sem_post()`），而另一个线程（线程 1）试图通过调用 `sem_wait()` 进入临界区时，会出现一个更有趣的情况。==

In this case, Thread 1 will decrement the value of the semaphore to -1, and thus wait (putting itself to sleep and relinquishing the processor).
==在这种情况下，线程 1 会将信号量的值减为 -1，从而进行等待（将自己置于睡眠状态并放弃处理器）。==

When Thread 0 runs again, it will eventually call `sem_post()`, incrementing the value of the semaphore back to zero, and then wake the waiting thread (Thread 1), which will then be able to acquire the lock for itself.
==当线程 0 再次运行时，它最终会调用 `sem_post()`，将信号量的值增加回 0，然后唤醒等待的线程（线程 1），线程 1 随后将能够自己获取锁。==

When Thread 1 finishes, it will again increment the value of the semaphore, restoring it to 1 again.
==当线程 1 完成时，它会再次增加信号量的值，将其恢复为 1。==

Figure 31.5 shows a trace of this example.
==图 31.5 显示了这个例子的追踪过程。==

In addition to thread actions, the figure shows the scheduler state of each thread: Run (the thread is running), Ready (i.e., runnable but not running), and Sleep (the thread is blocked).
==除了线程操作之外，该图还显示了每个线程的调度器状态：运行（线程正在运行）、就绪（即可运行但未运行）和睡眠（线程被阻塞）。==

Note that Thread 1 goes into the sleeping state when it tries to acquire the already-held lock;
==请注意，当线程 1 试图获取已被持有的锁时，它会进入睡眠状态；==

only when Thread 0 runs again can Thread 1 be awoken and potentially run again.
==只有当线程 0 再次运行时，线程 1 才能被唤醒并可能再次运行。==

If you want to work through your own example, try a scenario where multiple threads queue up waiting for a lock.
==如果你想自己通过一个例子来练习，试着设想一个多个线程排队等待锁的场景。==

What would the value of the semaphore be during such a trace?
==在这样的追踪过程中，信号量的值会是多少？==

Thus we are able to use semaphores as locks.
==因此，我们能够将信号量用作锁。==

Because locks only have two states (held and not held), we sometimes call a semaphore used as a lock a binary semaphore.
==因为锁只有两种状态（持有和未持有），我们有时将用作锁的信号量称为二值信号量。==

Note that if you are using a semaphore only in this binary fashion, it could be implemented in a simpler manner than the generalized semaphores we present here.
==请注意，如果你仅以这种二值方式使用信号量，它的实现方式可以比我们这里介绍的通用信号量更简单。==

31.3 Semaphores For Ordering
==31.3 用于排序的信号量==

Semaphores are also useful to order events in a concurrent program.
==信号量对于在并发程序中对事件进行排序也很有用。==

For example, a thread may wish to wait for a list to become non-empty, so it can delete an element from it.
==例如，一个线程可能希望等待列表变为非空，以便它可以从中删除一个元素。==

In this pattern of usage, we often find one thread waiting for something to happen, and another thread making that something happen and then signaling that it has happened, thus waking the waiting thread.
==在这种使用模式中，我们经常发现一个线程在等待某事发生，而另一个线程使该事发生，然后发出信号表明它已发生，从而唤醒等待的线程。==

We are thus using the semaphore as an ordering primitive (similar to our use of condition variables earlier).
==因此，我们将信号量用作排序原语（类似于我们之前使用条件变量）。==

A simple example is as follows.
==一个简单的例子如下。==

Imagine a thread creates another thread and then wants to wait for it to complete its execution (Figure 31.6).
==设想一个线程创建了另一个线程，然后想要等待它完成执行（图 31.6）。==

When this program runs, we would like to see the following:
==当这个程序运行时，我们希望看到以下内容：==

`parent: begin`
`child`
`parent: end`

The question, then, is how to use a semaphore to achieve this effect;
==那么问题来了，如何使用信号量来达到这种效果；==

as it turns out, the answer is relatively easy to understand.
==事实证明，答案相对容易理解。==

Figure 31.6: A Parent Waiting For Its Child
==图 31.6：父线程等待子线程==

As you can see in the code, the parent simply calls `sem_wait()` and the child `sem_post()` to wait for the condition of the child finishing its execution to become true.
==正如你在代码中看到的，父线程只需调用 `sem_wait()`，子线程调用 `sem_post()`，以等待子线程完成执行的条件变为真。==

However, this raises the question: what should the initial value of this semaphore be?
==然而，这就提出了一个问题：这个信号量的初始值应该是多少？==

(Again, think about it here, instead of reading ahead)
==（同样，在这里思考一下，而不是直接往下读）==

The answer, of course, is that the value of the semaphore should be set to is 0.
==答案当然是信号量的值应设置为 0。==

There are two cases to consider.
==有两种情况需要考虑。==

First, let us assume that the parent creates the child but the child has not run yet (i.e., it is sitting in a ready queue but not running).
==首先，让我们假设父线程创建了子线程，但子线程尚未运行（即它位于就绪队列中但未运行）。==

In this case (Figure 31.7), the parent will call `sem_wait()` before the child has called `sem_post()`;
==在这种情况下（图 31.7），父线程将在子线程调用 `sem_post()` 之前调用 `sem_wait()`；==

we'd like the parent to wait for the child to run.
==我们希望父线程等待子线程运行。==

The only way this will happen is if the value of the semaphore is not greater than 0;
==唯有信号量的值不大于 0 时，这才可能发生；==

hence, 0 is the initial value.
==因此，初始值为 0。==

The parent runs, decrements the semaphore (to -1), then waits (sleeping).
==父线程运行，递减信号量（至 -1），然后等待（睡眠）。==

When the child finally runs, it will call `sem_post()`, increment the value of the semaphore to 0, and wake the parent, which will then return from `sem_wait()` and finish the program.
==当子线程最终运行时，它将调用 `sem_post()`，将信号量的值增加到 0，并唤醒父线程，父线程随后将从 `sem_wait()` 返回并完成程序。==

Figure 31.7: Thread Trace: Parent Waiting For Child (Case 1)
==图 31.7：线程追踪：父线程等待子线程（情况 1）==

Figure 31.8: Thread Trace: Parent Waiting For Child (Case 2)
==图 31.8：线程追踪：父线程等待子线程（情况 2）==

The second case (Figure 31.8) occurs when the child runs to completion before the parent gets a chance to call `sem_wait()`.
==第二种情况（图 31.8）发生在子线程在父线程有机会调用 `sem_wait()` 之前就已运行完成时。==

In this case, the child will first call `sem_post()`, thus incrementing the value of the semaphore from 0 to 1.
==在这种情况下，子线程将首先调用 `sem_post()`，从而将信号量的值从 0 增加到 1。==

When the parent then gets a chance to run, it will call `sem_wait()` and find the value of the semaphore to be 1;
==当父线程随后有机会运行时，它将调用 `sem_wait()` 并发现信号量的值为 1；==

the parent will thus decrement the value (to 0) and return from `sem_wait()` without waiting, also achieving the desired effect.
==因此，父线程将递减该值（至 0）并从 `sem_wait()` 返回而无需等待，同样达到了预期的效果。==

31.4 The Producer/Consumer (Bounded Buffer) Problem
==31.4 生产者/消费者（有界缓冲区）问题==

The next problem we will confront in this chapter is known as the producer/consumer problem, or sometimes as the bounded buffer problem.
==我们在本章中面临的下一个问题被称为生产者/消费者问题，有时也称为有界缓冲区问题。==

This problem is described in detail in the previous chapter on condition variables; see there for details.
==这个问题在上一章关于条件变量的内容中有详细描述；详情请参阅那里。==

**ASIDE: SETTING THE VALUE OF A SEMAPHORE**
==**旁白：设置信号量的值**==

We've now seen two examples of initializing a semaphore.
==我们现在已经看到了两个初始化信号量的例子。==

In the first case, we set the value to 1 to use the semaphore as a lock;
==在第一种情况下，我们将值设置为 1，以将信号量用作锁；==

in the second, to 0, to use the semaphore for ordering.
==在第二种情况下，设置为 0，以使用信号量进行排序。==

So what's the general rule for semaphore initialization?
==那么信号量初始化的一般规则是什么？==

One simple way to think about it, thanks to Perry Kivolowitz, is to consider the number of resources you are willing to give away immediately after initialization.
==感谢 Perry Kivolowitz，一种简单的思考方法是考虑你在初始化后立即愿意分发的资源数量。==

With the lock, it was 1, because you are willing to have the lock locked (given away) immediately after initialization.
==对于锁，这个值是 1，因为你愿意在初始化后立即让锁被锁定（分发出去）。==

With the ordering case, it was 0, because there is nothing to give away at the start;
==对于排序的情况，这个值是 0，因为一开始没有什么可分发的；==

only when the child thread is done is the resource created, at which point, the value is incremented to 1.
==只有当子线程完成后，资源才被创建，此时，该值增加到 1。==

Try this line of thinking on future semaphore problems, and see if it helps.
==在未来的信号量问题上尝试这种思路，看看是否有帮助。==

**First Attempt**
==**初次尝试**==

Our first attempt at solving the problem introduces two semaphores, empty and full, which the threads will use to indicate when a buffer entry has been emptied or filled, respectively.
==我们要解决这个问题的初次尝试引入了两个信号量：empty 和 full，线程将使用它们分别指示缓冲区条目何时被清空或填充。==

The code for the put and get routines is in Figure 31.9, and our attempt at solving the producer and consumer problem is in Figure 31.10.
==put 和 get 例程的代码在图 31.9 中，我们要解决生产者和消费者问题的尝试在图 31.10 中。==

In this example, the producer first waits for a buffer to become empty in order to put data into it, and the consumer similarly waits for a buffer to become filled before using it.
==在这个例子中，生产者首先等待缓冲区变空以便将数据放入其中，消费者同样等待缓冲区被填满后再使用它。==

Let us first imagine that MAX = 1 (there is only one buffer in the array), and see if this works.
==让我们首先假设 MAX = 1（数组中只有一个缓冲区），看看这是否有效。==

Imagine again there are two threads, a producer and a consumer.
==再次设想有两个线程，一个生产者和一个消费者。==

Let us examine a specific scenario on a single CPU.
==让我们检查单个 CPU 上的特定场景。==

Assume the consumer gets to run first.
==假设消费者首先运行。==

Thus, the consumer will hit Line C1 in Figure 31.10, calling `sem_wait(&full)`.
==因此，消费者将执行图 31.10 中的 C1 行，调用 `sem_wait(&full)`。==

Because full was initialized to the value 0, the call will decrement full (to -1), block the consumer, and wait for another thread to call `sem_post()` on full, as desired.
==因为 full 初始化为 0，该调用将递减 full（至 -1），阻塞消费者，并等待另一个 thread 对 full 调用 `sem_post()`，正如预期的那样。==

Figure 31.9: The Put And Get Routines
==图 31.9：Put 和 Get 例程==

Figure 31.10: Adding The Full And Empty Conditions
==图 31.10：添加 Full 和 Empty 条件==

Assume the producer then runs.
==假设随后生产者运行。==

It will hit Line P1, thus calling the `sem_wait(&empty)` routine.
==它将执行 P1 行，从而调用 `sem_wait(&empty)` 例程。==

Unlike the consumer, the producer will continue through this line, because empty was initialized to the value MAX (in this case, 1).
==与消费者不同，生产者将继续执行该行，因为 empty 初始化为值 MAX（在本例中为 1）。==

Thus, empty will be decremented to 0 and the producer will put a data value into the first entry of buffer (Line P2).
==因此，empty 将递减为 0，生产者将把数据值放入缓冲区的第一个条目中（P2 行）。==

The producer will then continue on to P3 and call `sem_post(&full)`, changing the value of the full semaphore from -1 to 0 and waking the consumer (e.g., move it from blocked to ready).
==生产者随后将继续执行到 P3 并调用 `sem_post(&full)`，将 full 信号量的值从 -1 更改为 0 并唤醒消费者（例如，将其从阻塞状态移动到就绪状态）。==

In this case, one of two things could happen.
==在这种情况下，可能会发生两件事之一。==

If the producer continues to run, it will loop around and hit Line P1 again.
==如果生产者继续运行，它将循环并再次执行 P1 行。==

This time, however, it would block, as the empty semaphore's value is 0.
==然而这一次，它将被阻塞，因为 empty 信号量的值为 0。==

If the producer instead was interrupted and the consumer began to run, it would return from `sem_wait(&full)` (Line C1), find that the buffer was full, and consume it.
==如果生产者被中断并且消费者开始运行，它将从 `sem_wait(&full)`（C1 行）返回，发现缓冲区已满，并对其进行消费。==

In either case, we achieve the desired behavior.
==在任何一种情况下，我们要实现了预期的行为。==

You can try this same example with more threads (e.g., multiple producers, and multiple consumers).
==你可以尝试使用更多线程（例如，多个生产者和多个消费者）运行此同一示例。==

It should still work.
==它应该仍然有效。==

Figure 31.11: Adding Mutual Exclusion (Incorrectly)
==图 31.11：添加互斥（错误地）==

Let us now imagine that MAX is greater than 1 (say MAX=10).
==现在让我们想象 MAX 大于 1（比如 MAX=10）。==

For this example, let us assume that there are multiple producers and multiple consumers.
==对于此示例，让我们假设有多个生产者和多个消费者。==

We now have a problem: a race condition.
==我们现在有一个问题：竞态条件。==

Do you see where it occurs?
==你看到它发生在哪里了吗？==

(take some time and look for it) If you can't see it, here's a hint: look more closely at the `put()` and `get()` code.
==（花点时间找一下）如果你没看出来，这里有一个提示：更仔细地查看 `put()` 和 `get()` 代码。==

OK, let's understand the issue.
==好了，让我们来理解这个问题。==

Imagine two producers (Pa and Pb) both calling into `put()` at roughly the same time.
==想象两个生产者（Pa 和 Pb）几乎同时调用 `put()`。==

Assume producer Pa gets to run first, and just starts to fill the first buffer entry (fill=0 at Line F1).
==假设生产者 Pa 先运行，刚开始填充第一个缓冲区条目（F1 行，fill=0）。==

Before Pa gets a chance to increment the fill counter to 1, it is interrupted.
==在 Pa 有机会将 fill 计数器增加到 1 之前，它被中断了。==

Producer Pb starts to run, and at Line F1 it also puts its data into the 0th element of buffer, which means that the old data there is overwritten!
==生产者 Pb 开始运行，并且在 F1 行它也将其数据放入缓冲区的第 0 个元素中，这意味着那里的旧数据被覆盖了！==

This action is a no-no; we don't want any data from the producer to be lost.
==这种行为是绝对禁止的；我们不希望丢失来自生产者的任何数据。==

**A Solution: Adding Mutual Exclusion**
==**解决方案：添加互斥**==

As you can see, what we've forgotten here is mutual exclusion.
==如你所见，我们在这里忘记了互斥。==

The filling of a buffer and incrementing of the index into the buffer is a critical section, and thus must be guarded carefully.
==填充缓冲区和递增缓冲区索引是一个临界区，因此必须小心保护。==

So let's use our friend the binary semaphore and add some locks.
==所以让我们使用我们的朋友二值信号量并添加一些锁。==

Figure 31.11 shows our attempt.
==图 31.11 显示了我们的尝试。==

Now we've added some locks around the entire `put()`/`get()` parts of the code, as indicated by the NEW LINE comments.
==现在，我们在代码的整个 `put()`/`get()` 部分周围添加了一些锁，如 NEW LINE 注释所示。==

That seems like the right idea, but it also doesn't work.
==这似乎是个好主意，但它也不起作用。==

Why? Deadlock.
==为什么？死锁。==

Why does deadlock occur?
==为什么会发生死锁？==

Take a moment to consider it; try to find a case where deadlock arises.
==花点时间考虑一下；试着找出一个发生死锁的情况。==

What sequence of steps must happen for the program to deadlock?
==程序要发生死锁必须经过什么样的步骤序列？==

Figure 31.12: Adding Mutual Exclusion (Correctly)
==图 31.12：添加互斥（正确地）==

**Avoiding Deadlock**
==**避免死锁**==

OK, now that you figured it out, here is the answer.
==好了，既然你已经想通了，这里是答案。==

Imagine two threads, one producer and one consumer.
==想象两个线程，一个生产者和一个消费者。==

The consumer gets to run first.
==消费者先运行。==

It acquires the mutex (Line C0), and then calls `sem_wait()` on the full semaphore (Line C1);
==它获取互斥锁（C0 行），然后在 full 信号量上调用 `sem_wait()`（C1 行）；==

because there is no data yet, this call causes the consumer to block and thus yield the CPU;
==因为还没有数据，此调用会导致消费者阻塞，从而让出 CPU；==

importantly, though, the consumer still holds the lock.
==然而重要的是，消费者仍然持有锁。==

A producer then runs.
==然后一个生产者运行。==

It has data to produce and if it were able to run, it would be able to wake the consumer thread and all would be good.
==它有数据要生产，如果它能运行，它就能唤醒消费者线程，一切都会好起来。==

Unfortunately, the first thing it does is call `sem_wait()` on the binary mutex semaphore (Line P0).
==不幸的是，它做的第一件事是对二值互斥信号量调用 `sem_wait()`（P0 行）。==

The lock is already held.
==锁已经被持有了。==

Hence, the producer is now stuck waiting too.
==因此，生产者现在也卡在等待中。==

There is a simple cycle here.
==这里有一个简单的循环。==

The consumer holds the mutex and is waiting for the someone to signal full.
==消费者持有互斥锁并等待有人向 full 发出信号。==

The producer could signal full but is waiting for the mutex.
==生产者可以向 full 发出信号，但正在等待互斥锁。==

Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock.
==因此，生产者和消费者都卡在互相等待中：一个经典的死锁。==

**At Last, A Working Solution**
==**最后，一个可行的解决方案**==

To solve this problem, we simply must reduce the scope of the lock.
==为了解决这个问题，我们只需缩小锁的范围。==

Figure 31.12 shows the correct solution.
==图 31.12 显示了正确的解决方案。==

As you can see, we simply move the mutex acquire and release to be just around the critical section;
==如你所见，我们只需将互斥锁的获取和释放移动到仅围绕临界区的位置；==

the full and empty wait and signal code is left outside.
==full 和 empty 的等待和信号代码被留在外面。==

The result is a simple and working bounded buffer, a commonly-used pattern in multi-threaded programs.
==结果是一个简单且可用的有界缓冲区，这是多线程程序中常用的一种模式。==

Understand it now; use it later.
==现在理解它；以后使用它。==

You will thank us for years to come.
==你会感激我们很多年的。==

Or at least, you will thank us when the same question is asked on the final exam, or during a job interview.
==或者至少，当期末考试或求职面试中问到同样的问题时，你会感谢我们。==

31.5 Reader-Writer Locks
==31.5 读写锁==

Another classic problem stems from the desire for a more flexible locking primitive that admits that different data structure accesses might require different kinds of locking.
==另一个经典问题源于对更灵活的锁定原语的需求，这种原语承认不同的数据结构访问可能需要不同类型的锁定。==

For example, imagine a number of concurrent list operations, including inserts and simple lookups.
==例如，想象许多并发的列表操作，包括插入和简单的查找。==

While inserts change the state of the list (and thus a traditional critical section makes sense), lookups simply read the data structure;
==虽然插入会改变列表的状态（因此传统的临界区是有意义的），但查找只是读取数据结构；==

as long as we can guarantee that no insert is on-going, we can allow many lookups to proceed concurrently.
==只要我们能保证没有插入正在进行，我们就可以允许多个查找并发进行。==

The special type of lock we will now develop to support this type of operation is known as a reader-writer lock.
==我们现在要开发的用于支持此类操作的特殊类型的锁被称为读写锁。==

The code for such a lock is available in Figure 31.13.
==这种锁的代码在图 31.13 中提供。==

The code is pretty simple.
==代码非常简单。==

If some thread wants to update the data structure in question, it should call the new pair of synchronization operations: `rwlock_acquire_writelock()`, to acquire a write lock, and `rwlock_release_writelock()`, to release it.
==如果某个线程想要更新相关的数据结构，它应该调用新的一对同步操作：`rwlock_acquire_writelock()` 来获取写锁，以及 `rwlock_release_writelock()` 来释放它。==

Internally, these simply use the writelock semaphore to ensure that only a single writer can acquire the lock and thus enter the critical section to update the data structure in question.
==在内部，这些操作只是简单地使用 writelock 信号量来确保只有一个写入者可以获取锁，从而进入临界区更新相关的数据结构。==

More interesting is the pair of routines to acquire and release read locks.
==更有趣的是获取和释放读锁的那对例程。==

When acquiring a read lock, the reader first acquires lock and then increments the readers variable to track how many readers are currently inside the data structure.
==在获取读锁时，读取者首先获取 lock，然后递增 readers 变量以跟踪当前有多少读取者在数据结构内。==

The important step then taken within `rwlock_acquire_readlock()` occurs when the first reader acquires the lock;
==`rwlock_acquire_readlock()` 中采取的重要步骤发生在第一个读取者获取锁时；==

in that case, the reader also acquires the write lock by calling `sem_wait()` on the writelock semaphore, and then releasing the lock by calling `sem_post()`.
==在这种情况下，读取者还会通过对 writelock 信号量调用 `sem_wait()` 来获取写锁，然后通过调用 `sem_post()` 释放 lock。==

Thus, once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too;
==因此，一旦一个读取者获取了读锁，更多的读取者也将被允许获取读锁；==

however, any thread that wishes to acquire the write lock will have to wait until all readers are finished;
==然而，任何希望获取写锁的线程都必须等待，直到所有读取者完成；==

the last one to exit the critical section calls `sem_post()` on "writelock" and thus enables a waiting writer to acquire the lock.
==最后一个退出临界区的读取者会对 "writelock" 调用 `sem_post()`，从而使等待的写入者能够获取锁。==

Figure 31.13: A Simple Reader-Writer Lock
==图 31.13：一个简单的读写锁==

Finally, it should be noted that reader-writer locks should be used with some caution.
==最后，应该注意的是，使用读写锁应谨慎。==

They often add more overhead (especially with more sophisticated implementations), and thus do not end up speeding up performance as compared to just using simple and fast locking primitives.
==它们通常会增加更多的开销（特别是对于更复杂的实现），因此与仅使用简单快速的锁定原语相比，最终并没有提高性能。==

Either way, they showcase once again how we can use semaphores in an interesting and useful way.
==不管怎样，它们再次展示了我们如何以有趣和有用的方式使用信号量。==

**TIP: SIMPLE AND DUMB CAN BE BETTER (HILL'S LAW)**
==**提示：简单粗暴可能更好（HILL 定律）**==

You should never underestimate the notion that the simple and dumb approach can be the best one.
==你永远不应该低估这样一个观念：简单粗暴的方法可能是最好的方法。==

With locking, sometimes a simple spin lock works best, because it is easy to implement and fast.
==对于锁定，有时简单的自旋锁效果最好，因为它易于实现且速度快。==

Although something like reader/writer locks sounds cool, they are complex, and complex can mean slow.
==虽然像读/写锁这样的东西听起来很酷，但它们很复杂，而复杂可能意味着缓慢。==

Thus, always try the simple and dumb approach first.
==因此，总是先尝试简单粗暴的方法。==

This idea, of appealing to simplicity, is found in many places.
==这种追求简洁的想法在很多地方都有体现。==

One early source is Mark Hill's dissertation, which studied how to design caches for CPUs.
==早期的来源之一是 Mark Hill 的论文，该论文研究了如何为 CPU 设计缓存。==

Hill found that simple direct-mapped caches worked better than fancy set-associative designs (one reason is that in caching, simpler designs enable faster lookups).
==Hill 发现，简单的直接映射缓存比花哨的组相联设计效果更好（原因之一是，在缓存中，更简单的设计可以实现更快的查找）。==

As Hill succinctly summarized his work: "Big and dumb is better."
==正如 Hill 简洁地总结他的工作那样：“傻大黑粗（Big and dumb）更好。”==

And thus we call this similar advice Hill's Law.
==因此，我们将类似的建议称为 Hill 定律。==

31.6 The Dining Philosophers
==31.6 哲学家就餐问题==

One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the dining philosopher's problem.
==Dijkstra 提出并解决的最著名的并发问题之一被称为哲学家就餐问题。==

The problem is famous because it is fun and somewhat intellectually interesting;
==这个问题之所以出名，是因为它很有趣，而且在智力上也有点意思；==

however, its practical utility is low.
==然而，其实用价值很低。==

However, its fame forces its inclusion here;
==不过，它的名气迫使我们在这里将其收录；==

indeed, you might be asked about it on some interview, and you'd really hate your OS professor if you miss that question and don't get the job.
==的确，你可能会在某次面试中被问到这个问题，如果你错过了这个问题并没有得到这份工作，你会非常讨厌你的操作系统教授。==

Conversely, if you get the job, please feel free to send your OS professor a nice note, or some stock options.
==反之，如果你得到了这份工作，请随意给你的操作系统教授寄一张漂亮的便条，或者一些股票期权。==

The basic setup for the problem is this (as shown in Figure 31.14): assume there are five "philosophers" sitting around a table.
==问题的基本设置是这样的（如图 31.14 所示）：假设有五个“哲学家”围坐在一张桌子旁。==

Between each pair of philosophers is a single fork (and thus, five total).
==每对哲学家之间都有一把叉子（因此总共有五把）。==

The philosophers each have times where they think, and don't need any forks, and times where they eat.
==每位哲学家都有思考的时间（不需要任何叉子），也有进食的时间。==

In order to eat, a philosopher needs two forks, both the one on their left and the one on their right.
==为了进食，哲学家需要两把叉子，即左手边的和右手边的。==

The contention for these forks, and the synchronization problems that ensue, are what makes this a problem we study in concurrent programming.
==对这些叉子的争夺，以及随之而来的同步问题，使这成为我们在并发编程中研究的一个问题。==

Here is the basic loop of each philosopher, assuming each has a unique thread identifier p from 0 to 4 (inclusive):
==这是每位哲学家的基本循环，假设每位哲学家都有一个从 0 到 4（含）的唯一线程标识符 p：==

`while (1) { think(); get_forks(p); eat(); put_forks(p); }`
==（代码逻辑：思考，获取叉子，进食，放下叉子）==

The key challenge, then, is to write the routines `get_forks()` and `put_forks()` such that there is no deadlock, no philosopher starves and never gets to eat, and concurrency is high (i.e., as many philosophers can eat at the same time as possible).
==关键的挑战在于编写例程 `get_forks()` 和 `put_forks()`，使得没有死锁，没有哲学家挨饿而永远吃不到东西，并且并发性很高（即尽可能多的哲学家可以同时进食）。==

Figure 31.14: The Dining Philosophers
==图 31.14：哲学家就餐==

Following Downey's solutions, we'll use a few helper functions to get us towards a solution.
==遵循 Downey 的解决方案，我们将使用一些辅助函数来帮助我们找到解决方案。==

They are: `int left(int p) { return p; }` and `int right(int p) { return (p + 1) % 5; }`.
==它们是：`int left(int p) { return p; }` 和 `int right(int p) { return (p + 1) % 5; }`。==

When philosopher p wishes to refer to the fork on their left, they simply call `left(p)`.
==当哲学家 p 希望引用他们左边的叉子时，他们只需调用 `left(p)`。==

Similarly, the fork on the right of a philosopher p is referred to by calling `right(p)`;
==同样，哲学家 p 右边的叉子通过调用 `right(p)` 来引用；==

the modulo operator therein handles the one case where the last philosopher (p=4) tries to grab the fork on their right, which is fork 0.
==其中的模运算符处理了最后一位哲学家（p=4）试图抓住右边叉子（即叉子 0）的情况。==

We'll also need some semaphores to solve this problem.
==我们还需要一些信号量来解决这个问题。==

Let us assume we have five, one for each fork: `sem_t forks[5]`.
==让我们假设我们有五个信号量，每把叉子一个：`sem_t forks[5]`。==

**Broken Solution**
==**有问题的解决方案**==

We attempt our first solution to the problem.
==我们尝试该问题的第一个解决方案。==

Assume we initialize each semaphore (in the forks array) to a value of 1.
==假设我们将每个信号量（在 forks 数组中）初始化为值 1。==

Assume also that each philosopher knows its own number (p).
==还要假设每个哲学家都知道自己的编号（p）。==

We can thus write the `get_forks()` and `put_forks()` routine (Figure 31.15).
==我们因此可以编写 `get_forks()` 和 `put_forks()` 例程（图 31.15）。==

Figure 31.15: The `get_forks()` And `put_forks()` Routines
==图 31.15：`get_forks()` 和 `put_forks()` 例程==

The intuition behind this (broken) solution is as follows.
==这个（有问题的）解决方案背后的直觉如下。==

To acquire the forks, we simply grab a "lock" on each one: first the one on the left, and then the one on the right.
==为了获取叉子，我们只需获取每把叉子的“锁”：首先是左边的，然后是右边的。==

When we are done eating, we release them.
==当我们吃完后，我们释放它们。==

Simple, no?
==很简单，不是吗？==

Unfortunately, in this case, simple means broken.
==不幸的是，在这种情况下，简单意味着有问题。==

Can you see the problem that arises?
==你能看出出现的问题吗？==

Think about it.
==想一想。==

The problem is deadlock.
==问题是死锁。==

If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right, each will be stuck holding one fork and waiting for another, forever.
==如果每位哲学家都碰巧在任何哲学家能抓住右边的叉子之前抓住了左边的叉子，那么每位哲学家都会卡在手里拿着一把叉子并永远等待另一把叉子的状态。==

Specifically, philosopher 0 grabs fork 0, philosopher 1 grabs fork 1, philosopher 2 grabs fork 2, philosopher 3 grabs fork 3, and philosopher 4 grabs fork 4;
==具体来说，哲学家 0 抓住叉子 0，哲学家 1 抓住叉子 1，哲学家 2 抓住叉子 2，哲学家 3 抓住叉子 3，哲学家 4 抓住叉子 4；==

all the forks are acquired, and all the philosophers are stuck waiting for a fork that another philosopher possesses.
==所有的叉子都被获取了，所有的哲学家都卡在等待另一位哲学家拥有的叉子上。==

We'll study deadlock in more detail soon; for now, it is safe to say that this is not a working solution.
==我们很快将更详细地研究死锁；目前，可以肯定地说这不是一个可行的解决方案。==

**A Solution: Breaking The Dependency**
==**解决方案：打破依赖**==

The simplest way to attack this problem is to change how forks are acquired by at least one of the philosophers;
==解决这个问题最简单的方法是改变至少一位哲学家获取叉子的方式；==

indeed, this is how Dijkstra himself solved the problem.
==事实上，这也是 Dijkstra 本人解决这个问题的方法。==

Specifically, let's assume that philosopher 4 (the highest numbered one) gets the forks in a different order than the others (Figure 31.16);
==具体来说，让我们假设哲学家 4（编号最高的那位）以与其他哲学家不同的顺序获取叉子（图 31.16）；==

the `put_forks()` code remains the same.
==`put_forks()` 代码保持不变。==

Figure 31.16: Breaking The Dependency In `get_forks()`
==图 31.16：打破 `get_forks()` 中的依赖==

Because the last philosopher tries to grab right before left, there is no situation where each philosopher grabs one fork and is stuck waiting for another;
==因为最后一位哲学家试图在抓左边之前抓右边，所以不会出现每位哲学家都抓住一把叉子并卡在等待另一把叉子的情况；==

the cycle of waiting is broken.
==等待的循环被打破了。==

Think through the ramifications of this solution, and convince yourself that it works.
==仔细思考这个解决方案的后果，并让自己相信它是有效的。==

There are other "famous" problems like this one, e.g., the cigarette smoker's problem or the sleeping barber problem.
==还有其他类似的“著名”问题，例如吸烟者问题或理发师睡觉问题。==

Most of them are just excuses to think about concurrency; some of them have fascinating names.
==它们大多数只是思考并发性的借口；其中一些有迷人的名字。==

Look them up if you are interested in learning more, or just getting more practice thinking in a concurrent manner.
==如果你有兴趣了解更多，或者只是想获得更多以并发方式思考的练习，可以查阅它们。==

31.7 Thread Throttling
==31.7 线程节流==

One other simple use case for semaphores arises on occasion, and thus we present it here.
==信号量的另一个简单的用例偶尔会出现，因此我们在这里介绍它。==

The specific problem is this: how can a programmer prevent "too many" threads from doing something at once and bogging the system down?
==具体问题是这样的：程序员如何防止“太多”线程同时做某事而使系统陷入瘫痪？==

Answer: decide upon a threshold for "too many", and then use a semaphore to limit the number of threads concurrently executing the piece of code in question.
==答案：确定“太多”的阈值，然后使用信号量限制并发执行相关代码段的线程数量。==

We call this approach throttling, and consider it a form of admission control.
==我们将这种方法称为节流，并将其视为一种准入控制形式。==

Let's consider a more specific example.
==让我们考虑一个更具体的例子。==

Imagine that you create hundreds of threads to work on some problem in parallel.
==想象一下，你创建了数百个线程来并行处理某个问题。==

However, in a certain part of the code, each thread acquires a large amount of memory to perform part of the computation;
==然而，在代码的某个部分，每个线程都需要获取大量内存来执行部分计算；==

let's call this part of the code the memory-intensive region.
==我们将这部分代码称为内存密集型区域。==

If all of the threads enter the memory-intensive region at the same time, the sum of all the memory allocation requests will exceed the amount of physical memory on the machine.
==如果所有线程同时进入内存密集型区域，所有内存分配请求的总和将超过机器上的物理内存量。==

As a result, the machine will start thrashing (i.e., swapping pages to and from the disk), and the entire computation will slow to a crawl.
==结果，机器将开始颠簸（即，将页面换入和换出磁盘），整个计算将慢得像爬一样。==

A simple semaphore can solve this problem.
==一个简单的信号量就可以解决这个问题。==

By initializing the value of the semaphore to the maximum number of threads you wish to enter the memory-intensive region at once, and then putting a `sem_wait()` and `sem_post()` around the region, a semaphore can naturally throttle the number of threads that are ever concurrently in the dangerous region of the code.
==通过将信号量的值初始化为你希望同时进入内存密集型区域的最大线程数，然后在该区域周围放置 `sem_wait()` 和 `sem_post()`，信号量可以自然地限制同时处于代码危险区域的线程数量。==

31.8 How To Implement Semaphores
==31.8 如何实现信号量==

Finally, let's use our low-level synchronization primitives, locks and condition variables, to build our own version of semaphores called (drum roll here) ... Zemaphores.
==最后，让我们使用我们的低级同步原语，锁和条件变量，来构建我们自己版本的信号量，称为（此处击鼓）……Zemaphores。==

This task is fairly straightforward, as you can see in Figure 31.17.
==这个任务相当简单，如图 31.17 所示。==

In the code above, we use just one lock and one condition variable, plus a state variable to track the value of the semaphore.
==在上面的代码中，我们只使用了一个锁和一个条件变量，外加一个状态变量来跟踪信号量的值。==

Study the code for yourself until you really understand it.
==自己研究代码，直到你真正理解它。==

Do it!
==行动起来！==

One subtle difference between our Zemaphore and pure semaphores as defined by Dijkstra is that we don't maintain the invariant that the value of the semaphore, when negative, reflects the number of waiting threads;
==我们的 Zemaphore 与 Dijkstra 定义的纯信号量之间的一个细微差别是，我们不维护这样一个不变量：当信号量的值为负时，它反映了等待线程的数量；==

indeed, the value will never be lower than zero.
==事实上，该值永远不会低于零。==

This behavior is easier to implement and matches the current Linux implementation.
==这种行为更容易实现，并且与当前的 Linux 实现相匹配。==

Figure 31.17: Implementing Zemaphores With Locks And CVs
==图 31.17：用锁和 CV 实现 Zemaphores==

Curiously, building condition variables out of semaphores is a much trickier proposition.
==奇怪的是，用信号量构建条件变量是一个棘手得多的命题。==

Some highly experienced concurrent programmers tried to do this in the Windows environment, and many different bugs ensued.
==一些非常有经验的并发程序员试图在 Windows 环境中这样做，结果随之而来的是许多不同的错误。==

Try it yourself, and see if you can figure out why building condition variables out of semaphores is more challenging of a problem than it might appear.
==你自己试一试，看看你能否弄清楚为什么用信号量构建条件变量是一个比看起来更具挑战性的问题。==

31.9 Summary
==31.9 总结==

Semaphores are a powerful and flexible primitive for writing concurrent programs.
==信号量是编写并发程序的一种强大而灵活的原语。==

Some programmers use them exclusively, shunning locks and condition variables, due to their simplicity and utility.
==一些程序员因为信号量的简单和实用而只使用它们，避开锁和条件变量。==

In this chapter, we have presented just a few classic problems and solutions.
==在本章中，我们仅介绍了几个经典问题和解决方案。==

If you are interested in finding out more, there are many other materials you can reference.
==如果你有兴趣了解更多，还有许多其他资料可以参考。==

One great (and free reference) is Allen Downey's book on concurrency and programming with semaphores.
==一本很棒的（且免费的参考书）是 Allen Downey 关于并发和信号量编程的书。==

This book has lots of puzzles you can work on to improve your understanding of both semaphores in specific and concurrency in general.
==这本书有很多谜题，你可以通过解题来提高你对具体信号量和一般并发性的理解。==

**TIP: BE CAREFUL WITH GENERALIZATION**
==**提示：小心泛化**==

The abstract technique of generalization can thus be quite useful in systems design, where one good idea can be made slightly broader and thus solve a larger class of problems.
==因此，泛化的抽象技术在系统设计中非常有用，其中一个好的想法可以稍微扩展一下，从而解决更大的一类问题。==

However, be careful when generalizing;
==然而，泛化时要小心；==

as Lampson warns us "Don't generalize; generalizations are generally wrong".
==正如 Lampson 警告我们的那样：“不要泛化；泛化通常是错误的”。==

One could view semaphores as a generalization of locks and condition variables; however, is such a generalization needed?
==人们可以将信号量视为锁和条件变量的泛化；然而，这种泛化是必要的吗？==

And, given the difficulty of realizing a condition variable on top of a semaphore, perhaps this generalization is not as general as you might think.
==而且，鉴于在信号量之上实现条件变量的困难，这种泛化也许并不像你想象的那么通用。==

Becoming a real concurrency expert takes years of effort;
==成为一名真正的并发专家需要多年的努力；==

going beyond what you learn in this class is undoubtedly the key to mastering such a topic.
==超越你在本课程中学到的知识无疑是掌握这一主题的关键。==


SEMAPHORES

==信号量==

  

Homework (Code)

==作业（代码）==

  

In this homework, we'll use semaphores to solve some well-known concurrency problems.

==在本次作业中，我们将使用信号量来解决一些著名的并发问题。==

  

Many of these are taken from Downey's excellent "Little Book of Semaphores", which does a good job of pulling together a number of classic problems as well as introducing a few new variants.

==其中许多问题取自 Downey 优秀的《信号量小书》（Little Book of Semaphores），该书很好地汇集了许多经典问题，并介绍了一些新的变体。==

  

Interested readers should check out the Little Book for more fun.

==感兴趣的读者应该去看看这本小书以获取更多乐趣。==

  

Each of the following questions provides a code skeleton; your job is to fill in the code to make it work given semaphores.

==以下每个问题都提供了一个代码框架；你的任务是填充代码，使其在给定信号量的情况下正常工作。==

  

On Linux, you will be using native semaphores; on a Mac (where there is no semaphore support), you'll have to first build an implementation (using locks and condition variables, as described in the chapter).

==在 Linux 上，你将使用原生信号量；在 Mac 上（不支持信号量），你必须先构建一个实现（如本章所述，使用锁和条件变量）。==

  

Good luck!

==祝你好运！==

  

Questions

==问题==

  

1. The first problem is just to implement and test a solution to the fork/join problem, as described in the text.

==2. 第一个问题只是实现并测试 fork/join 问题的解决方案，如文中所述。==

  

Even though this solution is described in the text, the act of typing it in on your own is worthwhile.

==尽管文中描述了这个解决方案，但亲自输入代码是值得的。==

  

Even Bach would rewrite Vivaldi, allowing one soon-to-be master to learn from an existing one.

==即使是巴赫也会重写维瓦尔第的曲谱，这让一位未来的大师能够向现有的像大师学习。==

  

See fork-join.c for details.

==详见 fork-join.c。==

  

Add the call sleep (1) to the child to ensure it is working.

==在子进程中添加调用 sleep (1) 以确保其正常工作。==

  

2. Let's now generalize this a bit by investigating the rendezvous problem.

==3. 现在让我们通过研究会合（rendezvous）问题来对此进行一点推广。==

  

The problem is as follows: you have two threads, each of which are about to enter the rendezvous point in the code.

==问题如下：你有两个线程，每个线程都准备进入代码中的会合点。==

  

Neither should exit this part of the code before the other enters it.

==在另一个线程进入该部分代码之前，任何一个线程都不应退出。==

  

Consider using two semaphores for this task, and see rendezvous.c for details.

==考虑使用两个信号量来完成此任务，详见 rendezvous.c。==

  

3. Now go one step further by implementing a general solution to barrier synchronization.

==4. 现在更进一步，实现屏障（barrier）同步的通用解决方案。==

  

Assume there are two points in a sequential piece of code, called  and .

==假设在一段顺序代码中有两个点，分别称为  和 。==

  

Putting a barrier between  and  guarantees that all threads will execute  before any one thread executes .

==在  和  之间放置一个屏障，可以保证所有线程在任何一个线程执行  之前都已执行完 。==

  

Your task: write the code to implement a barrier() function that can be used in this manner.

==你的任务：编写代码来实现一个可以这种方式使用的 barrier() 函数。==

  

It is safe to assume you know N (the total number of threads in the running program) and that all N threads will try to enter the barrier.

==可以安全地假设你知道 N（运行程序中的线程总数），并且所有 N 个线程都会尝试进入屏障。==

  

Again, you should likely use two semaphores to achieve the solution, and some other integers to count things.

==同样，你可能应该使用两个信号量来实现解决方案，并使用其他一些整数来进行计数。==

  

See barrier.c for details.

==详见 barrier.c。==

  

4. Now let's solve the reader-writer problem, also as described in the text.

==5. 现在让我们解决读者-写者问题，正如文中所述。==

  

In this first take, don't worry about starvation.

==在第一次尝试中，不必担心饥饿问题。==

  

See the code in reader-writer.c for details.

==详见 reader-writer.c 中的代码。==

  

Add sleep() calls to your code to demonstrate it works as you expect.

==在你的代码中添加 sleep() 调用，以证明它按你的预期工作。==

  

Can you show the existence of the starvation problem?

==你能展示饥饿问题的存在吗？==

  

5. Let's look at the reader-writer problem again, but this time, worry about starvation.

==6. 让我们再次审视读者-写者问题，但这次要考虑饥饿问题。==

  

How can you ensure that all readers and writers eventually make progress?

==如何确保所有读者和写者最终都能取得进展？==

  

See reader-writer-nostarve.c for details.

==详见 reader-writer-nostarve.c。==

  

6. Use semaphores to build a no-starve mutex, in which any thread that tries to acquire the mutex will eventually obtain it.

==7. 使用信号量构建一个无饥饿互斥锁，任何尝试获取该互斥锁的线程最终都会获得它。==

  

See the code in mutex-nostarve.c for more information.

==更多信息请参见 mutex-nostarve.c 中的代码。==

  

7. Liked these problems?

==8. 喜欢这些问题吗？==

  

See Downey's free text for more just like them.

==参阅 Downey 的免费教材，了解更多类似问题。==

  

And don't forget, have fun!

==别忘了，玩得开心！==

  

But, you always do when you write code, no?

==不过，你在写代码的时候总是很开心的，不是吗？==

  

Common Concurrency Problems

==常见并发问题==

  

Researchers have spent a great deal of time and effort looking into concurrency bugs over many years.

==多年来，研究人员花费了大量的时间和精力来研究并发 bug。==

  

Much of the early work focused on deadlock, a topic which we've touched on in the past chapters but will now dive into deeply [C+71].

==早期的许多工作集中在死锁上，我们在前面的章节中已经涉及过这个话题，但现在将深入探讨 [C+71]。==

  

More recent work focuses on studying other types of common concurrency bugs (i.e., non-deadlock bugs).

==最近的工作集中在研究其他类型的常见并发 bug（即非死锁 bug）。==

  

In this chapter, we take a brief look at some example concurrency problems found in real code bases, to better understand what problems to look out for.

==在本章中，我们将简要介绍一些在真实代码库中发现的并发问题示例，以便更好地理解需要注意哪些问题。==

  

And thus our central issue for this chapter:

==因此，我们本章的核心问题是：==

  

CRUX: HOW TO HANDLE COMMON CONCURRENCY BUGS

==关键问题：如何处理常见的并发 BUG==

  

Concurrency bugs tend to come in a variety of common patterns.

==并发 bug 往往以各种常见的模式出现。==

  

Knowing which ones to look out for is the first step to writing more robust, correct concurrent code.

==了解需要注意哪些模式是编写更健壮、正确的并发代码的第一步。==

  

32.1 What Types Of Bugs Exist?

==32.1 存在哪些类型的 Bug？==

  

The first, and most obvious, question is this: what types of concurrency bugs manifest in complex, concurrent programs?

==第一个也是最明显的问题是：在复杂的并发程序中会显现出哪些类型的并发 bug？==

  

This question is difficult to answer in general, but fortunately, some others have done the work for us.

==这个问题通常很难回答，但幸运的是，其他人已经为我们完成了这项工作。==

  

Specifically, we rely upon a study by Lu et al. , which analyzes a number of popular concurrent applications in great detail to understand what types of bugs arise in practice.

==具体来说，我们依赖于 Lu 等人  的一项研究，该研究非常详细地分析了许多流行的并发应用程序，以了解实践中会出现哪些类型的 bug。==

  

The study focuses on four major and important open-source applications: MySQL (a popular database management system), Apache (a well-known web server), Mozilla (the famous web browser), and OpenOffice (a free version of the MS Office suite, which some people actually use).

==该研究侧重于四个主要且重要的开源应用程序：MySQL（流行的数据库管理系统）、Apache（著名的 Web 服务器）、Mozilla（著名的 Web 浏览器）和 OpenOffice（MS Office 套件的免费版本，实际上有些人即使是现在也在使用）。==

  

In the study, the authors examine concurrency bugs that have been found and fixed in each of these code bases, turning the developers' work into a quantitative bug analysis.

==在这项研究中，作者检查了在每个代码库中发现并修复的并发 bug，将开发人员的工作转化为定量的 bug 分析。==

  

Understanding these results can help you understand what types of problems actually occur in mature code bases.

==理解这些结果可以帮助你了解在成熟的代码库中实际会发生什么类型的问题。==

  

Figure 32.1 shows a summary of the bugs Lu and colleagues studied.

==图 32.1 显示了 Lu 及其同事研究的 bug 摘要。==

  

From the figure, you can see that there were 105 total bugs, most of which were not deadlock (74); the remaining 31 were deadlock bugs.

==从图中可以看出，总共有 105 个 bug，其中大多数不是死锁（74 个）；其余 31 个是死锁 bug。==

  

Figure 32.1: Bugs In Modern Applications

==图 32.1：现代应用程序中的 Bug==

  

Further, you can see the number of bugs studied from each application.

==此外，你可以看到每个应用程序中被研究的 bug 数量。==

  

While OpenOffice only had 8 total concurrency bugs, Mozilla had nearly 60.

==虽然 OpenOffice 总共只有 8 个并发 bug，但 Mozilla 有近 60 个。==

  

We now dive into these different classes of bugs (non-deadlock, deadlock) a bit more deeply.

==我们现在将更深入地探讨这些不同类别的 bug（非死锁，死锁）。==

  

For the first class of non-deadlock bugs, we use examples from the study to drive our discussion.

==对于第一类非死锁 bug，我们使用研究中的例子来推动我们的讨论。==

  

For the second class of deadlock bugs, we discuss the long line of work that has been done in either preventing, avoiding, or handling deadlock.

==对于第二类死锁 bug，我们讨论在预防、避免或处理死锁方面所做的一系列长期工作。==

  

32.2 Non-Deadlock Bugs

==32.2 非死锁 Bug==

  

Non-deadlock bugs make up a majority of concurrency bugs, according to Lu's study.

==根据 Lu 的研究，非死锁 bug 构成了并发 bug 的大多数。==

  

But what types of bugs are these?

==但这些是什么类型的 bug？==

  

How do they arise?

==它们是如何产生的？==

  

How can we fix them?

==我们要如何修复它们？==

  

We now discuss the two major types of non-deadlock bugs found by Lu et al.: atomicity violation bugs and order violation bugs.

==我们现在讨论 Lu 等人发现的两类主要的非死锁 bug：原子性违反 bug 和顺序违反 bug。==

  

Atomicity-Violation Bugs

==原子性违反 Bug==

  

The first type of problem encountered is referred to as an atomicity violation.

==遇到的第一类问题被称为原子性违反。==

  

Here is a simple example, found in MySQL.

==这是在 MySQL 中发现的一个简单示例。==

  

Before reading the explanation, try figuring out what the bug is.

==在阅读解释之前，试着找出 bug 是什么。==

  

Do it!

==动手吧！==

  

Figure 32.2: Atomicity Violation (atomicity.c)

==图 32.2：原子性违反 (atomicity.c)==

  

In the example, two different threads access the field proc_info in the structure thd.

==在这个例子中，两个不同的线程访问结构体 thd 中的字段 proc_info。==

  

The first thread checks if the value is non-NULL and then prints its value.

==第一个线程检查该值是否为非 NULL，然后打印其值。==

  

The second thread sets it to NULL.

==第二个线程将其设置为 NULL。==

  

Clearly, if the first thread performs the check but then is interrupted before the call to fputs, the second thread could run in-between, thus setting the pointer to NULL.

==显然，如果第一个线程执行了检查但在调用 fputs 之前被打断，第二个线程可能会在中间运行，从而将指针设置为 NULL。==

  

When the first thread resumes, it will crash, as a NULL pointer will be dereferenced by fputs.

==当第一个线程恢复时，它将崩溃，因为 fputs 将解引用一个 NULL 指针。==

  

The more formal definition of an atomicity violation, according to Lu et al, is this: "The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution)."

==根据 Lu 等人的说法，原子性违反的更正式定义是：“多次内存访问之间预期的可串行性被破坏（即，某个代码区域本应是原子的，但在执行过程中未强制执行原子性）。”==

  

In our example above, the code has an atomicity assumption (in Lu's words) about the check for non-NULL of proc_info and the usage of proc_info in the fputs() call.

==在上面的例子中，代码对 proc_info 的非 NULL 检查和 fputs() 调用中对 proc_info 的使用有一个原子性假设（用 Lu 的话说）。==

  

When the assumption is incorrect, the code will not work as desired.

==当这个假设不正确时，代码将无法按预期工作。==

  

Finding a fix for this type of problem is often (but not always) straightforward.

==找到这类问题的修复方法通常（但并非总是）很简单。==

  

Can you think of how to fix the code above?

==你能想到如何修复上面的代码吗？==

  

In this solution (Figure 32.3), we simply add locks around the shared-variable references, ensuring that when either thread accesses the proc_info field, it has a lock held (proc_info_lock).

==在这个解决方案（图 32.3）中，我们只是简单地在共享变量引用周围添加锁，确保当任一线程访问 proc_info 字段时，它都持有一个锁（proc_info_lock）。==

  

Of course, any other code that accesses the structure should also acquire this lock before doing so.

==当然，任何其他访问该结构的代码在访问之前也应该获取这个锁。==

  

Figure 32.3: Atomicity Violation Fixed (atomicity_fixed.c)

==图 32.3：原子性违反已修复 (atomicity_fixed.c)==

  

Order-Violation Bugs

==顺序违反 Bug==

  

Another common type of non-deadlock bug found by Lu et al. is known as an order violation.

==Lu 等人发现的另一种常见的非死锁 bug 称为顺序违反。==

  

Here is another simple example; once again, see if you can figure out why the code below has a bug in it.

==这是另一个简单的例子；再一次，看看你能不能找出为什么下面的代码中有 bug。==

  

Figure 32.4: Ordering Bug (ordering.c)

==图 32.4：顺序 Bug (ordering.c)==

  

As you probably figured out, the code in Thread 2 seems to assume that the variable mThread has already been initialized (and is not NULL).

==正如你可能已经发现的那样，线程 2 中的代码似乎假设变量 mThread 已经被初始化（并且不是 NULL）。==

  

However, if Thread 2 runs immediately once created, the value of mThread will not be set when it is accessed within mMain() in Thread 2, and will likely crash with a NULL-pointer dereference.

==然而，如果线程 2 在创建后立即运行，那么在线程 2 的 mMain() 中访问 mThread 时，它的值将不会被设置，并且可能会因 NULL 指针解引用而崩溃。==

  

Note that we assume the value of mThread is initially NULL.

==请注意，我们假设 mThread 的值最初为 NULL。==

  

If not, even stranger things could happen as arbitrary memory locations are accessed through the dereference in Thread 2.

==如果不是，甚至可能会发生更奇怪的事情，因为线程 2 中的解引用会访问任意内存位置。==

  

The more formal definition of an order violation is the following: "The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution)" [L+08].

==顺序违反的更正式定义如下：“两个（组）内存访问之间的预期顺序被颠倒（即，A 本应始终在 B 之前执行，但在执行过程中未强制执行该顺序）” [L+08]。==

  

The fix to this type of bug is generally to enforce ordering.

==这类 bug 的修复方法通常是强制执行顺序。==

  

As discussed previously, using condition variables is an easy and robust way to add this style of synchronization into modern code bases.

==正如前面所讨论的，使用条件变量是将这种同步风格添加到现代代码库中的一种简单而健壮的方法。==

  

In the example above, we could thus rewrite the code as seen in Figure 32.5.

==因此，在上面的例子中，我们可以像图 32.5 那样重写代码。==

  

Figure 32.5: Fixing The Ordering Violation (ordering_fixed.c)

==图 32.5：修复顺序违反 (ordering_fixed.c)==

  

In this fixed-up code sequence, we have added a condition variable (mtCond) and corresponding lock (mtLock), as well as a state variable (mtInit).

==在这个修正后的代码序列中，我们添加了一个条件变量 (mtCond) 和相应的锁 (mtLock)，以及一个状态变量 (mtInit)。==

  

When the initialization code runs, it sets the state of mtInit to 1 and signals that it has done so.

==当初始化代码运行时，它将 mtInit 的状态设置为 1，并发出已完成的信号。==

  

If Thread 2 had run before this point, it will be waiting for this signal and corresponding state change.

==如果线程 2 在此之前已经运行，它将等待此信号和相应的状态更改。==

  

If it runs later, it will check the state and see that the initialization has already occurred (i.e., mtInit is set to 1), and thus continue as is proper.

==如果它在之后运行，它将检查状态并看到初始化已经发生（即 mtInit 设置为 1），从而按正常流程继续。==

  

Note that we could likely use mThread as the state variable itself, but do not do so for the sake of simplicity here.

==请注意，我们可能直接使用 mThread 本身作为状态变量，但为了简单起见，这里没有这样做。==

  

When ordering matters between threads, condition variables (or semaphores) can come to the rescue.

==当线程之间的顺序很重要时，条件变量（或信号量）可以派上用场。==

  

Non-Deadlock Bugs: Summary

==非死锁 Bug：小结==

  

A large fraction (97%) of non-deadlock bugs studied by Lu et al. are either atomicity or order violations.

==Lu 等人研究的非死锁 bug 中，很大一部分（97%）要么是原子性违反，要么是顺序违反。==

  

Thus, by carefully thinking about these types of bug patterns, programmers can likely do a better job of avoiding them.

==因此，通过仔细思考这些类型的 bug 模式，程序员可能会更好地避免它们。==

  

Moreover, as more automated code-checking tools develop, they should likely focus on these two types of bugs as they constitute such a large fraction of non-deadlock bugs found in deployment.

==此外，随着更多自动化代码检查工具的开发，它们可能应该专注于这两类 bug，因为它们构成了部署中发现的非死锁 bug 的很大一部分。==

  

Unfortunately, not all bugs are as easily fixed as the examples we looked at above.

==不幸的是，并不是所有的 bug 都像我们在上面看到的例子那样容易修复。==

  

Some require a deeper understanding of what the program is doing, or a larger amount of code or data structure reorganization to fix.

==有些需要对程序正在做什么有更深入的理解，或者需要对代码或数据结构进行大量的重组才能修复。==

  

Read Lu et al.'s excellent (and readable) paper for more details.

==阅读 Lu 等人优秀（且通俗易懂）的论文以了解更多细节。==

  

32.3 Deadlock Bugs

==32.3 死锁 Bug==

  

Beyond the concurrency bugs mentioned above, a classic problem that arises in many concurrent systems with complex locking protocols is known as deadlock.

==除了上面提到的并发 bug 之外，许多具有复杂锁定协议的并发系统中出现的一个经典问题被称为死锁。==

  

Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock (L1) and waiting for another one (L2).

==例如，当一个线程（比如线程 1）持有一个锁（L1）并等待另一个锁（L2）时，就会发生死锁。==

  

Unfortunately, the thread (Thread 2) that holds lock L2 is waiting for L1 to be released.

==不幸的是，持有锁 L2 的线程（线程 2）正在等待 L1 被释放。==

  

Here is a code snippet that demonstrates such a potential deadlock:

==这是一个演示这种潜在死锁的代码片段：==

  

Figure 32.6: Simple Deadlock (deadlock.c)

==图 32.6：简单死锁 (deadlock.c)==

  

Note that if this code runs, deadlock does not necessarily occur.

==请注意，如果运行此代码，死锁并不一定会发生。==

  

Rather, it may occur, if, for example, Thread 1 grabs lock L1 and then a context switch occurs to Thread 2.

==确切地说，它可能会发生，例如，如果线程 1 获取了锁 L1，然后上下文切换到线程 2。==

  

At that point, Thread 2 grabs L2, and tries to acquire L1.

==此时，线程 2 获取 L2，并尝试获取 L1。==

  

Thus we have a deadlock, as each thread is waiting for the other and neither can run.

==因此我们陷入了死锁，因为每个线程都在等待对方，谁也无法运行。==

  

See Figure 32.7 for a graphical depiction; the presence of a cycle in the graph is indicative of the deadlock.

==图解见图 32.7；图中循环的存在表明了死锁。==

  

Figure 32.7: The Deadlock Dependency Graph

==图 32.7：死锁依赖图==

  

CRUX: HOW TO DEAL WITH DEADLOCK

==关键问题：如何处理死锁==

  

How should we build systems to prevent, avoid, or at least detect and recover from deadlock?

==我们应该如何构建系统来预防、避免，或者至少检测并从死锁中恢复？==

  

Is this a real problem in systems today?

==这在今天的系统中是一个真正的问题吗？==

  

Why Do Deadlocks Occur?

==为什么会发生死锁？==

  

As you may be thinking, simple deadlocks such as the one above seem readily avoidable.

==正如你可能在想的那样，像上面那样简单的死锁似乎很容易避免。==

  

For example, if Thread 1 and 2 both made sure to grab locks in the same order, the deadlock would never arise.

==例如，如果线程 1 和 2 都确保按相同的顺序获取锁，死锁就永远不会出现。==

  

So why do deadlocks happen?

==那么为什么会发生死锁呢？==

  

One reason is that in large code bases, complex dependencies arise between components.

==原因之一是，在大型代码库中，组件之间会出现复杂的依赖关系。==

  

Take the operating system, for example.

==以操作系统为例。==

  

The virtual memory system might need to access the file system in order to page in a block from disk.

==虚拟内存系统可能需要访问文件系统，以便从磁盘调入一个块。==

  

The file system might subsequently require a page of memory to read the block into and thus contact the virtual memory system.

==文件系统随后可能需要一页内存来读入该块，从而联系虚拟内存系统。==

  

Thus, the design of locking strategies in large systems must be carefully done to avoid deadlock in the case of circular dependencies that may occur naturally in the code.

==因此，在大型系统中，锁定策略的设计必须小心谨慎，以避免在代码中自然发生的循环依赖情况下出现死锁。==

  

Another reason is due to the nature of encapsulation.

==另一个原因是由于封装的性质。==

  

As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way.

==作为软件开发人员，我们被教导要隐藏实现细节，从而使软件更容易以模块化的方式构建。==

  

Unfortunately, such modularity does not mesh well with locking.

==不幸的是，这种模块化与锁定配合得不好。==

  

As Jula et al. point out , some seemingly innocuous interfaces almost invite you to deadlock.

==正如 Jula 等人指出的 ，一些看似无害的接口几乎是在邀请你去死锁。==

  

For example, take the Java Vector class and the method AddAll().

==例如，以 Java Vector 类和方法 AddAll() 为例。==

  

This routine would be called as follows:

==该例程将如下调用：==

  

`Vector v1, v2; v1.AddAll(v2);`

`Vector v1, v2; v1.AddAll(v2);`

  

Internally, because the method needs to be multi-thread safe, locks for both the vector being added to (v1) and the parameter (v2) need to be acquired.

==在内部，因为该方法需要是多线程安全的，所以需要获取被添加到的向量 (v1) 和参数 (v2) 的锁。==

  

The routine acquires said locks in some arbitrary order (say v1 then v2) in order to add the contents of v2 to v1.

==为了将 v2 的内容添加到 v1，该例程以某种任意顺序（比如先 v1 后 v2）获取上述锁。==

  

If some other thread calls v2.AddAll(v1) at nearly the same time, we have the potential for deadlock, all in a way that is quite hidden from the calling application.

==如果其他线程几乎同时调用 v2.AddAll(v1)，我们就有了发生死锁的可能性，这一切对于调用应用程序来说都是相当隐蔽的。==

  

Conditions for Deadlock

==死锁的条件==

  

Four conditions need to hold for a deadlock to occur [C+71]:

==发生死锁需要满足四个条件 [C+71]：==

  

• Mutual exclusion: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).

==• 互斥：线程对其所需的资源要求独占控制（例如，线程获取锁）。==

  

• Hold-and-wait: Threads hold resources allocated to them (e.g., locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).

==• 占有并等待：线程持有分配给它们的资源（例如，它们已经获取的锁），同时等待额外的资源（例如，它们希望获取的锁）。==

  

• No preemption: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.

==• 不可抢占：资源（例如，锁）不能从持有它们的线程中强行移除。==

  

• Circular wait: There exists a circular chain of threads such that each thread holds one or more resources (e.g., locks) that are being requested by the next thread in the chain.

==• 循环等待：存在一个线程的循环链，使得每个线程都持有一个或多个链中下一个线程正在请求的资源（例如，锁）。==

  

If any of these four conditions are not met, deadlock cannot occur.

==如果这四个条件中的任何一个不满足，死锁就不会发生。==

  

Thus, we first explore techniques to prevent deadlock; each of these strategies seeks to prevent one of the above conditions from arising and thus is one approach to handling the deadlock problem.

==因此，我们首先探讨预防死锁的技术；这些策略中的每一个都旨在防止上述条件之一的出现，因此是处理死锁问题的一种方法。==

  

Prevention

==预防==

  

Circular Wait

==循环等待==

  

Probably the most practical prevention technique (and certainly one that is frequently employed) is to write your locking code such that you never induce a circular wait.

==也许最实用的预防技术（也是经常使用的技术）是编写锁定代码，使其永远不会导致循环等待。==

  

The most straightforward way to do that is to provide a total ordering on lock acquisition.

==做到这一点最直接的方法是对锁的获取提供全序（total ordering）。==

  

For example, if there are only two locks in the system (L1 and L2), you can prevent deadlock by always acquiring L1 before L2.

==例如，如果系统中只有两个锁（L1 和 L2），你可以通过总是先获取 L1 再获取 L2 来防止死锁。==

  

Such strict ordering ensures that no cyclical wait arises; hence, no deadlock.

==这种严格的顺序确保不会出现循环等待；因此，不会有死锁。==

  

Of course, in more complex systems, more than two locks will exist, and thus total lock ordering may be difficult to achieve (and perhaps is unnecessary anyhow).

==当然，在更复杂的系统中，会存在两个以上的锁，因此全序锁排序可能很难实现（而且也许无论如何都是不必要的）。==

  

Thus, a partial ordering can be a useful way to structure lock acquisition so as to avoid deadlock.

==因此，偏序（partial ordering）可能是构建锁获取以避免死锁的有用方法。==

  

An excellent real example of partial lock ordering can be seen in the memory mapping code in Linux [T+94] (v5.2).

==Linux 内存映射代码 [T+94] (v5.2) 中可以看到偏序锁排序的一个极好的真实示例。==

  

The comment at the top of the source code reveals ten different groups of lock acquisition orders, including simple ones such as "i_mutex before i_mmap_rwsem" and more complex orders such as "i_mmap_rwsem before private_lock before swap_lock before i_pages_lock".

==源代码顶部的注释揭示了十组不同的锁获取顺序，包括简单的如“i_mutex 在 i_mmap_rwsem 之前”，以及更复杂的顺序如“i_mmap_rwsem 在 private_lock 之前，private_lock 在 swap_lock 之前，swap_lock 在 i_pages_lock 之前”。==

  

TIP: ENFORCE LOCK ORDERING BY LOCK ADDRESS

==提示：通过锁地址强制执行锁顺序==

  

In some cases, a function must grab two (or more) locks.

==在某些情况下，一个函数必须获取两个（或更多）锁。==

  

Thus, we know we must be careful or deadlock could arise.

==因此，我们知道必须小心，否则可能会出现死锁。==

  

Imagine a function that is called as follows: do_something(mutex_t *m1, mutex_t *m2).

==想象一个如下调用的函数：do_something(mutex_t *m1, mutex_t *m2)。==

  

If the code always grabs m1 before m2 (or always m2 before m1), it could deadlock, because one thread could call do_something(L1, L2) while another thread could call do_something(L2, L1).

==如果代码总是先获取 m1 再获取 m2（或总是先 m2 后 m1），它可能会死锁，因为一个线程可能调用 do_something(L1, L2)，而另一个线程可能调用 do_something(L2, L1)。==

  

To avoid this particular issue, the clever programmer can use the address of each lock as a way of ordering lock acquisition.

==为了避免这个特定问题，聪明的程序员可以使用每个锁的地址作为排序锁获取的一种方式。==

  

By acquiring locks in either high-to-low or low-to-high address order, do_something() can guarantee that it always acquires locks in the same order, regardless of which order they are passed in.

==通过按地址从高到低或从低到高的顺序获取锁，do_something() 可以保证它总是以相同的顺序获取锁，而不管它们是以什么顺序传入的。==

  

The code would look something like this:

==代码看起来像这样：==

  

```c

if (m1 > m2) { // grab in high-to-low address order

    pthread_mutex_lock(m1);

    pthread_mutex_lock(m2);

} else {

    pthread_mutex_lock(m2);

    pthread_mutex_lock(m1);

}

  

```

  

```c

==if (m1 > m2) { // 按地址从高到低的顺序获取==

    pthread_mutex_lock(m1);

    pthread_mutex_lock(m2);

} else {

    pthread_mutex_lock(m2);

    pthread_mutex_lock(m1);

}

  

```

  

By using this simple technique, a programmer can ensure a simple and efficient deadlock-free implementation of multi-lock acquisition.

==通过使用这种简单的技术，程序员可以确保多锁获取的简单且高效的无死锁实现。==

  

As you can imagine, both total and partial ordering require careful design of locking strategies and must be constructed with great care.

==可以想象，全序和偏序都需要仔细设计锁定策略，并且必须非常小心地构建。==

  

Further, ordering is just a convention, and a sloppy programmer can easily ignore the locking protocol and potentially cause deadlock.

==此外，排序只是一种约定，粗心的程序员很容易忽略锁定协议，从而可能导致死锁。==

  

Finally, lock ordering requires a deep understanding of the code base, and how various routines are called.

==最后，锁排序需要对代码库以及如何调用各种例程有深入的理解。==

  

Just one mistake could result in the "D" word¹.

==只要犯一个错误就可能导致“D”字头的问题¹。（¹注：“D”代表“Deadlock”，即死锁）==

  

Hold-and-wait

==占有并等待==

  

The hold-and-wait requirement for deadlock can be avoided by acquiring all locks at once, atomically.

==通过原子地一次性获取所有锁，可以避免死锁的“占有并等待”要求。==

  

In practice, this could be achieved as follows:

==在实践中，这可以通过以下方式实现：==

  

```c

pthread_mutex_lock(prevention); // begin acquisition

pthread_mutex_lock(L1);

pthread_mutex_lock(L2);

pthread_mutex_unlock(prevention); // end

  

```

  

```c

==pthread_mutex_lock(prevention); // 开始获取==

pthread_mutex_lock(L1);

pthread_mutex_lock(L2);

==pthread_mutex_unlock(prevention); // 结束==

  

```

  

By first grabbing the lock prevention, this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided.

==通过首先获取 prevention 锁，此代码保证在锁获取过程中不会发生不合时宜的线程切换，从而再次避免死锁。==

  

Of course, it requires that any time any thread grabs a lock, it first acquires the global prevention lock.

==当然，它要求任何线程在获取锁时，都必须首先获取全局 prevention 锁。==

  

For example, if another thread was trying to grab locks L1 and L2 in a different order, it would be OK, because it would be holding the prevention lock while doing so.

==例如，如果另一个线程试图以不同的顺序获取锁 L1 和 L2，也没关系，因为它在这样做时会持有 prevention 锁。==

  

Note that the solution is problematic for a number of reasons.

==请注意，该解决方案存在许多问题。==

  

As before, encapsulation works against us: when calling a routine, this approach requires us to know exactly which locks must be held and to acquire them ahead of time.

==如前所述，封装对我们不利：在调用例程时，这种方法要求我们确切地知道必须持有包含哪些锁，并提前获取它们。==

  

This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.

==这种技术也可能会降低并发性，因为所有锁必须在早期（一次性）获取，而不是在真正需要时获取。==

  

No Preemption

==不可抢占==

  

Because we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another.

==因为我们通常认为锁在调用解锁之前一直被持有，所以多重锁获取经常会给我们带来麻烦，因为在等待一个锁时，我们正持有另一个锁。==

  

Many thread libraries provide a more flexible set of interfaces to help avoid this situation.

==许多线程库提供了一组更灵活的接口来帮助避免这种情况。==

  

Specifically, the routine pthread_mutex_trylock() either grabs the lock (if it is available) and returns success or returns an error code indicating the lock is held.

==具体来说，例程 pthread_mutex_trylock() 要么获取锁（如果可用）并返回成功，要么返回指示锁已被持有的错误代码。==

  

In the latter case, you can try again later if you want to grab that lock.

==在后一种情况下，如果你想获取那个锁，你可以稍后再试。==

  

Such an interface could be used as follows to build a deadlock-free, ordering-robust lock acquisition protocol:

==这样的接口可以如下使用，以构建一个无死锁、顺序健壮的锁获取协议：==

  

```c

top:

pthread_mutex_lock(L1);

if (pthread_mutex_trylock(L2) != 0) {

    pthread_mutex_unlock(L1);

    goto top;

}

  

```

  

```c

top:

pthread_mutex_lock(L1);

if (pthread_mutex_trylock(L2) != 0) {

    pthread_mutex_unlock(L1);

    goto top;

}

  

```

  

Note that another thread could follow the same protocol but grab the locks in the other order (L2 then L1) and the program would still be deadlock free.

==请注意，另一个线程可以遵循相同的协议，但以相反的顺序获取锁（先 L2 后 L1），程序仍然是无死锁的。==

  

One new problem does arise, however: livelock.

==然而，确实出现了一个新问题：活锁（livelock）。==

  

It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks.

==两个线程有可能（虽然不太可能）都重复尝试此序列，并重复未能获取这两个锁。==

  

In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name livelock.

==在这种情况下，两个系统都在一遍又一遍地运行此代码序列（因此它不是死锁），但没有取得进展，因此得名活锁。==

  

There are solutions to the livelock problem, too: for example, one could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.

==活锁问题也有解决方案：例如，可以在循环返回并重试整个操作之前添加一个随机延迟，从而降低竞争线程之间重复干扰的几率。==

  

One point about this solution: it skirts around the hard parts of using a trylock approach.

==关于这个解决方案的一点：它避开了使用 trylock 方法的困难部分。==

  

The first problem that would likely exist again arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more complex to implement.

==可能存在的第一个问题再次归因于封装：如果这些锁中的一个被埋在某个正在被调用的例程中，那么跳回开头的实现就会变得更加复杂。==

  

If the code had acquired some resources (other than L1) along the way, it must make sure to carefully release them as well.

==如果代码在途中获取了一些资源（除了 L1），它必须确保也仔细地释放它们。==

  

For example, if after acquiring L1, the code had allocated some memory, it would have to release that memory upon failure to acquire L2, before jumping back to the top to try the entire sequence again.

==例如，如果在获取 L1 后，代码分配了一些内存，那么在获取 L2 失败时，它必须在跳回顶部再次尝试整个序列之前释放该内存。==

  

However, in limited circumstances (e.g., the Java vector method mentioned earlier), this type of approach could work well.

==然而，在有限的情况下（例如前面提到的 Java vector 方法），这种方法可以很好地工作。==

  

You might also notice that this approach doesn't really add preemption (the forcible action of taking a lock away from a thread that owns it), but rather uses the trylock approach to allow a developer to back out of lock ownership (i.e., preempt their own ownership) in a graceful way.

==你可能还会注意到，这种方法并没有真正增加抢占（从拥有锁的线程中强行夺走锁的强制行为），而是使用 trylock 方法允许开发人员以一种优雅的方式退出锁的所有权（即，抢占他们自己的所有权）。==

  

However, it is a practical approach, and thus we include it here, despite its imperfection in this regard.

==然而，这是一个实用的方法，因此我们将其包括在这里，尽管它在这方面并不完美。==

  

Mutual Exclusion

==互斥==

  

The final prevention technique would be to avoid the need for mutual exclusion at all.

==最后的预防技术是完全避免对互斥的需求。==

  

In general, we know this is difficult, because the code we wish to run does indeed have critical sections.

==一般来说，我们知道这很难，因为我们希望运行的代码确实有临界区。==

  

So what can we do?

==那么我们能做什么呢？==

  

Herlihy had the idea that one could design various data structures without locks at all [H91, H93].

==Herlihy 有一个想法，即可以设计完全不需要锁的各种数据结构 [H91, H93]。==

  

The idea behind these lock-free (and related wait-free) approaches here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking.

==这些无锁（以及相关的无等待）方法背后的想法很简单：使用强大的硬件指令，你可以以一种不需要显式锁定的方式构建数据结构。==

  

As a simple example, let us assume we have a compare-and-swap instruction, which as you may recall is an atomic instruction provided by the hardware that does the following:

==作为一个简单的例子，让我们假设我们有一个比较并交换（compare-and-swap）指令，你可能还记得，这是硬件提供的一条原子指令，它执行以下操作：==

  

```c

int CompareAndSwap(int *address, int expected, int new) {

    if (*address == expected) {

        *address = new;

        return 1; // success

    }

    return 0; // failure

}

  

```

  

```c

int CompareAndSwap(int *address, int expected, int new) {

    if (*address == expected) {

        *address = new;

==        return 1; // 成功==

    }

==    return 0; // 失败==

}

  

```

  

Imagine we now wanted to atomically increment a value by a certain amount, using compare-and-swap.

==想象一下，我们现在想使用比较并交换来原子地将一个值增加一定的量。==

  

We could do so with the following simple function:

==我们可以用下面这个简单的函数来做到这一点：==

  

```c

void AtomicIncrement(int *value, int amount) {

    do {

        int old = *value;

    } while (CompareAndSwap(value, old, old + amount) == 0);

}

  

```

  

```c

void AtomicIncrement(int *value, int amount) {

    do {

        int old = *value;

    } while (CompareAndSwap(value, old, old + amount) == 0);

}

  

```

  

Instead of acquiring a lock, doing the update, and then releasing it, we have instead built an approach that repeatedly tries to update the value to the new amount and uses the compare-and-swap to do so.

==我们没有获取锁、执行更新然后释放锁，而是构建了一种方法，该方法反复尝试将值更新为新数量，并使用比较并交换来做到这一点。==

  

In this manner, no lock is acquired, and no deadlock can arise (though livelock is still a possibility, and thus a robust solution will be more complex than the simple code snippet above).

==通过这种方式，没有获取锁，也不会出现死锁（尽管活锁仍然是一种可能性，因此一个健壮的解决方案将比上面的简单代码片段更复杂）。==

  

Let us consider a slightly more complex example: list insertion.

==让我们考虑一个稍微复杂的例子：列表插入。==

  

Here is code that inserts at the head of a list:

==这是在列表头部插入的代码：==

  

```c

void insert(int value) {

    node_t *n = malloc(sizeof(node_t));

    assert(n != NULL);

    n->value = value;

    n->next = head;

    head = n;

}

  

```

  

```c

void insert(int value) {

    node_t *n = malloc(sizeof(node_t));

    assert(n != NULL);

    n->value = value;

    n->next = head;

    head = n;

}

  

```

  

This code performs a simple insertion, but if called by multiple threads at the "same time", has a race condition.

==这段代码执行简单的插入，但如果被多个线程在“同一时间”调用，则存在竞态条件。==

  

Can you figure out why? (draw a picture of what could happen to a list if two concurrent insertions take place, assuming, as always, a malicious scheduling interleaving).

==你能找出原因吗？（画一张图，说明如果发生两次并发插入，列表会发生什么，像往常一样假设存在恶意的调度交错）。==

  

Of course, we could solve this by surrounding this code with a lock acquire and release:

==当然，我们可以通过用锁获取和释放包围这段代码来解决这个问题：==

  

```c

void insert(int value) {

    node_t *n = malloc(sizeof(node_t));

    assert(n != NULL);

    n->value = value;

    pthread_mutex_lock(listlock); // begin critical section

    n->next = head;

    head = n;

    pthread_mutex_unlock(listlock); // end critical section

}

  

```

  

```c

void insert(int value) {

    node_t *n = malloc(sizeof(node_t));

    assert(n != NULL);

    n->value = value;

==    pthread_mutex_lock(listlock); // 开始临界区==

    n->next = head;

    head = n;

==    pthread_mutex_unlock(listlock); // 结束临界区==

}

  

```

  

In this solution, we are using locks in the traditional manner.

==在这个解决方案中，我们以传统方式使用锁。==

  

Instead, let us try to perform this insertion in a lock-free manner simply using the compare-and-swap instruction.

==取而代之的是，让我们尝试仅使用比较并交换指令以无锁方式执行此插入。==

  

Here is one possible approach:

==这是一种可能的方法：==

  

```c

void insert(int value) {

    node_t *n = malloc(sizeof(node_t));

    assert(n != NULL);

    n->value = value;

    do {

        n->next = head;

    } while (CompareAndSwap(&head, n->next, n) == 0);

}

  

```

  

```c

void insert(int value) {

    node_t *n = malloc(sizeof(node_t));

    assert(n != NULL);

    n->value = value;

    do {

        n->next = head;

    } while (CompareAndSwap(&head, n->next, n) == 0);

}

  

```

  

The astute reader might be asking why we grabbed the lock so late, instead of right when entering insert().

==敏锐的读者可能会问，为什么我们这么晚才获取锁，而不是在进入 insert() 时立即获取。==

  

Can you, astute reader, figure out why that is likely correct?

==敏锐的读者，你能找出为什么这可能是正确的吗？==

  

What assumptions does the code make, for example, about the call to malloc()?

==例如，代码对 malloc() 的调用做了什么假设？==

  

The code here updates the next pointer to point to the current head, and then tries to swap the newly-created node into position as the new head of the list.

==这里的代码更新 next 指针以指向当前的 head，然后尝试将新创建的节点交换到位，作为列表的新 head。==

  

However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head.

==但是，如果其他线程同时成功换入了新的 head，这将失败，导致该线程使用新的 head 重试。==

  

Of course, building a useful list requires more than just a list insert, and not surprisingly building a list that you can insert into, delete from, and perform lookups on in a lock-free manner is non-trivial.

==当然，构建一个有用的列表不仅仅需要列表插入，毫不奇怪，构建一个可以无锁地插入、删除和执行查找的列表并非易事。==

  

Read the rich literature on lock-free and wait-free synchronization to learn more [H01, H91, H93].

==阅读关于无锁和无等待同步的丰富文献以了解更多信息 [H01, H91, H93]。==

  

Deadlock Avoidance via Scheduling

==通过调度避免死锁==

  

Instead of deadlock prevention, in some scenarios deadlock avoidance is preferable.

==在某些场景下，死锁避免比死锁预防更可取。==

  

Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently schedules said threads in a way as to guarantee no deadlock can occur.

==避免需要一些关于各种线程在执行期间可能获取哪些锁的全局知识，随后以保证不会发生死锁的方式调度这些线程。==

  

For example, assume we have two processors and four threads which must be scheduled upon them.

==例如，假设我们有两个处理器和四个必须在它们之上调度的线程。==

  

Assume further we know that Thread 1 (T1) grabs locks L1 and L2 (in some order, at some point during its execution), T2 grabs L1 and L2 as well, T3 grabs just L2, and T4 grabs no locks at all.

==进一步假设我们知道线程 1 (T1) 获取锁 L1 和 L2（以某种顺序，在其执行期间的某一点），T2 也获取 L1 和 L2，T3 只获取 L2，而 T4 根本不获取锁。==

  

We can show these lock acquisition demands of the threads in tabular form:

==我们可以用表格形式显示线程的这些锁获取需求：==

  

L1: T1(yes), T2(yes), T3(no), T4(no)

==L1: T1(是), T2(是), T3(否), T4(否)==

  

L2: T1(yes), T2(yes), T3(yes), T4(no)

==L2: T1(是), T2(是), T3(是), T4(否)==

  

A smart scheduler could thus compute that as long as T1 and T2 are not run at the same time, no deadlock could ever arise.

==因此，智能调度程序可以计算出，只要 T1 和 T2 不同时运行，死锁就永远不会出现。==

  

Here is one such schedule:

==这是一个这样的时间表：==

  

CPU 1: T3, T4

CPU 1: T3, T4

  

CPU 2: T1, T2

CPU 2: T1, T2

  

Note that it is OK for (T3 and T1) or (T3 and T2) to overlap.

==请注意，(T3 和 T1) 或 (T3 和 T2) 重叠是可以的。==

  

Even though T3 grabs lock L2, it can never cause a deadlock by running concurrently with other threads because it only grabs one lock.

==即使 T3 获取了锁 L2，它与其他线程并发运行也永远不会导致死锁，因为它只获取一个锁。==

  

Let's look at one more example.

==让我们再看一个例子。==

  

In this one, there is more contention for the same resources (again, locks L1 and L2), as indicated by the following contention table:

==在这个例子中，对相同资源（还是锁 L1 和 L2）的争用更多，如下面的争用表所示：==

  

L1: T1(yes), T2(yes), T3(yes), T4(no)

==L1: T1(是), T2(是), T3(是), T4(否)==

  

L2: T1(yes), T2(yes), T3(yes), T4(no)

==L2: T1(是), T2(是), T3(是), T4(否)==

  

TIP: DON'T ALWAYS DO IT PERFECTLY (TOM WEST'S LAW)

==提示：不要总是追求完美（汤姆·韦斯特定律）==

  

Tom West, famous as the subject of the classic computer-industry book Soul of a New Machine [K81], says famously: "Not everything worth doing is worth doing well", which is a terrific engineering maxim.

==汤姆·韦斯特，作为计算机行业经典书籍《新机器的灵魂》[K81] 的主角而闻名，他有一句名言：“不是每一件值得做的事情都值得做好”，这是一句极好的工程格言。==

  

If a bad thing happens rarely, certainly one should not spend a great deal of effort to prevent it, particularly if the cost of the bad thing occurring is small.

==如果一件坏事很少发生，那么当然不应该花费大量精力去预防它，特别是如果坏事发生的代价很小。==

  

If, on the other hand, you are building a space shuttle, and the cost of something going wrong is the space shuttle blowing up, well, perhaps you should ignore this piece of advice.

==另一方面，如果你正在建造航天飞机，而出错的代价是航天飞机爆炸，好吧，也许你应该忽略这条建议。==

  

Some readers object: "This sounds like you are suggesting mediocrity as a solution!"

==有些读者反对：“这听起来像是你在建议用平庸作为解决方案！”==

  

Perhaps they are right, that we should be careful with advice such as this.

==也许他们是对的，我们在对待此类建议时应该小心。==

  

However, our experience tells us that in the world of engineering, with pressing deadlines and other real-world concerns, one will always have to decide which aspects of a system to build well and which to put aside for another day.

==然而，我们的经验告诉我们，在工程世界中，面对紧迫的截止日期和其他现实世界的担忧，人们总是必须决定系统的哪些方面要建好，哪些要留待以后解决。==

  

The hard part is knowing which to do when, a bit of insight only gained through experience and dedication to the task at hand.

==困难的部分在于知道何时做什么，这是一种只能通过经验和对当前任务的专注才能获得的洞察力。==

  

In particular, threads T1, T2, and T3 all need to grab both locks L1 and L2 at some point during their execution.

==特别是，线程 T1、T2 和 T3 都需要在执行期间的某个时刻同时获取锁 L1 和 L2。==

  

Here is a possible schedule that guarantees that no deadlock could ever occur:

==这是一个可能的时间表，保证永远不会发生死锁：==

  

CPU 1: T4

CPU 1: T4

  

CPU 2: T1, T2, T3

CPU 2: T1, T2, T3

  

As you can see, static scheduling leads to a conservative approach where T1, T2, and T3 are all run on the same processor, and thus the total time to complete the jobs is lengthened considerably.

==如你所见，静态调度导致了一种保守的方法，即 T1、T2 和 T3 都在同一个处理器上运行，从而大大延长了完成任务的总时间。==

  

Though it may have been possible to run these tasks concurrently, the fear of deadlock prevents us from doing so, and the cost is performance.

==虽然本来可能并发运行这些任务，但对死锁的恐惧阻止了我们这样做，代价就是性能。==

  

One famous example of an approach like this is Dijkstra's Banker's Algorithm [D64], and many similar approaches have been described in the literature.

==这种方法的一个著名例子是 Dijkstra 的银行家算法 [D64]，文献中也描述了许多类似的方法。==

  

Unfortunately, they are only useful in very limited environments, for example, in an embedded system where one has full knowledge of the entire set of tasks that must be run and the locks that they need.

==不幸的是，它们仅在非常有限的环境中有用，例如，在嵌入式系统中，人们完全了解必须运行的整套任务以及它们所需的锁。==

  

Further, such approaches can limit concurrency, as we saw in the second example above.

==此外，正如我们在上面的第二个示例中所看到的，此类方法会限制并发性。==

  

Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution.

==因此，通过调度避免死锁并不是一种广泛使用的通用解决方案。==

  

Detect and Recover

==检测并恢复==

  

One final general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been detected.

==最后一个通用策略是允许死锁偶尔发生，然后一旦检测到此类死锁就采取一些措施。==

  

For example, if an OS froze once a year, you would just reboot it and get happily (or grumpily) on with your work.

==例如，如果操作系统一年死机一次，你只需重新启动它，然后愉快地（或脾气暴躁地）继续工作。==

  

If deadlocks are rare, such a non-solution is indeed quite pragmatic.

==如果死锁很少见，那么这种非解决方案确实非常实用。==

  

Many database systems employ deadlock detection and recovery techniques.

==许多数据库系统采用死锁检测和恢复技术。==

  

A deadlock detector runs periodically, building a resource graph and checking it for cycles.

==死锁检测器定期运行，构建资源图并检查其是否存在循环。==

  

In the event of a cycle (deadlock), the system needs to be restarted.

==如果出现循环（死锁），系统需要重新启动。==

  

If more intricate repair of data structures is first required, a human being may be involved to ease the process.

==如果首先需要对数据结构进行更复杂的修复，可能需要人工干预来简化过程。==

  

More detail on database concurrency, deadlock, and related issues can be found elsewhere [B+87, K87].

==关于数据库并发、死锁和相关问题的更多详细信息，可以在其他地方找到 [B+87, K87]。==

  

Read these works, or better yet, take a course on databases to learn more about this rich and interesting topic.

==阅读这些作品，或者更好的是，选修一门数据库课程，以了解更多关于这个丰富而有趣的主题的信息。==

  

32.4 Summary

==32.4 小结==

  

In this chapter, we have studied the types of bugs that occur in concurrent programs.

==在本章中，我们研究了并发程序中出现的 bug 类型。==

  

The first type, non-deadlock bugs, are surprisingly common, but often are easier to fix.

==第一种类型，即非死锁 bug，出奇地常见，但通常更容易修复。==

  

They include atomicity violations, in which a sequence of instructions that should have been executed together was not, and order violations, in which the needed order between two threads was not enforced.

==它们包括原子性违反（本应一起执行的指令序列未一起执行）和顺序违反（未强制执行两个线程之间所需的顺序）。==

  

We have also briefly discussed deadlock: why it occurs, and what can be done about it.

==我们还简要讨论了死锁：它为什么会发生，以及可以为此做些什么。==

  

The problem is as old as concurrency itself, and many hundreds of papers have been written about the topic.

==这个问题与并发本身一样古老，关于该主题的论文已有数百篇。==

  

The best solution in practice is to be careful, develop a lock acquisition order, and thus prevent deadlock from occurring in the first place.

==实践中最好的解决方案是小心谨慎，制定锁获取顺序，从而首先防止死锁的发生。==

  

Wait-free approaches also have promise, as some wait-free data structures are now finding their way into commonly-used libraries and critical systems, including Linux.

==无等待方法也有希望，因为一些无等待数据结构现在正在进入常用库和关键系统，包括 Linux。==

  

However, their lack of generality and the complexity to develop a new wait-free data structure will likely limit the overall utility of this approach.

==然而，它们缺乏通用性以及开发新的无等待数据结构的复杂性，可能会限制这种方法的整体效用。==

  

Perhaps the best solution is to develop new concurrent programming models: in systems such as MapReduce (from Google) [GD02], programmers can describe certain types of parallel computations without any locks whatsoever.

==也许最好的解决方案是开发新的并发编程模型：在 MapReduce（来自 Google）[GD02] 等系统中，程序员可以在没有任何锁的情况下描述某些类型的并行计算。==

  

Locks are problematic by their very nature; perhaps we should seek to avoid using them unless we truly must.

==锁在本质上是有问题的；也许我们应该寻求避免使用它们，除非我们真的必须这样做。==

  

References

==参考文献==

  

[B+87] "Concurrency Control and Recovery in Database Systems" by Philip A. Bernstein, Vassos Hadzilacos, Nathan Goodman.

==[B+87] 《数据库系统中的并发控制与恢复》，作者 Philip A. Bernstein, Vassos Hadzilacos, Nathan Goodman。==

  

Addison-Wesley, 1987. The classic text on concurrency in database management systems.

==Addison-Wesley, 1987。关于数据库管理系统中并发的经典教材。==

  

As you can tell, understanding concurrency, deadlock, and other topics in the world of databases is a world unto itself.

==你可以看出，理解数据库世界中的并发、死锁和其他主题本身就是一个世界。==

  

Study it and find out for yourself.

==研究它并亲自找出答案。==

  

[C+71] "System Deadlocks" by E.G. Coffman, M.J. Elphick, A. Shoshani.

==[C+71] 《系统死锁》，作者 E.G. Coffman, M.J. Elphick, A. Shoshani。==

  

ACM Computing Surveys, 3:2, June 1971. The classic paper outlining the conditions for deadlock and how you might go about dealing with it.

==ACM Computing Surveys, 3:2, 1971年6月。概述死锁条件以及如何处理死锁的经典论文。==

  

There are certainly some earlier papers on this topic; see the references within this paper for details.

==当然，关于这个主题还有一些更早的论文；详见本文中的参考文献。==

  

[D64] "Een algorithme ter voorkoming van de dodelijke omarming" by Edsger Dijkstra. 1964.

==[D64] 《避免死锁的算法》（荷兰语标题），作者 Edsger Dijkstra，1964年。==

  

Indeed, not only did Dijkstra come up with a number of solutions to the deadlock problem, he was the first to note its existence, at least in written form.

==确实，Dijkstra 不仅提出了死锁问题的许多解决方案，而且他是第一个注意到其存在的人，至少在书面形式上是这样。==

  

However, he called it the "deadly embrace", which (thankfully) did not catch on.

==然而，他称之为“致命拥抱”（deadly embrace），幸好这个叫法没有流行起来。==

  

[GD02] "MapReduce: Simplified Data Processing on Large Clusters" by Sanjay Ghemawhat, Jeff Dean.

==[GD02] 《MapReduce：大型集群上的简化数据处理》，作者 Sanjay Ghemawhat, Jeff Dean。==

  

OSDI '04, San Francisco, CA, October 2004. The MapReduce paper ushered in the era of large-scale data processing, and proposes a framework for performing such computations on clusters of generally unreliable machines.

==OSDI '04，加州旧金山，2004年10月。MapReduce 论文开启了大规模数据处理的时代，并提出了一个框架，用于在通常不可靠的机器集群上执行此类计算。==

  

[H01] "A Pragmatic Implementation of Non-blocking Linked-lists" by Tim Harris.

==[H01] 《非阻塞链表的实用实现》，作者 Tim Harris。==

  

International Conference on Distributed Computing (DISC), 2001. A relatively modern example of the difficulties of building something as simple as a concurrent linked list without locks.

==国际分布式计算会议 (DISC)，2001年。一个相对现代的例子，说明了在没有锁的情况下构建像并发链表这样简单的东西是多么困难。==

  

[H91] "Wait-free Synchronization" by Maurice Herlihy. ACM TOPLAS, 13:1, January 1991.

==[H91] 《无等待同步》，作者 Maurice Herlihy。ACM TOPLAS, 13:1，1991年1月。==

  

Herlihy's work pioneers the ideas behind wait-free approaches to writing concurrent programs.

==Herlihy 的工作开创了编写并发程序的无等待方法背后的思想。==

  

These approaches tend to be complex and hard, often more difficult than using locks correctly, probably limiting their success in the real world.

==这些方法往往复杂且困难，通常比正确使用锁更难，这可能会限制它们在现实世界中的成功。==

  

[H93] "A Methodology for Implementing Highly Concurrent Data Objects" by Maurice Herlihy.

==[H93] 《实现高并发数据对象的方法论》，作者 Maurice Herlihy。==

  

ACM TOPLAS, 15:5, November 1993. A nice overview of lock-free and wait-free structures.

==ACM TOPLAS, 15:5，1993年11月。对无锁和无等待结构的精彩概述。==

  

Both approaches eschew locks, but wait-free approaches are harder to realize, as they try to ensure that any operation on a concurrent structure will terminate in a finite number of steps (e.g., no unbounded looping).

==这两种方法都避开了锁，但无等待方法更难实现，因为它们试图确保对并发结构的任何操作都将在有限的步骤内终止（例如，没有无限循环）。==

  

[J+08] "Deadlock Immunity: Enabling Systems To Defend Against Deadlocks" by Horatiu Jula, Daniel Tralamazza, Cristian Zamfir, George Candea.

==[J+08] 《死锁免疫：使系统能够抵御死锁》，作者 Horatiu Jula, Daniel Tralamazza, Cristian Zamfir, George Candea。==

  

OSDI '08, San Diego, CA, December 2008. An excellent recent paper on deadlocks and how to avoid getting caught in the same ones over and over again in a particular system.

==OSDI '08，加州圣地亚哥，2008年12月。一篇关于死锁以及如何避免在特定系统中一遍又一遍地陷入同一死锁的优秀近期论文。==

  

[K81] "Soul of a New Machine" by Tracy Kidder. Backbay Books, 2000 (reprint of 1980 version).

==[K81] 《新机器的灵魂》，作者 Tracy Kidder。Backbay Books，2000年（1980年版重印）。==

  

A must-read for any systems builder or engineer, detailing the early days of how a team inside Data General (DG), led by Tom West, worked to produce a "new machine."

==任何系统构建者或工程师的必读书籍，详细介绍了 Data General (DG) 内部由 Tom West 领导的团队如何致力于生产“新机器”的早期日子。==

  

Kidder's other books are also excellent, including "Mountains beyond Mountains."

==Kidder 的其他书也很棒，包括《Mountains beyond Mountains》。==

  

Or maybe you don't agree with us, comma?

==或者也许你不同意我们的观点，逗号？==

  

[K87] "Deadlock Detection in Distributed Databases" by Edgar Knapp. ACM Computing Surveys, 19:4, December 1987.

==[K87] 《分布式数据库中的死锁检测》，作者 Edgar Knapp。ACM Computing Surveys, 19:4，1987年12月。==

  

An excellent overview of deadlock detection in distributed database systems.

==分布式数据库系统中死锁检测的精彩概述。==

  

Also points to a number of other related works, and thus is a good place to start your reading.

==也指向了许多其他相关作品，因此是开始阅读的好地方。==

  

[L+08] "Learning from Mistakes A Comprehensive Study on Real World Concurrency Bug Characteristics" by Shan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou.

==[L+08] 《从错误中学习：关于现实世界并发 Bug 特征的综合研究》，作者 Shan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou。==

  

ASPLOS '08, March 2008, Seattle, Washington. The first in-depth study of concurrency bugs in real software, and the basis for this chapter.

==ASPLOS '08，2008年3月，华盛顿州西雅图。对真实软件中并发 bug 的首次深入研究，也是本章的基础。==

  

Look at Y.Y. Zhou's or Shan Lu's web pages for many more interesting papers on bugs.

==查看 Y.Y. Zhou 或 Shan Lu 的网页，了解更多关于 bug 的有趣论文。==

  

[T+94] "Linux File Memory Map Code" by Linus Torvalds and many others.

==[T+94] 《Linux 文件内存映射代码》，作者 Linus Torvalds 等人。==

  

Thanks to Michael Walfish (NYU) for pointing out this precious example.

==感谢 Michael Walfish (NYU) 指出了这个珍贵的例子。==

  

The real world, as you can see in this file, can be a bit more complex than the simple clarity found in textbooks...

==正如你在这个文件中看到的那样，现实世界可能比教科书中简单的清晰度要复杂一些……==

  

Homework (Code)

==作业（代码）==

  

This homework lets you explore some real code that deadlocks (or avoids deadlock).

==这份作业让你探索一些会死锁（或避免死锁）的真实代码。==

  

The different versions of code correspond to different approaches to avoiding deadlock in a simplified vector_add() routine.

==不同版本的代码对应于在简化的 vector_add() 例程中避免死锁的不同方法。==

  

See the README for details on these programs and their common substrate.

==有关这些程序及其通用基础的详细信息，请参阅 README。==

  

Questions

==问题==

  

1. First let's make sure you understand how the programs generally work, and some of the key options.

==2. 首先，确保你了解程序的一般工作原理，以及一些关键选项。==

  

Study the code in vector-deadlock.c, as well as in main-common.c and related files.

==研究 vector-deadlock.c 以及 main-common.c 和相关文件中的代码。==

  

Now, run `./vector-deadlock -n 2 -l 1 -v`, which instantiates two threads (-n 2), each of which does one vector add (-l 1), and does so in verbose mode (-v).

==现在，运行 `./vector-deadlock -n 2 -l 1 -v`，它实例化两个线程 (-n 2)，每个线程执行一次向量加法 (-l 1)，并以详细模式 (-v) 运行。==

  

Make sure you understand the output.

==确保你理解输出。==

  

How does the output change from run to run?

==每次运行的输出有何变化？==

  

2. Now add the -d flag, and change the number of loops (-l) from 1 to higher numbers.

==3. 现在添加 -d 标志，并将循环次数 (-l) 从 1 更改为更大的数字。==

  

What happens?

==会发生什么？==

  

Does the code (always) deadlock?

==代码（总是）会死锁吗？==

  

3. How does changing the number of threads (-n) change the outcome of the program?

==4. 更改线程数 (-n) 如何改变程序的结果？==

  

Are there any values of -n that ensure no deadlock occurs?

==是否有任何 -n 的值可以确保不发生死锁？==

  

4. Now examine the code in vector-global-order.c.

==5. 现在检查 vector-global-order.c 中的代码。==

  

First, make sure you understand what the code is trying to do; do you understand why the code avoids deadlock?

==首先，确保你理解代码试图做什么；你明白为什么代码避免了死锁吗？==

  

Also, why is there a special case in this vector_add() routine when the source and destination vectors are the same?

==另外，当源向量和目标向量相同时，为什么这个 vector_add() 例程中有一个特殊情况？==

  

5. Now run the code with the following flags: `-t -n 2 -l 100000 -d`.

==6. 现在使用以下标志运行代码：`-t -n 2 -l 100000 -d`。==

  

How long does the code take to complete?

==代码需要多长时间才能完成？==

  

How does the total time change when you increase the number of loops, or the number of threads?

==当你增加循环次数或线程数时，总时间如何变化？==

  

6. What happens if you turn on the parallelism flag (-p)?

==7. 如果打开并行标志 (-p) 会发生什么？==

  

How much would you expect performance to change when each thread is working on adding different vectors (which is what -p enables) versus working on the same ones?

==当每个线程致力于添加不同的向量（这就是 -p 启用的功能）与致力于相同的向量时，你预期性能会有多大变化？==

  

7. Now let's study vector-try-wait.c. First make sure you understand the code.

==8. 现在让我们研究 vector-try-wait.c。首先确保你理解代码。==

  

Is the first call to pthread_mutex_trylock() really needed?

==第一次调用 pthread_mutex_trylock() 真的有必要吗？==

  

Now run the code.

==现在运行代码。==

  

How fast does it run compared to the global order approach?

==与全局排序方法相比，它的运行速度如何？==

  

How does the number of retries, as counted by the code, change as the number of threads increases?

==随着线程数的增加，代码计数的重试次数如何变化？==

  

8. Now let's look at vector-avoid-hold-and-wait.c.

==9. 现在让我们看看 vector-avoid-hold-and-wait.c。==

  

What is the main problem with this approach?

==这种方法的主要问题是什么？==

  

How does its performance compare to the other versions, when running both with -p and without it?

==在使用 -p 和不使用 -p 运行时，它的性能与其他版本相比如何？==

  

9. Finally, let's look at vector-nolock.c.

==10. 最后，让我们看看 vector-nolock.c。==

  

This version doesn't use locks at all; does it provide the exact same semantics as the other versions?

==这个版本完全不使用锁；它是否提供了与其他版本完全相同的语义？==

  

Why or why not?

==为什么是或为什么不是？==

  

10. Now compare its performance to the other versions, both when threads are working on the same two vectors (no -p) and when each thread is working on separate vectors (-p).

==11. 现在将其性能与其他版本进行比较，包括线程处理相同的两个向量（无 -p）时以及每个线程处理单独的向量（-p）时。==

  

How does this no-lock version perform?

==这个无锁版本的性能如何？==

  

Event-based Concurrency (Advanced)

==基于事件的并发（进阶）==

  

Thus far, we've written about concurrency as if the only way to build concurrent applications is to use threads.

==到目前为止，我们在撰写关于并发的内容时，似乎构建并发应用程序的唯一方法是使用线程。==

  

Like many things in life, this is not completely true.

==就像生活中的许多事情一样，这并不完全正确。==

  

Specifically, a different style of concurrent programming is often used in both GUI-based applications [O96] as well as some types of internet servers [PDZ99].

==具体来说，基于 GUI 的应用程序 [O96] 以及某些类型的 Internet 服务器 [PDZ99] 通常使用一种不同风格的并发编程。==

  

This style, known as event-based concurrency, has become popular in some modern systems, including server-side frameworks such as node.js [N13], but its roots are found in C/UNIX systems that we'll discuss below.

==这种被称为基于事件的并发的风格在一些现代系统中已经变得流行，包括像 node.js [N13] 这样的服务器端框架，但它的根源在于我们将在下面讨论的 C/UNIX 系统。==

  

The problem that event-based concurrency addresses is two-fold.

==基于事件的并发解决的问题是双重的。==

  

The first is that managing concurrency correctly in multi-threaded applications can be challenging; as we've discussed, missing locks, deadlock, and other nasty problems can arise.

==首先是，在多线程应用程序中正确管理并发可能具有挑战性；正如我们所讨论的，可能会出现缺少锁、死锁和其他令人讨厌的问题。==

  

The second is that in a multi-threaded application, the developer has little or no control over what is scheduled at a given moment in time; rather, the programmer simply creates threads and then hopes that the underlying OS schedules them in a reasonable manner across available CPUs.

==其次是，在多线程应用程序中，开发人员对给定时刻调度什么几乎没有控制权；相反，程序员只是创建线程，然后希望底层操作系统以合理的方式在可用 CPU 上调度它们。==

  

Given the difficulty of building a general-purpose scheduler that works well in all cases for all workloads, sometimes the OS will schedule work in a manner that is less than optimal.

==鉴于构建一个在所有情况下都能很好地处理所有工作负载的通用调度程序的难度，有时操作系统会以不太理想的方式调度工作。==

  

And thus, we have ...

==因此，我们有……==

  

THE CRUX: HOW TO BUILD CONCURRENT SERVERS WITHOUT THREADS

==关键问题：如何在不使用线程的情况下构建并发服务器==

  

How can we build a concurrent server without using threads, and thus retain control over concurrency as well as avoid some of the problems that seem to plague multi-threaded applications?

==我们如何才能在不使用线程的情况下构建并发服务器，从而保持对并发的控制，并避免一些困扰多线程应用程序的问题？==

  

33.1 The Basic Idea: An Event Loop

==33.1 基本理念：事件循环==

  

The basic approach we'll use, as stated above, is called event-based concurrency.

==如上所述，我们将使用的基本方法称为基于事件的并发。==

  

The approach is quite simple: you simply wait for something (i.e., an "event") to occur; when it does, you check what type of event it is and do the small amount of work it requires (which may include issuing I/O requests, or scheduling other events for future handling, etc.).

==这种方法非常简单：你只是等待某事（即“事件”）发生；当它发生时，你检查它是什么类型的事件，并做它需要的少量工作（其中可能包括发出 I/O 请求，或调度其他事件以供将来处理等）。==

  

That's it!

==就是这样！==

  

Before getting into the details, let's first examine what a canonical event-based server looks like.

==在深入细节之前，让我们首先检查一下典型的基于事件的服务器是什么样子的。==

  

Such applications are based around a simple construct known as the event loop.

==此类应用程序基于一种简单的结构，称为事件循环。==

  

Pseudocode for an event loop looks like this:

==事件循环的伪代码如下所示：==

  

```c

while (1) {

    events = getEvents();

    for (e in events)

        processEvent(e);

}

  

```

  

```c

while (1) {

    events = getEvents();

    for (e in events)

        processEvent(e);

}

  

```

  

It's really that simple.

==真的就这么简单。==

  

The main loop simply waits for something to do (by calling getEvents() in the code above) and then, for each event returned, processes them, one at a time; the code that processes each event is known as an event handler.

==主循环只是等待做某事（通过调用上面的代码中的 getEvents()），然后，对于返回的每个事件，一次一个地处理它们；处理每个事件的代码称为事件处理程序。==

  

Importantly, when a handler processes an event, it is the only activity taking place in the system; thus, deciding which event to handle next is equivalent to scheduling.

==重要的是，当处理程序处理事件时，它是系统中唯一发生的活动；因此，决定接下来处理哪个事件等同于调度。==

  

This explicit control over scheduling is one of the fundamental advantages of the event-based approach.

==这种对调度的显式控制是基于事件的方法的基本优势之一。==

  

But this discussion leaves us with a bigger question: how exactly does an event-based server determine which events are taking place, in particular with regards to network and disk I/O?

==但是这个讨论给我们留下了一个更大的问题：基于事件的服务器究竟如何确定正在发生哪些事件，特别是关于网络和磁盘 I/O？==

  

Specifically, how can an event server tell if a message has arrived for it?

==具体来说，事件服务器如何知道是否有消息到达？==

  

33.2 An Important API: select() (or poll())

==33.2 一个重要的 API：select()（或 poll()）==

  

With that basic event loop in mind, we next must address the question of how to receive events.

==考虑到那个基本的事件循环，我们接下来必须解决如何接收事件的问题。==

  

In most systems, a basic API is available, via either the select() or poll() system calls.

==在大多数系统中，通过 select() 或 poll() 系统调用可以使用一个基本的 API。==

  

What these interfaces enable a program to do is simple: check whether there is any incoming I/O that should be attended to.

==这些接口使程序能够做的事情很简单：检查是否有任何需要处理的传入 I/O。==

  

For example, imagine that a network application (such as a web server) wishes to check whether any network packets have arrived, in order to service them.

==例如，假设一个网络应用程序（如 Web 服务器）希望检查是否有任何网络数据包到达，以便为它们提供服务。==

  

These system calls let you do exactly that.

==这些系统调用让你正是做这件事。==

  

Take select() for example.

==以 select() 为例。==

  

The manual page (on a Mac) describes the API in this manner:

==手册页（在 Mac 上）以这种方式描述了 API：==

  

```c

int select(int nfds,

    fd_set *restrict readfds,

    fd_set *restrict writefds,

    fd_set *restrict errorfds,

    struct timeval *restrict timeout);

  

```

  

```c

int select(int nfds,

    fd_set *restrict readfds,

    fd_set *restrict writefds,

    fd_set *restrict errorfds,

    struct timeval *restrict timeout);

  

```

  

The actual description from the man page: select() examines the I/O descriptor sets whose addresses are passed in readfds, writefds, and errorfds to see if some of their descriptors are ready for reading, are ready for writing, or have an exceptional condition pending, respectively.

==手册页中的实际描述：select() 检查其地址传入 readfds、writefds 和 errorfds 的 I/O 描述符集，以分别查看其中是否有描述符已准备好读取、准备好写入或有待处理的异常情况。==

  

The first nfds descriptors are checked in each set, i.e., the descriptors from 0 through nfds-1 in the descriptor sets are examined.

==每个集合中的前 nfds 个描述符会被检查，即检查描述符集中从 0 到 nfds-1 的描述符。==

  

On return, select() replaces the given descriptor sets with subsets consisting of those descriptors that are ready for the requested operation.

==返回时，select() 用由那些已准备好进行请求操作的描述符组成的子集替换给定的描述符集。==

  

select() returns the total number of ready descriptors in all the sets.

==select() 返回所有集合中就绪描述符的总数。==

  

ASIDE: BLOCKING VS. NON-BLOCKING INTERFACES

==旁白：阻塞与非阻塞接口==

  

Blocking (or synchronous) interfaces do all of their work before returning to the caller; non-blocking (or asynchronous) interfaces begin some work but return immediately, thus letting whatever work that needs to be done get done in the background.

==阻塞（或同步）接口在返回给调用者之前完成所有工作；非阻塞（或异步）接口开始一些工作但立即返回，从而让需要做的任何工作在后台完成。==

  

The usual culprit in blocking calls is I/O of some kind.

==阻塞调用的通常罪魁祸首是某种 I/O。==

  

For example, if a call must read from disk in order to complete, it might block, waiting for the I/O request that has been sent to the disk to return.

==例如，如果一个调用必须从磁盘读取才能完成，它可能会阻塞，等待已发送到磁盘的 I/O 请求返回。==

  

Non-blocking interfaces can be used in any style of programming (e.g., with threads), but are essential in the event-based approach, as a call that blocks will halt all progress.

==非阻塞接口可用于任何风格的编程（例如，使用线程），但在基于事件的方法中是必不可少的，因为阻塞的调用将停止所有进度。==

  

A couple of points about select().

==关于 select() 的几点。==

  

First, note that it lets you check whether descriptors can be read from as well as written to; the former lets a server determine that a new packet has arrived and is in need of processing, whereas the latter lets the service know when it is OK to reply (i.e., the outbound queue is not full).

==首先，请注意它允许你检查描述符是否可读以及可写；前者让服务器确定新数据包已到达并需要处理，而后者让服务知道何时可以回复（即出站队列未满）。==

  

Second, note the timeout argument.

==其次，注意 timeout 参数。==

  

One common usage here is to set the timeout to NULL, which causes select() to block indefinitely, until some descriptor is ready.

==这里的一个常见用法是将 timeout 设置为 NULL，这将导致 select() 无限期阻塞，直到某个描述符就绪。==

  

However, more robust servers will usually specify some kind of timeout; one common technique is to set the timeout to zero, and thus use the call to select() to return immediately.

==然而，更健壮的服务器通常会指定某种超时；一种常见的技术是将超时设置为零，从而使用对 select() 的调用立即返回。==

  

The poll() system call is quite similar.

==poll() 系统调用非常相似。==

  

See its manual page, or Stevens and Rago [SR05], for details.

==详见其手册页，或 Stevens 和 Rago [SR05]。==

  

Either way, these basic primitives give us a way to build a non-blocking event loop, which simply checks for incoming packets, reads from sockets with messages upon them, and replies as needed.

==无论哪种方式，这些基本原语都为我们提供了一种构建非阻塞事件循环的方法，该循环只需检查传入的数据包，从有消息的套接字中读取，并根据需要进行回复。==

  

33.3 Using select()

==33.3 使用 select()==

  

To make this more concrete, let's examine how to use select() to see which network descriptors have incoming messages upon them.

==为了使其更具体，让我们检查如何使用 select() 来查看哪些网络描述符上有传入消息。==

  

Figure 33.1 shows a simple example.

==图 33.1 显示了一个简单的例子。==

  

This code is actually fairly simple to understand.

==这段代码实际上相当容易理解。==

  

After some initialization, the server enters an infinite loop.

==在一些初始化之后，服务器进入无限循环。==

  

Inside the loop, it uses the FD_ZERO() macro to first clear the set of file descriptors, and then uses FD_SET() to include all of the file descriptors from minFD to maxFD in the set.

==在循环内部，它首先使用 FD_ZERO() 宏清除文件描述符集，然后使用 FD_SET() 将从 minFD 到 maxFD 的所有文件描述符包含在集合中。==

Figure 33.1: Simple Code Using select ()

==图 33.1：使用 select() 的简单代码==

  

This set of descriptors might represent, for example, all of the network sockets to which the server is paying attention.

==这组描述符可能代表（例如）服务器正在关注的所有网络套接字。==

  

Finally, the server calls select () to see which of the connections have data available upon them.

==最后，服务器调用 select() 来查看哪些连接上有可用数据。==

  

By then using FD_ISSET() in a loop, the event server can see which of the descriptors have data ready and process the incoming data.

==通过随后在一个循环中使用 FD_ISSET()，事件服务器可以看到哪些描述符有数据准备好，并处理传入的数据。==

  

Of course, a real server would be more complicated than this, and require logic to use when sending messages, issuing disk I/O, and many other details.

==当然，一个真正的服务器会比这更复杂，并且需要包含发送消息、发出磁盘 I/O 以及许多其他细节时的逻辑。==

  

For further information, see Stevens and Rago [SR05] for API information, or Pai et. al or Welsh et al. for a good overview of the general flow of event-based servers [PDZ99, WCB01].

==欲了解更多信息，请参阅 Stevens 和 Rago [SR05] 获取 API 信息，或参阅 Pai 等人或 Welsh 等人的文章，以很好地了解基于事件的服务器的一般流程 [PDZ99, WCB01]。==

  

TIP: DON'T BLOCK IN EVENT-BASED SERVERS

==提示：不要在基于事件的服务器中阻塞==

  

Event-based servers enable fine-grained control over scheduling of tasks.

==基于事件的服务器允许对任务调度进行细粒度的控制。==

  

However, to maintain such control, no call that blocks the execution of the caller can ever be made.

==然而，为了维持这种控制，绝对不能进行任何会阻塞调用者执行的调用。==

  

Failing to obey this design tip will result in a blocked event-based server, frustrated clients, and serious questions as to whether you ever read this part of the book.

==如果不遵守这个设计提示，将导致基于事件的服务器被阻塞，客户端感到沮丧，并且人们会严重怀疑你是否读过本书的这一部分。==

  

33.4 Why Simpler? No Locks Needed

==33.4 为什么更简单？无需锁==

  

With a single CPU and an event-based application, the problems found in concurrent programs are no longer present.

==在单 CPU 和基于事件的应用程序中，并发程序中常见的问题不再存在。==

  

Specifically, because only one event is being handled at a time, there is no need to acquire or release locks.

==具体来说，因为一次只处理一个事件，所以不需要获取或释放锁。==

  

The event-based server cannot be interrupted by another thread because it is decidedly single threaded.

==基于事件的服务器不会被另一个线程中断，因为它显然是单线程的。==

  

Thus, concurrency bugs common in threaded programs do not manifest in the basic event-based approach.

==因此，线程程序中常见的并发 bug 不会在基本的基于事件的方法中出现。==

  

33.5 A Problem: Blocking System Calls

==33.5 一个问题：阻塞系统调用==

  

Thus far, event-based programming sounds great, right?

==到目前为止，基于事件的编程听起来很棒，对吧？==

  

You program a simple loop, and handle events as they arise.

==你编写一个简单的循环，并在事件发生时处理它们。==

  

You don't even need to think about locking!

==你甚至不需要考虑锁的问题！==

  

But there is an issue: what if an event requires that you issue a system call that might block?

==但是有一个问题：如果一个事件需要你发出一个可能会阻塞的系统调用怎么办？==

  

For example, imagine a request comes from a client into a server to read a file from disk and return its contents to the requesting client (much like a simple HTTP request).

==例如，设想一个请求从客户端进入服务器，要求从磁盘读取文件并将其内容返回给请求的客户端（很像一个简单的 HTTP 请求）。==

  

To service such a request, some event handler will eventually have to issue an open () system call to open the file, followed by a series of read () calls to read the file.

==为了服务这样的请求，某个事件处理程序最终将不得不发出 open() 系统调用来打开文件，随后是一系列 read() 调用来读取文件。==

  

When the file is read into memory, the server will likely start sending the results to the client.

==当文件被读入内存后，服务器可能会开始将结果发送给客户端。==

  

Both the open () and read () calls may issue I/O requests to the storage system (when the needed metadata or data is not in memory already), and thus may take a long time to service.

==open() 和 read() 调用都可能向存储系统发出 I/O 请求（当所需的元数据或数据尚未在内存中时），因此可能需要很长时间才能完成服务。==

  

With a thread-based server, this is no issue: while the thread issuing the I/O request suspends (waiting for the I/O to complete), other threads can run, thus enabling the server to make progress.

==对于基于线程的服务器，这不是问题：当发出 I/O 请求的线程挂起（等待 I/O 完成）时，其他线程可以运行，从而使服务器能够继续处理。==

  

Indeed, this natural overlap of I/O and other computation is what makes thread-based programming quite natural and straightforward.

==事实上，这种 I/O 和其他计算的自然重叠正是基于线程的编程非常自然和直观的原因。==

  

With an event-based approach, however, there are no other threads to run: just the main event loop.

==然而，在基于事件的方法中，没有其他线程可以运行：只有一个主事件循环。==

  

And this implies that if an event handler issues a call that blocks, the entire server will do just that: block until the call completes.

==这意味着如果事件处理程序发出了一个阻塞调用，整个服务器就会这样做：阻塞直到调用完成。==

  

When the event loop blocks, the system sits idle, and thus is a huge potential waste of resources.

==当事件循环阻塞时，系统处于空闲状态，因此这是对资源的巨大潜在浪费。==

  

We thus have a rule that must be obeyed in event-based systems: no blocking calls are allowed.

==因此，我们在基于事件的系统中必须遵守一条规则：不允许阻塞调用。==

  

33.6 A Solution: Asynchronous I/O

==33.6 解决方案：异步 I/O==

  

To overcome this limit, many modern operating systems have introduced new ways to issue I/O requests to the disk system, referred to generically as asynchronous I/O.

==为了克服这一限制，许多现代操作系统引入了向磁盘系统发出 I/O 请求的新方法，统称为异步 I/O。==

  

These interfaces enable an application to issue an I/O request and return control immediately to the caller, before the I/O has completed.

==这些接口允许应用程序发出 I/O 请求并在 I/O 完成之前立即将控制权返回给调用者。==

  

Additional interfaces enable an application to determine whether various I/Os have completed.

==额外的接口允许应用程序确定各种 I/O 是否已完成。==

  

For example, let us examine the interface provided on a Mac (other systems have similar APIs).

==例如，让我们检查一下 Mac 上提供的接口（其他系统有类似的 API）。==

  

The APIs revolve around a basic structure, the struct aiocb or AIO control block in common terminology.

==这些 API 围绕一个基本结构展开，即 struct aiocb，通俗术语称为 AIO 控制块。==

  

A simplified version of the structure looks like this (see the manual pages for more information):

==该结构的简化版本如下所示（更多信息请参阅手册页）：==

  

```c

struct aiocb {

    int aio_fildes; // File descriptor

    off_t aio_offset; // File offset

    volatile void *aio_buf; // Location of buffer

    size_t aio_nbytes; // Length of transfer

};

  

```

  

```c

struct aiocb {

==    int aio_fildes; // 文件描述符==

==    off_t aio_offset; // 文件偏移量==

==    volatile void *aio_buf; // 缓冲区位置==

==    size_t aio_nbytes; // 传输长度==

};

  

```

  

To issue an asynchronous read to a file, an application should first fill in this structure with the relevant information: the file descriptor of the file to be read (aio_fildes), the offset within the file (aio_offset) as well as the length of the request (aio_nbytes), and finally the target memory location into which the results of the read should be copied (aio_buf).

==要向文件发出异步读取，应用程序应首先用相关信息填充此结构：要读取的文件的文件描述符 (aio_fildes)、文件内的偏移量 (aio_offset) 以及请求的长度 (aio_nbytes)，最后是读取结果应复制到的目标内存位置 (aio_buf)。==

  

After this structure is filled in, the application must issue the asynchronous call to read the file.

==填充完此结构后，应用程序必须发出异步调用来读取文件。==

  

On a Mac, this API is simply the asynchronous read API:

==在 Mac 上，此 API 仅仅是异步读取 API：==

  

`int aio_read(struct aiocb *aiocbp);`

`int aio_read(struct aiocb *aiocbp);`

  

This call tries to issue the I/O; if successful, it simply returns right away and the application (i.e., the event-based server) can continue with its work.

==此调用尝试发出 I/O；如果成功，它只是立即返回，应用程序（即基于事件的服务器）可以继续其工作。==

  

There is one last piece of the puzzle we must solve, however.

==然而，我们必须解决拼图的最后一块。==

  

How can we tell when an I/O is complete, and thus that the buffer (pointed to by aio_buf) now has the requested data within it?

==我们如何知道 I/O 何时完成，从而知道缓冲区（由 aio_buf 指向）现在包含请求的数据？==

  

One last API is needed. On a Mac, it is referred to (somewhat confusingly) as aio_error().

==需要最后一个 API。在 Mac 上，它（有点令人困惑地）被称为 aio_error()。==

  

The API looks like this:

==API 如下所示：==

  

`int aio_error(const struct aiocb *aiocbp);`

`int aio_error(const struct aiocb *aiocbp);`

  

This system call checks whether the request referred to by aiocbp has completed.

==此系统调用检查由 aiocbp 引用的请求是否已完成。==

  

If it has, the routine returns success (indicated by a zero); if not, EINPROGRESS is returned.

==如果已完成，该例程返回成功（由零表示）；如果未完成，则返回 EINPROGRESS。==

  

Thus, for every outstanding asynchronous I/O, an application can periodically poll the system via a call to aio_error() to determine whether said I/O has yet completed.

==因此，对于每一个未完成的异步 I/O，应用程序可以通过调用 aio_error() 定期轮询系统，以确定该 I/O 是否已完成。==

  

One thing you might have noticed is that it is painful to check whether an I/O has completed.

==你可能已经注意到的一件事是，检查 I/O 是否完成是很痛苦的。==

  

If a program has tens or hundreds of I/Os issued at a given point in time, should it simply keep checking each of them repeatedly, or wait a little while first, or ... ?

==如果一个程序在给定的时间点发出了数十或数百个 I/O，它是应该简单地重复检查每一个，还是先等一会儿，或者……？==

  

To remedy this issue, some systems provide an approach based on the interrupt.

==为了解决这个问题，一些系统提供了一种基于中断的方法。==

  

This method uses UNIX signals to inform applications when an asynchronous I/O completes, thus removing the need to repeatedly ask the system.

==这种方法使用 UNIX 信号在异步 I/O 完成时通知应用程序，从而消除了重复询问系统的需要。==

  

This polling vs. interrupts issue is seen in devices too, as you will see (or already have seen) in the chapter on I/O devices.

==这种轮询与中断的问题在设备中也存在，正如你将在（或已经）在 I/O 设备章节中看到的那样。==

  

In systems without asynchronous I/O, the pure event-based approach cannot be implemented.

==在没有异步 I/O 的系统中，无法实现纯粹的基于事件的方法。==

  

However, clever researchers have derived methods that work fairly well in their place.

==然而，聪明的、研究人员已经衍生出了在这些地方工作得相当好的方法。==

  

For example, Pai et al. [PDZ99] describe a hybrid approach in which events are used to process network packets, and a thread pool is used to manage outstanding I/Os.

==例如，Pai 等人 [PDZ99] 描述了一种混合方法，其中事件用于处理网络数据包，而线程池用于管理未完成的 I/O。==

  

Read their paper for details.

==阅读他们的论文以获取详细信息。==

  

33.7 Another Problem: State Management

==33.7 另一个问题：状态管理==

  

Another issue with the event-based approach is that such code is generally more complicated to write than traditional thread-based code.

==基于事件的方法的另一个问题是，此类代码通常比传统的基于线程的代码编写起来更复杂。==

  

The reason is as follows: when an event handler issues an asynchronous I/O, it must package up some program state for the next event handler to use when the I/O finally completes.

==原因如下：当事件处理程序发出异步 I/O 时，它必须打包一些程序状态，以便下一个事件处理程序在 I/O 最终完成时使用。==

  

This additional work is not needed in thread-based programs, as the state the program needs is on the stack of the thread.

==这种额外的工作在基于线程的程序中是不需要的，因为程序所需的状态位于线程的栈上。==

  

Adya et al. call this work manual stack management, and it is fundamental to event-based programming [A+02].

==Adya 等人称这项工作为手动栈管理，它是基于事件编程的基础 [A+02]。==

  

To make this point more concrete, let's look at a simple example in which a thread-based server needs to read from a file descriptor (fd) and, once complete, write the data that it read from the file to a network socket descriptor (sd).

==为了使这一点更具体，让我们看一个简单的例子，其中基于线程的服务器需要从文件描述符 (fd) 读取，并且一旦完成，将从文件读取的数据写入网络套接字描述符 (sd)。==

  

The code (ignoring error checking) looks like this:

==代码（忽略错误检查）如下所示：==

  

```c

int rc = read (fd, buffer, size);

rc = write (sd, buffer, size);

  

```

  

```c

int rc = read (fd, buffer, size);

rc = write (sd, buffer, size);

  

```

  

As you can see, in a multi-threaded program, doing this kind of work is trivial.

==如你所见，在多线程程序中，做这种工作是微不足道的。==

  

When the read () finally returns, the code immediately knows which socket to write to because that information is on the stack of the thread (in the variable sd).

==当 read() 最终返回时，代码立即知道要写入哪个套接字，因为该信息位于线程的栈上（在变量 sd 中）。==

  

In an event-based system, life is not so easy.

==在基于事件的系统中，生活并非如此轻松。==

  

To perform the same task, we'd first issue the read asynchronously, using the AIO calls described above.

==要执行相同的任务，我们首先要使用上述 AIO 调用异步发出读取。==

  

Let's say we then periodically check for completion of the read using the aio_error() call.

==假设我们随后使用 aio_error() 调用定期检查读取的完成情况。==

  

When that call informs us that the read is complete, how does the event-based server know what to do?

==当该调用通知我们读取已完成时，基于事件的服务器如何知道该做什么？==

  

ASIDE: UNIX SIGNALS

==旁白：UNIX 信号==

  

A huge and fascinating infrastructure known as signals is present in all modern UNIX variants.

==在所有现代 UNIX 变体中，都存在一个巨大而迷人的基础设施，称为信号。==

  

At its simplest, signals provide a way to communicate with a process.

==简而言之，信号提供了一种与进程通信的方式。==

  

Specifically, a signal can be delivered to an application.

==具体来说，信号可以被传递给应用程序。==

  

Doing so stops the application from whatever it is doing to run a signal handler, i.e., some code in the application to handle that signal.

==这样做会停止应用程序当前正在做的任何事情，转而运行信号处理程序，即应用程序中用于处理该信号的某些代码。==

  

When finished, the process just resumes its previous behavior.

==完成后，进程将恢复其先前的行为。==

  

Each signal has a name, such as HUP (hang up), INT (interrupt), SEGV (segmentation violation), etc.; see the man page for details.

==每个信号都有一个名称，例如 HUP（挂起）、INT（中断）、SEGV（段错误）等；详情请参阅手册页。==

  

Interestingly, sometimes it is the kernel itself that does the signaling.

==有趣的是，有时是内核本身在发出信号。==

  

For example, when your program encounters a segmentation violation, the OS sends it a SIGSEGV (prepending SIG to signal names is common).

==例如，当你的程序遇到段错误时，操作系统会向其发送 SIGSEGV（在信号名称前加上 SIG 是很常见的）。==

  

If your program is configured to catch that signal, you can actually run some code in response to this erroneous program behavior (which is helpful for debugging).

==如果你的程序配置为捕获该信号，实际上你可以运行一些代码来响应这种错误的程序行为（这有助于调试）。==

  

When a signal is sent to a process not configured to handle a signal, the default behavior is enacted.

==当信号发送到未配置为处理该信号的进程时，将执行默认行为。==

  

For SEGV, the process is killed.

==对于 SEGV，进程会被终止。==

  

Here is a simple program that goes into an infinite loop, but has first set up a signal handler to catch SIGHUP:

==这是一个简单的程序，它进入无限循环，但首先设置了一个信号处理程序来捕获 SIGHUP：==

  

[Code Snippet Omitted for Brevity - Standard Signal Handler C Code]

==[代码片段]==

  

You can send signals to it with the kill command line tool (yes, this is an odd and aggressive name).

==你可以使用 kill 命令行工具向它发送信号（是的，这是一个奇怪且具有攻击性的名字）。==

  

Doing so will interrupt the main while loop in the program and run the handler code handle():

==这样做将中断程序中的主 while 循环并运行处理程序代码 handle()：==

  

prompt> ./main &

prompt> ./main &

  

prompt> kill -HUP 36705

prompt> kill -HUP 36705

  

stop wakin' me up...

==别再吵醒我了……==

  

There is a lot more to learn about signals, so much that a single chapter, much less a single page, does not nearly suffice.

==关于信号还有很多东西要学，一章的内容，更不用说一页了，完全不够。==

  

As always, there is one great source: Stevens and Rago [SR05]. Read more if interested.

==一如既往，有一个很好的来源：Stevens 和 Rago [SR05]。如果有兴趣，请阅读更多内容。==

  

The solution, as described by Adya et al. [A+02], is to use an old programming language construct known as a continuation [FHK84].

==正如 Adya 等人 [A+02] 所描述的，解决方案是使用一种称为延续（continuation）的古老编程语言结构 [FHK84]。==

  

Though it sounds complicated, the idea is rather simple: basically, record the needed information to finish processing this event in some data structure.

==虽然听起来很复杂，但这个想法相当简单：基本上，将完成处理此事件所需的信息记录在某个数据结构中。==

  

When the event happens (i.e., when the disk I/O completes), look up the needed information and process the event.

==当事件发生时（即，当磁盘 I/O 完成时），查找所需的信息并处理该事件。==

  

In this specific case, the solution would be to record the socket descriptor (sd) in some kind of data structure (e.g., a hash table), indexed by the file descriptor (fd).

==在这个特定案例中，解决方案是将套接字描述符 (sd) 记录在某种数据结构（例如哈希表）中，以文件描述符 (fd) 为索引。==

  

When the disk I/O completes, the event handler would use the file descriptor to look up the continuation, which will return the value of the socket descriptor to the caller.

==当磁盘 I/O 完成时，事件处理程序将使用文件描述符来查找延续，这将把套接字描述符的值返回给调用者。==

  

At this point (finally), the server can then do the last bit of work to write the data to the socket.

==此时（终于），服务器可以进行最后一点工作，将数据写入套接字。==

  

33.8 What Is Still Difficult With Events

==33.8 事件还有哪些困难==

  

There are a few other difficulties with the event-based approach that we should mention.

==基于事件的方法还有一些其他困难，我们应该提一下。==

  

For example, when systems moved from a single CPU to multiple CPUs, some of the simplicity of the event-based approach disappeared.

==例如，当系统从单 CPU 转移到多 CPU 时，基于事件的方法的一些简单性消失了。==

  

Specifically, in order to utilize more than one CPU, the event server has to run multiple event handlers in parallel.

==具体来说，为了利用多个 CPU，事件服务器必须并行运行多个事件处理程序。==

  

When doing so, the usual synchronization problems (e.g., critical sections) arise, and the usual solutions (e.g., locks) must be employed.

==这样做时，通常的同步问题（例如临界区）就会出现，并且必须采用通常的解决方案（例如锁）。==

  

Thus, on modern multicore systems, simple event handling without locks is no longer possible.

==因此，在现代多核系统上，没有锁的简单事件处理已不再可能。==

  

Another problem with the event-based approach is that it does not integrate well with certain kinds of systems activity, such as paging.

==基于事件的方法的另一个问题是，它不能很好地与某些类型的系统活动（如分页）集成。==

  

For example, if an event-handler page faults, it will block, and thus the server will not make progress until the page fault completes.

==例如，如果事件处理程序发生页面错误，它将被阻塞，因此服务器在页面错误完成之前不会有任何进展。==

  

Even though the server has been structured to avoid explicit blocking, this type of implicit blocking due to page faults is hard to avoid and thus can lead to large performance problems when prevalent.

==即使服务器已被构建为避免显式阻塞，这种由于页面错误引起的隐式阻塞也很难避免，因此在普遍存在时会导致严重的性能问题。==

  

A third issue is that event-based code can be hard to manage over time, as the exact semantics of various routines changes [A+02].

==第三个问题是，随着时间的推移，基于事件的代码可能难以管理，因为各种例程的确切语义会发生变化 [A+02]。==

  

For example, if a routine changes from non-blocking to blocking, the event handler that calls that routine must also change to accommodate its new nature, by ripping itself into two pieces.

==例如，如果一个例程从非阻塞变为阻塞，调用该例程的事件处理程序也必须改变以适应其新性质，将其自身拆分为两部分。==

  

Because blocking is so disastrous for event-based servers, a programmer must always be on the lookout for such changes in the semantics of the APIs each event uses.

==因为阻塞对基于事件的服务器来说是灾难性的，程序员必须时刻警惕每个事件使用的 API 语义中的这种变化。==

  

Finally, though asynchronous disk I/O is now possible on most platforms, it has taken a long time to get there [PDZ99], and it never quite integrates with asynchronous network I/O in as simple and uniform a manner as you might think.

==最后，虽然异步磁盘 I/O 现在在大多数平台上都是可能的，但实现这一目标花了很长时间 [PDZ99]，而且它从未像你想象的那样简单统一地与异步网络 I/O 集成。==

  

For example, while one would simply like to use the select () interface to manage all outstanding I/Os, usually some combination of select () for networking and the AIO calls for disk I/O are required.

==例如，虽然人们只想使用 select() 接口来管理所有未完成的 I/O，但通常需要结合用于网络的 select() 和用于磁盘 I/O 的 AIO 调用。==

  

33.9 Summary

==33.9 总结==

  

We've presented a bare bones introduction to a different style of concurrency based on events.

==我们简要介绍了一种基于事件的不同并发风格。==

  

Event-based servers give control of scheduling to the application itself, but do so at some cost in complexity and difficulty of integration with other aspects of modern systems (e.g., paging).

==基于事件的服务器将调度控制权交给应用程序本身，但这样做在复杂性和与现代系统其他方面（例如分页）集成的难度方面付出了一些代价。==

  

Because of these challenges, no single approach has emerged as best.

==由于这些挑战，没有一种单一的方法成为最佳方法。==

  

Thus, both threads and events are likely to persist as two different approaches to the same concurrency problem for many years to come.

==因此，线程和事件可能会作为解决同一并发问题的两种不同方法持续存在很多年。==

  

Read some research papers (e.g., [A+02, PDZ99, vB+03, WCB01]) or better yet, write some event-based code, to learn more.

==阅读一些研究论文（例如 [A+02, PDZ99, vB+03, WCB01]），或者更好的是，编写一些基于事件的代码，以了解更多信息。==

  

Homework (Code)

==作业（代码）==

  

In this (short) homework, you'll gain some experience with event-based code and some of its key concepts. Good luck!

==在这个（简短的）作业中，你将获得一些基于事件的代码及其关键概念的经验。祝好运！==

  

Questions

==问题==

  

1. First, write a simple server that can accept and serve TCP connections.

==2. 首先，编写一个可以接受并服务 TCP 连接的简单服务器。==

  

You'll have to poke around the Internet a bit if you don't already know how to do this.

==如果你还不知道怎么做，你必须在互联网上搜索一下。==

  

Build this to serve exactly one request at a time; have each request be very simple, e.g., to get the current time of day.

==构建它以一次仅服务一个请求；让每个请求都非常简单，例如，获取当前时间。==

  

2. Now, add the select () interface.

==3. 现在，添加 select() 接口。==

  

Build a main program that can accept multiple connections, and an event loop that checks which file descriptors have data on them, and then read and process those requests.

==构建一个可以接受多个连接的主程序，以及一个检查哪些文件描述符上有数据的事件循环，然后读取并处理这些请求。==

  

Make sure to carefully test that you are using select () correctly.

==确保仔细测试你是否正确使用了 select()。==

  

3. Next, let's make the requests a little more interesting, to mimic a simple web or file server.

==4. 接下来，让我们让请求更有趣一点，模仿一个简单的 Web 或文件服务器。==

  

Each request should be to read the contents of a file (named in the request), and the server should respond by reading the file into a buffer, and then returning the contents to the client.

==每个请求都应该是读取文件（在请求中命名）的内容，服务器应通过将文件读入缓冲区然后将内容返回给客户端来响应。==

  

Use the standard open (), read(), close() system calls to implement this feature.

==使用标准的 open()、read()、close() 系统调用来实现此功能。==

  

Be a little careful here: if you leave this running for a long time, someone may figure out how to use it to read all the files on your computer!

==这里要小心一点：如果你让它长时间运行，有人可能会弄清楚如何使用它来读取你计算机上的所有文件！==

  

4. Now, instead of using standard I/O system calls, use the asynchronous I/O interfaces as described in the chapter.

==5. 现在，不要使用标准的 I/O 系统调用，而是使用本章中描述的异步 I/O 接口。==

  

How hard was it to incorporate asynchronous interfaces into your program?

==将异步接口合并到你的程序中有多难？==

  

5. For fun, add some signal handling to your code.

==6. 为了好玩，在你的代码中添加一些信号处理。==

  

One common use of signals is to poke a server to reload some kind of configuration file, or take some other kind of administrative action.

==信号的一种常见用途是提示服务器重新加载某种配置文件，或采取某种其他管理操作。==

  

Perhaps one natural way to play around with this is to add a user-level file cache to your server, which stores recently accessed files.

==也许玩这个的一种自然方式是在你的服务器中添加一个用户级文件缓存，用于存储最近访问的文件。==

  

Implement a signal handler that clears the cache when the signal is sent to the server process.

==实现一个信号处理程序，当信号发送到服务器进程时清除缓存。==

  

6. Finally, we have the hard part: how can you tell if the effort to build an asynchronous, event-based approach are worth it?

==7. 最后，我们遇到了困难的部分：你怎么知道构建异步、基于事件的方法的努力是否值得？==

  

Can you create an experiment to show the benefits?

==你能创建一个实验来展示其好处吗？==

  

How much implementation complexity did your approach add?

==你的方法增加了多少实现复杂性？==

  

Summary Dialogue on Concurrency

==关于并发的总结对话==

  

Professor: So, does your head hurt now?

==教授：那么，你的头现在疼吗？==

  

Student: (taking two Motrin tablets) Well, some. It's hard to think about all the ways threads can interleave.

==学生：（服用了两片布洛芬）嗯，有点。很难思考线程交错的所有方式。==

  

Professor: Indeed it is. I am always amazed that when concurrent execution is involved, just a few lines of code can become nearly impossible to understand.

==教授：确实如此。我总是惊讶于当涉及并发执行时，只有几行代码也会变得几乎无法理解。==

  

Student: Me too! It's kind of embarrassing, as a Computer Scientist, not to be able to make sense of five lines of code.

==学生：我也是！作为一名计算机科学家，搞不懂五行代码有点尴尬。==

  

Professor: Oh, don't feel too badly. If you look through the first papers on concurrent algorithms, they are sometimes wrong!

==教授：哦，别太难过。如果你查看关于并发算法的最早的论文，它们有时也是错的！==

  

And the authors often professors!

==而且作者通常是教授！==

  

Student: (gasps) Professors can be umm... wrong?

==学生：（倒吸一口气）教授也可能……呃……犯错？==

  

Professor: Yes, it is true. Though don't tell anybody it's one of our trade secrets.

==教授：是的，这是真的。不过别告诉任何人，这是我们的商业秘密之一。==

  

Student: I am sworn to secrecy. But if concurrent code is so hard to think about, and so hard to get right, how are we supposed to write correct concurrent code?

==学生：我发誓保密。但是，如果并发代码如此难以思考，并且如此难以正确编写，我们应该如何编写正确的并发代码呢？==

  

Professor: Well that is the real question, isn't it? I think it starts with a few simple things.

==教授：这才是真正的问题，不是吗？我认为这始于几件简单的事情。==

  

First, keep it simple! Avoid complex interactions between threads, and use well-known and tried-and-true ways to manage thread interactions.

==首先，保持简单！避免线程之间的复杂交互，并使用众所周知的、经过验证的方法来管理线程交互。==

  

Student: Like simple locking, and maybe a producer-consumer queue?

==学生：比如简单的锁，或者生产者-消费者队列？==

  

Professor: Exactly! Those are common paradigms, and you should be able to produce the working solutions given what you've learned.

==教授：没错！这些是常见的范例，根据你所学的知识，你应该能够生成有效的解决方案。==

  

Second, only use concurrency when absolutely needed; avoid it if at all possible.

==其次，仅在绝对需要时才使用并发；如果可能的话，尽量避免使用它。==

  

There is nothing worse than premature optimization of a program.

==没有什么比程序的过早优化更糟糕的了。==

  

Student: I see why add threads if you don't need them?

==学生：我明白了，如果你不需要线程，为什么要添加它们？==

  

Professor: Exactly. Third, if you really need parallelism, seek it in other simplified forms.

==教授：没错。第三，如果你真的需要并行性，请以其他简化形式寻求它。==

  

For example, the Map-Reduce method for writing parallel data analysis code is an excellent example of achieving parallelism without having to handle any of the horrific complexities of locks, condition variables, and the other nasty things we've talked about.

==例如，编写并行数据分析代码的 Map-Reduce 方法就是实现并行性的绝佳示例，而无需处理锁、条件变量和我们讨论过的其他讨厌事物的可怕复杂性。==

  

Student: Map-Reduce, huh? Sounds interesting - I'll have to read more about it on my own.

==学生：Map-Reduce，嗯？听起来很有趣——我得自己去多读读。==

  

Professor: Good! You should. In the end, you'll have to do a lot of that, as what we learn together can only serve as the barest introduction to the wealth of knowledge that is out there.

==教授：很好！你应该这样做。最后，你将不得不做很多这样的事情，因为我们一起学到的东西只能作为对现有丰富知识的最基本的介绍。==

  

Read, read, and read some more! And then try things out, write some code, and then write some more too.

==阅读，阅读，再阅读！然后尝试一下，写一些代码，然后再写更多代码。==

  

And practice more, too; beyond what's in this book, there are plenty of other resources out there.

==还要多练习；除了这本书里的内容，外面还有很多其他资源。==

  

As Gladwell talks about in his book "Outliers", you need to put roughly 10,000 hours into something in order to become a real expert.

==正如 Gladwell 在他的书《异类》中所说，你需要投入大约 10,000 小时做某事才能成为真正的专家。==

  

You can't do that all inside of class time!

==你不可能在课堂时间内完成所有这些！==

  

Student: Wow, I'm not sure if that is depressing, or uplifting. But I'll assume the latter, and get to work!

==学生：哇，我不确定这是令人沮丧还是令人振奋。但我假设是后者，然后开始工作！==

  

Time to write some more concurrent code...

==是时候多写点并发代码了……==

  

Here is a link to one, in gamified form: [https://deadlockempire.github.io/](https://deadlockempire.github.io/)

==这里有一个链接，是游戏化形式的：[https://deadlockempire.github.io/](https://deadlockempire.github.io/)==

  

Part III Persistence

==第三部分 持久性==

  

A Dialogue on Persistence

==关于持久性的对话==

  

Professor: And thus we reach the third of our four... err... three pillars of operating systems: persistence.

==教授：因此，我们到达了操作系统的四个……呃……三个支柱中的第三个：持久性。==

  

Student: Did you say there were three pillars, or four? What is the fourth?

==学生：你是说有三个支柱，还是四个？第四个是什么？==

  

Professor: No. Just three, young student, just three. Trying to keep it simple here.

==教授：不。只有三个，年轻的学生，只有三个。这里尽量保持简单。==

  

Student: OK, fine. But what is persistence, oh fine and noble professor?

==学生：好吧，行。但是什么是持久性，哦，优秀而高贵的教授？==

  

Professor: Actually, you probably know what it means in the traditional sense, right?

==教授：实际上，你可能知道它在传统意义上意味着什么，对吧？==

  

As the dictionary would say: "a firm or obstinate continuance in a course of action in spite of difficulty or opposition."

==正如字典所说：“在困难或反对面前坚定或顽固地继续行动。”==

  

Student: It's kind of like taking your class: some obstinance required.

==学生：这有点像上你的课：需要一些固执。==

  

Professor: Ha! Yes. But persistence here means something else. Let me explain.

==教授：哈！是的。但这里的持久性意味着别的东西。让我解释一下。==

  

Imagine you are outside, in a field, and you pick a

==教授：想象你在外面，在田野里，你摘了一个==

  

Student: (interrupting) I know! A peach! From a peach tree!

==学生：（打断）我知道！一个桃子！从桃树上摘的！==

  

Professor: I was going to say apple, from an apple tree. Oh well; we'll do it your way, I guess.

==教授：我本来想说苹果，从苹果树上摘的。哦，好吧；我想我们就按你说的来吧。==

  

Student: (stares blankly)

==学生：（茫然地注视着）==

  

Professor: Anyhow, you pick a peach; in fact, you pick many many peaches, but you want to make them last for a long time.

==教授：不管怎样，你摘了一个桃子；事实上，你摘了很多很多桃子，但你想让它们保存很长时间。==

  

Winter is hard and cruel in Wisconsin, after all. What do you do?

==毕竟，威斯康星州的冬天艰难而残酷。你做什么？==

  

Student: Well, I think there are some different things you can do. You can pickle it! Or bake a pie.

==学生：嗯，我想你可以做一些不同的事情。你可以把它腌起来！或者烤个派。==

  

Or make a jam of some kind. Lots of fun!

==或者做某种果酱。很有趣！==

  

Professor: Fun? Well, maybe. Certainly, you have to do a lot more work to make the peach persist.

==教授：有趣？嗯，也许吧。当然，你必须做更多的工作才能让桃子持久保存。==

  

And so it is with information as well; making information persist, despite computer crashes, disk failures, or power outages is a tough and interesting challenge.

==信息也是如此；即使在计算机崩溃、磁盘故障或停电的情况下，使信息持久化也是一项艰巨而有趣的挑战。==

  

Student: Nice segue; you're getting quite good at that.

==学生：很好的过渡；你很擅长这个。==

  

Professor: Thanks! A professor can always use a few kind words, you know.

==教授：谢谢！教授总是需要一些好听的话，你知道的。==

  

Student: I'll try to remember that. I guess it's time to stop talking peaches, and start talking computers?

==学生：我会尽量记住这一点。我想是时候停止谈论桃子，开始谈论计算机了？==

  

Professor: Yes, it is that time...

==教授：是的，是时候了……==

  

I/O Devices

==I/O 设备==

  

Before delving into the main content of this part of the book (on persistence), we first introduce the concept of an input/output (I/O) device and show how the operating system might interact with such an entity.

==在深入探讨本书这一部分的主要内容（关于持久性）之前，我们首先介绍输入/输出 (I/O) 设备的概念，并展示操作系统如何与此类实体交互。==

  

I/O is quite critical to computer systems, of course; imagine a program without any input (it produces the same result each time);

==I/O 对计算机系统当然至关重要；想象一个没有任何输入的程序（它每次都产生相同的结果）；==

  

Now imagine a program with no output (what was the purpose of it running?).

==现在想象一个没有输出的程序（运行它的目的是什么？）。==

  

Clearly, for computer systems to be interesting, both input and output are required.

==显然，要让计算机系统变得有趣，输入和输出都是必需的。==

  

And thus, our general problem:

==因此，我们的一般性问题是：==

  

CRUX: HOW TO INTEGRATE I/O INTO SYSTEMS

==关键：如何将 I/O 集成到系统中==

  

How should I/O be integrated into systems?

==I/O 应该如何集成到系统中？==

  

What are the general mechanisms?

==一般机制是什么？==

  

How can we make them efficient?

==我们要如何使它们高效？==

  

36.1 System Architecture

==36.1 系统架构==

  

To begin our discussion, let's look at a "classical" diagram of a typical system (Figure 36.1, page 2).

==为了开始我们的讨论，让我们看一个典型系统的“经典”图（图 36.1，第 2 页）。==

  

The picture shows a single CPU attached to the main memory of the system via some kind of memory bus or interconnect.

==该图显示了单个 CPU 通过某种内存总线或互连连接到系统的主内存。==

  

Some devices are connected to the system via a general I/O bus, which in many modern systems would be PCI (or one of its many derivatives);

==一些设备通过通用 I/O 总线连接到系统，在许多现代系统中，这将是 PCI（或其众多衍生产品之一）；==

  

Graphics and some other higher-performance I/O devices might be found here.

==图形和其他一些高性能 I/O 设备可能会在这里找到。==

  

Finally, even lower down are one or more of what we call a peripheral bus, such as SCSI, SATA, or USB.

==最后，再往下是一个或多个我们称为外围总线的东西，例如 SCSI、SATA 或 USB。==

  

These connect slow devices to the system, including disks, mice, and keyboards.

==这些总线将慢速设备连接到系统，包括磁盘、鼠标和键盘。==

  

One question you might ask is: why do we need a hierarchical structure like this?

==你可能会问的一个问题是：为什么我们需要这种分层结构？==

  

Put simply: physics, and cost.

==简而言之：物理学和成本。==

  

The faster a bus is, the shorter it must be;

==总线越快，它就必须越短；==

  

Thus, a high-performance memory bus does not have much room to plug devices and such into it.

==因此，高性能内存总线没有太多空间来插入设备等。==

  

In addition, engineering a bus for high performance is quite costly.

==此外，设计高性能总线的成本相当高。==

  

Thus, system designers have adopted this hierarchical approach, where components that demand high performance (such as the graphics card) are nearer the CPU.

==因此，系统设计人员采用了这种分层方法，其中要求高性能的组件（例如显卡）更靠近 CPU。==

  

Lower performance components are further away.

==性能较低的组件则更远。==

  

The benefits of placing disks and other slow devices on a peripheral bus are manifold;

==将磁盘和其他慢速设备放在外围总线上的好处是多方面的；==

  

In particular, you can place a large number of devices on it.

==特别是，你可以在上面放置大量设备。==

  

Figure 36.1: Prototypical System Architecture

==图 36.1：原型系统架构==

  

Of course, modern systems increasingly use specialized chipsets and faster point-to-point interconnects to improve performance.

==当然，现代系统越来越多地使用专用芯片组和更快的点对点互连来提高性能。==

  

Figure 36.2 (page 3) shows an approximate diagram of Intel's Z270 Chipset [H17].

==图 36.2（第 3 页）显示了英特尔 Z270 芯片组的近似图 [H17]。==

  

Along the top, the CPU connects most closely to the memory system, but also has a high-performance connection to the graphics card (and thus, the display) to enable gaming (oh, the horror!) and other graphics-intensive applications.

==在顶部，CPU 与内存系统的连接最紧密，但也与显卡（以及显示器）有高性能连接，以支持游戏（哦，太可怕了！）和其他图形密集型应用程序。==

  

The CPU connects to an I/O chip via Intel's proprietary DMI (Direct Media Interface), and the rest of the devices connect to this chip via a number of different interconnects.

==CPU 通过英特尔专有的 DMI（直接媒体接口）连接到 I/O 芯片，其余设备通过许多不同的互连连接到该芯片。==

  

On the right, one or more hard drives connect to the system via the eSATA interface;

==在右侧，一个或多个硬盘驱动器通过 eSATA 接口连接到系统；==

  

ATA (the AT Attachment, in reference to providing connection to the IBM PC AT), then SATA (for Serial ATA), and now eSATA (for external SATA) represent an evolution of storage interfaces over the past decades, with each step forward increasing performance to keep pace with modern storage devices.

==ATA（AT 附件，指提供与 IBM PC AT 的连接），然后是 SATA（串行 ATA），现在是 eSATA（外部 SATA），代表了过去几十年存储接口的演变，每一步都提高了性能以跟上现代存储设备的步伐。==

  

Below the I/O chip are a number of USB (Universal Serial Bus) connections, which in this depiction enable a keyboard and mouse to be attached to the computer.

==在 I/O 芯片下方有许多 USB（通用串行总线）连接，在此描述中，这些连接允许键盘和鼠标连接到计算机。==

  

On many modern systems, USB is used for low performance devices such as these.

==在许多现代系统上，USB 用于此类低性能设备。==

  

Finally, on the left, other higher performance devices can be connected to the system via PCIe (Peripheral Component Interconnect Express).

==最后，在左侧，其他高性能设备可以通过 PCIe（外围组件互连高速）连接到系统。==

  

In this diagram, a network interface is attached to the system here;

==在该图中，网络接口连接在此处；==

  

Higher performance storage devices (such as NVMe persistent storage devices) are often connected here.

==更高性能的存储设备（例如 NVMe 持久存储设备）通常连接在这里。==

  

Figure 36.2: Modern System Architecture

==图 36.2：现代系统架构==

  

36.2 A Canonical Device

==36.2 典型设备==

  

Let us now look at a canonical device (not a real one), and use this device to drive our understanding of some of the machinery required to make device interaction efficient.

==现在让我们看一个典型设备（不是真实的），并使用该设备来加深我们对使设备交互高效所需的某些机制的理解。==

  

From Figure 36.3 (page 4), we can see that a device has two important components.

==从图 36.3（第 4 页）可以看出，一个设备有两个重要组件。==

  

The first is the hardware interface it presents to the rest of the system.

==第一个是它呈现给系统其余部分的硬件接口。==

  

Just like a piece of software, hardware must also present some kind of interface that allows the system software to control its operation.

==就像软件一样，硬件也必须呈现某种允许系统软件控制其操作的接口。==

  

Thus, all devices have some specified interface and protocol for typical interaction.

==因此，所有设备都有一些指定的接口和协议来进行典型交互。==

  

The second part of any device is its internal structure.

==任何设备的第二部分都是其内部结构。==

  

This part of the device is implementation specific and is responsible for implementing the abstraction the device presents to the system.

==设备的这部分是特定于实现的，负责实现设备向系统呈现的抽象。==

  

Very simple devices will have one or a few hardware chips to implement their functionality;

==非常简单的设备将拥有一个或几个硬件芯片来实现其功能；==

  

More complex devices will include a simple CPU, some general purpose memory, and other device-specific chips to get their job done.

==更复杂的设备将包括一个简单的 CPU、一些通用内存和其他特定于设备的芯片来完成其工作。==

  

For example, modern RAID controllers might consist of hundreds of thousands of lines of firmware (i.e., software within a hardware device) to implement its functionality.

==例如，现代 RAID 控制器可能包含数十万行固件（即硬件设备中的软件）来实现其功能。==

  

Figure 36.3: A Canonical Device

==图 36.3：典型设备==

  

36.3 The Canonical Protocol

==36.3 典型协议==

  

In the picture above, the (simplified) device interface is comprised of three registers: a status register, which can be read to see the current status of the device;

==在上图中，（简化的）设备接口由三个寄存器组成：一个状态寄存器，可以读取它来查看设备的当前状态；==

  

A command register, to tell the device to perform a certain task;

==一个命令寄存器，告诉设备执行特定任务；==

  

And a data register to pass data to the device, or get data from the device.

==以及一个数据寄存器，用于向设备传递数据，或从设备获取数据。==

  

By reading and writing these registers, the operating system can control device behavior.

==通过读写这些寄存器，操作系统可以控制设备行为。==

  

Let us now describe a typical interaction that the OS might have with the device in order to get the device to do something on its behalf.

==现在让我们描述操作系统可能与设备进行的典型交互，以便让设备为其执行某些操作。==

  

The protocol is as follows:

==协议如下：==

  

```c

While (STATUS == BUSY)

    ; // wait until device is not busy

Write data to DATA register

Write command to COMMAND register

(starts the device and executes the command)

While (STATUS == BUSY)

    ; // wait until device is done with your request

  

```

  

```c

While (STATUS == BUSY)

==    ; // 等待直到设备不忙==

Write data to DATA register

Write command to COMMAND register

==(启动设备并执行命令)==

While (STATUS == BUSY)

==    ; // 等待直到设备完成你的请求==

  

```

  

The protocol has four steps.

==该协议有四个步骤。==

  

In the first, the OS waits until the device is ready to receive a command by repeatedly reading the status register;

==第一步，OS 通过重复读取状态寄存器来等待设备准备好接收命令；==

  

We call this polling the device (basically, just asking it what is going on).

==我们要称之为轮询设备（基本上就是问它发生了什么）。==

  

Second, the OS sends some data down to the data register;

==第二，OS 将一些数据发送到数据寄存器；==

  

One can imagine that if this were a disk, for example, that multiple writes would need to take place to transfer a disk block (say 4KB) to the device.

==可以想象，例如，如果这是一个磁盘，则需要进行多次写入才能将一个磁盘块（比如 4KB）传输到设备。==

  

When the main CPU is involved with the data movement (as in this example protocol), we refer to it as programmed I/O (PIO).

==当主 CPU 参与数据移动时（如本示例协议中所示），我们将其称为程序控制 I/O (PIO)。==

  

Third, the OS writes a command to the command register;

==第三，OS 将命令写入命令寄存器；==

  

Doing so implicitly lets the device know that both the data is present and that it should begin working on the command.

==这样做隐含地让设备知道数据已存在，并且它应该开始处理命令。==

  

Finally, the OS waits for the device to finish by again polling it in a loop, waiting to see if it is finished (it may then get an error code to indicate success or failure).

==最后，OS 通过再次在循环中轮询设备来等待设备完成，等待查看它是否已完成（它可能会获得一个错误代码来指示成功或失败）。==

  

This basic protocol has the positive aspect of being simple and working.

==这个基本协议的积极方面是简单且有效。==

  

However, there are some inefficiencies and inconveniences involved.

==但是，这涉及一些低效和不便之处。==

  

The first problem you might notice in the protocol is that polling seems inefficient;

==你可能在协议中注意到的第一个问题是轮询似乎效率低下；==

  

Specifically, it wastes a great deal of CPU time just waiting for the (potentially slow) device to complete its activity, instead of switching to another ready process and thus better utilizing the CPU.

==具体来说，它浪费了大量的 CPU 时间来等待（可能很慢的）设备完成其活动，而不是切换到另一个就绪进程，从而更好地利用 CPU。==

  

THE CRUX: How To AVOID THE COSTS OF POLLING

==关键：如何避免轮询的成本==

  

How can the OS check device status without frequent polling, and thus lower the CPU overhead required to manage the device?

==OS 如何在不频繁轮询的情况下检查设备状态，从而降低管理设备所需的 CPU 开销？==

  

36.4 Lowering CPU Overhead With Interrupts

==36.4 使用中断降低 CPU 开销==

  

The invention that many engineers came upon years ago to improve this interaction is something we've seen already: the interrupt.

==许多工程师多年前发明来改善这种交互的东西是我们已经见过的：中断。==

  

Instead of polling the device repeatedly, the OS can issue a request, put the calling process to sleep, and context switch to another task.

==OS 可以发出请求，让调用进程进入睡眠状态，并上下文切换到另一个任务，而不是重复轮询设备。==

  

When the device is finally finished with the operation, it will raise a hardware interrupt, causing the CPU to jump into the OS at a predetermined interrupt service routine (ISR) or more simply an interrupt handler.

==当设备最终完成操作时，它将引发硬件中断，导致 CPU 跳转到 OS 中预定的中断服务程序 (ISR) 或更简单地说中断处理程序。==

  

The handler is just a piece of operating system code that will finish the request (for example, by reading data and perhaps an error code from the device) and wake the process waiting for the I/O, which can then proceed as desired.

==处理程序只是一段操作系统代码，它将完成请求（例如，通过从设备读取数据和可能的错误代码）并唤醒等待 I/O 的进程，该进程随后可以按预期继续。==

  

Interrupts thus allow for overlap of computation and I/O which is key for improved utilization.

==因此，中断允许计算和 I/O 重叠，这是提高利用率的关键。==

  

In the diagram, Process 1 runs on the CPU for some time (indicated by a repeated 1 on the CPU line), and then issues an I/O request to the disk to read some data.

==在该图中，进程 1 在 CPU 上运行一段时间（由 CPU 行上重复的 1 表示），然后向磁盘发出 I/O 请求以读取一些数据。==

  

Without interrupts, the system simply spins, polling the status of the device repeatedly until the I/O is complete (indicated by a p).

==如果没有中断，系统只会空转，重复轮询设备的状态，直到 I/O 完成（由 p 表示）。==

  

The disk services the request and finally Process 1 can run again.

==磁盘服务请求，最后进程 1 可以再次运行。==

  

If instead we utilize interrupts and allow for overlap, the OS can do something else while waiting for the disk.

==相反，如果我们利用中断并允许重叠，OS 可以在等待磁盘时做其他事情。==

  

In this example, the OS runs Process 2 on the CPU while the disk services Process 1's request.

==在此示例中，OS 在磁盘服务进程 1 的请求时在 CPU 上运行进程 2。==

  

When the disk request is finished, an interrupt occurs, and the OS wakes up Process 1 and runs it again.

==当磁盘请求完成时，会发生中断，OS 唤醒进程 1 并再次运行它。==

  

Thus, both the CPU and the disk are properly utilized during the middle stretch of time.

==因此，CPU 和磁盘在中间段时间内都得到了适当的利用。==

  

Note that using interrupts is not always the best solution.

==请注意，使用中断并不总是最佳解决方案。==

  

For example, imagine a device that performs its tasks very quickly: the first poll usually finds the device to be done with task.

==例如，设想一个执行任务非常快的设备：第一次轮询通常会发现设备已完成任务。==

  

Using an interrupt in this case will actually slow down the system: switching to another process, handling the interrupt, and switching back to the issuing process is expensive.

==在这种情况下使用中断实际上会降低系统速度：切换到另一个进程、处理中断以及切换回发出请求的进程是昂贵的。==

  

Thus, if a device is fast, it may be best to poll; if it is slow, interrupts, which allow overlap, are best.

==因此，如果设备很快，最好进行轮询；如果它很慢，允许重叠的中断是最好的。==

I/O DEVICES
I/O 设备

TIP: INTERRUPTS NOT ALWAYS BETTER THAN POLLING
==提示：中断并不总是优于轮询==

Although interrupts allow for overlap of computation and , they only really make sense for slow devices.
==尽管中断允许计算和  重叠，但它们实际上只对慢速设备有意义。==

Otherwise, the cost of interrupt handling and context switching may outweigh the benefits interrupts provide.
==否则，中断处理和上下文切换的成本可能会超过中断带来的好处。==

There are also cases where a flood of interrupts may overload a system and lead it to livelock [MR96]; in such cases, polling provides more control to the OS in its scheduling and thus is again useful.
==还有一些情况是，大量的中断可能会使系统过载并导致活锁（livelock）[MR96]；在这种情况下，轮询可以为操作系统提供更多的调度控制权，因此又变得有用了。==

If the speed of the device is not known, or sometimes fast and sometimes slow, it may be best to use a hybrid that polls for a little while and then, if the device is not yet finished, uses interrupts.
==如果设备的速度未知，或者时快时慢，最好的办法可能是使用混合策略：先轮询一小会儿，如果设备尚未完成，再使用中断。==

This two-phased approach may achieve the best of both worlds.
==这种两阶段的方法可以兼得两者的优点。==

Another reason not to use interrupts arises in networks [MR96].
==不使用中断的另一个原因出现在网络中 [MR96]。==

When a huge stream of incoming packets each generate an interrupt, it is possible for the OS to livelock, that is, find itself only processing interrupts and never allowing a user-level process to run and actually service the requests.
==当巨大的传入数据包流每一个都产生一个中断时，操作系统就有可能陷入活锁，即发现自己只在处理中断，而从未允许用户级进程运行来实际处理这些请求。==

For example, imagine a web server that experiences a load burst because it became the top-ranked entry on hacker news [H18].
==例如，想象一个网络服务器因为成为 Hacker News [H18] 上的头条而经历了负载爆发。==

In this case, it is better to occasionally use polling to better control what is happening in the system and allow the web server to service some requests before going back to the device to check for more packet arrivals.
==在这种情况下，最好偶尔使用轮询，以便更好地控制系统中发生的事情，并允许网络服务器在返回设备检查更多数据包到达之前处理一些请求。==

Another interrupt-based optimization is coalescing.
==另一个基于中断的优化是合并（coalescing）。==

In such a setup, a device which needs to raise an interrupt first waits for a bit before delivering the interrupt to the CPU.
==在这种设置中，需要发起中断的设备在向 CPU 传递中断之前先等待一会儿。==

While waiting, other requests may soon complete, and thus multiple interrupts can be coalesced into a single interrupt delivery, thus lowering the overhead of interrupt processing.
==在等待期间，其他请求可能很快就会完成，因此多个中断可以合并为一次中断传递，从而降低中断处理的开销。==

Of course, waiting too long will increase the latency of a request, a common trade-off in systems.
==当然，等待太久会增加请求的延迟，这是系统中常见的权衡。==

See Ahmad et al. [A+11] for an excellent summary.
==有关精彩的总结，请参阅 Ahmad 等人的文章 [A+11]。==

36.5 More Efficient Data Movement With DMA
36.5 利用 DMA 实现更高效的数据移动

Unfortunately, there is one other aspect of our canonical protocol that requires our attention.
==不幸的是，我们规范协议中还有另一个方面需要我们要关注。==

In particular, when using programmed  (PIO) to transfer a large chunk of data to a device, the CPU is once again overburdened with a rather trivial task, and thus wastes a lot of time and effort that could better be spent running other processes.
==特别是，当使用程序控制 （PIO）将大量数据传输到设备时，CPU 会再次因相当琐碎的任务而不堪重负，从而浪费了大量本可以用于运行其他进程的时间和精力。==

This timeline illustrates the problem:
==这个时间轴说明了这个问题：==

In the timeline, Process 1 is running and then wishes to write some data to the disk.
==在时间轴中，进程 1 正在运行，然后希望将一些数据写入磁盘。==

It then initiates the  which must copy the data from memory to the device explicitly, one word at a time (marked c in the diagram).
==然后它启动 ，这必须显式地将数据从内存复制到设备，一次一个字（在图中标记为 c）。==

When the copy is complete, the  begins on the disk and the CPU can finally be used for something else.
==当复制完成后， 在磁盘上开始，CPU 终于可以用于其他事情了。==

THE CRUX: How To LOWER PIO OVERHEADS
==关键问题：如何降低 PIO 开销==

With PIO, the CPU spends too much time moving data to and from devices by hand.
==使用 PIO，CPU 花费太多时间手动将数据移入和移出设备。==

How can we offload this work and thus allow the CPU to be more effectively utilized?
==我们如何卸载这项工作，从而允许 CPU 被更有效地利用？==

The solution to this problem is something we refer to as Direct Memory Access (DMA).
==这个问题的解决方案就是我们所说的直接内存访问（DMA）。==

A DMA engine is essentially a very specific device within a system that can orchestrate transfers between devices and main memory without much CPU intervention.
DMA 引擎本质上是系统内的一个非常特殊的设备，它可以协调设备和主内存之间的传输，而无需太多 CPU 干预。

DMA works as follows.
DMA 的工作原理如下。

To transfer data to the device, for example, the OS would program the DMA engine by telling it where the data lives in memory, how much data to copy, and which device to send it to.
==例如，为了将数据传输到设备，操作系统会对 DMA 引擎进行编程，告诉它数据在内存中的位置、要复制多少数据以及将其发送到哪个设备。==

At that point, the OS is done with the transfer and can proceed with other work.
==此时，操作系统完成了传输任务，可以继续进行其他工作。==

When the DMA is complete, the DMA controller raises an interrupt, and the OS thus knows the transfer is complete.
==当 DMA 完成时，DMA 控制器会发起一个中断，操作系统因此知道传输已完成。==

The revised timeline:
==修改后的时间轴：==

From the timeline, you can see that the copying of data is now handled by the DMA controller.
==从时间轴可以看出，数据的复制现在由 DMA 控制器处理。==

Because the CPU is free during that time, the OS can do something else, here choosing to run Process 2.
==因为 CPU 在那段时间是空闲的，操作系统可以做其他事情，这里选择运行进程 2。==

Process 2 thus gets to use more CPU before Process 1 runs again.
==因此，在进程 1 再次运行之前，进程 2 获得了更多的 CPU 使用权。==

36.6 Methods Of Device Interaction
36.6 设备交互的方法

Now that we have some sense of the efficiency issues involved with performing  there are a few other problems we need to handle to incorporate devices into modern systems.
==既然我们对执行  涉及的效率问题有了一些了解，我们还需要处理其他几个问题，以便将设备整合到现代系统中。==

One problem you may have noticed thus far: we have not really said anything about how the OS actually communicates with the device!
==到目前为止，你可能已经注意到了一个问题：我们还没有真正讨论过操作系统实际上是如何与设备通信的！==

Thus, the problem:
==因此，问题在于：==

THE CRUX: How To COMMUNICATE WITH DEVICES
==关键问题：如何与设备通信==

How should the hardware communicate with a device?
==硬件应该如何与设备通信？==

Should there be explicit instructions?
==应该有显式的指令吗？==

Or are there other ways to do it?
==还是有其他方法可以做到？==

Over time, two primary methods of device communication have developed.
==随着时间的推移，已经发展出了两种主要的设备通信方法。==

The first, oldest method (used by IBM mainframes for many years) is to have explicit I/O instructions.
==第一种也是最古老的方法（被 IBM 大型机使用了多年）是拥有显式的 I/O 指令。==

These instructions specify a way for the OS to send data to specific device registers and thus allow the construction of the protocols described above.
==这些指令指定了操作系统向特定设备寄存器发送数据的方式，从而允许构建上述协议。==

For example, on x86, the in and out instructions can be used to communicate with devices.
==例如，在 x86 上，in 和 out 指令可用于与设备通信。==

For example, to send data to a device, the caller specifies a register with the data in it, and a specific port which names the device.
==例如，为了向设备发送数据，调用者指定一个包含数据的寄存器，以及一个命名该设备的特定端口。==

Executing the instruction leads to the desired behavior.
==执行该指令会产生所需的行为。==

Such instructions are usually privileged.
==此类指令通常是特权指令。==

The OS controls devices, and the OS thus is the only entity allowed to directly communicate with them.
==操作系统控制设备，因此操作系统是唯一允许直接与它们通信的实体。==

Imagine if any program could read or write the disk, for example: total chaos (as always), as any user program could use such a loophole to gain complete control over the machine.
==试想一下，如果任何程序都可以读写磁盘，例如：将会是一片混乱（一如既往），因为任何用户程序都可以利用这个漏洞来获得对机器的完全控制。==

The second method to interact with devices is known as memory-mapped I/O.
==与设备交互的第二种方法称为内存映射 I/O（memory-mapped I/O）。==

With this approach, the hardware makes device registers available as if they were memory locations.
==通过这种方法，硬件使设备寄存器像内存位置一样可用。==

To access a particular register, the OS issues a load (to read) or store (to write) the address; the hardware then routes the load/store to the device instead of main memory.
==要访问特定的寄存器，操作系统发出加载（读取）或存储（写入）该地址的指令；然后硬件将加载/存储路由到设备而不是主内存。==

There is not some great advantage to one approach or the other.
==这两种方法并没有哪一种具有巨大的优势。==

The memory-mapped approach is nice in that no new instructions are needed to support it, but both approaches are still in use today.
==内存映射方法的优点是不需要新指令来支持它，但这两种方法今天仍在使用。==

36.7 Fitting Into The OS: The Device Driver
36.7 融入操作系统：设备驱动程序

One final problem we will discuss: how to fit devices, each of which have very specific interfaces, into the OS, which we would like to keep as general as possible.
==我们将讨论的最后一个问题是：如何将每个都具有非常特定接口的设备，融入到我们要尽可能保持通用的操作系统中。==

For example, consider a file system.
==例如，考虑一个文件系统。==

We'd like to build a file system that worked on top of SCSI disks, IDE disks, USB keychain drives, and so forth, and we'd like the file system to be relatively oblivious to all of the details of how to issue a read or write request to these different types of drives.
==我们希望构建一个可以在 SCSI 磁盘、IDE 磁盘、USB 闪存驱动器等之上工作的文件系统，并且我们希望文件系统相对忽略如何向这些不同类型的驱动器发出读或写请求的所有细节。==

Thus, our problem:
==因此，我们的问题是：==

THE CRUX: How TO BUILD A DEVICE-NEUTRAL OS
==关键问题：如何构建一个设备中立的操作系统==

How can we keep most of the OS device-neutral, thus hiding the details of device interactions from major OS subsystems?
==我们如何保持大部分操作系统设备中立，从而向主要的操作系统子系统隐藏设备交互的细节？==

The problem is solved through the age-old technique of abstraction.
==这个问题通过古老的抽象技术得以解决。==

At the lowest level, a piece of software in the OS must know in detail how a device works.
==在最低层，操作系统中的一个软件片段必须详细了解设备是如何工作的。==

We call this piece of software a device driver, and any specifics of device interaction are encapsulated within.
==我们将这个软件片段称为设备驱动程序，所有设备交互的细节都封装在其中。==

Let us see how this abstraction might help OS design and implementation by examining the Linux file system software stack.
==让我们通过检查 Linux 文件系统软件栈，来看看这种抽象如何帮助操作系统的设计和实现。==

Figure 36.4 is a rough and approximate depiction of the Linux software organization.
==图 36.4 是 Linux 软件组织的粗略和近似描绘。==

As you can see from the diagram, a file system (and certainly, an application above) is completely oblivious to the specifics of which disk class it is using; it simply issues block read and write requests to the generic block layer, which routes them to the appropriate device driver, which handles the details of issuing the specific request.
==从图中可以看出，文件系统（当然还有上面的应用程序）完全不知道它正在使用哪种磁盘类别；它只是向通用块层发出块读取和写入请求，通用块层将它们路由到适当的设备驱动程序，由驱动程序处理发出具体请求的细节。==

Although simplified, the diagram shows how such detail can be hidden from most of the OS.
==虽然简化了，但该图展示了如何向大部分操作系统隐藏这些细节。==

Figure 36.4: The File System Stack
==图 36.4：文件系统栈==

The diagram also shows a raw interface to devices, which enables special applications (such as a file-system checker, described later [AD14], or a disk defragmentation tool) to directly read and write blocks without using the file abstraction.
==该图还显示了设备的原始接口，这使得特殊的应用程序（如稍后描述的文件系统检查器 [AD14] 或磁盘碎片整理工具）能够不使用文件抽象而直接读写块。==

Most systems provide this type of interface to support these low-level storage management applications.
==大多数系统提供这种类型的接口来支持这些低级存储管理应用程序。==

Note that the encapsulation seen above can have its downside as well.
==请注意，上面看到的封装也可能有其缺点。==

For example, if there is a device that has many special capabilities, but has to present a generic interface to the rest of the kernel, those special capabilities will go unused.
==例如，如果有一个设备具有许多特殊功能，但必须向内核的其余部分呈现通用接口，那么这些特殊功能将不会被使用。==

This situation arises, for example, in Linux with SCSI devices, which have very rich error reporting; because other block devices (e.g., ATA/IDE) have much simpler error handling, all that higher levels of software ever receive is a generic EIO (generic IO error) error code; any extra detail that SCSI may have provided is thus lost to the file system [G08].
==这种情况例如出现在 Linux 的 SCSI 设备中，它具有非常丰富的错误报告；因为其他块设备（如 ATA/IDE）具有简单得多的错误处理，高层软件收到的只是一个通用的 EIO（通用 IO 错误）错误代码；SCSI 可能提供的任何额外细节因此对文件系统来说都丢失了 [G08]。==

Interestingly, because device drivers are needed for any device you might plug into your system, over time they have come to represent a huge percentage of kernel code.
==有趣的是，因为任何你可能插入系统的设备都需要设备驱动程序，随着时间的推移，它们已占据了内核代码的巨大比例。==

Studies of the Linux kernel reveal that over 70% of OS code is found in device drivers [C01]; for Windows-based systems, it is likely quite high as well.
==对 Linux 内核的研究表明，超过 70% 的操作系统代码是在设备驱动程序中 [C01]；对于基于 Windows 的系统，这个比例可能也相当高。==

Thus, when people tell you that the OS has millions of lines of code, what they are really saying is that the OS has millions of lines of device-driver code.
==因此，当人们告诉你操作系统有数百万行代码时，他们实际上是在说操作系统有数百万行设备驱动程序代码。==

Of course, for any given installation, most of that code may not be active (i.e., only a few devices are connected to the system at a time).
==当然，对于任何给定的安装，大部分代码可能并未处于活动状态（即，一次只有少数设备连接到系统）。==

Perhaps more depressingly, as drivers are often written by "amateurs" (instead of full-time kernel developers), they tend to have many more bugs and thus are a primary contributor to kernel crashes [S03].
==也许更令人沮丧的是，由于驱动程序通常由“业余爱好者”（而不是全职内核开发人员）编写，它们往往有更多的漏洞，因此是内核崩溃的主要原因 [S03]。==

36.8 Case Study: A Simple IDE Disk Driver
36.8 案例研究：一个简单的 IDE 磁盘驱动程序

To dig a little deeper here, let's take a quick look at an actual device: an IDE disk drive [L94].
==为了在这里挖掘得更深一点，让我们快速看一个实际的设备：IDE 磁盘驱动器 [L94]。==

We summarize the protocol as described in this reference [W10]; we'll also peek at the xv6 source code for a simple example of a working IDE driver [CK+08].
==我们总结了此参考文献 [W10] 中描述的协议；我们还将看看 xv6 源代码，以获得一个工作的 IDE 驱动程序的简单示例 [CK+08]。==

Figure 36.5: The IDE Interface
==图 36.5：IDE 接口==

An IDE disk presents a simple interface to the system, consisting of four types of register: control, command block, status, and error.
IDE 磁盘向系统呈现一个简单的接口，由四种类型的寄存器组成：控制、命令块、状态和错误。

These registers are available by reading or writing to specific "I/O addresses" (such as 0x3F6 below) using (on x86) the in and out I/O instructions.
==通过使用（在 x86 上）in 和 out I/O 指令读取或写入特定的“I/O 地址”（如下面的 0x3F6），可以使用这些寄存器。==

The basic protocol to interact with the device is as follows, assuming it has already been initialized.
==假设设备已经初始化，与其交互的基本协议如下。==

Wait for drive to be ready.
==等待驱动器准备就绪。==

Read Status Register (0x1F7) until drive is READY and not BUSY.
==读取状态寄存器（0x1F7）直到驱动器处于 READY 且非 BUSY 状态。==

Write parameters to command registers.
==将参数写入命令寄存器。==

Write the sector count, logical block address (LBA) of the sectors to be accessed, and drive number (master=0x00 or slave = 0x10, as IDE permits just two drives) to command registers (0x1F2-0x1F6).
==将扇区计数、要访问的扇区的逻辑块地址（LBA）以及驱动器编号（master=0x00 或 slave=0x10，因为 IDE 仅允许两个驱动器）写入命令寄存器（0x1F2-0x1F6）。==

Start the I/O.
==启动 I/O。==

Write READ | WRITE command to command register (0x1F7).
==将 READ | WRITE 命令写入命令寄存器（0x1F7）。==

Data transfer (for writes): Wait until drive status is READY and DRQ (drive request for data); write data to data port.
==数据传输（用于写入）：等待直到驱动器状态为 READY 和 DRQ（驱动器请求数据）；将数据写入数据端口。==

Handle interrupts.
==处理中断。==

In the simplest case, handle an interrupt for each sector transferred; more complex approaches allow batching and thus one final interrupt when the entire transfer is complete.
==在最简单的情况下，处理每个传输扇区的中断；更复杂的方法允许批处理，从而在整个传输完成时产生一个最终中断。==

Error handling.
==错误处理。==

After each operation, read the status register. If the ERROR bit is on, read the error register for details.
==每次操作后，读取状态寄存器。如果 ERROR 位开启，读取错误寄存器以获取详细信息。==

Most of this protocol is found in the xv6 IDE driver (Figure 36.6), which (after initialization) works through four primary functions.
==这个协议的大部分可以在 xv6 IDE 驱动程序（图 36.6）中找到，它（初始化后）通过四个主要函数工作。==

The first is ide_rw(), which queues a request (if there are others pending), or issues it directly to the disk (via ide start_request()); in either case, the routine waits for the request to complete and the calling process is put to sleep.
==第一个是 ide_rw()，它将请求排队（如果有其他请求待处理），或者直接将其发布到磁盘（通过 ide start_request()）；在任何一种情况下，该例程都会等待请求完成，并将调用进程置于睡眠状态。==

The second is ide start_request(), which is used to send a request (and perhaps data, in the case of a write) to the disk; the in and out x86 instructions are called to read and write device registers, respectively.
==第二个是 ide start_request()，用于向磁盘发送请求（如果是写入，可能还有数据）；调用 in 和 out x86 指令分别读写设备寄存器。==

The start request routine uses the third function, ide_wait_ready (), to ensure the drive is ready before issuing a request to it.
==启动请求例程使用第三个函数 ide_wait_ready()，以确保在向驱动器发出请求之前驱动器已准备就绪。==

Finally, ide_intr() is invoked when an interrupt takes place; it reads data from the device (if the request is a read, not a write), wakes the process waiting for the  to complete, and (if there are more requests in the I/O queue), launches the next I/O via i de_start_request().
==最后，当发生中断时调用 ide_intr()；它从设备读取数据（如果请求是读取，而非写入），唤醒等待  完成的进程，并且（如果 I/O 队列中有更多请求），通过 ide_start_request() 启动下一个 I/O。==

36.9 Historical Notes
36.9 历史注记

Before ending, we include a brief historical note on the origin of some of these fundamental ideas.
==在结束之前，我们就其中一些基本思想的起源包含一个简短的历史注记。==

If you are interested in learning more, read Smotherman's excellent summary [S08].
==如果你有兴趣了解更多，请阅读 Smotherman 的精彩总结 [S08]。==

Interrupts are an ancient idea, existing on the earliest of machines.
==中断是一个古老的概念，存在于最早的机器上。==

For example, the UNIVAC in the early 1950's had some form of interrupt vectoring, although it is unclear in exactly which year this feature was available [S08].
==例如，20 世纪 50 年代初的 UNIVAC 具有某种形式的中断向量，尽管尚不清楚该功能确切是在哪一年可用的 [S08]。==

Sadly, even in its infancy, we are beginning to lose the origins of computing history.
==可悲的是，即使在它尚处婴儿期，我们已经开始遗失计算历史的起源。==

There is also some debate as to which machine first introduced the idea of DMA.
==关于哪台机器首先引入了 DMA 的概念也存在一些争论。==

For example, Knuth and others point to the DYSEAC (a "mobile" machine, which at the time meant it could be hauled in a trailer), whereas others think the IBM SAGE may have been the first [S08].
==例如，Knuth 等人指向 DYSEAC（一台“移动”机器，当时意味着它可以用拖车拖运），而其他人则认为 IBM SAGE 可能是第一个 [S08]。==

Either way, by the mid 50's, systems with I/O devices that communicated directly with memory and interrupted the CPU when finished existed.
==无论哪种方式，到了 50 年代中期，带有直接与内存通信并在完成时中断 CPU 的 I/O 设备的系统已经存在。==

The history here is difficult to trace because the inventions are tied to real, and sometimes obscure, machines.
==这里的历史很难追溯，因为这些发明与真实的、有时是晦涩难懂的机器联系在一起。==

For example, some think that the Lincoln Labs TX-2 machine was first with vectored interrupts [S08], but this is hardly clear.
==例如，有人认为林肯实验室的 TX-2 机器最早采用向量中断 [S08]，但这很难确定。==

Figure 36.6: The xv6 IDE Disk Driver (Simplified)
==图 36.6：xv6 IDE 磁盘驱动程序（简化版）==

Because the ideas are relatively obvious no Einsteinian leap is required to come up with the idea of letting the CPU do something else while a slow  is pending - perhaps our focus on "who first?" is misguided.
==因为这些想法相对明显——不需要爱因斯坦式的飞跃就能想出在慢速  挂起时让 CPU 做其他事情的主意——也许我们要关注“谁是第一？”是被误导了。==

What is certainly clear: as people built these early machines, it became obvious that I/O support was needed.
==可以肯定的是：当人们构建这些早期机器时，显然需要 I/O 支持。==

Interrupts, DMA, and related ideas are all direct outcomes of the nature of fast CPUs and slow devices; if you were there at the time, you might have had similar ideas.
==中断、DMA 和相关想法都是快速 CPU 和慢速设备这一本质的直接产物；如果你当时在场，你可能也会有类似的想法。==

36.10 Summary
36.10 总结

You should now have a very basic understanding of how an OS interacts with a device.
==你现在应该对操作系统如何与设备交互有了一个非常基本的了解。==

Two techniques, the interrupt and DMA, have been introduced to help with device efficiency, and two approaches to accessing device registers, explicit I/O instructions and memory-mapped  have been described.
==介绍了两种技术，中断和 DMA，以帮助提高设备效率，并描述了两种访问设备寄存器的方法，显式 I/O 指令和内存映射 。==

Finally, the notion of a device driver has been presented, showing how the OS itself can encapsulate low-level details and thus make it easier to build the rest of the OS in a device-neutral fashion.
==最后，提出了设备驱动程序的概念，展示了操作系统本身如何封装低级细节，从而使以设备中立的方式构建操作系统的其余部分变得更加容易。==

References
==参考文献==

[A+11] "vIC: Interrupt Coalescing for Virtual Machine Storage Device IO" by Irfan Ahmad, Ajay Gulati, Ali Mashtizadeh. USENIX '11.
[A+11] "vIC: Interrupt Coalescing for Virtual Machine Storage Device IO" 作者 Irfan Ahmad, Ajay Gulati, Ali Mashtizadeh. USENIX '11.

A terrific survey of interrupt coalescing in traditional and virtualized environments.
==对传统和虚拟化环境中中断合并的一项极好的调查。==

[AD14] "Operating Systems: Three Easy Pieces" (Chapters: Crash Consistency: FSCK and Journaling and Log-Structured File Systems) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014.
[AD14] "Operating Systems: Three Easy Pieces"（章节：Crash Consistency: FSCK and Journaling and Log-Structured File Systems）作者 Remzi Arpaci-Dusseau 和 Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014.

A description of a file-system checker and how it works, which requires low-level access to disk devices not normally provided by the file system directly.
==描述了文件系统检查器及其工作原理，这需要文件系统通常不直接提供的对磁盘设备的低级访问。==

[C01] "An Empirical Study of Operating System Errors" by Andy Chou, Junfeng Yang, Benjamin Chelf, Seth Hallem, Dawson Engler. SOSP '01.
[C01] "An Empirical Study of Operating System Errors" 作者 Andy Chou, Junfeng Yang, Benjamin Chelf, Seth Hallem, Dawson Engler. SOSP '01.

One of the first papers to systematically explore how many bugs are in modern operating systems.
==第一批系统地探索现代操作系统中有多少错误的论文之一。==

Among other neat findings, the authors show that device drivers have something like seven times more bugs than mainline kernel code.
==在其他巧妙的发现中，作者表明设备驱动程序的错误大约是主线内核代码的七倍。==

[CK+08] "The xv6 Operating System" by Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich. From: [http://pdos.csail.mit.edu/6.828/2008/index.html](http://pdos.csail.mit.edu/6.828/2008/index.html).
[CK+08] "The xv6 Operating System" 作者 Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich. 来源: [http://pdos.csail.mit.edu/6.828/2008/index.html](http://pdos.csail.mit.edu/6.828/2008/index.html).

See ide.c for the IDE device driver, with a few more details therein.
==有关 IDE 设备驱动程序，请参阅 ide.c，其中包含更多细节。==

[D07] "What Every Programmer Should Know About Memory" by Ulrich Drepper. November, 2007. Available: [http://www.akkadia.org/drepper/cpumemory.pdf](http://www.akkadia.org/drepper/cpumemory.pdf).
[D07] "What Every Programmer Should Know About Memory" 作者 Ulrich Drepper. 2007 年 11 月. 可用地址: [http://www.akkadia.org/drepper/cpumemory.pdf](http://www.akkadia.org/drepper/cpumemory.pdf).

A fantastic read about modern memory systems, starting at DRAM and going all the way up to virtualization and cache-optimized algorithms.
==关于现代内存系统的一本精彩读物，从 DRAM 开始，一直讲到虚拟化和缓存优化算法。==

[G08] "EIO: Error-handling is Occasionally Correct" by Haryadi Gunawi, Cindy Rubio-Gonzalez, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, Ben Liblit. FAST '08, San Jose, CA, February 2008.
[G08] "EIO: Error-handling is Occasionally Correct" 作者 Haryadi Gunawi, Cindy Rubio-Gonzalez, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, Ben Liblit. FAST '08, San Jose, CA, 2008 年 2 月.

Our own work on building a tool to find code in Linux file systems that does not handle error return properly.
==我们自己的工作，关于构建一个工具来查找 Linux 文件系统中未正确处理错误返回的代码。==

We found hundreds and hundreds of bugs, many of which have now been fixed.
==我们发现了成百上千个错误，其中许多现在已经修复。==

[H17] "Intel Core i7-7700K review: Kaby Lake Debuts for Desktop" by Joel Hruska. January 3, 2017.
[H17] "Intel Core i7-7700K review: Kaby Lake Debuts for Desktop" 作者 Joel Hruska. 2017 年 1 月 3 日.

An in-depth review of a recent Intel chipset, including CPUs and the 1/0 subsystem.
==对最近的英特尔芯片组的深入回顾，包括 CPU 和 I/O 子系统。==

[H18] "Hacker News" by Many contributors. Available: [https://news.ycombinator.com](https://news.ycombinator.com).
[H18] "Hacker News" 作者 Many contributors. 可用地址: [https://news.ycombinator.com](https://news.ycombinator.com).

One of the better aggregators for tech-related stuff.
==科技相关内容的较好聚合器之一。==

Once back in 2014, this book became a highly-ranked entry, leading to 1 million chapter downloads in just one day!
==早在 2014 年，这本书就成为了排名很高的条目，仅仅一天就有 100 万次章节下载！==

Sadly, we have yet to re-experience such a high.
==遗憾的是，我们还没有再次体验过如此的高峰。==

[L94] "AT Attachment Interface for Disk Drives" by Lawrence J. Lamers. Reference number: ANSI X3.221, 1994.
[L94] "AT Attachment Interface for Disk Drives" 作者 Lawrence J. Lamers. 参考编号: ANSI X3.221, 1994.

A rather dry document about device interfaces. Read it at your own peril.
==一份关于设备接口的相当枯燥的文档。阅读风险自负。==

[MR96] "Eliminating Receive Livelock in an Interrupt-driven Kernel" by Jeffrey Mogul, K. K. Ramakrishnan. USENIX '96, San Diego, CA, January 1996.
[MR96] "Eliminating Receive Livelock in an Interrupt-driven Kernel" 作者 Jeffrey Mogul, K. K. Ramakrishnan. USENIX '96, San Diego, CA, 1996 年 1 月.

Mogul and colleagues did a great deal of pioneering work on web server network performance.
Mogul 和同事在 Web 服务器网络性能方面做了大量开创性工作。

This paper is but one example.
==这篇论文只是其中一个例子。==

[S08] "Interrupts" by Mark Smotherman. July '08. Available: [http://people.cs.clemson.edu/~mark/interrupts.html](http://people.cs.clemson.edu/~mark/interrupts.html).
[S08] "Interrupts" 作者 Mark Smotherman. 08 年 7 月. 可用地址: [http://people.cs.clemson.edu/~mark/interrupts.html](http://people.cs.clemson.edu/~mark/interrupts.html).

A treasure trove of information on the history of interrupts, DMA, and related early ideas in computing.
==关于中断历史、DMA 和相关早期计算思想的信息宝库。==

[S03] "Improving the Reliability of Commodity Operating Systems" by Michael M. Swift, Brian N. Bershad, Henry M. Levy. SOSP '03.
[S03] "Improving the Reliability of Commodity Operating Systems" 作者 Michael M. Swift, Brian N. Bershad, Henry M. Levy. SOSP '03.

Swift's work revived interest in a more microkernel-like approach to operating systems; minimally, it finally gave some good reasons why address-space based protection could be useful in a modern OS.
Swift 的工作重新引起了人们对更像微内核的操作系统方法的兴趣；至少，它最终给出了一些很好的理由，说明为什么基于地址空间的保护在现代操作系统中是有用的。

[W10] "Hard Disk Driver" by Washington State Course Homepage. Available online at this site: [http://eecs.wsu.edu/cs460/cs560/HDdriver.html](http://eecs.wsu.edu/cs460/cs560/HDdriver.html).
[W10] "Hard Disk Driver" 作者 Washington State Course Homepage. 可在此站点在线获取: [http://eecs.wsu.edu/cs460/cs560/HDdriver.html](http://eecs.wsu.edu/cs460/cs560/HDdriver.html).

A nice summary of a simple IDE disk drive's interface and how to build a device driver for it.
==对简单 IDE 磁盘驱动器接口以及如何为其构建设备驱动程序的很好的总结。==

Hard Disk Drives
==硬盘驱动器==

37
37

The last chapter introduced the general concept of an I/O device and showed you how the OS might interact with such a beast.
==上一章介绍了 I/O 设备的一般概念，并向你展示了操作系统如何与此类庞然大物进行交互。==

In this chapter, we dive into more detail about one device in particular: the hard disk drive.
==在本章中，我们将深入探讨一种特定的设备：硬盘驱动器。==

These drives have been the main form of persistent data storage in computer systems for decades and much of the development of file system technology (coming soon) is predicated on their behavior.
==几十年来，这些驱动器一直是计算机系统中持久数据存储的主要形式，并且文件系统技术（即将推出）的大部分开发都基于它们的行为。==

Thus, it is worth understanding the details of a disk's operation before building the file system software that manages it.
==因此，在构建管理磁盘的文件系统软件之前，值得了解磁盘操作的细节。==

Many of these details are available in excellent papers by Ruemmler and Wilkes [RW92] and Anderson, Dykes, and Riedel [ADR03].
==许多细节都可以在 Ruemmler 和 Wilkes [RW92] 以及 Anderson、Dykes 和 Riedel [ADR03] 的优秀论文中找到。==

CRUX: HOW TO STORE AND ACCESS DATA ON DISK
==关键问题：如何在磁盘上存储和访问数据==

How do modern hard-disk drives store data?
==现代硬盘驱动器如何存储数据？==

What is the interface?
==接口是什么？==

How is the data actually laid out and accessed?
==数据实际上是如何布局和访问的？==

How does disk scheduling improve performance?
==磁盘调度如何提高性能？==

37.1 The Interface
37.1 接口

Let's start by understanding the interface to a modern disk drive.
==让我们从了解现代磁盘驱动器的接口开始。==

The basic interface for all modern drives is straightforward.
==所有现代驱动器的基本接口都很简单。==

The drive consists of a large number of sectors (512-byte blocks), each of which can be read or written.
==驱动器由大量扇区（512 字节块）组成，每个扇区都可以被读取或写入。==

The sectors are numbered from 0 to  on a disk with n sectors.
==在具有 n 个扇区的磁盘上，扇区编号从 0 到 。==

Thus, we can view the disk as an array of sectors; 0 to  is thus the address space of the drive.
==因此，我们可以将磁盘视为扇区数组；0 到  因此是驱动器的地址空间。==

Multi-sector operations are possible; indeed, many file systems will read or write 4KB at a time (or more).
==多扇区操作是可能的；实际上，许多文件系统一次会读取或写入 4KB（或更多）。==

However, when updating the disk, the only guarantee drive manufacturers make is that a single 512-byte write is atomic (i.e., it will either complete in its entirety or it won't complete at all); thus, if an untimely power loss occurs, only a portion of a larger write may complete (sometimes called a torn write).
==但是，在更新磁盘时，驱动器制造商做出的唯一保证是单个 512 字节的写入是原子的（即，它要么完全完成，要么根本不完成）；因此，如果发生不合时宜的断电，较大写入操作可能只有一部分完成（有时称为撕裂写入 torn write）。==

There are some assumptions most clients of disk drives make, but that are not specified directly in the interface; Schlosser and Ganger have called this the "unwritten contract" of disk drives [SG04].
==磁盘驱动器的大多数客户端会做出一些假设，但这些假设并未在接口中直接指定；Schlosser 和 Ganger 将其称为磁盘驱动器的“不成文契约”[SG04]。==

Specifically, one can usually assume that accessing two blocks near one-another within the drive's address space will be faster than accessing two blocks that are far apart.
==具体来说，人们通常可以假设，在驱动器的地址空间内访问彼此靠近的两个块，要比访问相距较远的两个块更快。==

One can also usually assume that accessing blocks in a contiguous chunk (i.e., a sequential read or write) is the fastest access mode, and usually much faster than any more random access pattern.
==人们通常还可以假设，以连续块（即顺序读或写）访问块是最快的访问模式，并且通常比任何随机访问模式快得多。==

37.2 Basic Geometry
37.2 基本几何结构

Let's start to understand some of the components of a modern disk.
==让我们开始了解现代磁盘的一些组件。==

We start with a platter, a circular hard surface on which data is stored persistently by inducing magnetic changes to it.
==我们从盘片（platter）开始，这是一个圆形的坚硬表面，通过在其上感应磁性变化来持久地存储数据。==

A disk may have one or more platters; each platter has 2 sides, each of which is called a surface.
==一个磁盘可能有一个或多个盘片；每个盘片有 2 个面，每个面都称为一个表面（surface）。==

These platters are usually made of some hard material (such as aluminum), and then coated with a thin magnetic layer that enables the drive to persistently store bits even when the drive is powered off.
==这些盘片通常由某种坚硬材料（如铝）制成，然后涂上一层薄薄的磁性层，使驱动器即使在断电时也能持久地存储比特。==

The platters are all bound together around the spindle, which is connected to a motor that spins the platters around (while the drive is powered on) at a constant (fixed) rate.
==盘片都围绕主轴（spindle）绑定在一起，主轴连接到一个电机，该电机（在驱动器通电时）以恒定（固定）的速率旋转盘片。==

The rate of rotation is often measured in rotations per minute (RPM), and typical modern values are in the 7,200 RPM to 15,000 RPM range.
==旋转速率通常以每分钟转数（RPM）来衡量，典型的现代值在 7,200 RPM 到 15,000 RPM 范围内。==

Note that we will often be interested in the time of a single rotation, e.g., a drive that rotates at 10,000 RPM means that a single rotation takes about 6 milliseconds (6 ms).
==请注意，我们通常会对单次旋转的时间感兴趣，例如，一个以 10,000 RPM 旋转的驱动器意味着单次旋转大约需要 6 毫秒（6 ms）。==

Data is encoded on each surface in concentric circles of sectors; we call one such concentric circle a track.
==数据以扇区的同心圆形式编码在每个表面上；我们将这样一个同心圆称为磁道（track）。==

A single surface contains many thousands and thousands of tracks, tightly packed together, with hundreds of tracks fitting into the width of a human hair.
==单个表面包含成千上万个紧密排列在一起的磁道，几百个磁道就可以塞进一根头发的宽度。==

To read and write from the surface, we need a mechanism that allows us to either sense (i.e., read) the magnetic patterns on the disk or to induce a change in (i.e., write) them.
==为了从表面读取和写入，我们需要一种机制，允许我们感应（即读取）磁盘上的磁性图案或引起它们的改变（即写入）。==

This process of reading and writing is accomplished by the disk head; there is one such head per surface of the drive.
==这种读写过程由磁盘磁头（disk head）完成；驱动器的每个表面都有一个这样的磁头。==

The disk head is attached to a single disk arm, which moves across the surface to position the head over the desired track.
==磁盘磁头连接到一个磁盘臂（disk arm）上，磁盘臂在表面上移动，将磁头定位在所需的磁道上。==

Figure 37.1: A Disk With Just A Single Track
==图 37.1：只有一个磁道的磁盘==

Figure 37.2: A Single Track Plus A Head
==图 37.2：单磁道加磁头==

37.3 A Simple Disk Drive
37.3 一个简单的磁盘驱动器

Let's understand how disks work by building up a model one track at a time.
==让我们通过一次建立一个磁道的模型来了解磁盘是如何工作的。==

Assume we have a simple disk with a single track (Figure 37.1).
==假设我们有一个带有单个磁道的简单磁盘（图 37.1）。==

This track has just 12 sectors, each of which is 512 bytes in size (our typical sector size, recall) and addressed therefore by the numbers 0 through 11.
==该磁道只有 12 个扇区，每个扇区大小为 512 字节（回想一下，这是我们典型的扇区大小），因此通过数字 0 到 11 进行寻址。==

The single platter we have here rotates around the spindle, to which a motor is attached.
==我们这里的单个盘片围绕着主轴旋转，主轴上连接着一个电机。==

Of course, the track by itself isn't too interesting; we want to be able to read or write those sectors, and thus we need a disk head, attached to a disk arm, as we now see (Figure 37.2).
==当然，磁道本身并不太有趣；我们希望能够读写这些扇区，因此我们需要一个连接到磁盘臂上的磁盘磁头，正如我们现在所看到的（图 37.2）。==

In the figure, the disk head, attached to the end of the arm, is positioned over sector 6, and the surface is rotating counter-clockwise.
==在图中，连接到手臂末端的磁盘磁头位于扇区 6 上方，表面逆时针旋转。==

Single-track Latency: The Rotational Delay
==单磁道延迟：旋转延迟==

To understand how a request would be processed on our simple, one-track disk, imagine we now receive a request to read block 0.
==为了理解在我们简单的单磁道磁盘上如何处理请求，想象一下我们现在收到了读取块 0 的请求。==

How should the disk service this request?
==磁盘应该如何处理这个请求？==

In our simple disk, the disk doesn't have to do much.
==在我们简单的磁盘中，磁盘不需要做太多事情。==

In particular, it must just wait for the desired sector to rotate under the disk head.
==具体来说，它必须只是等待所需的扇区旋转到磁盘磁头下方。==

This wait happens often enough in modern drives, and is an important enough component of I/O service time, that it has a special name: rotational delay (sometimes rotation delay, though that sounds weird).
==这种等待在现代驱动器中经常发生，并且是 I/O 服务时间中足够重要的组成部分，因此它有一个特殊的名称：旋转延迟（rotational delay）（有时也叫 rotation delay，虽然听起来很奇怪）。==

In the example, if the full rotational delay is R, the disk has to incur a rotational delay of about  to wait for 0 to come under the read/write head (if we start at 6).
==在示例中，如果完整的旋转延迟为 R，磁盘必须承担大约  的旋转延迟以等待 0 来到读/写头下方（如果我们从 6 开始）。==

A worst-case request on this single track would be to sector 5, causing nearly a full rotational delay in order to service such a request.
==这个单磁道上最坏情况的请求是针对扇区 5，为了处理该请求会导致几乎整整一圈的旋转延迟。==

Multiple Tracks: Seek Time
==多磁道：寻道时间==

So far our disk just has a single track, which is not too realistic; modern disks of course have many millions.
==到目前为止，我们的磁盘只有一个磁道，这不太现实；现代磁盘当然有数百万个磁道。==

Let's thus look at an ever-so-slightly more realistic disk surface, this one with three tracks (Figure 37.3, left).
==因此，让我们看一个稍微现实一点的磁盘表面，这个有三个磁道（图 37.3，左）。==

In the figure, the head is currently positioned over the innermost track (which contains sectors 24 through 35); the next track over contains the next set of sectors (12 through 23), and the outermost track contains the first sectors (0 through 11).
==在图中，磁头当前位于最内层磁道（包含扇区 24 到 35）上方；旁边的一个磁道包含下一组扇区（12 到 23），最外层磁道包含第一组扇区（0 到 11）。==

To understand how the drive might access a given sector, we now trace what would happen on a request to a distant sector, e.g., a read to sector 11.
==为了理解驱动器如何访问给定的扇区，我们现在追踪对一个遥远扇区的请求会发生什么，例如，读取扇区 11。==

To service this read, the drive has to first move the disk arm to the correct track (in this case, the outermost one), in a process known as a seek.
==为了处理这个读取，驱动器必须首先将磁盘臂移动到正确的磁道（在本例中为最外层），这个过程称为寻道（seek）。==

Seeks, along with rotations, are one of the most costly disk operations.
==寻道和旋转一样，是最昂贵的磁盘操作之一。==

The seek, it should be noted, has many phases: first an acceleration phase as the disk arm gets moving; then coasting as the arm is moving at full speed, then deceleration as the arm slows down; finally settling as the head is carefully positioned over the correct track.
==需要注意的是，寻道有许多阶段：首先是加速阶段，磁盘臂开始移动；然后是滑行阶段，磁盘臂全速移动；接着是减速阶段，磁盘臂减慢速度；最后是稳定阶段（settling），磁头被仔细定位在正确的磁道上。==

The settling time is often quite significant, e.g., 0.5 to 2 ms, as the drive must be certain to find the right track (imagine if it just got close instead!).
==稳定时间通常相当长，例如 0.5 到 2 ms，因为驱动器必须确保找到正确的磁道（想象一下如果它只是靠近而已！）。==

After the seek, the disk arm has positioned the head over the right track.
==寻道后，磁盘臂已将磁头定位在正确的磁道上方。==

A depiction of the seek is found in Figure 37.3 (right).
==寻道的描绘见图 37.3（右）。==

Figure 37.3: Three Tracks Plus A Head (Right: With Seek)
==图 37.3：三个磁道加一个磁头（右：带寻道）==

As we can see, during the seek, the arm has been moved to the desired track, and the platter of course has rotated, in this case about 3 sectors.
==我们可以看到，在寻道期间，手臂已移至所需的磁道，当然盘片也旋转了，在本例中旋转了大约 3 个扇区。==

Thus, sector 9 is just about to pass under the disk head, and we must only endure a short rotational delay to complete the transfer.
==因此，扇区 9 正好要在磁盘磁头下方通过，我们只需要忍受很短的旋转延迟即可完成传输。==

When sector 11 passes under the disk head, the final phase of  will take place, known as the transfer, where data is either read from or written to the surface.
==当扇区 11 在磁盘磁头下方通过时，将发生  的最后阶段，称为传输（transfer），即数据从表面读取或写入表面。==

And thus, we have a complete picture of  time: first a seek, then waiting for the rotational delay, and finally the transfer.
==因此，我们有了  时间的完整画面：首先是寻道，然后等待旋转延迟，最后是传输。==

Some Other Details
==其他一些细节==

Though we won't spend too much time on it, there are some other interesting details about how hard drives operate.
==虽然我们不会在上面花费太多时间，但关于硬盘驱动器如何运行还有其他一些有趣的细节。==

Many drives employ some kind of track skew to make sure that sequential reads can be properly serviced even when crossing track boundaries.
==许多驱动器采用某种磁道倾斜（track skew），以确保即使在跨越磁道边界时也能正确处理顺序读取。==

In our simple example disk, this might appear as seen in Figure 37.4 (page 5).
==在我们简单的示例磁盘中，这可能如图 37.4（第 5 页）所示。==

Figure 37.4: Three Tracks: Track Skew Of 2
==图 37.4：三个磁道：磁道倾斜为 2==

Sectors are often skewed like this because when switching from one track to another, the disk needs time to reposition the head (even to neighboring tracks).
==扇区通常会像这样倾斜，因为当从一个磁道切换到另一个磁道时，磁盘需要时间来重新定位磁头（即使是到相邻的磁道）。==

Without such skew, the head would be moved to the next track but the desired next block would have already rotated under the head, and thus the drive would have to wait almost the entire rotational delay to access the next block.
==如果没有这种倾斜，磁头会被移动到下一个磁道，但所需的下一个块已经旋转到磁头下方，因此驱动器将不得不等待几乎整个旋转延迟来访问下一个块。==

Another reality is that outer tracks tend to have more sectors than inner tracks, which is a result of geometry; there is simply more room out there.
==另一个现实是，外层磁道往往比内层磁道拥有更多的扇区，这是几何结构的结果；外面的空间简直更大。==

These tracks are often referred to as multi-zoned disk drives, where the disk is organized into multiple zones, and where a zone is consecutive set of tracks on a surface.
==这些磁道通常被称为多区域磁盘驱动器（multi-zoned disk drives），其中磁盘被组织成多个区域（zone），一个区域是表面上一组连续的磁道。==

Each zone has the same number of sectors per track, and outer zones have more sectors than inner zones.
==每个区域的每个磁道具有相同数量的扇区，并且外层区域比内层区域具有更多的扇区。==

Finally, an important part of any modern disk drive is its cache, for historical reasons sometimes called a track buffer.
==最后，任何现代磁盘驱动器的一个重要部分是它的缓存（cache），由于历史原因有时称为磁道缓冲区（track buffer）。==

This cache is just some small amount of memory (usually around 8 or 16 MB) which the drive can use to hold data read from or written to the disk.
==这个缓存只是一小部分内存（通常约为 8 或 16 MB），驱动器可以使用它来保存从磁盘读取或写入磁盘的数据。==

For example, when reading a sector from the disk, the drive might decide to read in all of the sectors on that track and cache them in its memory; doing so allows the drive to quickly respond to any subsequent requests to the same track.
==例如，当从磁盘读取一个扇区时，驱动器可能会决定读入该磁道上的所有扇区并将它们缓存在其内存中；这样做允许驱动器快速响应对同一磁道的任何后续请求。==

On writes, the drive has a choice: should it acknowledge the write has completed when it has put the data in its memory, or after the write has actually been written to disk?
==在写入时，驱动器有一个选择：它是应该在将数据放入其内存时确认写入已完成，还是在数据实际写入磁盘后确认？==

The former is called write back caching (or sometimes immediate reporting), and the latter write through.
==前者称为回写缓存（write back caching）（或有时称为立即报告），后者称为直写（write through）。==

Write back caching sometimes makes the drive appear "faster", but can be dangerous; if the file system or applications require that data be written to disk in a certain order for correctness, write-back caching can lead to problems (read the chapter on file-system journaling for details).
==回写缓存有时会使驱动器看起来“更快”，但也可能是危险的；如果文件系统或应用程序要求数据按特定顺序写入磁盘以保证正确性，回写缓存可能会导致问题（阅读有关文件系统日志记录的章节了解详情）。==

ASIDE: DIMENSIONAL ANALYSIS
==旁注：量纲分析==

Remember in Chemistry class, how you solved virtually every problem by simply setting up the units such that they canceled out, and somehow the answers popped out as a result?
==还记得在化学课上，你是如何通过简单地设置单位使它们相互抵消，从而解决了几乎所有问题的吗？结果答案就这么蹦出来了？==

That chemical magic is known by the highfalutin name of dimensional analysis and it turns out it is useful in computer systems analysis too.
==那种化学魔法有一个高大上的名字叫量纲分析，事实证明它在计算机系统分析中也很有用。==

Let's do an example to see how dimensional analysis works and why it is useful.
==让我们做一个例子来看看量纲分析是如何工作的，以及为什么它很有用。==

In this case, assume you have to figure out how long, in milliseconds, a single rotation of a disk takes.
==在这种情况下，假设你必须弄清楚磁盘旋转一圈需要多长时间（以毫秒为单位）。==

Unfortunately, you are given only the RPM of the disk, or rotations per minute.
==不幸的是，你只知道磁盘的 RPM，即每分钟转数。==

Let's assume we're talking about a 10K RPM disk (i.e., it rotates 10,000 times per minute).
==让我们假设我们讨论的是一个 10K RPM 的磁盘（即，它每分钟旋转 10,000 次）。==

How do we set up the dimensional analysis so that we get time per rotation in milliseconds?
==我们要如何设置量纲分析，以便我们得到以毫秒为单位的每次旋转时间？==

To do so, we start by putting the desired units on the left; in this case, we wish to obtain the time (in milliseconds) per rotation, so that is exactly what we write down:  .
==为此，我们首先将所需的单位放在左边；在这种情况下，我们希望获得每次旋转的时间（以毫秒为单位），所以这正是我们写下的： 。==

We then write down everything we know, making sure to cancel units where possible.
==然后我们写下我们要知道的一切，确保在可能的情况下抵消单位。==

First, we obtain  (keeping rotation on the bottom, as that's where it is on the left), then transform minutes into seconds with  , and then finally transform seconds in milliseconds with 
==首先，我们得到 （将旋转保持在底部，因为这就是它在左侧的位置），然后用  将分钟转换为秒，最后用  将秒转换为毫秒。==

The final result is the following (with units nicely canceled):
==最终结果如下（单位已完美抵消）：==




As you can see from this example, dimensional analysis makes what seems intuitive into a simple and repeatable process.
==正如你从这个例子中看到的，量纲分析使看起来直观的东西变成了一个简单且可重复的过程。==

Beyond the RPM calculation above, it comes in handy with I/O analysis regularly.
==除了上面的 RPM 计算之外，它在 I/O 分析中也经常派上用场。==

For example, you will often be given the transfer rate of a disk, e.g., 100 MB/second, and then asked: how long does it take to transfer a 512 KB block (in milliseconds)?
==例如，经常会给你磁盘的传输速率，比如 100 MB/秒，然后问：传输一个 512 KB 的块需要多长时间（以毫秒为单位）？==

With dimensional analysis, it's easy:
==有了量纲分析，这很容易：==




37.4 I/O Time: Doing The Math
37.4 I/O 时间：做数学计算

Now that we have an abstract model of the disk, we can use a little analysis to better understand disk performance.
==既然我们有了磁盘的抽象模型，我们可以使用一些分析来更好地理解磁盘性能。==

In particular, we can now represent  time as the sum of three major components:
==具体来说，我们现在可以将  时间表示为三个主要部分的和：==




Figure 37.5: Disk Drive Specs: SCSI Versus SATA
==图 37.5：磁盘驱动器规格：SCSI 对比 SATA==

Note that the rate of  which is often more easily used for comparison between drives (as we will do below), is easily computed from the time.
==请注意， 速率  通常更容易用于驱动器之间的比较（如下所示），它可以很容易地从时间计算出来。==

Simply divide the size of the transfer by the time it took:
==只需将传输大小除以所用时间：==




To get a better feel for  time, let us perform the following calculation.
==为了更好地感受  时间，让我们执行以下计算。==

Assume there are two workloads we are interested in.
==假设我们对两种工作负载感兴趣。==

The first, known as the random workload, issues small (e.g., 4KB) reads to random locations on the disk.
==第一种称为随机工作负载，向磁盘上的随机位置发出小的（例如 4KB）读取。==

Random workloads are common in many important applications, including database management systems.
==随机工作负载在许多重要的应用程序中很常见，包括数据库管理系统。==

The second, known as the sequential workload, simply reads a large number of sectors consecutively from the disk, without jumping around.
==第二种称为顺序工作负载，简单地从磁盘连续读取大量扇区，而不跳来跳去。==

Sequential access patterns are quite common and thus important as well.
==顺序访问模式非常常见，因此也很重要。==

To understand the difference in performance between random and sequential workloads, we need to make a few assumptions about the disk drive first.
==为了理解随机工作负载和顺序工作负载之间的性能差异，我们需要先对磁盘驱动器做一些假设。==

Let's look at a couple of modern disks from Seagate.
==让我们看看希捷的几款现代磁盘。==

The first, known as the Cheetah 15K.5 [S09b], is a high-performance SCSI drive.
==第一款名为 Cheetah 15K.5 [S09b]，是一款高性能 SCSI 驱动器。==

The second, the Barracuda [S09a], is a drive built for capacity.
==第二款 Barracuda [S09a]，是一款为容量而构建的驱动器。==

Details on both are found in Figure 37.5.
==有关这两款产品的详细信息，请参见图 37.5。==

As you can see, the drives have quite different characteristics, and in many ways nicely summarize two important components of the disk drive market.
==正如你所看到的，这些驱动器具有截然不同的特性，并且在许多方面很好地总结了磁盘驱动器市场的两个重要组成部分。==

The first is the "high performance" drive market, where drives are engineered to spin as fast as possible, deliver low seek times, and transfer data quickly.
==第一个是“高性能”驱动器市场，在这个市场中，驱动器被设计为尽可能快地旋转，提供低寻道时间，并快速传输数据。==

The second is the "capacity" market, where cost per byte is the most important aspect; thus, the drives are slower but pack as many bits as possible into the space available.
==第二个是“容量”市场，在这个市场中，每字节成本是最重要的方面；因此，驱动器较慢，但在可用空间中塞入了尽可能多的比特。==

From these numbers, we can start to calculate how well the drives would do under our two workloads outlined above.
==根据这些数字，我们可以开始计算驱动器在上述两种工作负载下的表现。==

Let's start by looking at the random workload.
==让我们从随机工作负载开始。==

Assuming each 4 KB read occurs at a random location on disk, we can calculate how long each such read would take.
==假设每次 4 KB 读取都发生在磁盘上的随机位置，我们可以计算每次此类读取需要多长时间。==

On the Cheetah:
==在 Cheetah 上：==




The average seek time (4 milliseconds) is just taken as the average time reported by the manufacturer; note that a full seek (from one end of the surface to the other) would likely take two or three times longer.
==平均寻道时间（4 毫秒）取自制造商报告的平均时间；请注意，完整的寻道（从表面的一端到另一端）可能需要两倍或三倍的时间。==

TIP: USE DISKS SEQUENTIALLY
==提示：顺序使用磁盘==

When at all possible, transfer data to and from disks in a sequential manner.
==只要有可能，就以顺序方式向磁盘传输数据或从磁盘传输数据。==

If sequential is not possible, at least think about transferring data in large chunks: the bigger, the better.
==如果无法顺序传输，至少考虑以大块传输数据：越大越好。==

If  is done in little random pieces, I/O performance will suffer dramatically.
==如果以小的随机片段进行 ，I/O 性能将急剧下降。==

Also, users will suffer.
==此外，用户也会受苦。==

Also, you will suffer, knowing what suffering you have wrought with your careless random .
==此外，你自己也会受苦，因为你知道你用粗心的随机  造成了多大的痛苦。==

The average rotational delay is calculated from the RPM directly.
==平均旋转延迟直接根据 RPM 计算。==

15000 RPM is equal to 250 RPS (rotations per second); thus, each rotation takes 4 ms.
15000 RPM 等于 250 RPS（每秒转数）；因此，每转需要 4 ms。

On average, the disk will encounter a half rotation and thus 2 ms is the average time.
==平均而言，磁盘会遇到半圈旋转，因此平均时间为 2 ms。==

Finally, the transfer time is just the size of the transfer over the peak transfer rate; here it is vanishingly small (30 microseconds; note that we need 1000 microseconds just to get 1 millisecond!).
==最后，传输时间只是传输大小除以峰值传输速率；在这里它微乎其微（30 微秒；请注意，我们需要 1000 微秒才能得到 1 毫秒！）。==

Thus, from our equation above,  for the Cheetah roughly equals 6 ms.
==因此，根据上面的公式，Cheetah 的  大约等于 6 ms。==

To compute the rate of  we just divide the size of the transfer by the average time, and thus arrive at  for the Cheetah under the random workload of about 0.66 
==为了计算  速率，我们只需将传输大小除以平均时间，从而得出 Cheetah 在随机工作负载下的  约为 0.66 。==

The same calculation for the Barracuda yields a  of about 13.2 ms, more than twice as slow, and thus a rate of about 0.31 .
==对 Barracuda 进行相同的计算得出  约为 13.2 ms，慢了两倍多，因此速率约为 0.31 。==

Now let's look at the sequential workload.
==现在让我们看看顺序工作负载。==

Here we can assume there is a single seek and rotation before a very long transfer.
==在这里，我们可以假设在非常长的传输之前只有一次寻道和旋转。==

For simplicity, assume the size of the transfer is 100 MB.
==为简单起见，假设传输大小为 100 MB。==

Thus,  for the Cheetah and Barracuda is about 800 ms and 950 ms, respectively.
==因此，Cheetah 和 Barracuda 的  分别约为 800 ms 和 950 ms。==

The rates of  are thus very nearly the peak transfer rates of 125  and 105 MB/s, respectively.
==因此  速率非常接近 125  和 105 MB/s 的峰值传输速率。==

Figure 37.6: Disk Drive Performance: SCSI Versus SATA
==图 37.6：磁盘驱动器性能：SCSI 对比 SATA==

The figure shows us a number of important things.
==该图向我们展示了许多重要的事情。==

First, and most importantly, there is a huge gap in drive performance between random and sequential workloads, almost a factor of 200 or so for the Cheetah and more than a factor 300 difference for the Barracuda.
==首先，也是最重要的一点，在随机和顺序工作负载之间存在巨大的驱动器性能差距，Cheetah 几乎相差 200 倍左右，而 Barracuda 相差超过 300 倍。==

And thus we arrive at the most obvious design tip in the history of computing.
==因此，我们得出了计算史上最明显的设计技巧。==

A second, more subtle point: there is a large difference in performance between high-end "performance" drives and low-end "capacity" drives.
==第二点，更微妙的一点：高端“性能”驱动器和低端“容量”驱动器之间存在巨大的性能差异。==

For this reason (and others), people are often willing to pay top dollar for the former while trying to get the latter as cheaply as possible.
==由于这个原因（以及其他原因），人们通常愿意为前者支付高价，而试图尽可能便宜地获得后者。==

ASIDE: COMPUTING THE "AVERAGE" SEEK
==旁注：计算“平均”寻道==

In many books and papers, you will see average disk-seek time cited as being roughly one-third of the full seek time.
==在许多书籍和论文中，你会看到平均磁盘寻道时间被引用为大约是全寻道时间的三分之一。==

Where does this come from?
==这是从哪里来的？==

Turns out it arises from a simple calculation based on average seek distance, not time.
==事实证明，这源于基于平均寻道距离而非时间的简单计算。==

Imagine the disk as a set of tracks, from 0 to N.
==想象磁盘是一组磁道，从 0 到 N。==

The seek distance between any two tracks x and y is thus computed as the absolute value of the difference between them: .
==因此，任意两个磁道 x 和 y 之间的寻道距离计算为它们之间差值的绝对值：。==

To compute the average seek distance, all you need to do is to first add up all possible seek distances:
==要计算平均寻道距离，你所需要做的就是首先将所有可能的寻道距离加起来：==




Then, divide this by the number of different possible seeks: .
==然后，将其除以不同可能寻道的数量：。==

Το compute the sum, we'll just use the integral form:
==为了计算总和，我们只使用积分形式：==




To compute the inner integral, let's break out the absolute value:
==为了计算内部积分，让我们拆开绝对值：==




Solving this leads to  which can be simplified to .
==求解此式得到 ，可以简化为 。==

Now we have to compute the outer integral:
==现在我们必须计算外部积分：==




which results in:
==结果是：==




Remember that we still have to divide by the total number of seeks  to compute the average seek distance: 
==记住，我们要必须除以寻道总数  来计算平均寻道距离：==

Thus the average seek distance on a disk, over all possible seeks, is one-third the full distance.
==因此，磁盘上的平均寻道距离（在所有可能的寻道中）是全距离的三分之一。==

And now when you hear that an average seek is one-third of a full seek, you'll know where it came from.
==现在，当你听到平均寻道是全寻道的三分之一时，你就知道它从何而来了。==

Figure 37.7: SSTF: Scheduling Requests 21 And 2
==图 37.7：SSTF：调度请求 21 和 2==

37.5 Disk Scheduling
37.5 磁盘调度

Because of the high cost of  the OS has historically played a role in deciding the order of  issued to the disk.
==由于  的高成本，操作系统在历史上一直扮演着决定向磁盘发出  顺序的角色。==

More specifically, given a set of  requests, the disk scheduler examines the requests and decides which one to schedule next [SCO90, JW91].
==更具体地说，给定一组  请求，磁盘调度程序（disk scheduler）会检查这些请求并决定下一个调度哪一个 [SCO90, JW91]。==

Unlike job scheduling, where the length of each job is usually unknown, with disk scheduling, we can make a good guess at how long a "job" (i.e., disk request) will take.
==与作业长度通常未知的作业调度不同，在磁盘调度中，我们可以很好地猜测一个“作业”（即磁盘请求）将花费多长时间。==

By estimating the seek and possible rotational delay of a request, the disk scheduler can know how long each request will take, and thus (greedily) pick the one that will take the least time to service first.
==通过估计请求的寻道和可能的旋转延迟，磁盘调度程序可以知道每个请求将花费多长时间，从而（贪婪地）选择首先服务花费时间最少的那个。==

Thus, the disk scheduler will try to follow the principle of SJF (shortest job first) in its operation.
==因此，磁盘调度程序将尝试在其操作中遵循 SJF（最短作业优先）原则。==

SSTF: Shortest Seek Time First
SSTF：最短寻道时间优先

One early disk scheduling approach is known as shortest-seek-time-first (SSTF) (also called shortest-seek-first or SSF).
==一种早期的磁盘调度方法被称为最短寻道时间优先（SSTF）（也称为最短寻道优先或 SSF）。==

SSTF orders the queue of  requests by track, picking requests on the nearest track to complete first.
SSTF 按磁道对  请求队列进行排序，选择最近磁道上的请求先完成。

For example, assuming the current position of the head is over the inner track, and we have requests for sectors 21 (middle track) and 2 (outer track), we would then issue the request to 21 first, wait for it to complete, and then issue the request to 2 (Figure 37.7).
==例如，假设磁头的当前位置在内层磁道上方，并且我们有对扇区 21（中间磁道）和 2（外层磁道）的请求，我们将首先向 21 发出请求，等待其完成，然后再向 2 发出请求（图 37.7）。==

SSTF works well in this example, seeking to the middle track first and then the outer track.
SSTF 在此示例中运行良好，首先寻道到中间磁道，然后是外层磁道。

However, SSTF is not a panacea, for the following reasons.
==然而，SSTF 并非万能药，原因如下。==

First, the drive geometry is not available to the host OS; rather, it sees an array of blocks.
==首先，驱动器几何结构对主机操作系统不可用；相反，它看到的是一个块数组。==

Fortunately, this problem is rather easily fixed.
==幸运的是，这个问题很容易解决。==

Instead of SSTF, an OS can simply implement nearest-block-first (NBF), which schedules the request with the nearest block address next.
==操作系统可以简单地实现最近块优先（NBF），而不是 SSTF，它调度下一个具有最近块地址的请求。==

The second problem is more fundamental: starvation.
==第二个问题更为根本：饥饿（starvation）。==

Imagine in our example above if there were a steady stream of requests to the inner track, where the head currently is positioned.
==想象一下在上面的例子中，如果有一股稳定的请求流向磁头当前所在的内层磁道。==

Requests to any other tracks would then be ignored completely by a pure SSTF approach.
==纯 SSTF 方法将完全忽略对任何其他磁道的请求。==

And thus the crux of the problem:
==因此问题的关键在于：==

CRUX: HOW TO HANDLE DISK STARVATION
==关键问题：如何处理磁盘饥饿==

How can we implement SSTF-like scheduling but avoid starvation?
==我们如何实现类似 SSTF 的调度但避免饥饿？==

Elevator (a.k.a. SCAN or C-SCAN)
==电梯算法（又名 SCAN 或 C-SCAN）==

The answer to this query was developed some time ago (see [CKR72] for example), and is relatively straightforward.
==这个问题的答案是很久以前开发出来的（例如参见 [CKR72]），并且相对简单。==

The algorithm, originally called SCAN, simply moves back and forth across the disk servicing requests in order across the tracks.
==该算法最初称为 SCAN，它只是在磁盘上来回移动，按顺序跨磁道处理请求。==

Let's call a single pass across the disk (from outer to inner tracks, or inner to outer) a sweep.
==我们将穿过磁盘的一次单向移动（从外层到内层磁道，或从内层到外层）称为一次扫描（sweep）。==

Thus, if a request comes for a block on a track that has already been serviced on this sweep of the disk, it is not handled immediately, but rather queued until the next sweep (in the other direction).
==因此，如果一个请求针对的是在该次磁盘扫描中已经处理过的磁道上的块，它不会立即被处理，而是排队等到下一次扫描（反方向）。==

SCAN has a number of variants, all of which do about the same thing.
SCAN 有许多变体，所有变体都做大致相同的事情。

For example, Coffman et al. introduced F-SCAN, which freezes the queue to be serviced when it is doing a sweep [CKR72]; this action places requests that come in during the sweep into a queue to be serviced later.
==例如，Coffman 等人引入了 F-SCAN，它在进行扫描时冻结要处理的队列 [CKR72]；此操作将扫描期间进入的请求放入队列以便稍后处理。==

Doing so avoids starvation of far-away requests, by delaying the servicing of late-arriving (but nearer by) requests.
==这样做通过延迟处理迟到的（但距离较近的）请求，避免了远距离请求的饥饿。==

C-SCAN is another common variant, short for Circular SCAN.
C-SCAN 是另一种常见的变体，是 Circular SCAN（循环 SCAN）的缩写。

Instead of sweeping in both directions across the disk, the algorithm only sweeps from outer-to-inner, and then resets at the outer track to begin again.
==该算法不是在磁盘上双向扫描，而是仅从外向内扫描，然后在重置到外层磁道重新开始。==

Doing so is a bit more fair to inner and outer tracks, as pure back-and-forth SCAN favors the middle tracks, i.e., after servicing the outer track, SCAN passes through the middle twice before coming back to the outer track again.
==这样做对内层和外层磁道更公平一些，因为纯粹的来回 SCAN 偏向于中间磁道，即在服务外层磁道之后，SCAN 在再次回到外层磁道之前会两次经过中间磁道。==

For reasons that should now be clear, the SCAN algorithm (and its cousins) is sometimes referred to as the elevator algorithm, because it behaves like an elevator which is either going up or down and not just servicing requests to floors based on which floor is closer.
==出于现在应该很清楚的原因，SCAN 算法（及其表亲）有时被称为电梯算法，因为它的行为就像电梯一样，要么上升要么下降，而不是仅仅根据哪个楼层更近来服务楼层请求。==

Imagine how annoying it would be if you were going down from floor 10 to 1, and somebody got on at 3 and pressed 4, and the elevator went up to 4 because it was "closer" than 1!
==想象一下，如果你正从 10 楼下到 1 楼，有人在 3 楼上来按了 4 楼，电梯因为 4 楼比 1 楼“更近”而上到 4 楼，那是多么恼人！==

As you can see, the elevator algorithm, when used in real life, prevents fights from taking place on elevators.
==正如你所看到的，电梯算法在现实生活中使用时，可以防止电梯里发生打架事件。==

In disks, it just prevents starvation.
==在磁盘中，它只是防止饥饿。==

Unfortunately, SCAN and its cousins do not represent the best scheduling technology.
==不幸的是，SCAN 及其表亲并不代表最好的调度技术。==

In particular, SCAN (or SSTF even) does not actually adhere as closely to the principle of SJF as they could.
==特别是，SCAN（甚至 SSTF）实际上并没有像它们本可以做到的那样紧密地遵循 SJF 原则。==

In particular, they ignore rotation.
==特别是，它们忽略了旋转。==

And thus, another crux:
==因此，另一个关键问题是：==

Rotates this way
==沿此方向旋转==

HARD DISK DRIVES
==硬盘驱动器==

CRUX: HOW TO ACCOUNT FOR DISK ROTATION COSTS
==关键问题：如何计算磁盘旋转成本==

How can we implement an algorithm that more closely approximates SJF by taking both seek and rotation into account?
==我们如何实现一种算法，通过同时考虑寻道和旋转，从而更接近最短作业优先（SJF）？==

SPTF: Shortest Positioning Time First
==SPTF：最短定位时间优先==

Before discussing shortest positioning time first or SPTF scheduling (sometimes also called shortest access time first or SATF), which is the solution to our problem, let us make sure we understand the problem in more detail.
==在讨论作为解决方案的最短定位时间优先或 SPTF 调度（有时也称为最短访问时间优先或 SATF）之前，让我们确保更详细地理解这个问题。==

Figure 37.8 presents an example.
==图 37.8 展示了一个例子。==

In the example, the head is currently positioned over sector 30 on the inner track.
==在这个例子中，磁头当前位于内圈磁道的扇区 30 上方。==

The scheduler thus has to decide: should it schedule sector 16 (on the middle track) or sector 8 (on the outer track) for its next request.
==因此调度器必须决定：下一个请求应该调度扇区 16（在中间磁道）还是扇区 8（在外圈磁道）。==

So which should it service next?
==那么它应该接下来服务哪一个呢？==

The answer, of course, is "it depends".
==答案当然是“视情况而定”。==

In engineering, it turns out "it depends" is almost always the answer, reflecting that trade-offs are part of the life of the engineer;
==在工程学中，事实证明“视情况而定”几乎总是答案，这反映了权衡取舍是工程师生活的一部分；==

such maxims are also good in a pinch, e.g., when you don't know an answer to your boss's question, you might want to try this gem.
==这种格言在紧要关头也很好用，例如，当你不知道老板问题的答案时，你可能想试试这个锦囊妙计。==

However, it is almost always better to know why it depends, which is what we discuss here.
==然而，知道“为什么要视情况而定”几乎总是更好的，这正是我们要在这里讨论的。==

What it depends on here is the relative time of seeking as compared to rotation.
==这里取决于寻道时间与旋转时间的相对大小。==

If, in our example, seek time is much higher than rotational delay, then SSTF (and variants) are just fine.
==如果在这个例子中，寻道时间远高于旋转延迟，那么 SSTF（及其变体）就很好。==

However, imagine if seek is quite a bit faster than rotation.
==然而，想象一下如果寻道比旋转快得多。==

Then, in our example, it would make more sense to seek further to service request 8 on the outer track than it would to perform the shorter seek to the middle track to service 16, which has to rotate all the way around before passing under the disk head.
==那么，在我们的例子中，寻道至更远的外圈磁道去服务请求 8，比执行较短的寻道去中间磁道服务请求 16 更有意义，因为后者在经过磁头下方之前必须旋转一整圈。==

Figure 37.8: SSTF: Sometimes Not Good Enough
==图 37.8：SSTF：有时还不够好==

TIP: IT ALWAYS DEPENDS (LIVNY'S LAW)
==提示：永远视情况而定（LIVNY 定律）==

Almost any question can be answered with "it depends", as our colleague Miron Livny always says.
==正如我们的同事 Miron Livny 常说的那样，几乎任何问题都可以用“视情况而定”来回答。==

However, use with caution, as if you answer too many questions this way, people will stop asking you questions altogether.
==不过，使用时要小心，因为如果你用这种方式回答太多问题，人们就会完全不再问你问题了。==

For example, somebody asks: "want to go to lunch?"
==例如，有人问：“想去吃午饭吗？”==

You reply: "it depends, are you coming along?"
==你回答：“视情况而定，你也去吗？”==

On modern drives, as we saw above, both seek and rotation are roughly equivalent (depending, of course, on the exact requests), and thus SPTF is useful and improves performance.
==在现代驱动器上，正如我们上面所看到的，寻道和旋转大致相当（当然取决于具体的请求），因此 SPTF 是有用的并且可以提高性能。==

However, it is even more difficult to implement in an OS, which generally does not have a good idea where track boundaries are or where the disk head currently is (in a rotational sense).
==然而，在操作系统中实现它更加困难，因为操作系统通常不太清楚磁道边界在哪里，或者磁头当前在哪里（在旋转意义上）。==

Thus, SPTF is usually performed inside a drive, described below.
==因此，SPTF 通常在驱动器内部执行，如下所述。==

Other Scheduling Issues
==其他调度问题==

There are many other issues we do not discuss in this brief description of basic disk operation, scheduling, and related topics.
==在这个关于基本磁盘操作、调度和相关主题的简要描述中，还有许多其他问题我们没有讨论。==

One such issue is this: where is disk scheduling performed on modern systems?
==其中一个问题是：现代系统的磁盘调度是在哪里执行的？==

In older systems, the operating system did all the scheduling;
==在旧系统中，操作系统负责所有的调度；==

after looking through the set of pending requests, the OS would pick the best one, and issue it to the disk.
==在浏览了一组未决请求后，操作系统会挑选最好的一个，并将其发送给磁盘。==

When that request completed, the next one would be chosen, and so forth.
==当该请求完成后，会选择下一个，依此类推。==

Disks were simpler then, and so was life.
==那时的磁盘比较简单，生活也是如此。==

In modern systems, disks can accommodate multiple outstanding requests, and have sophisticated internal schedulers themselves (which can implement SPTF accurately; inside the disk controller, all relevant details are available, including exact head position).
==在现代系统中，磁盘可以容纳多个未完成的请求，并且自身拥有复杂的内部调度器（它可以准确地实现 SPTF；在磁盘控制器内部，所有相关的细节都是可用的，包括精确的磁头位置）。==

Thus, the OS scheduler usually picks what it thinks the best few requests are (say 16) and issues them all to disk;
==因此，操作系统调度器通常会挑选它认为最好的几个请求（比如 16 个），并将它们全部发送给磁盘；==

the disk then uses its internal knowledge of head position and detailed track layout information to service said requests in the best possible (SPTF) order.
==然后，磁盘利用其内部的磁头位置知识和详细的磁道布局信息，以尽可能最好的顺序（SPTF）服务这些请求。==

Another important related task performed by disk schedulers is I/O merging.
==磁盘调度器执行的另一个重要的相关任务是 I/O 合并。==

For example, imagine a series of requests to read blocks 33, then 8, then 34, as in Figure 37.8.
==例如，想象一系列读取块 33，然后是 8，然后是 34 的请求，如图 37.8 所示。==

In this case, the scheduler should merge the requests for blocks 33 and 34 into a single two-block request;
==在这种情况下，调度器应该将块 33 和 34 的请求合并为一个单独的双块请求；==

any reordering that the scheduler does is performed upon the merged requests.
==调度器所做的任何重新排序都是针对合并后的请求执行的。==

Merging is particularly important at the OS level, as it reduces the number of requests sent to the disk and thus lowers overheads.
==合并在操作系统层面特别重要，因为它减少了发送到磁盘的请求数量，从而降低了开销。==

One final problem that modern schedulers address is this: how long should the system wait before issuing an  to disk?
==现代调度器解决的最后一个问题是：系统在向磁盘发出 I/O 请求之前应该等待多久？==

One might naively think that the disk, once it has even a single I/O, should immediately issue the request to the drive;
==人们可能会天真地认为，磁盘一旦有一个 I/O 请求，就应该立即向驱动器发出请求；==

this approach is called work-conserving, as the disk will never be idle if there are requests to serve.
==这种方法被称为工作守恒（work-conserving），因为如果有请求需要服务，磁盘将永远不会空闲。==

However, research on anticipatory disk scheduling has shown that sometimes it is better to wait for a bit [ID01], in what is called a non-work-conserving approach.
==然而，关于预期磁盘调度的研究表明，有时等待一会儿会更好 [ID01]，这被称为非工作守恒（non-work-conserving）方法。==

By waiting, a new and "better" request may arrive at the disk, and thus overall efficiency is increased.
==通过等待，一个新的且“更好”的请求可能会到达磁盘，从而提高整体效率。==

Of course, deciding when to wait, and for how long, can be tricky;
==当然，决定何时等待以及等待多久可能很棘手；==

see the research paper for details, or check out the Linux kernel implementation to see how such ideas are transitioned into practice (if you are the ambitious sort).
==详情请参阅研究论文，或者查看 Linux 内核的实现，看看这些想法是如何转化为实践的（如果你是有抱负的那类人）。==

37.6 Summary
==37.6 总结==

We have presented a summary of how disks work.
==我们已经总结了磁盘是如何工作的。==

The summary is actually a detailed functional model;
==这个总结实际上是一个详细的功能模型；==

it does not describe the amazing physics, electronics, and material science that goes into actual drive design.
==它没有描述实际驱动器设计中涉及的惊人的物理学、电子学和材料科学。==

For those interested in even more details of that nature, we suggest a different major (or perhaps minor);
==对于那些对这些细节感兴趣的人，我们建议选择不同的专业（或许是辅修）；==

for those that are happy with this model, good!
==对于那些对这个模型感到满意的人，很好！==

We can now proceed to using the model to build more interesting systems on top of these incredible devices.
==我们现在可以继续利用该模型，在这些不可思议的设备之上构建更有趣的系统。==

References
==参考文献==

[ADR03] "More Than an Interface: SCSI vs. ATA" by Dave Anderson, Jim Dykes, Erik Riedel.
==[ADR03] "More Than an Interface: SCSI vs. ATA" 作者：Dave Anderson, Jim Dykes, Erik Riedel。==

FAST '03, 2003. One of the best recent-ish references on how modern disk drives really work;
==FAST '03, 2003。关于现代磁盘驱动器真正如何工作的最佳近期参考文献之一；==

a must read for anyone interested in knowing more.
==任何有兴趣了解更多信息的人的必读之作。==

[CKR72] "Analysis of Scanning Policies for Reducing Disk Seek Times" E.G. Coffman, L.A. Klimko, B. Ryan
==[CKR72] "Analysis of Scanning Policies for Reducing Disk Seek Times" 作者：E.G. Coffman, L.A. Klimko, B. Ryan==

SIAM Journal of Computing, September 1972, Vol 1. No 3. Some of the early work in the field of disk scheduling.
==SIAM Journal of Computing, 1972 年 9 月, 第 1 卷第 3 期。磁盘调度领域的一些早期工作。==

[HK+17] "The Unwritten Contract of Solid State Drives" by Jun He, Sudarsun Kannan, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau.
==[HK+17] "The Unwritten Contract of Solid State Drives" 作者：Jun He, Sudarsun Kannan, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau。==

EuroSys '17, Belgrade, Serbia, April 2017. We take the idea of the unwritten contract, and extend it to SSDs.
==EuroSys '17，塞尔维亚贝尔格莱德，2017 年 4 月。我们采用了不成文契约的想法，并将其扩展到了 SSD。==

Using SSDs well seems as complicated as hard drives, and sometimes more so.
==用好 SSD 似乎和硬盘一样复杂，有时甚至更复杂。==

[ID01] "Anticipatory Scheduling: A Disk-scheduling Framework To Overcome Deceptive Idleness In Synchronous I/O" by Sitaram Iyer, Peter Druschel.
==[ID01] "Anticipatory Scheduling: A Disk-scheduling Framework To Overcome Deceptive Idleness In Synchronous I/O" 作者：Sitaram Iyer, Peter Druschel。==

SOSP '01, October 2001. A cool paper showing how waiting can improve disk scheduling: better requests may be on their way!
==SOSP '01，2001 年 10 月。一篇很酷的论文，展示了等待如何改善磁盘调度：更好的请求可能正在路上！==

[JW91] "Disk Scheduling Algorithms Based On Rotational Position" by D. Jacobson, J. Wilkes.
==[JW91] "Disk Scheduling Algorithms Based On Rotational Position" 作者：D. Jacobson, J. Wilkes。==

Technical Report HPL-CSP-91-7rev1, Hewlett-Packard, February 1991. A more modern take on disk scheduling.
==技术报告 HPL-CSP-91-7rev1，惠普，1991 年 2 月。对磁盘调度的更现代的看法。==

It remains a technical report (and not a published paper) because the authors were scooped by Seltzer et al. [SCO90].
==它仍然是一份技术报告（而不是已发表的论文），因为作者被 Seltzer 等人抢先了一步 [SCO90]。==

[RW92] "An Introduction to Disk Drive Modeling" by C. Ruemmler, J. Wilkes.
==[RW92] "An Introduction to Disk Drive Modeling" 作者：C. Ruemmler, J. Wilkes。==

IEEE Computer, 27:3, March 1994. A terrific introduction to the basics of disk operation.
==IEEE Computer, 27:3, 1994 年 3 月。对磁盘操作基础知识极好的介绍。==

Some pieces are out of date, but most of the basics remain.
==有些部分已经过时，但大部分基础知识仍然适用。==

[SCO90] "Disk Scheduling Revisited" by Margo Seltzer, Peter Chen, John Ousterhout.
==[SCO90] "Disk Scheduling Revisited" 作者：Margo Seltzer, Peter Chen, John Ousterhout。==

USENIX 1990. A paper that talks about how rotation matters too in the world of disk scheduling.
==USENIX 1990。一篇讨论在磁盘调度领域中旋转也很重要的论文。==

[SG04] "MEMS-based storage devices and standard disk interfaces: A square peg in a round hole?" Steven W. Schlosser, Gregory R. Ganger
==[SG04] "MEMS-based storage devices and standard disk interfaces: A square peg in a round hole?" 作者：Steven W. Schlosser, Gregory R. Ganger==

FAST '04, pp. 87-100, 2004. While the MEMS aspect of this paper hasn't yet made an impact, the discussion of the contract between file systems and disks is wonderful and a lasting contribution.
==FAST '04, pp. 87-100, 2004。虽然这篇论文中关于 MEMS 的方面尚未产生影响，但关于文件系统和磁盘之间契约的讨论非常精彩，是一个持久的贡献。==

We later build on this work to study the "Unwritten Contract of Solid State Drives" [HK+17]
==我们后来在这项工作的基础上研究了“固态硬盘的不成文契约” [HK+17]==

[S09a] "Barracuda ES.2 data sheet" by Seagate, Inc..
==[S09a] "Barracuda ES.2 data sheet" 作者：希捷公司（Seagate, Inc.）。==

Available at this website, at least, it was: [http://www.seagate.com/docs/pdf/datasheet/disc/ds_barracuda_es.pdf](http://www.seagate.com/docs/pdf/datasheet/disc/ds_barracuda_es.pdf).
==该网站提供下载，至少以前是：[http://www.seagate.com/docs/pdf/datasheet/disc/ds_barracuda_es.pdf](http://www.seagate.com/docs/pdf/datasheet/disc/ds_barracuda_es.pdf)。==

A data sheet; read at your own risk.
==一份数据表；阅读风险自负。==

Risk of what?
==什么风险？==

Boredom.
==无聊。==

[S09b] "Cheetah 15K.5" by Seagate, Inc..
==[S09b] "Cheetah 15K.5" 作者：希捷公司（Seagate, Inc.）。==

Available at this website, we're pretty sure it is: [http://www.seagate.com/docs/pdf/datasheet/disc/ds-cheetah-15k-5-us.pdf](http://www.seagate.com/docs/pdf/datasheet/disc/ds-cheetah-15k-5-us.pdf).
==该网站提供下载，我们很确定是：[http://www.seagate.com/docs/pdf/datasheet/disc/ds-cheetah-15k-5-us.pdf](http://www.seagate.com/docs/pdf/datasheet/disc/ds-cheetah-15k-5-us.pdf)。==

See above commentary on data sheets.
==参见上面关于数据表的评论。==

Homework (Simulation)
==作业（模拟）==

This homework uses disk.py to familiarize you with how a modern hard drive works.
==本作业使用 `disk.py` 来让你熟悉现代硬盘驱动器是如何工作的。==

It has a lot of different options, and unlike most of the other simulations, has a graphical animator to show you exactly what happens when the disk is in action.
==它有许多不同的选项，而且与大多数其他模拟不同，它有一个图形动画演示器，向你展示磁盘运行时究竟发生了什么。==

See the README for details.
==详情请参阅 README。==

1. Compute the seek, rotation, and transfer times for the following sets of requests: `-a 0`, `-a 6`, `-a 30`, `-a 7,30,8`, and finally `-a 10,11,12,13`.
==2. 计算以下几组请求的寻道、旋转和传输时间：`-a 0`，`-a 6`，`-a 30`，`-a 7,30,8`，最后是 `-a 10,11,12,13`。==
2. Do the same requests above, but change the seek rate to different values: `-S 2`, `-S 4`, `-S 8`, `-S 10`, `-S 40`, `-S 0.1`.
==4. 执行与上面相同的请求，但将寻道速率更改为不同的值：`-S 2`，`-S 4`，`-S 8`，`-S 10`，`-S 40`，`-S 0.1`。==

How do the times change?
==时间是如何变化的？==

3. Do the same requests above, but change the rotation rate: `-R 0.1`, `-R 0.5`, `-R 0.01`.
==4. 执行与上面相同的请求，但更改旋转速率：`-R 0.1`，`-R 0.5`，`-R 0.01`。==

How do the times change?
==时间是如何变化的？==

4. FIFO is not always best, e.g., with the request stream `-a 7,30,8`, what order should the requests be processed in?
==5. 先进先出（FIFO）并不总是最好的，例如，对于请求流 `-a 7,30,8`，应该按什么顺序处理请求？==

Run the shortest seek-time first (SSTF) scheduler (`-p SSTF`) on this workload;
==在此工作负载上运行最短寻道时间优先（SSTF）调度器（`-p SSTF`）；==

how long should it take (seek, rotation, transfer) for each request to be served?
==每个请求被服务需要多长时间（寻道、旋转、传输）？==

5. Now use the shortest access-time first (SATF) scheduler (`-p SATF`).
==6. 现在使用最短访问时间优先（SATF）调度器（`-p SATF`）。==

Does it make any difference for `-a 7,30,8` workload?
==这对于 `-a 7,30,8` 工作负载有什么区别吗？==

Find a set of requests where SATF outperforms SSTF;
==找到一组 SATF 优于 SSTF 的请求；==

more generally, when is SATF better than SSTF?
==更一般地讲，SATF 什么时候比 SSTF 更好？==

6. Here is a request stream to try: `-a 10,11,12,13`.
==7. 这里有一个可以尝试的请求流：`-a 10,11,12,13`。==

What goes poorly when it runs?
==运行时什么地方表现得很差？==

Try adding track skew to address this problem (`-o skew`).
==尝试添加磁道倾斜（track skew）来解决这个问题（`-o skew`）。==

Given the default seek rate, what should the skew be to maximize performance?
==在给定默认寻道速率的情况下，倾斜度应该是多少才能使性能最大化？==

What about for different seek rates (e.g., `-S 2`, `-S 4`)?
==对于不同的寻道速率（例如 `-S 2`，`-S 4`）呢？==

In general, could you write a formula to figure out the skew?
==总的来说，你能写一个公式来计算倾斜度吗？==

7. Specify a disk with different density per zone, e.g., `-z 10,20,30`, which specifies the angular difference between blocks on the outer, middle, and inner tracks.
==8. 指定一个各区域密度不同的磁盘，例如 `-z 10,20,30`，它指定了外圈、中间和内圈磁道上块之间的角度差。==

Run some random requests (e.g., `-a -1 -A 5,-1,0`, which specifies that random requests should be used via the `-a -1` flag and that five requests ranging from 0 to the max be generated), and compute the seek, rotation, and transfer times.
==运行一些随机请求（例如 `-a -1 -A 5,-1,0`，它通过 `-a -1` 标志指定应使用随机请求，并生成 5 个范围从 0 到最大值的请求），并计算寻道、旋转和传输时间。==

Use different random seeds.
==使用不同的随机种子。==

What is the bandwidth (in sectors per unit time) on the outer, middle, and inner tracks?
==外圈、中间和内圈磁道的带宽（每单位时间的扇区数）是多少？==

8. A scheduling window determines how many requests the disk can examine at once.
==9. 调度窗口决定了磁盘一次可以检查多少个请求。==

Generate random workloads (e.g., `-A 1000,-1,0`, with different seeds) and see how long the SATF scheduler takes when the scheduling window is changed from 1 up to the number of requests.
==生成随机工作负载（例如 `-A 1000,-1,0`，使用不同的种子），并观察当调度窗口从 1 变为请求总数时，SATF 调度器需要多长时间。==

How big of a window is needed to maximize performance?
==多大的窗口才能使性能最大化？==

Hint: use the flag and don't turn on graphics (`-G`) to run these quickly.
==提示：使用该标志且不要打开图形界面（`-G`）以快速运行这些。==

When the scheduling window is set to 1, does it matter which policy you are using?
==当调度窗口设置为 1 时，使用哪种策略有关系吗？==

9. Create a series of requests to starve a particular request, assuming an SATF policy.
==10. 创建一系列请求来“饿死”某个特定请求，假设使用的是 SATF 策略。==

Given that sequence, how does it perform if you use a bounded SATF (BSATF) scheduling approach?
==给定该序列，如果你使用有界 SATF（BSATF）调度方法，它的表现如何？==

In this approach, you specify the scheduling window (e.g., `-w 4`);
==在这种方法中，你指定调度窗口（例如 `-w 4`）；==

the scheduler only moves onto the next window of requests when all requests in the current window have been serviced.
==调度器只有在当前窗口中的所有请求都得到服务后，才会移动到下一个请求窗口。==

Does this solve starvation?
==这解决了饥饿问题吗？==

How does it perform, as compared to SATF?
==与 SATF 相比，它的表现如何？==

In general, how should a disk make this trade-off between performance and starvation avoidance?
==一般来说，磁盘应该如何在性能和避免饥饿之间进行权衡？==

10. All the scheduling policies we have looked at thus far are greedy;
==11. 到目前为止，我们看到的所有调度策略都是贪婪的；==

they pick the next best option instead of looking for an optimal schedule.
==它们挑选下一个最佳选项，而不是寻找最佳时间表。==

Can you find a set of requests in which greedy is not optimal?
==你能找到一组贪婪算法并非最优的请求吗？==

Redundant Arrays of Inexpensive Disks (RAIDS)
==廉价磁盘冗余阵列（RAID）==

38
38

When we use a disk, we sometimes wish it to be faster;
==当我们要使用磁盘时，有时候希望它能更快；==

 operations are slow and thus can be the bottleneck for the entire system.
==I/O 操作很慢，因此可能成为整个系统的瓶颈。==

When we use a disk, we sometimes wish it to be larger;
==当我们要使用磁盘时，有时候希望它能更大；==

more and more data is being put online and thus our disks are getting fuller and fuller.
==越来越多的数据被放到网上，因此我们的磁盘变得越来越满。==

When we use a disk, we sometimes wish for it to be more reliable;
==当我们要使用磁盘时，有时候希望它更可靠；==

when a disk fails, if our data isn't backed up, all that valuable data is gone.
==当磁盘发生故障时，如果我们的数据没有备份，所有那些宝贵的数据都会丢失。==

CRUX: HOW TO MAKE A LARGE, FAST, RELIABLE DISK
==关键问题：如何构建大容量、快速且可靠的磁盘==

How can we make a large, fast, and reliable storage system?
==我们要如何构建一个大容量、快速且可靠的存储系统？==

What are the key techniques?
==关键技术有哪些？==

What are trade-offs between different approaches?
==不同方法之间的权衡是什么？==

In this chapter, we introduce the Redundant Array of Inexpensive Disks better known as RAID [P+88], a technique to use multiple disks in concert to build a faster, bigger, and more reliable disk system.
==在本章中，我们将介绍廉价磁盘冗余阵列，即人们熟知的 RAID [P+88]，这是一种协同使用多个磁盘来构建更快、更大、更可靠的磁盘系统的技术。==

The term was introduced in the late 1980s by a group of researchers at U.C. Berkeley (led by Professors David Patterson and Randy Katz and then student Garth Gibson);
==这个术语是 20 世纪 80 年代末由加州大学伯克利分校的一组研究人员（由 David Patterson 教授和 Randy Katz 教授以及当时的学生 Garth Gibson 领导）提出的；==

it was around this time that many different researchers simultaneously arrived upon the basic idea of using multiple disks to build a better storage system [BG88, K86, K88, PB86, SG86].
==大约在这个时候，许多不同的研究人员不约而同地得出了利用多个磁盘构建更好存储系统的基本想法 [BG88, K86, K88, PB86, SG86]。==

Externally, a RAID looks like a disk: a group of blocks one can read or write.
==在外部看来，RAID 就像一个磁盘：一组可以读写的块。==

Internally, the RAID is a complex beast, consisting of multiple disks, memory (both volatile and non-), and one or more processors to manage the system.
==在内部，RAID 是一个复杂的庞然大物，由多个磁盘、内存（易失性和非易失性）以及一个或多个用于管理系统的处理器组成。==

A hardware RAID is very much like a computer system, specialized for the task of managing a group of disks.
==硬件 RAID 非常像一个计算机系统，专门用于管理一组磁盘的任务。==

RAIDs offer a number of advantages over a single disk.
==RAID 相比单个磁盘提供了许多优势。==

One advantage is performance.
==一个优势是性能。==

Using multiple disks in parallel can greatly speed up  times.
==并行使用多个磁盘可以大大加快 I/O 速度。==

Another benefit is capacity.
==另一个好处是容量。==

Large data sets demand large disks.
==大数据集需要大磁盘。==

Finally, RAIDs can improve reliability;
==最后，RAID 可以提高可靠性；==

spreading data across multiple disks (without RAID techniques) makes the data vulnerable to the loss of a single disk;
==将数据分散到多个磁盘（不使用 RAID 技术）会使数据容易因单个磁盘丢失而受损；==

with some form of redundancy, RAIDs can tolerate the loss of a disk and keep operating as if nothing were wrong.
==通过某种形式的冗余，RAID 可以容忍磁盘丢失并像什么都没发生一样继续运行。==

TIP: TRANSPARENCY ENABLES DEPLOYMENT
==提示：透明性有助于部署==

When considering how to add new functionality to a system, one should always consider whether such functionality can be added transparently, in a way that demands no changes to the rest of the system.
==当考虑如何向系统添加新功能时，应始终考虑是否可以透明地添加该功能，即不需要更改系统的其余部分。==

Requiring a complete rewrite of the existing software (or radical hardware changes) lessens the chance of impact of an idea.
==要求完全重写现有软件（或彻底的硬件更改）会降低一个想法产生影响的机会。==

RAID is a perfect example, and certainly its transparency contributed to its success;
==RAID 是一个完美的例子，它的透明性无疑促成了它的成功；==

administrators could install a SCSI-based RAID storage array instead of a SCSI disk, and the rest of the system (host computer, OS, etc.) did not have to change one bit to start using it.
==管理员可以安装基于 SCSI 的 RAID 存储阵列来代替 SCSI 磁盘，而系统的其余部分（主机、操作系统等）完全不需要更改即可开始使用它。==

By solving this problem of deployment, RAID was made more successful from day one.
==通过解决这个部署问题，RAID 从第一天起就更加成功。==

Amazingly, RAIDs provide these advantages transparently to systems that use them, i.e., a RAID just looks like a big disk to the host system.
==令人惊讶的是，RAID 透明地向使用它们的系统提供这些优势，也就是说，RAID 在主机系统看来就像一个大磁盘。==

The beauty of transparency, of course, is that it enables one to simply replace a disk with a RAID and not change a single line of software;
==当然，透明之美在于它使人们可以简单地用 RAID 替换磁盘，而无需更改任何一行软件代码；==

the operating system and client applications continue to operate without modification.
==操作系统和客户端应用程序无需修改即可继续运行。==

In this manner, transparency greatly improves the deployability of RAID, enabling users and administrators to put a RAID to use without worries of software compatibility.
==通过这种方式，透明性极大地提高了 RAID 的可部署性，使用户和管理员能够使用 RAID 而无需担心软件兼容性。==

We now discuss some of the important aspects of RAIDs.
==我们现在讨论 RAID 的一些重要方面。==

We begin with the interface, fault model, and then discuss how one can evaluate a RAID design along three important axes: capacity, reliability, and performance.
==我们从接口、故障模型开始，然后讨论如何沿三个重要轴线评估 RAID 设计：容量、可靠性和性能。==

We then discuss a number of other issues that are important to RAID design and implementation.
==然后我们将讨论对 RAID 设计和实现很重要的其他一些问题。==

38.1 Interface And RAID Internals
==38.1 接口和 RAID 内部结构==

To a file system above, a RAID looks like a big, (hopefully) fast, and (hopefully) reliable disk.
==对于上层的文件系统来说，RAID 看起来像是一个大的、（希望是）快速的、（希望是）可靠的磁盘。==

Just as with a single disk, it presents itself as a linear array of blocks, each of which can be read or written by the file system (or other client).
==就像单个磁盘一样，它呈现为一个线性的块数组，文件系统（或其他客户端）可以读取或写入其中的每一个块。==

When a file system issues a logical I/O request to the RAID, the RAID internally must calculate which disk (or disks) to access in order to complete the request, and then issue one or more physical I/Os to do so.
==当文件系统向 RAID 发出逻辑 I/O 请求时，RAID 内部必须计算要访问哪个（或哪些）磁盘以完成请求，然后发出一各或多个物理 I/O 来执行此操作。==

The exact nature of these physical I/Os depends on the RAID level, as we will discuss in detail below.
==这些物理 I/O 的确切性质取决于 RAID 级别，我们将在下面详细讨论。==

However, as a simple example, consider a RAID that keeps two copies of each block (each one on a separate disk);
==然而，作为一个简单的例子，考虑一个保留每个块的两个副本的 RAID（每个副本在单独的磁盘上）；==

when writing to such a mirrored RAID system, the RAID will have to perform two physical I/Os for every one logical I/O it is issued.
==当向这样的镜像 RAID 系统写入时，RAID 必须为发出的每一个逻辑 I/O 执行两个物理 I/O。==

A RAID system is often built as a separate hardware box, with a standard connection (e.g., SCSI, or SATA) to a host.
==RAID 系统通常被构建为一个独立的硬件盒，通过标准连接（例如 SCSI 或 SATA）连接到主机。==

Internally, however, RAIDs are fairly complex, consisting of a microcontroller that runs firmware to direct the operation of the RAID, volatile memory such as DRAM to buffer data blocks as they are read and written, and in some cases, non-volatile memory to buffer writes safely and perhaps even specialized logic to perform parity calculations (useful in some RAID levels, as we will also see below).
==然而在内部，RAID 相当复杂，包括运行固件以指导 RAID 操作的微控制器，用于在读写时缓冲数据块的易失性内存（如 DRAM），在某些情况下，还有用于安全缓冲写入的非易失性内存，甚至可能有用于执行奇偶校验计算的专用逻辑（在某些 RAID 级别中有用，我们也将在下面看到）。==

At a high level, a RAID is very much a specialized computer system: it has a processor, memory, and disks;
==从高层来看，RAID 非常像一个专用的计算机系统：它有处理器、内存和磁盘；==

however, instead of running applications, it runs specialized software designed to operate the RAID.
==然而，它不是运行应用程序，而是运行专门设计用来操作 RAID 的软件。==

38.2 Fault Model
==38.2 故障模型==

To understand RAID and compare different approaches, we must have a fault model in mind.
==要理解 RAID 并比较不同的方法，我们必须在脑海中有一个故障模型。==

RAIDs are designed to detect and recover from certain kinds of disk faults;
==RAID 旨在检测并从某些类型的磁盘故障中恢复；==

thus, knowing exactly which faults to expect is critical in arriving upon a working design.
==因此，确切地知道预期会有哪些故障对于达成一个有效的设计至关重要。==

The first fault model we will assume is quite simple, and has been called the fail-stop fault model [S84].
==我们将假设的第一个故障模型非常简单，被称为故障-停止（fail-stop）故障模型 [S84]。==

In this model, a disk can be in exactly one of two states: working or failed.
==在这个模型中，磁盘只能处于两种状态之一：工作或故障。==

With a working disk, all blocks can be read or written.
==工作中的磁盘，所有块都可以读写。==

In contrast, when a disk has failed, we assume it is permanently lost.
==相反，当磁盘发生故障时，我们假设它永久丢失了。==

One critical aspect of the fail-stop model is what it assumes about fault detection.
==故障-停止模型的一个关键方面是它对故障检测的假设。==

Specifically, when a disk has failed, we assume that this is easily detected.
==具体来说，当磁盘发生故障时，我们假设这很容易被检测到。==

For example, in a RAID array, we would assume that the RAID controller hardware (or software) can immediately observe when a disk has failed.
==例如，在 RAID 阵列中，我们会假设 RAID 控制器硬件（或软件）可以立即观察到磁盘何时发生故障。==

Thus, for now, we do not have to worry about more complex "silent" failures such as disk corruption.
==因此，目前我们不必担心更复杂的“静默”故障，如磁盘损坏。==

We also do not have to worry about a single block becoming inaccessible upon an otherwise working disk (sometimes called a latent sector error).
==我们也不必担心在其他方面工作正常的磁盘上单个块变得无法访问（有时称为潜在扇区错误）。==

We will consider these more complex (and unfortunately, more realistic) disk faults later.
==我们稍后将考虑这些更复杂（不幸的是，也更现实）的磁盘故障。==

38.3 How To Evaluate A RAID
==38.3 如何评估 RAID==

As we will soon see, there are a number of different approaches to building a RAID.
==正如我们将很快看到的，构建 RAID 有许多不同的方法。==

Each of these approaches has different characteristics which are worth evaluating, in order to understand their strengths and weaknesses.
==每种方法都有值得评估的不同特征，以便了解它们的优缺点。==

Specifically, we will evaluate each RAID design along three axes.
==具体来说，我们将沿三个轴线评估每种 RAID 设计。==

The first axis is capacity;
==第一个轴线是容量；==

given a set of N disks each with B blocks, how much useful capacity is available to clients of the RAID?
==给定一组 N 个磁盘，每个磁盘有 B 个块，RAID 客户端可用的有用容量是多少？==

Without redundancy, the answer is ; in contrast, if we have a system that keeps two copies of each block (called mirroring), we obtain a useful capacity of .
==没有冗余时，答案是 ；相反，如果我们有一个保留每个块两个副本的系统（称为镜像），我们将获得  的有用容量。==

Different schemes (e.g., parity-based ones) tend to fall in between.
==不同的方案（例如基于奇偶校验的方案）往往介于两者之间。==

The second axis of evaluation is reliability.
==评估的第二个轴线是可靠性。==

How many disk faults can the given design tolerate?
==给定的设计可以容忍多少个磁盘故障？==

In alignment with our fault model, we assume only that an entire disk can fail;
==根据我们的故障模型，我们假设只有整个磁盘会发生故障；==

in later chapters (i.e., on data integrity), we'll think about how to handle more complex failure modes.
==在后面的章节（即关于数据完整性的章节）中，我们将思考如何处理更复杂的故障模式。==

Finally, the third axis is performance.
==最后，第三个轴线是性能。==

Performance is somewhat challenging to evaluate, because it depends heavily on the workload presented to the disk array.
==性能评估有些挑战性，因为它在很大程度上取决于提交给磁盘阵列的工作负载。==

Thus, before evaluating performance, we will first present a set of typical workloads that one should consider.
==因此，在评估性能之前，我们将首先提出一组应考虑的典型工作负载。==

We now consider three important RAID designs: RAID Level 0 (striping), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity-based redundancy).
==我们现在考虑三种重要的 RAID 设计：RAID 0 级（条带化）、RAID 1 级（镜像）和 RAID 4/5 级（基于奇偶校验的冗余）。==

The naming of each of these designs as a "level" stems from the pioneering work of Patterson, Gibson, and Katz at Berkeley [P+88].
==将这些设计中的每一个命名为“级别”源于伯克利的 Patterson、Gibson 和 Katz 的开创性工作 [P+88]。==

38.4 RAID Level 0: Striping
==38.4 RAID 0 级：条带化==

The first RAID level is actually not a RAID level at all, in that there is no redundancy.
==第一个 RAID 级别实际上根本不是 RAID 级别，因为它没有冗余。==

However, RAID level 0, or striping as it is better known, serves as an excellent upper-bound on performance and capacity and thus is worth understanding.
==然而，RAID 0 级，或者更为人熟知的条带化，是性能和容量的极好上限，因此值得理解。==

The simplest form of striping will stripe blocks across the disks of the system as follows (assume here a 4-disk array):
==最简单的条带化形式将在系统的磁盘上条带化块，如下所示（这里假设是一个 4 磁盘阵列）：==

Figure 38.1: RAID-0: Simple Striping
==图 38.1：RAID-0：简单条带化==

From Figure 38.1, you get the basic idea: spread the blocks of the array across the disks in a round-robin fashion.
==从图 38.1 中，你得到了基本思路：以轮询方式将阵列的块分散到磁盘上。==

This approach is designed to extract the most parallelism from the array when requests are made for contiguous chunks of the array (as in a large, sequential read, for example).
==这种方法旨在当请求阵列的连续块时（例如在大型顺序读取中），从阵列中提取最大的并行性。==

We call the blocks in the same row a stripe;
==我们将同一行中的块称为一个条带；==

thus, blocks 0, 1, 2, and 3 are in the same stripe above.
==因此，上面的块 0、1、2 和 3 处于同一个条带中。==

In the example, we have made the simplifying assumption that only 1 block (each of say size 4KB) is placed on each disk before moving on to the next.
==在这个例子中，我们做了一个简化的假设，即在移至下一个磁盘之前，每个磁盘上只放置 1 个块（每个大小约为 4KB）。==

However, this arrangement need not be the case.
==然而，这种安排不一定是必须的。==

For example, we could arrange the blocks across disks as in Figure 38.2:
==例如，我们可以像图 38.2 那样在磁盘上排列块：==

Figure 38.2: Striping With A Bigger Chunk Size
==图 38.2：具有更大块大小的条带化==

In this example, we place two 4KB blocks on each disk before moving on to the next disk.
==在这个例子中，我们在移至下一个磁盘之前，在每个磁盘上放置两个 4KB 的块。==

Thus, the chunk size of this RAID array is 8KB, and a stripe thus consists of 4 chunks or 32KB of data.
==因此，这个 RAID 阵列的块大小是 8KB，一个条带因此由 4 个块或 32KB 数据组成。==

ASIDE: THE RAID MAPPING PROBLEM
==旁注：RAID 映射问题==

Before studying the capacity, reliability, and performance characteristics of the RAID, we first present an aside on what we call the mapping problem.
==在研究 RAID 的容量、可靠性和性能特征之前，我们首先插入一段关于我们所谓的映射问题的讨论。==

This problem arises in all RAID arrays;
==这个问题出现在所有的 RAID 阵列中；==

simply put, given a logical block to read or write, how does the RAID know exactly which physical disk and offset to access?
==简单地说，给定一个要读写的逻辑块，RAID 如何确切地知道要访问哪个物理磁盘和偏移量？==

For these simple RAID levels, we do not need much sophistication in order to correctly map logical blocks onto their physical locations.
==对于这些简单的 RAID 级别，我们不需要太复杂的技术就能正确地将逻辑块映射到它们的物理位置。==

Take the first striping example above (chunk size = 1 block = 4KB).
==以上面的第一个条带化示例为例（块大小 = 1 个块 = 4KB）。==

In this case, given a logical block address A, the RAID can easily compute the desired disk and offset with two simple equations:
==在这种情况下，给定逻辑块地址 A，RAID 可以用两个简单的方程轻松计算出所需的磁盘和偏移量：==

Disk = A % number_of_disks
==磁盘 = A % 磁盘数量==

Offset = A / number_of_disks
==偏移量 = A / 磁盘数量==

Note that these are all integer operations (e.g.,  not 1.33333...).
==注意这些都是整数运算（例如， 而不是 1.33333...）。==

Let's see how these equations work for a simple example.
==让我们看一个简单的例子，看看这些方程是如何工作的。==

Imagine in the first RAID above that a request arrives for block 14.
==想象在上面的第一个 RAID 中，来了一个对块 14 的请求。==

Given that there are 4 disks, this would mean that the disk we are interested in is (): disk 2.
==鉴于有 4 个磁盘，这意味着我们感兴趣的磁盘是 ()：磁盘 2。==

The exact block is calculated as (): block 3.
==具体的块计算为 ()：块 3。==

Thus, block 14 should be found on the fourth block (block 3, starting at 0) of the third disk (disk 2, starting at 0), which is exactly where it is.
==因此，块 14 应该位于第三个磁盘（磁盘 2，从 0 开始）的第四个块（块 3，从 0 开始）上，这正是它所在的位置。==

You can think about how these equations would be modified to support different chunk sizes.
==你可以思考一下如何修改这些方程以支持不同的块大小。==

Try it!
==试一试！==

It's not too hard.
==并不太难。==

Chunk Sizes
==块大小（Chunk Sizes）==

Chunk size mostly affects performance of the array.
==块大小主要影响阵列的性能。==

For example, a small chunk size implies that many files will get striped across many disks, thus increasing the parallelism of reads and writes to a single file;
==例如，较小的块大小意味着许多文件将被条带化到许多磁盘上，从而增加了对单个文件的读写并行性；==

however, the positioning time to access blocks across multiple disks increases, because the positioning time for the entire request is determined by the maximum of the positioning times of the requests across all drives.
==然而，跨多个磁盘访问块的定位时间会增加，因为整个请求的定位时间由所有驱动器上请求的定位时间的最大值决定。==

A big chunk size, on the other hand, reduces such intra-file parallelism, and thus relies on multiple concurrent requests to achieve high throughput.
==另一方面，大的块大小减少了这种文件内并行性，因此依赖于多个并发请求来实现高吞吐量。==

However, large chunk sizes reduce positioning time;
==然而，大的块大小减少了定位时间；==

if, for example, a single file fits within a chunk and thus is placed on a single disk, the positioning time incurred while accessing it will just be the positioning time of a single disk.
==例如，如果单个文件适合放入一个块中，从而被放置在单个磁盘上，那么访问它时产生的定位时间将仅仅是单个磁盘的定位时间。==

Thus, determining the "best" chunk size is hard to do, as it requires a great deal of knowledge about the workload presented to the disk system [CL95].
==因此，确定“最佳”块大小很难，因为它需要大量关于提交给磁盘系统的工作负载的知识 [CL95]。==

For the rest of this discussion, we will assume that the array uses a chunk size of a single block (4KB).
==在接下来的讨论中，我们将假设阵列使用单个块（4KB）的块大小。==

Most arrays use larger chunk sizes (e.g., 64 KB), but for the issues we discuss below, the exact chunk size does not matter;
==大多数阵列使用更大的块大小（例如 64 KB），但对于我们下面讨论的问题，确切的块大小并不重要；==

thus we use a single block for the sake of simplicity.
==因此为简单起见，我们使用单个块。==

Back To RAID-0 Analysis
==回到 RAID-0 分析==

Let us now evaluate the capacity, reliability, and performance of striping.
==现在让我们评估条带化的容量、可靠性和性能。==

From the perspective of capacity, it is perfect: given N disks each of size B blocks, striping delivers  blocks of useful capacity.
==从容量的角度来看，它是完美的：给定 N 个大小为 B 块的磁盘，条带化提供  块的有用容量。==

From the standpoint of reliability, striping is also perfect, but in the bad way: any disk failure will lead to data loss.
==从可靠性的角度来看，条带化也是完美的，但是是糟糕的那种：任何磁盘故障都会导致数据丢失。==

Finally, performance is excellent: all disks are utilized, often in parallel, to service user  requests.
==最后，性能非常好：所有磁盘都被利用，通常是并行地，来服务用户的 I/O 请求。==

Evaluating RAID Performance
==评估 RAID 性能==

In analyzing RAID performance, one can consider two different performance metrics.
==在分析 RAID 性能时，可以考虑两个不同的性能指标。==

The first is single-request latency.
==第一个是单请求延迟。==

Understanding the latency of a single  request to a RAID is useful as it reveals how much parallelism can exist during a single logical  operation.
==了解 RAID 单个 I/O 请求的延迟很有用，因为它揭示了在单个逻辑 I/O 操作期间可以存在多少并行性。==

The second is steady-state throughput of the RAID, i.e., the total bandwidth of many concurrent requests.
==第二个是 RAID 的稳态吞吐量，即许多并发请求的总带宽。==

Because RAIDs are often used in high-performance environments, the steady-state bandwidth is critical, and thus will be the main focus of our analyses.
==由于 RAID 通常用于高性能环境，因此稳态带宽至关重要，因此将成为我们分析的重点。==

To understand throughput in more detail, we need to put forth some workloads of interest.
==为了更详细地理解吞吐量，我们需要提出一些感兴趣的工作负载。==

We will assume, for this discussion, that there are two types of workloads: sequential and random.
==在这个讨论中，我们将假设有两种类型的工作负载：顺序的和随机的。==

With a sequential workload, we assume that requests to the array come in large contiguous chunks;
==对于顺序工作负载，我们假设对阵列的请求是以大的连续块形式出现的；==

for example, a request (or series of requests) that accesses 1 MB of data, starting at block  and ending at block , would be deemed sequential.
==例如，访问 1 MB 数据的请求（或一系列请求），从块  开始并在块  结束，将被视为顺序的。==

Sequential workloads are common in many environments (think of searching through a large file for a keyword), and thus are considered important.
==顺序工作负载在许多环境中都很常见（想想在大型文件中搜索关键字），因此被认为是重要的。==

For random workloads, we assume that each request is rather small, and that each request is to a different random location on disk.
==对于随机工作负载，我们假设每个请求都相当小，并且每个请求都指向磁盘上不同的随机位置。==

For example, a random stream of requests may first access 4KB at logical address 10, then at logical address 550,000, then at 20,100, and so forth.
==例如，随机请求流可能首先访问逻辑地址 10 处的 4KB，然后在逻辑地址 550,000 处，然后在 20,100 处，依此类推。==

Some important workloads, such as transactional workloads on a database management system (DBMS), exhibit this type of access pattern, and thus it is considered an important workload.
==一些重要的工作负载，如数据库管理系统（DBMS）上的事务性工作负载，表现出这种类型的访问模式，因此它被认为是一种重要的工作负载。==

Of course, real workloads are not so simple, and often have a mix of sequential and random-seeming components as well as behaviors in-between the two.
==当然，实际的工作负载并不是那么简单，通常混合了顺序和看似随机的成分，以及介于两者之间的行为。==

For simplicity, we just consider these two possibilities.
==为了简单起见，我们只考虑这两种可能性。==

As you can tell, sequential and random workloads will result in widely different performance characteristics from a disk.
==正如你所知，顺序和随机工作负载将导致磁盘产生截然不同的性能特征。==

With sequential access, a disk operates in its most efficient mode, spending little time seeking and waiting for rotation and most of its time transferring data.
==对于顺序访问，磁盘以其最高效的模式运行，花费很少的时间寻道和等待旋转，而大部分时间用于传输数据。==

With random access, just the opposite is true: most time is spent seeking and waiting for rotation and relatively little time is spent transferring data.
==对于随机访问，情况正好相反：大部分时间花在寻道和等待旋转上，相对较少的时间花在传输数据上。==

To capture this difference in our analysis, we will assume that a disk can transfer data at  under a sequential workload, and  when under a random workload.
==为了在我们的分析中捕捉这种差异，我们将假设磁盘在顺序工作负载下可以以  的速度传输数据，而在随机工作负载下以  的速度传输。==

In general, S is much greater than R (i.e., ).
==通常，S 远大于 R（即 ）。==

To make sure we understand this difference, let's do a simple exercise.
==为了确保我们理解这种差异，让我们做一个简单的练习。==

Specifically, let's calculate S and R given the following disk characteristics.
==具体来说，让我们根据以下磁盘特性计算 S 和 R。==

Assume a sequential transfer of size 10 MB on average, and a random transfer of 10 KB on average.
==假设平均顺序传输大小为 10 MB，平均随机传输大小为 10 KB。==

Also, assume the following disk characteristics:
==此外，假设以下磁盘特性：==

Average seek time: 7 ms
==平均寻道时间：7 ms==

Average rotational delay: 3 ms
==平均旋转延迟：3 ms==

Transfer rate of disk: 
==磁盘传输速率：==

To compute S, we need to first figure out how time is spent in a typical 10 MB transfer.
==为了计算 S，我们需要首先弄清楚典型的 10 MB 传输是如何花费时间的。==

First, we spend 7 ms seeking, and then 3 ms rotating.
==首先，我们花费 7 ms 寻道，然后 3 ms 旋转。==

Finally, transfer begins;
==最后，传输开始；==

10 MB @ 50  leads to  of a second, or 200 ms, spent in transfer.
==10 MB @ 50  导致花费  秒，即 200 ms 进行传输。==

Thus, for each 10 MB request, we spend 210 ms completing the request.
==因此，对于每个 10 MB 请求，我们花费 210 ms 完成请求。==

To compute S, we just need to divide:
==要计算 S，我们只需进行除法：==




As we can see, because of the large time spent transferring data, S is very near the peak bandwidth of the disk (the seek and rotational costs have been amortized).
==我们可以看到，由于大部分时间花在传输数据上，S 非常接近磁盘的峰值带宽（寻道和旋转成本已被摊销）。==

We can compute R similarly.
==我们可以类似地计算 R。==

Seek and rotation are the same;
==寻道和旋转是一样的；==

we then compute the time spent in transfer, which is 10 KB @ 50  or 0.195 ms.
==然后我们计算传输所花费的时间，即 10 KB @ 50  或 0.195 ms。==




As we can see, R is less than 1 , and  is almost 50.
==正如我们所见，R 小于 1 ，且  接近 50。==

Back To RAID-0 Analysis, Again
==再次回到 RAID-0 分析==

Let's now evaluate the performance of striping.
==现在让我们评估条带化的性能。==

As we said above, it is generally good.
==正如我们上面所说，通常很好。==

From a latency perspective, for example, the latency of a single-block request should be just about identical to that of a single disk;
==例如，从延迟的角度来看，单块请求的延迟应该与单个磁盘的延迟几乎相同；==

after all, RAID-0 will simply redirect that request to one of its disks.
==毕竟，RAID-0 只是将该请求重定向到其磁盘之一。==

From the perspective of steady-state sequential throughput, we'd expect to get the full bandwidth of the system.
==从稳态顺序吞吐量的角度来看，我们期望获得系统的全部带宽。==

Thus, throughput equals N (the number of disks) multiplied by S (the sequential bandwidth of a single disk).
==因此，吞吐量等于 N（磁盘数量）乘以 S（单个磁盘的顺序带宽）。==

For a large number of random , we can again use all of the disks, and thus obtain .
==对于大量的随机 I/O，我们可以再次使用所有磁盘，从而获得 。==

As we will see below, these values are both the simplest to calculate and will serve as an upper bound in comparison with other RAID levels.
==正如我们将在下面看到的，这些值既是最容易计算的，也将作为与其他 RAID 级别比较的上限。==

38.5 RAID Level 1: Mirroring
==38.5 RAID 1 级：镜像==

Our first RAID level beyond striping is known as RAID level 1, or mirroring.
==除了条带化之外，我们的第一个 RAID 级别被称为 RAID 1 级，或镜像。==

With a mirrored system, we simply make more than one copy of each block in the system;
==对于镜像系统，我们只是简单地为系统中的每个块制作多个副本；==

each copy should be placed on a separate disk, of course.
==当然，每个副本应该放在单独的磁盘上。==

By doing so, we can tolerate disk failures.
==这样做，我们可以容忍磁盘故障。==

In a typical mirrored system, we will assume that for each logical block, the RAID keeps two physical copies of it.
==在一个典型的镜像系统中，我们将假设对于每个逻辑块，RAID 保留两个物理副本。==

Here is an example:
==这里有一个例子：==

Figure 38.3: Simple RAID-1: Mirroring
==图 38.3：简单 RAID-1：镜像==

In the example, disk 0 and disk 1 have identical contents, and disk 2 and disk 3 do as well;
==在这个例子中，磁盘 0 和磁盘 1 有相同的内容，磁盘 2 和磁盘 3 也是如此；==

the data is striped across these mirror pairs.
==数据在这些镜像对之间条带化。==

In fact, you may have noticed that there are a number of different ways to place block copies across the disks.
==事实上，你可能已经注意到有许多不同的方式在磁盘上放置块副本。==

The arrangement above is a common one and is sometimes called RAID-10 (or RAID , stripe of mirrors) because it uses mirrored pairs (RAID-1) and then stripes (RAID-0) on top of them;
==上面的排列是一种常见的方式，有时被称为 RAID-10（或 RAID ，镜像的条带），因为它使用镜像对（RAID-1），然后在它们之上进行条带化（RAID-0）；==

another common arrangement is RAID-01 (or RAID , mirror of stripes), which contains two large striping (RAID-0) arrays, and then mirrors (RAID-1) on top of them.
==另一种常见的排列是 RAID-01（或 RAID ，条带的镜像），它包含两个大的条带化（RAID-0）阵列，然后在它们之上进行镜像（RAID-1）。==

For now, we will just talk about mirroring assuming the above layout.
==目前，我们将假设上述布局来讨论镜像。==

When reading a block from a mirrored array, the RAID has a choice: it can read either copy.
==当从镜像阵列读取块时，RAID 有一个选择：它可以读取任意一个副本。==

For example, if a read to logical block 5 is issued to the RAID, it is free to read it from either disk 2 or disk 3.
==例如，如果向 RAID 发出一个读取逻辑块 5 的请求，它可以自由地从磁盘 2 或磁盘 3 读取它。==

When writing a block, though, no such choice exists: the RAID must update both copies of the data, in order to preserve reliability.
==然而，当写入一个块时，不存在这样的选择：RAID 必须更新数据的两个副本，以保持可靠性。==

Do note, though, that these writes can take place in parallel;
==请注意，这些写入可以并行进行；==

for example, a write to logical block 5 could proceed to disks 2 and 3 at the same time.
==例如，对逻辑块 5 的写入可以同时在磁盘 2 和 3 上进行。==

RAID-1 Analysis
==RAID-1 分析==

Let us assess RAID-1.
==让我们评估一下 RAID-1。==

From a capacity standpoint, RAID-1 is expensive;
==从容量的角度来看，RAID-1 是昂贵的；==

with the mirroring level , we only obtain half of our peak useful capacity.
==当镜像级别  时，我们只获得峰值有用容量的一半。==

With N disks of B blocks, RAID-1 useful capacity is .
==对于 N 个磁盘，每个磁盘有 B 个块，RAID-1 的有用容量是 。==

From a reliability standpoint, RAID-1 does well.
==从可靠性的角度来看，RAID-1 表现良好。==

It can tolerate the failure of any one disk.
==它可以容忍任何一个磁盘的故障。==

You may also notice RAID-1 can actually do better than this, with a little luck.
==你可能也注意到，运气好的话，RAID-1 实际上可以做得比这更好。==

Imagine, in the figure above, that disk 0 and disk 2 both failed.
==想象一下，在上图中，磁盘 0 和磁盘 2 都坏了。==

In such a situation, there is no data loss!
==在这种情况下，没有数据丢失！==

More generally, a mirrored system (with mirroring level of 2) can tolerate 1 disk failure for certain, and up to  failures depending on which disks fail.
==更一般地说，一个镜像系统（镜像级别为 2）可以确定地容忍 1 个磁盘故障，并且最多可以容忍  个故障，具体取决于哪些磁盘发生故障。==

In practice, we generally don't like to leave things like this to chance;
==在实践中，我们通常不喜欢把这种事情留给运气；==

thus most people consider mirroring to be good for handling a single failure.
==因此大多数人认为镜像适合处理单个故障。==

Finally, we analyze performance.
==最后，我们分析性能。==

From the perspective of the latency of a single read request, we can see it is the same as the latency on a single disk;
==从单个读取请求的延迟角度来看，我们可以看到它与单个磁盘的延迟相同；==

all the RAID-1 does is direct the read to one of its copies.
==RAID-1 所做的只是将读取指向其副本之一。==

A write is a little different: it requires two physical writes to complete before it is done.
==写入稍有不同：它需要完成两个物理写入才算完成。==

These two writes happen in parallel, and thus the time will be roughly equivalent to the time of a single write;
==这两个写入是并行发生的，因此时间将大致相当于单个写入的时间；==

however, because the logical write must wait for both physical writes to complete, it suffers the worst-case seek and rotational delay of the two requests, and thus (on average) will be slightly higher than a write to a single disk.
==然而，因为逻辑写入必须等待两个物理写入都完成，所以它会遭受两个请求中最坏情况的寻道和旋转延迟，因此（平均而言）会略高于对单个磁盘的写入。==

ASIDE: THE RAID CONSISTENT-UPDATE PROBLEM
==旁注：RAID 一致性更新问题==

Before analyzing RAID-1, let us first discuss a problem that arises in any multi-disk RAID system, known as the consistent-update problem [DAA05].
==在分析 RAID-1 之前，让我们首先讨论任何多磁盘 RAID 系统中都会出现的一个问题，即一致性更新问题 [DAA05]。==

The problem occurs on a write to any RAID that has to update multiple disks during a single logical operation.
==当对任何需要在单个逻辑操作期间更新多个磁盘的 RAID 进行写入时，就会出现此问题。==

In this case, let us assume we are considering a mirrored disk array.
==在这种情况下，让我们假设我们正在考虑一个镜像磁盘阵列。==

Imagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1.
==想象写入请求被发往 RAID，然后 RAID 决定必须将其写入两个磁盘，磁盘 0 和磁盘 1。==

The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs.
==然后 RAID 向磁盘 0 发出写入，但在 RAID 向磁盘 1 发出请求之前，发生了断电（或系统崩溃）。==

In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued).
==在这个不幸的情况下，让我们假设对磁盘 0 的请求已完成（但显然对磁盘 1 的请求没有完成，因为它从未发出）。==

The result of this untimely power loss is that the two copies of the block are now inconsistent;
==这种不合时宜的断电导致的结果是，该块的两个副本现在不一致了；==

the copy on disk 0 is the new version, and the copy on disk 1 is the old.
==磁盘 0 上的副本是新版本，而磁盘 1 上的副本是旧版本。==

What we would like to happen is for the state of both disks to change atomically, i.e., either both should end up as the new version or neither.
==我们要希望发生的是两个磁盘的状态原子性地改变，即要么都变成新版本，要么都不是。==

The general way to solve this problem is to use a write-ahead log of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it.
==解决这个问题的一般方法是使用某种预写日志，在执行操作之前先记录 RAID 将要做什么（即，用特定数据更新两个磁盘）。==

By taking this approach, we can ensure that in the presence of a crash, the right thing will happen;
==通过采取这种方法，我们可以确保在发生崩溃时，会发生正确的事情；==

by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync.
==通过运行一个重放所有挂起事务到 RAID 的恢复程序，我们可以确保没有两个镜像副本（在 RAID-1 案例中）是不同步的。==

One last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging.
==最后一点：因为在每次写入时都记录到磁盘极其昂贵，大多数 RAID 硬件包含少量非易失性 RAM（例如，电池供电的），并在那里执行此类日志记录。==

Thus, consistent update is provided without the high cost of logging to disk.
==因此，在没有记录到磁盘的高昂成本的情况下提供了一致性更新。==

To analyze steady-state throughput, let us start with the sequential workload.
==为了分析稳态吞吐量，让我们从顺序工作负载开始。==

When writing out to disk sequentially, each logical write must result in two physical writes;
==当顺序写入磁盘时，每个逻辑写入必须导致两个物理写入；==

for example, when we write logical block 0 (in the figure above), the RAID internally would write it to both disk 0 and disk 1.
==例如，当我们写入逻辑块 0（在上图中）时，RAID 内部会将其写入磁盘 0 和磁盘 1。==

Thus, we can conclude that the maximum bandwidth obtained during sequential writing to a mirrored array is , or half the peak bandwidth.
==因此，我们可以得出结论，镜像阵列顺序写入期间获得的最大带宽是 ，即峰值带宽的一半。==

Unfortunately, we obtain the exact same performance during a sequential read.
==不幸的是，在顺序读取期间我们获得了完全相同的性能。==

One might think that a sequential read could do better, because it only needs to read one copy of the data, not both.
==人们可能认为顺序读取可以做得更好，因为它只需要读取数据的一个副本，而不是两个。==

However, let's use an example to illustrate why this doesn't help much.
==然而，让我们用一个例子来说明为什么这没有太大帮助。==

Imagine we need to read blocks 0, 1, 2, 3, 4, 5, 6, and 7.
==想象一下我们需要读取块 0、1、2、3、4、5、6 和 7。==

Let's say we issue the read of 0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the read of 3 to disk 3.
==假设我们将 0 的读取发给磁盘 0，1 的读取发给磁盘 2，2 的读取发给磁盘 1，3 的读取发给磁盘 3。==

We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1, and 3, respectively.
==我们继续将 4、5、6 和 7 的读取分别发给磁盘 0、2、1 和 3。==

One might naively think that because we are utilizing all disks, we are achieving the full bandwidth of the array.
==人们可能会天真地认为，因为我们在利用所有磁盘，所以我们实现了阵列的全部带宽。==

To see that this is not (necessarily) the case, however, consider the requests a single disk receives (say disk 0).
==然而，要明白情况（未必）如此，请考虑单个磁盘（比如说磁盘 0）收到的请求。==

First, it gets a request for block 0; then, it gets a request for block 4 (skipping block 2).
==首先，它收到块 0 的请求；然后，它收到块 4 的请求（跳过了块 2）。==

In fact, each disk receives a request for every other block.
==事实上，每个磁盘每隔一个块接收一个请求。==

While it is rotating over the skipped block, it is not delivering useful bandwidth to the client.
==当它在被跳过的块上旋转时，它并没有向客户端提供有用的带宽。==

Thus, each disk will only deliver half its peak bandwidth.
==因此，每个磁盘将只提供其峰值带宽的一半。==

And thus, the sequential read will only obtain a bandwidth of .
==因此，顺序读取将只获得  的带宽。==

Random reads are the best case for a mirrored RAID.
==随机读取是镜像 RAID 的最佳情况。==

In this case, we can distribute the reads across all the disks, and thus obtain the full possible bandwidth.
==在这种情况下，我们可以将读取分布到所有磁盘上，从而获得全部可能的带宽。==

Thus, for random reads, RAID-1 delivers .
==因此，对于随机读取，RAID-1 提供 。==

Finally, random writes perform as you might expect: .
==最后，随机写入的表现正如你所预期的那样：。==

Each logical write must turn into two physical writes, and thus while all the disks will be in use, the client will only perceive this as half the available bandwidth.
==每个逻辑写入必须变成两个物理写入，因此虽然所有磁盘都将被使用，但客户端只能感知到可用带宽的一半。==

Even though a write to logical block x turns into two parallel writes to two different physical disks, the bandwidth of many small requests only achieves half of what we saw with striping.
==即使对逻辑块 x 的写入变成了对两个不同物理磁盘的两个并行写入，许多小请求的带宽也只能达到我们在条带化中看到的一半。==

As we will soon see, getting half the available bandwidth is actually pretty good!
==正如我们很快就会看到的，获得可用带宽的一半实际上已经相当不错了！==

38.6 RAID Level 4: Saving Space With Parity
==38.6 RAID 4 级：利用奇偶校验节省空间==

We now present a different method of adding redundancy to a disk array known as parity.
==我们现在介绍一种不同的向磁盘阵列添加冗余的方法，称为奇偶校验。==

Parity-based approaches attempt to use less capacity and thus overcome the huge space penalty paid by mirrored systems.
==基于奇偶校验的方法试图使用更少的容量，从而克服镜像系统付出的巨大空间代价。==

They do so at a cost, however: performance.
==然而，这样做是有代价的：性能。==

Figure 38.4: RAID-4 With Parity
==图 38.4：具有奇偶校验的 RAID-4==

Here is an example five-disk RAID-4 system (Figure 38.4).
==这里有一个五磁盘 RAID-4 系统的例子（图 38.4）。==

For each stripe of data, we have added a single parity block that stores the redundant information for that stripe of blocks.
==对于每个数据条带，我们添加了一个奇偶校验块，用于存储该条带块的冗余信息。==

For example, parity block P1 has redundant information that it calculated from blocks 4, 5, 6, and 7.
==例如，奇偶校验块 P1 具有从块 4、5、6 和 7 计算得出的冗余信息。==

To compute parity, we need to use a mathematical function that enables us to withstand the loss of any one block from our stripe.
==为了计算奇偶校验，我们需要使用一个数学函数，使我们能够承受条带中任何一个块的丢失。==

It turns out the simple function XOR does the trick quite nicely.
==事实证明，简单的异或（XOR）函数就能很好地达到目的。==

For a given set of bits, the XOR of all of those bits returns a 0 if there are an even number of 1's in the bits, and a 1 if there are an odd number of 1's.
==对于给定的一组位，如果其中 1 的个数是偶数，则所有这些位的异或返回 0，如果是奇数，则返回 1。==

For example:
==例如：==

In the first row (0,0,1,1), there are two 1's (C2, C3), and thus XOR of all of those values will be 0 (P);
==在第一行 (0,0,1,1) 中，有两个 1 (C2, C3)，因此所有这些值的异或将是 0 (P)；==

similarly, in the second row there is only one 1 (C1), and thus the XOR must be 1 (P).
==同样，在第二行中只有一个 1 (C1)，因此异或必须是 1 (P)。==

You can remember this in a simple way: that the number of 1s in any row, including the parity bit, must be an even (not odd) number;
==你可以用一种简单的方式记住这一点：任何一行中 1 的数量，包括奇偶校验位，必须是偶数（不是奇数）；==

that is the invariant that the RAID must maintain in order for parity to be correct.
==这是 RAID 必须维护的不变性，以便奇偶校验是正确的。==

From the example above, you might also be able to guess how parity information can be used to recover from a failure.
==从上面的例子中，你也许能猜出奇偶校验信息如何用于从故障中恢复。==

Imagine the column labeled C2 is lost.
==想象一下标记为 C2 的列丢失了。==

To figure out what values must have been in the column, we simply have to read in all the other values in that row (including the XOR'd parity bit) and reconstruct the right answer.
==为了弄清楚该列中一定是什么值，我们只需读入该行中的所有其他值（包括异或的奇偶校验位）并重构正确的答案。==

Specifically, assume the first row's value in column C2 is lost (it is a 1);
==具体来说，假设第一行 C2 列的值丢失（它是 1）；==

by reading the other values in that row (0 from C0, 0 from C1, 1 from C3, and 0 from the parity column P), we get the values 0, 0, 1, and 0.
==通过读取该行中的其他值（来自 C0 的 0，来自 C1 的 0，来自 C3 的 1，以及来自奇偶校验列 P 的 0），我们得到值 0、0、1 和 0。==

Because we know that XOR keeps an even number of 1's in each row, we know what the missing data must be: a 1.
==因为我们知道异或保持每一行中 1 的数量为偶数，我们知道丢失的数据一定是什么：一个 1。==

And that is how reconstruction works in a XOR-based parity scheme!
==这就是基于异或的奇偶校验方案中重建的工作原理！==

Note also how we compute the reconstructed value: we just XOR the data bits and the parity bits together, in the same way that we calculated the parity in the first place.
==还要注意我们如何计算重构的值：我们只是将数据位和奇偶校验位异或在一起，就像我们最初计算奇偶校验一样。==

Now you might be wondering: we are talking about XORing all of these bits, and yet from above we know that the RAID places 4KB (or larger) blocks on each disk;
==现在你可能会想：我们在谈论异或所有这些位，但从上面我们知道 RAID 在每个磁盘上放置 4KB（或更大）的块；==

how do we apply XOR to a bunch of blocks to compute the parity?
==我们如何对一堆块应用异或来计算奇偶校验？==

It turns out this is easy as well.
==事实证明这也容易。==

Simply perform a bitwise XOR across each bit of the data blocks;
==只需对数据块的每一位执行按位异或；==

put the result of each bitwise XOR into the corresponding bit slot in the parity block.
==将每个按位异或的结果放入奇偶校验块中相应的位槽中。==

As you can see from the figure, the parity is computed for each bit of each block and the result placed in the parity block.
==正如图中所示，奇偶校验是为每个块的每一位计算的，结果放置在奇偶校验块中。==

RAID-4 Analysis
==RAID-4 分析==

Let us now analyze RAID-4.
==现在让我们分析 RAID-4。==

From a capacity standpoint, RAID-4 uses 1 disk for parity information for every group of disks it is protecting.
==从容量的角度来看，RAID-4 为它保护的每组磁盘使用 1 个磁盘用于奇偶校验信息。==

Thus, our useful capacity for a RAID group is .
==因此，我们的 RAID 组的有用容量是 。==

Reliability is also quite easy to understand: RAID-4 tolerates 1 disk failure and no more.
==可靠性也很容易理解：RAID-4 可以容忍 1 个磁盘故障，不能更多。==

If more than one disk is lost, there is simply no way to reconstruct the lost data.
==如果丢失超过一个磁盘，根本无法重构丢失的数据。==

Figure 38.5: Full-stripe Writes In RAID-4
==图 38.5：RAID-4 中的全条带写入==

Finally, there is performance.
==最后是性能。==

This time, let us start by analyzing steady-state throughput.
==这一次，让我们从分析稳态吞吐量开始。==

Sequential read performance can utilize all of the disks except for the parity disk, and thus deliver a peak effective bandwidth of  (an easy case).
==顺序读取性能可以利用除奇偶校验磁盘以外的所有磁盘，从而提供  的峰值有效带宽（这是一个简单的情况）。==

To understand the performance of sequential writes, we must first understand how they are done.
==要理解顺序写入的性能，我们必须首先了解它们是如何完成的。==

When writing a big chunk of data to disk, RAID-4 can perform a simple optimization known as a full-stripe write.
==当向磁盘写入一大块数据时，RAID-4 可以执行一种称为全条带写入的简单优化。==

For example, imagine the case where the blocks 0, 1, 2, and 3 have been sent to the RAID as part of a write request (Figure 38.5).
==例如，想象块 0、1、2 和 3 作为写入请求的一部分已发送到 RAID 的情况（图 38.5）。==

In this case, the RAID can simply calculate the new value of P0 (by performing an XOR across the blocks 0, 1, 2, and 3) and then write all of the blocks (including the parity block) to the five disks above in parallel (highlighted in gray in the figure).
==在这种情况下，RAID 可以简单地计算 P0 的新值（通过对块 0、1、2 和 3 执行异或），然后并行地将所有块（包括奇偶校验块）写入上面的五个磁盘（图中以灰色突出显示）。==

Thus, full-stripe writes are the most efficient way for RAID-4 to write to disk.
==因此，全条带写入是 RAID-4 写入磁盘的最有效方式。==

Once we understand the full-stripe write, calculating the performance of sequential writes on RAID-4 is easy;
==一旦我们理解了全条带写入，计算 RAID-4 上顺序写入的性能就很容易了；==

the effective bandwidth is also .
==有效带宽也是 。==

Even though the parity disk is constantly in use during the operation, the client does not gain performance advantage from it.
==即使奇偶校验磁盘在操作过程中一直被使用，客户端也不会从中获得性能优势。==

Now let us analyze the performance of random reads.
==现在让我们分析随机读取的性能。==

As you can also see from the figure above, a set of 1-block random reads will be spread across the data disks of the system but not the parity disk.
==正如你从上图中看到的那样，一组 1 块大小的随机读取将分布在系统的数据磁盘上，但不会分布在奇偶校验磁盘上。==

Thus, the effective performance is: .
==因此，有效性能是：。==

Random writes, which we have saved for last, present the most interesting case for RAID-4.
==我们留到最后的随机写入，展示了 RAID-4 最有趣的情况。==

Imagine we wish to overwrite block 1 in the example above.
==想象一下我们希望覆盖上面例子中的块 1。==

We could just go ahead and overwrite it, but that would leave us with a problem: the parity block P0 would no longer accurately reflect the correct parity value of the stripe;
==我们可以直接覆盖它，但这会给我们留下一个问题：奇偶校验块 P0 将不再准确反映条带的正确奇偶校验值；==

in this example, P0 must also be updated.
==在这个例子中，P0 也必须更新。==

How can we update it both correctly and efficiently?
==我们如何既正确又有效地更新它？==

It turns out there are two methods.
==事实证明有两种方法。==

The first, known as additive parity, requires us to do the following.
==第一种称为加法奇偶校验，需要我们要执行以下操作。==

To compute the value of the new parity block, read in all of the other data blocks in the stripe in parallel (in the example, blocks 0, 2, and 3) and XOR those with the new block (1).
==为了计算新奇偶校验块的值，并行读入条带中的所有其他数据块（在示例中为块 0、2 和 3），并将它们与新块 (1) 进行异或。==

The result is your new parity block.
==结果就是你的新奇偶校验块。==

To complete the write, you can then write the new data and new parity to their respective disks, also in parallel.
==要完成写入，你可以随后将新数据和新奇偶校验并行写入各自的磁盘。==

The problem with this technique is that it scales with the number of disks, and thus in larger RAIDs requires a high number of reads to compute parity.
==这种技术的问题在于它随磁盘数量扩展，因此在较大的 RAID 中需要大量的读取来计算奇偶校验。==

Thus, the subtractive parity method.
==因此，有了减法奇偶校验方法。==

For example, imagine this string of bits (4 data bits, one parity):
==例如，想象这串位（4 个数据位，1 个奇偶校验位）：==

C0 C1 C2 C3 P
0 0 1 1 0

XOR(0,0,1,1) = 0
XOR(0,0,1,1) = 0

Let's imagine that we wish to overwrite bit C2 with a new value which we will call .
==让我们想象一下，我们希望用一个新值覆盖位 C2，我们称之为 。==

The subtractive method works in three steps.
==减法分三步进行。==

First, we read in the old data at C2 () and the old parity ().
==首先，我们读入 C2 处的旧数据（）和旧奇偶校验（）。==

Then, we compare the old data and the new data;
==然后，我们比较旧数据和新数据；==

if they are the same (e.g., ), then we know the parity bit will also remain the same (i.e., ).
==如果它们相同（例如，），那么我们知道奇偶校验位也将保持不变（即 ）。==

If, however, they are different, then we must flip the old parity bit to the opposite of its current state, that is, if ,  will be set to 0;
==然而，如果它们不同，那么我们必须将旧奇偶校验位翻转为其当前状态的反面，也就是说，如果 ， 将被设置为 0；==

if ,  will be set to 1.
==如果 ， 将被设置为 1。==

We can express this whole mess neatly with XOR:
==我们可以用异或整洁地表达这一团乱麻：==




Because we are dealing with blocks, not bits, we perform this calculation over all the bits in the block (e.g., 4096 bytes in each block multiplied by 8 bits per byte).
==因为我们处理的是块，而不是位，所以我们对块中的所有位执行此计算（例如，每个块 4096 字节乘以每字节 8 位）。==

Thus, in most cases, the new block will be different than the old block and thus the new parity block will too.
==因此，在大多数情况下，新块将不同于旧块，因此新奇偶校验块也将不同。==

You should now be able to figure out when we would use the additive parity calculation and when we would use the subtractive method.
==你现在应该能够弄清楚我们要何时使用加法奇偶校验计算，何时使用减法。==

Think about how many disks would need to be in the system so that the additive method performs fewer I/Os than the subtractive method;
==想一想系统中需要有多少个磁盘，才能使加法方法执行的 I/O 少于减法方法；==

what is the cross-over point?
==交叉点是多少？==

For this performance analysis, let us assume we are using the subtractive method.
==对于这个性能分析，让我们假设我们使用的是减法。==

Thus, for each write, the RAID has to perform 4 physical  (two reads and two writes).
==因此，对于每次写入，RAID 必须执行 4 次物理 I/O（两次读取和两次写入）。==

Now imagine there are lots of writes submitted to the RAID; how many can RAID-4 perform in parallel?
==现在想象有大量写入提交给 RAID；RAID-4 可以并行执行多少个？==

To understand, let us again look at the RAID-4 layout (Figure 38.6).
==为了理解，让我们再次看看 RAID-4 的布局（图 38.6）。==

Figure 38.6: Example: Writes To 4, 13, And Respective Parity Blocks
==图 38.6：示例：写入 4、13 和各自的奇偶校验块==

Now imagine there were 2 small writes submitted to the RAID-4 at about the same time, to blocks 4 and 13.
==现在想象大约在同一时间向 RAID-4 提交了 2 个小写入，分别写入块 4 和 13。==

The data for those disks is on disks 0 and 1, and thus the read and write to data could happen in parallel, which is good.
==这些磁盘的数据位于磁盘 0 和 1 上，因此数据的读写可以并行发生，这很好。==

The problem that arises is with the parity disk; both the requests have to read the related parity blocks for 4 and 13, parity blocks 1 and 3.
==出现的问题在于奇偶校验磁盘；两个请求都必须读取 4 和 13 的相关奇偶校验块，即奇偶校验块 1 和 3。==

Hopefully, the issue is now clear: the parity disk is a bottleneck under this type of workload;
==希望现在问题已经很清楚了：在这种类型的工作负载下，奇偶校验磁盘是一个瓶颈；==

we sometimes thus call this the small-write problem for parity-based RAIDs.
==因此我们有时将此称为基于奇偶校验的 RAID 的小写入问题。==

Thus, even though the data disks could be accessed in parallel, the parity disk prevents any parallelism from materializing;
==因此，即使数据磁盘可以并行访问，奇偶校验磁盘也阻碍了任何并行性的实现；==

all writes to the system will be serialized because of the parity disk.
==由于奇偶校验磁盘，所有对系统的写入都将被序列化。==

Because the parity disk has to perform two  (one read, one write) per logical  we can compute the performance of small random writes in RAID-4 by computing the parity disk's performance on those two , and thus we achieve  MB/s.
==因为奇偶校验磁盘必须为每个逻辑 I/O 执行两个 I/O（一次读取，一次写入），我们可以通过计算奇偶校验磁盘在这两个 I/O 上的性能来计算 RAID-4 中小随机写入的性能，因此我们达到了  MB/s。==

RAID-4 throughput under random small writes is terrible;
==随机小写入下的 RAID-4 吞吐量很糟糕；==

it does not improve as you add disks to the system.
==随着向系统添加磁盘，它并没有改善。==

We conclude by analyzing I/O latency in RAID-4.
==最后我们分析 RAID-4 中的 I/O 延迟。==

As you now know, a single read (assuming no failure) is just mapped to a single disk, and thus its latency is equivalent to the latency of a single disk request.
==正如你现在所知，单个读取（假设没有故障）只是映射到单个磁盘，因此其延迟相当于单个磁盘请求的延迟。==

The latency of a single write requires two reads and then two writes;
==单个写入的延迟需要两次读取，然后是两次写入；==

the reads can happen in parallel, as can the writes, and thus total latency is about twice that of a single disk (with some differences because we have to wait for both reads to complete and thus get the worst-case positioning time, but then the updates don't incur seek cost and thus may be a better-than-average positioning cost).
==读取可以并行发生，写入也可以，因此总延迟大约是单个磁盘的两倍（有一些差异，因为我们必须等待两个读取都完成，从而得到最坏情况的定位时间，但随后的更新不会产生寻道成本，因此可能具有优于平均水平的定位成本）。==

38.7 RAID Level 5: Rotating Parity
==38.7 RAID 5 级：旋转奇偶校验==

To address the small-write problem (at least, partially), Patterson, Gibson, and Katz introduced RAID-5.
==为了解决小写入问题（至少是部分解决），Patterson、Gibson 和 Katz 推出了 RAID-5。==

RAID-5 works almost identically to RAID-4, except that it rotates the parity block across drives (Figure 38.7).
==RAID-5 的工作原理与 RAID-4 几乎相同，除了它会在驱动器之间旋转奇偶校验块（图 38.7）。==

Figure 38.7: RAID-5 With Rotated Parity
==图 38.7：具有旋转奇偶校验的 RAID-5==

As you can see, the parity block for each stripe is now rotated across the disks, in order to remove the parity-disk bottleneck for RAID-4.
==正如你所看到的，每个条带的奇偶校验块现在在磁盘之间旋转，以消除 RAID-4 的奇偶校验磁盘瓶颈。==

RAID-5 Analysis
==RAID-5 分析==

Much of the analysis for RAID-5 is identical to RAID-4.
==RAID-5 的大部分分析与 RAID-4 相同。==

For example, the effective capacity and failure tolerance of the two levels are identical.
==例如，这两个级别的有效容量和故障容忍度是相同的。==

So are sequential read and write performance.
==顺序读写性能也是如此。==

The latency of a single request (whether a read or a write) is also the same as RAID-4.
==单个请求的延迟（无论是读还是写）也与 RAID-4 相同。==

Random read performance is a little better, because we can now utilize all disks.
==随机读取性能稍微好一点，因为我们现在可以利用所有磁盘。==

Finally, random write performance improves noticeably over RAID-4, as it allows for parallelism across requests.
==最后，随机写入性能比 RAID-4 显着提高，因为它允许跨请求并行。==

Imagine a write to block 1 and a write to block 10;
==想象一下对块 1 的写入和对块 10 的写入；==

this will turn into requests to disk 1 and disk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for block 10 and its parity).
==这将变成对磁盘 1 和磁盘 4 的请求（针对块 1 及其奇偶校验），以及对磁盘 0 和磁盘 2 的请求（针对块 10 及其奇偶校验）。==

Thus, they can proceed in parallel.
==因此，它们可以并行进行。==

Figure 38.8: RAID Capacity, Reliability, and Performance
==图 38.8：RAID 容量、可靠性和性能==

In fact, we can generally assume that given a large number of random requests, we will be able to keep all the disks about evenly busy.
==事实上，我们通常可以假设，给定大量的随机请求，我们将能够使所有磁盘大致均匀地忙碌。==

If that is the case, then our total bandwidth for small writes will be .
==如果是这种情况，那么我们小写入的总带宽将是 。==

The factor of four loss is due to the fact that each RAID-5 write still generates 4 total  operations, which is simply the cost of using parity-based RAID.
==四倍的损失是由于每个 RAID-5 写入仍然产生 4 个总 I/O 操作，这仅仅是使用基于奇偶校验的 RAID 的成本。==

Because RAID-5 is basically identical to RAID-4 except in the few cases where it is better, it has almost completely replaced RAID-4 in the marketplace.
==因为 RAID-5 除了在少数情况下更好之外，基本上与 RAID-4 相同，所以它在市场上几乎完全取代了 RAID-4。==

The only place where it has not is in systems that know they will never perform anything other than a large write, thus avoiding the small-write problem altogether [HLM94];
==唯一的例外是那些知道除了大写入之外永远不会执行其他操作的系统，从而完全避免了小写入问题 [HLM94]；==

in those cases, RAID-4 is sometimes used as it is slightly simpler to build.
==在这些情况下，有时会使用 RAID-4，因为它的构建稍微简单一些。==

38.8 RAID Comparison: A Summary
==38.8 RAID 比较：总结==

We now summarize our simplified comparison of RAID levels in Figure 38.8.
==我们现在在图 38.8 中总结我们对 RAID 级别的简化比较。==

Note that we have omitted a number of details to simplify our analysis.
==请注意，为了简化分析，我们省略了许多细节。==

For example, when writing in a mirrored system, the average seek time is a little higher than when writing to just a single disk, because the seek time is the max of two seeks (one on each disk).
==例如，在镜像系统中写入时，平均寻道时间比仅写入单个磁盘时稍高，因为寻道时间是两次寻道（每个磁盘一次）的最大值。==

Thus, random write performance to two disks will generally be a little less than random write performance of a single disk.
==因此，对两个磁盘的随机写入性能通常会略低于单个磁盘的随机写入性能。==

Also, when updating the parity disk in RAID-4/5, the first read of the old parity will likely cause a full seek and rotation, but the second write of the parity will only result in rotation.
==此外，在 RAID-4/5 中更新奇偶校验磁盘时，第一次读取旧奇偶校验可能会导致完全的寻道和旋转，但第二次写入奇偶校验将只会导致旋转。==

Finally, sequential I/O to mirrored RAIDs pay a 2x performance penalty as compared to other approaches.
==最后，与其他方法相比，镜像 RAID 的顺序 I/O 付出了 2 倍的性能代价。==

The  penalty assumes a naive read/write pattern for mirroring;
== 的代价假设了镜像的朴素读写模式；==

a more sophisticated approach that issued large  requests to differing parts of each mirror could potentially achieve full bandwidth.
==一种更复杂的方法，即向每个镜像的不同部分发出大型 I/O 请求，可能实现全带宽。==

Think about this to see if you can figure out why.
==思考一下，看看你是否能弄清楚原因。==
However, the comparison in Figure 38.8 does capture the essential differences, and is useful for understanding tradeoffs across RAID levels.
==然而，图 38.8 中的比较确实捕捉到了本质区别，并且对于理解不同 RAID 级别之间的权衡非常有用。==

For the latency analysis, we simply use $T$ to represent the time that a request to a single disk would take.
==对于延迟分析，我们简单地使用 $T$ 来表示对单个磁盘的请求所需的时间。==

To conclude, if you strictly want performance and do not care about reliability, striping is obviously best.
==总而言之，如果你只追求性能而不关心可靠性，条带化（striping）显然是最好的。==

If, however, you want random I/O performance and reliability, mirroring is the best; the cost you pay is in lost capacity.
==然而，如果你想要随机 I/O 性能和可靠性，镜像（mirroring）是最好的；你付出的代价是容量的损失。==

If capacity and reliability are your main goals, then RAID-5 is the winner; the cost you pay is in small-write performance.
==如果容量和可靠性是你的主要目标，那么 RAID-5 就是赢家；你付出的代价是小写（small-write）性能。==

Finally, if you are always doing sequential I/O and want to maximize capacity, RAID-5 also makes the most sense.
==最后，如果你总是进行顺序 I/O 并希望最大化容量，RAID-5 也是最合理的选择。==

**38.9 Other Interesting RAID Issues**
==**38.9 其他有趣的 RAID 问题**==

There are a number of other interesting ideas that one could (and perhaps should) discuss when thinking about RAID.
==在思考 RAID 时，还有许多其他有趣的想法可以（也或许应该）讨论。==

Here are some things we might eventually write about.
==以下是我们最终可能会写到的一些内容。==

For example, there are many other RAID designs, including Levels 2 and 3 from the original taxonomy, and Level 6 to tolerate multiple disk faults [C+04].
==例如，还有许多其他的 RAID 设计，包括原始分类中的级别 2 和 3，以及可以容忍多个磁盘故障的级别 6 [C+04]。==

There is also what the RAID does when a disk fails; sometimes it has a **hot spare** sitting around to fill in for the failed disk.
==还有 RAID 在磁盘故障时的表现；有时它会有一个**热备盘**（hot spare）在旁边随时待命，以填补故障磁盘的空缺。==

What happens to performance under failure, and performance during reconstruction of the failed disk?
==故障下的性能表现如何，以及故障磁盘重建期间的性能表现又如何？==

There are also more realistic fault models, to take into account **latent sector errors** or **block corruption** [B+08], and lots of techniques to handle such faults (see the data integrity chapter for details).
==还有更现实的故障模型，需要考虑**潜伏扇区错误**或**数据块损坏** [B+08]，以及许多处理此类故障的技术（详见数据完整性章节）。==

Finally, you can even build RAID as a software layer: such **software RAID** systems are cheaper but have other problems, including the **consistent-update problem** [DAA05].
==最后，你甚至可以将 RAID 构建为软件层：这种**软件 RAID** 系统更便宜，但也有其他问题，包括**一致性更新问题** [DAA05]。==

**38.10 Summary**
==**38.10 总结**==

We have discussed RAID.
==我们已经讨论了 RAID。==

RAID transforms a number of independent disks into a large, more capacious, and more reliable single entity; importantly, it does so transparently, and thus hardware and software above is relatively oblivious to the change.
==RAID 将多个独立的磁盘转化为一个巨大、容量更大且更可靠的单一实体；重要的是，它是透明地完成这一过程的，因此上层的硬件和软件对这种变化相对无感。==

There are many possible RAID levels to choose from, and the exact RAID level to use depends heavily on what is important to the end-user.
==有许多可能的 RAID 级别可供选择，具体使用哪种 RAID 级别很大程度上取决于对终端用户而言什么才是重要的。==

For example, mirrored RAID is simple, reliable, and generally provides good performance but at a high capacity cost.
==例如，镜像 RAID 简单、可靠，通常提供良好的性能，但容量成本很高。==

RAID-5, in contrast, is reliable and better from a capacity standpoint, but performs quite poorly when there are small writes in the workload.
==相比之下，RAID-5 是可靠的，从容量角度看更好，但当工作负载中存在小额写入时，性能表现相当糟糕。==

Picking a RAID and setting its parameters (chunk size, number of disks, etc.) properly for a particular workload is challenging, and remains more of an art than a science.
==为特定工作负载选择合适的 RAID 并正确设置其参数（块大小、磁盘数量等）是具有挑战性的，这在很大程度上仍然是一门艺术而非科学。==

**39 Interlude: Files and Directories**
==**39 插曲：文件和目录**==

Thus far we have seen the development of two key operating system abstractions: the **process**, which is a virtualization of the CPU, and the **address space**, which is a virtualization of memory.
==到目前为止，我们已经看到了两个关键操作系统抽象的发展：**进程**（CPU 的虚拟化）和**地址空间**（内存的虚拟化）。==

In tandem, these two abstractions allow a program to run as if it is in its own private, isolated world; as if it has its own processor (or processors); as if it has its own memory.
==这两个抽象协同工作，使程序运行起来就好像处于自己私有的、隔离的世界中；就好像它拥有自己的处理器（或多个处理器）；就好像它拥有自己的内存。==

This illusion makes programming the system much easier and thus is prevalent today not only on desktops and servers but increasingly on all programmable platforms including mobile phones and the like.
==这种错觉使系统编程变得更加容易，因此在当今不仅在台式机和服务器上流行，而且越来越多地出现在包括移动电话在内的所有可编程平台上。==

In this section, we add one more critical piece to the virtualization puzzle: **persistent storage**.
==在本节中，我们为虚拟化拼图增加了一个关键部分：**持久性存储**。==

A persistent-storage device, such as a classic **hard disk drive** or a more modern **solid-state storage device**, stores information permanently (or at least, for a long time).
==持久性存储设备，如经典的**硬盘驱动器**或更现代的**固态存储设备**，可以永久（或者至少是长时间地）存储信息。==

Unlike memory, whose contents are lost when there is a power loss, a persistent-storage device keeps such data intact.
==与内存不同（内存的内容在断电时会丢失），持久性存储设备可以保持此类数据的完整。==

Thus, the OS must take extra care with such a device: this is where users keep data that they really care about.
==因此，操作系统必须对这种设备格外小心：这是用户存放他们真正关心的数据的地方。==

**CRUX: HOW TO MANAGE A PERSISTENT DEVICE**
==**核心问题：如何管理持久性设备**==

How should the OS manage a persistent device?
==操作系统应该如何管理持久性设备？==

What are the APIs?
==有哪些 API？==

What are the important aspects of the implementation?
==实现中的重要方面有哪些？==

Thus, in the next few chapters, we will explore critical techniques for managing persistent data, focusing on methods to improve performance and reliability.
==因此，在接下来的几章中，我们将探索管理持久性数据的关键技术，重点关注提高性能和可靠性的方法。==

We begin, however, with an overview of the API: the interfaces you’ll expect to see when interacting with a UNIX file system.
==然而，我们首先概述 API：即在与 UNIX 文件系统交互时你期望看到的接口。==

**39.1 Files And Directories**
==**39.1 文件和目录**==

Two key abstractions have developed over time in the virtualization of storage.
==在存储虚拟化的过程中，随着时间的推移发展出了两个关键抽象。==

The first is the **file**.
==第一个是**文件**。==

A file is simply a linear array of bytes, each of which you can read or write.
==文件只是一个简单的字节线性数组，你可以读取或写入其中的每一个字节。==

Each file has some kind of **low-level name**, usually a number of some kind; often, the user is not aware of this name (as we will see).
==每个文件都有某种**低级名称**，通常是某种数字；通常情况下，用户并不知道这个名称（我们稍后会看到）。==

For historical reasons, the low-level name of a file is often referred to as its **inode number** (**i-number**).
==由于历史原因，文件的低级名称通常被称为其 **inode 编号**（**i-number**）。==

We’ll be learning a lot more about inodes in future chapters; for now, just assume that each file has an inode number associated with it.
==在未来的章节中，我们将学习更多关于 inode 的知识；目前，只需假设每个文件都关联一个 inode 编号。==

In most systems, the OS does not know much about the structure of the file (e.g., whether it is a picture, or a text file, or C code); rather, the responsibility of the file system is simply to store such data persistently on disk and make sure that when you request the data again, you get what you put there in the first place.
==在大多数系统中，操作系统对文件的结构了解不多（例如，它是图片、文本文件还是 C 代码）；相反，文件系统的责任只是将此类数据持久地存储在磁盘上，并确保当你再次请求数据时，你能得到最初存放的内容。==

Doing so is not as simple as it seems!
==这样做并不像看起来那么简单！==

The second abstraction is that of a **directory**.
==第二个抽象是**目录**。==

A directory, like a file, also has a low-level name (i.e., an inode number), but its contents are quite specific: it contains a list of (user-readable name, low-level name) pairs.
==目录和文件一样，也有一个低级名称（即 inode 编号），但它的内容非常特殊：它包含一个（用户可读名称，低级名称）对的列表。==

For example, let’s say there is a file with the low-level name “10”, and it is referred to by the user-readable name of “foo”.
==例如，假设有一个低级名称为“10”的文件，用户可读的名称为“foo”。==

The directory that “foo” resides in thus would have an entry (“foo”, “10”) that maps the user-readable name to the low-level name.
==因此，“foo”所在的目录将包含一个条目（“foo”，“10”），该条目将用户可读名称映射到低级名称。==

Each entry in a directory refers to either files or other directories.
==目录中的每个条目要么指向文件，要么指向其他目录。==

By placing directories within other directories, users are able to build an arbitrary **directory tree** (or **directory hierarchy**), under which all files and directories are stored.
==通过将目录放置在其他目录中，用户能够构建一个任意的**目录树**（或**目录层级**），所有的文件和目录都存储在其中。==

The directory hierarchy starts at a **root directory** (in UNIX-based systems, the root directory is simply referred to as /) and uses some kind of separator to name subsequent sub-directories until the desired file or directory is named.
==目录层级始于**根目录**（在基于 UNIX 的系统中，根目录简称为 /），并使用某种分隔符来命名后续的子目录，直到命名所需的文件夹或文件。==

For example, if a user created a directory `foo` in the root directory `/`, and then created a file `bar.txt` in the directory `foo`, we could refer to the file by its **absolute pathname**, which in this case would be `/foo/bar.txt`.
==例如，如果用户在根目录 `/` 中创建了一个目录 `foo`，然后在目录 `foo` 中创建了一个文件 `bar.txt`，我们可以通过其**绝对路径名**来引用该文件，在这种情况下就是 `/foo/bar.txt`。==

See Figure 39.1 for a more complex directory tree; valid directories in the example are `/`, `/foo`, `/bar`, `/bar/bar`, `/bar/foo` and valid files are `/foo/bar.txt` and `/bar/foo/bar.txt`.
==请参见图 39.1 了解更复杂的目录树；示例中的有效目录是 `/`, `/foo`, `/bar`, `/bar/bar`, `/bar/foo`，有效文件是 `/foo/bar.txt` 和 `/bar/foo/bar.txt`。==

**TIP: THINK CAREFULLY ABOUT NAMING**
==**提示：仔细思考命名问题**==

Naming is an important aspect of computer systems [SK09].
==命名是计算机系统的一个重要方面 [SK09]。==

In UNIX systems, virtually everything that you can think of is named through the file system.
==在 UNIX 系统中，几乎所有你能想到的东西都是通过文件系统命名的。==

Beyond just files, devices, pipes, and even processes [K84] can be found in what looks like a plain old file system.
==除了文件之外，设备、管道甚至进程 [K84] 都可以出现在看起来像普通文件系统的结构中。==

This uniformity of naming eases your conceptual model of the system, and makes the system simpler and more modular.
==这种命名的统一性简化了你对系统的概念模型，使系统更简单、更模块化。==

Thus, whenever creating a system or interface, think carefully about what names you are using.
==因此，每当创建系统或接口时，请仔细思考你正在使用的名称。==

Directories and files can have the same name as long as they are in different locations in the file-system tree (e.g., there are two files named `bar.txt` in the figure, `/foo/bar.txt` and `/bar/foo/bar.txt`).
==只要目录和文件位于文件系统树的不同位置，它们就可以拥有相同的名称（例如，图中两个名为 `bar.txt` 的文件，分别是 `/foo/bar.txt` 和 `/bar/foo/bar.txt`）。==

You may also notice that the file name in this example often has two parts: `bar` and `txt`, separated by a period.
==你可能还会注意到，本例中的文件名通常由两部分组成：`bar` 和 `txt`，由句点分隔。==

The first part is an arbitrary name, whereas the second part of the file name is usually used to indicate the **type** of the file, e.g., whether it is C code (e.g., `.c`), or an image (e.g., `.jpg`), or a music file (e.g., `.mp3`).
==第一部分是任意名称，而文件名的第二部分通常用于指示文件的**类型**，例如，它是 C 代码（如 `.c`）、图像（如 `.jpg`）还是音乐文件（如 `.mp3`）。==

However, this is usually just a **convention**: there is usually no enforcement that the data contained in a file named `main.c` is indeed C source code.
==然而，这通常只是一个**惯例**：通常并不强制要求名为 `main.c` 的文件中包含的数据确实是 C 源代码。==

Thus, we can see one great thing provided by the file system: a convenient way to **name** all the files we are interested in.
==因此，我们可以看到文件系统提供的一件伟大的事情：一种方便的方式来**命名**我们感兴趣的所有文件。==

Names are important in systems as the first step to accessing any resource is being able to name it.
==名称在系统中非常重要，因为访问任何资源的第一步就是能够对其命名。==

In UNIX systems, the file system thus provides a unified way to access files on disk, USB stick, CD-ROM, many other devices, and in fact many other things, all located under the single directory tree.
==因此，在 UNIX 系统中，文件系统提供了一种统一的方式来访问磁盘、USB 闪存盘、CD-ROM、许多其他设备以及实际上许多其他东西，所有这些都位于单一的目录树下。==

**39.2 The File System Interface**
==**39.2 文件系统接口**==

Let’s now discuss the file system interface in more detail.
==现在让我们更详细地讨论文件系统接口。==

We’ll start with the basics of creating, accessing, and deleting files.
==我们将从创建、访问和删除文件的基础知识开始。==

You may think this is straightforward, but along the way we’ll discover the mysterious call that is used to remove files, known as `unlink()`.
==你可能认为这很简单，但在此过程中我们将发现用于删除文件的神秘调用，称为 `unlink()`。==

Hopefully, by the end of this chapter, this mystery won’t be so mysterious to you!
==希望到本章结束时，这个秘密对你来说不再那么神秘！==

**39.3 Creating Files**
==**39.3 创建文件**==

We’ll start with the most basic of operations: creating a file.
==我们将从最基本的操作开始：创建一个文件。==

This can be accomplished with the `open` system call; by calling `open()` and passing it the `O_CREAT` flag, a program can create a new file.
==这可以通过 `open` 系统调用来完成；通过调用 `open()` 并传递 `O_CREAT` 标志，程序可以创建一个新文件。==

Here is some example code to create a file called “foo” in the current working directory:
==下面是在当前工作目录中创建名为“foo”的文件的示例代码：==

```c
int fd = open("foo", O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S_IWUSR);
```

**ASIDE: THE CREAT() SYSTEM CALL**
==**补充：CREAT() 系统调用**==

The older way of creating a file is to call `creat()`, as follows:
==创建文件的旧方法是调用 `creat()`，如下所示：==

```c
==// 选项：添加第二个标志以设置权限==
int fd = creat("foo");
```

You can think of `creat()` as `open()` with the following flags: `O_CREAT | O_WRONLY | O_TRUNC`.
==你可以将 `creat()` 视为带有以下标志的 `open()`：`O_CREAT | O_WRONLY | O_TRUNC`。==

Because `open()` can create a file, the usage of `creat()` has somewhat fallen out of favor (indeed, it could just be implemented as a library call to `open()`); however, it does hold a special place in UNIX lore.
==因为 `open()` 也可以创建文件，所以 `creat()` 的使用在某种程度上已经不再受青睐（实际上，它可以简单地实现为对 `open()` 的库调用）；然而，它在 UNIX 传说中确实占有特殊地位。==

Specifically, when Ken Thompson was asked what he would do differently if he were redesigning UNIX, he replied: “I’d spell creat with an e.”
==具体来说，当 Ken Thompson 被问到如果重新设计 UNIX 他会做哪些不同的改进时，他回答道：“我会给 creat 加个 e。”==

The routine `open()` takes a number of different flags.
==`open()` 例程接受许多不同的标志。==

In this example, the second parameter creates the file (`O_CREAT`) if it does not exist, ensures that the file can only be written to (`O_WRONLY`), and, if the file already exists, truncates it to a size of zero bytes thus removing any existing content (`O_TRUNC`).
==在本例中，第二个参数在文件不存在时创建文件（`O_CREAT`），确保文件只能被写入（`O_WRONLY`），并且如果文件已经存在，则将其截断为零字节，从而删除任何现有内容（`O_TRUNC`）。==

The third parameter specifies permissions, in this case making the file readable and writable by the owner.
==第三个参数指定权限，在本例中使文件对所有者可读写。==

One important aspect of `open()` is what it returns: a **file descriptor**.
==`open()` 的一个重要方面是它的返回值：一个**文件描述符**。==

A file descriptor is just an integer, private per process, and is used in UNIX systems to access files; thus, once a file is opened, you use the file descriptor to read or write the file, assuming you have permission to do so.
==文件描述符只是一个整数，对每个进程而言是私有的，在 UNIX 系统中用于访问文件；因此，一旦文件被打开，你就可以使用文件描述符来读取或写入文件，前提是你拥有相应的权限。==

In this way, a file descriptor is a **capability** [L84], i.e., an opaque handle that gives you the power to perform certain operations.
==通过这种方式，文件描述符是一种**能力**（capability）[L84]，即一个赋予你执行某些操作权限的不透明句柄。==

Another way to think of a file descriptor is as a pointer to an object of type file; once you have such an object, you can call other “methods” to access the file, like `read()` and `write()` (we’ll see how to do so below).
==另一种理解文件描述符的方式是将其视为指向文件类型对象的指针；一旦拥有了这样一个对象，你就可以调用其他“方法”来访问文件，例如 `read()` 和 `write()`（我们将在下面看到如何操作）。==

As stated above, file descriptors are managed by the operating system on a per-process basis.
==如上所述，文件描述符由操作系统在每个进程的基础上进行管理。==

This means some kind of simple structure (e.g., an array) is kept in the `proc` structure on UNIX systems.
==这意味着在 UNIX 系统的 `proc` 结构中保留了某种简单的结构（例如数组）。==

Here is the relevant piece from the xv6 kernel [CK+08]:
==以下是 xv6 内核中相关的部分 [CK+08]：==

```c
struct proc {
  ...
==  struct file *ofile[NOFILE]; // 已打开的文件==
  ...
};
```

A simple array (with a maximum of `NOFILE` open files), indexed by the file descriptor, tracks which files are opened on a per-process basis.
==一个简单的数组（最多可容纳 `NOFILE` 个打开文件），由文件描述符索引，在每个进程的基础上跟踪哪些文件已打开。==

Each entry of the array is actually just a pointer to a `struct file`, which will be used to track information about the file being read or written; we’ll discuss this further below.
==数组的每个条目实际上只是一个指向 `struct file` 的指针，它将用于跟踪正在读取或写入的文件信息；我们将在下文中进一步讨论。==




The declaration below shows the information available within each directory entry in the **struct dirent** data structure:
==下面的声明显示了 **struct dirent** 数据结构中每个目录项（directory entry）中可用的信息：==

```c
struct dirent {
    char           d_name[256]; // filename
    ino_t          d_ino;       // inode number
    off_t          d_off;       // offset to the next dirent
    unsigned short d_reclen;    // length of this record
    unsigned char  d_type;      // type of file
};
```
```c
struct dirent {
==    char           d_name[256]; // 文件名==
==    ino_t          d_ino;       // i 节点号==
==    off_t          d_off;       // 到下一个目录项的偏移量==
==    unsigned short d_reclen;    // 本条记录的长度==
==    unsigned char  d_type;      // 文件类型==
};
```

Because directories are light on information (basically, just mapping the name to the inode number, along with a few other details), a program may want to call **stat()** on each file to get more information on each, such as its length or other detailed information.
==由于目录包含的信息很少（基本上只是将名称映射到 i 节点号，以及其他一些细节），程序可能希望对每个文件调用 **stat()** 以获取有关每个文件的更多信息，例如其长度或其他详细信息。==

Indeed, this is exactly what **ls** does when you pass it the **-l** flag; try **strace** on **ls** with and without that flag to see for yourself.
==事实上，当你给 **ls** 传递 **-l** 标志时，它正是这么做的；你可以尝试对带和不带该标志的 **ls** 使用 **strace**，亲自查看效果。==

39.13 Deleting Directories
==39.13 删除目录==

Finally, you can delete a directory with a call to **rmdir()** (which is used by the program of the same name, **rmdir**).
==最后，你可以通过调用 **rmdir()** 来删除目录（同名程序 **rmdir** 使用的就是该调用）。==

Unlike file deletion, however, removing directories is more dangerous, as you could potentially delete a large amount of data with a single command.
==然而，与删除文件不同，删除目录更加危险，因为你可能会通过单个命令删除大量数据。==

Thus, **rmdir()** has the requirement that the directory be empty (i.e., only has "." and ".." entries) before it is deleted.
==因此，**rmdir()** 要求目录在被删除之前必须为空（即，仅包含“.”和“..”条目）。==

If you try to delete a non-empty directory, the call to **rmdir()** simply will fail.
==如果你尝试删除一个非空目录，对 **rmdir()** 的调用将直接失败。==

39.14 Hard Links
==39.14 硬链接==

We now come back to the mystery of why removing a file is performed via **unlink()**, by understanding a new way to make an entry in the file system tree, through a system call known as **link()**.
==通过理解一种在文件系统树中创建条目的新方法——即通过名为 **link()** 的系统调用，我们现在回到“为什么删除文件是通过 **unlink()** 执行”的谜题。==

The **link()** system call takes two arguments, an old pathname and a new one; when you "link" a new file name to an old one, you essentially create another way to refer to the same file.
==**link()** 系统调用接受两个参数：一个旧路径名和一个新路径名；当你将一个新文件名“链接”到一个旧文件名时，你实际上创建了引用同一个文件的另一种方式。==

The command-line program **ln** is used to do this, as we see in this example:
==命令行程序 **ln** 用于执行此操作，如我们在本例中所见：==

```
prompt> echo hello > file
prompt> cat file
hello
prompt> ln file file2
prompt> cat file2
hello
```
```
prompt> echo hello > file
prompt> cat file
hello
prompt> ln file file2
prompt> cat file2
hello
```

The way **link()** works is that it simply creates another name in the directory you are creating the link to, and refers it to the same inode number (i.e., low-level name) of the original file.
==**link()** 的工作原理是，它只是在你创建链接的目录中创建另一个名称，并将其指向原始文件的同一个 i 节点号（即低级名称）。==

The file is not copied in any way; rather, you now just have two human-readable names (**file** and **file2**) that both refer to the same file.
==文件本身并没有被以任何方式复制；相反，你现在只是有了两个人类可读的名称（**file** 和 **file2**），它们都指向同一个文件。==

We can even see this in the directory itself, by printing out the inode number of each file:
==我们甚至可以在目录本身中看到这一点，通过打印出每个文件的 i 节点号：==

```
prompt> ls -i file file2
67158084 file
67158084 file2
```
```
prompt> ls -i file file2
67158084 file
67158084 file2
```

By passing the **-i** flag to **ls**, it prints out the inode number of each file (as well as the file name).
==通过给 **ls** 传递 **-i** 标志，它会打印出每个文件的 i 节点号（以及文件名）。==

And thus you can see what link really has done: just make a new reference to the same exact inode number (67158084 in this example).
==因此，你可以看到链接真正做了什么：只是为完全相同的 i 节点号（本例中为 67158084）创建了一个新的引用。==

By now you might be starting to see why **unlink()** is called **unlink()**.
==到目前为止，你可能已经开始理解为什么 **unlink()** 被称为 **unlink()** 了。==

When you create a file, you are really doing two things.
==当你创建一个文件时，你实际上在做两件事。==

First, you are making a structure (the inode) that will track virtually all relevant information about the file, including its size, where its blocks are on disk, and so forth.
==首先，你正在创建一个结构（i 节点），它将跟踪几乎所有与文件相关的信息，包括其大小、其数据块在磁盘上的位置等等。==

Second, you are linking a human-readable name to that file, and putting that link into a directory.
==其次，你正在将一个人类可读的名称链接到该文件，并将该链接放入一个目录中。==

After creating a hard link to a file, the file system perceives no difference between the original file name (**file**) and the newly created file name (**file2**); indeed, they are both just links to the underlying metadata about the file, which is found in inode number 67158084.
==在为文件创建硬链接后，文件系统感知不到原始文件名（**file**）和新创建文件名（**file2**）之间有任何区别；事实上，它们都只是指向该文件底层元数据的链接，这些元数据位于 67158084 号 i 节点中。==

Thus, to remove a file from the file system, we call **unlink()**.
==因此，为了从文件系统中移除一个文件，我们调用 **unlink()**。==

In the example above, we could for example remove the file named **file**, and still access the file without difficulty:
==在上面的例子中，例如我们可以删除名为 **file** 的文件，但仍然可以毫无困难地访问该文件：==

```
prompt> rm file
removed 'file'
prompt> cat file2
hello
```
```
prompt> rm file
removed 'file'
prompt> cat file2
hello
```

The reason this works is because when the file system unlinks file, it checks a **reference count** within the inode number.
==这种方法之所以有效，是因为当文件系统取消对文件的链接时，它会检查 i 节点号中的**引用计数**（reference count）。==

This reference count (sometimes called the **link count**) allows the file system to track how many different file names have been linked to this particular inode.
==这个引用计数（有时称为**链接计数**）允许文件系统跟踪有多少个不同的文件名已链接到这个特定的 i 节点。==

When **unlink()** is called, it removes the "link" between the human-readable name (the file that is being deleted) to the given inode number, and decrements the reference count; only when the reference count reaches zero does the file system also free the inode and related data blocks, and thus truly "delete" the file.
==当 **unlink()** 被调用时，它会移除人类可读名称（正在被删除的文件）与给定 i 节点号之间的“链接”，并递减引用计数；只有当引用计数达到零时，文件系统才会释放 i 节点和相关的数据块，从而真正“删除”文件。==

You can see the reference count of a file using **stat()** of course.
==当然，你可以使用 **stat()** 查看文件的引用计数。==

39.15 Symbolic Links
==39.15 符号链接==

There is one other type of link that is really useful, and it is called a **symbolic link** or sometimes a **soft link**.
==还有另一种非常有用的链接类型，它被称为**符号链接**（symbolic link），有时也称为**软链接**（soft link）。==

Hard links are somewhat limited: you can't create one to a directory (for fear that you will create a cycle in the directory tree); you can't hard link to files in other disk partitions (because inode numbers are only unique within a particular file system, not across file systems); etc.
==硬链接有一定的局限性：你不能创建一个指向目录的硬链接（因为担心会在目录树中创建环路）；你不能硬链接到其他磁盘分区中的文件（因为 i 节点号仅在特定的文件系统内唯一，而不能跨文件系统唯一）；等等。==

Thus, a new type of link called the symbolic link was created.
==因此，一种称为符号链接的新型链接被创造了出来。==

To create such a link, you can use the same program **ln**, but with the **-s** flag.
==要创建这样的链接，你可以使用同一个程序 **ln**，但要加上 **-s** 标志。==

```
prompt> echo hello > file
prompt> ln -s file file2
prompt> cat file2
hello
```
```
prompt> echo hello > file
prompt> ln -s file file2
prompt> cat file2
hello
```

However, beyond this surface similarity, symbolic links are actually quite different from hard links.
==然而，除了这种表面上的相似性之外，符号链接实际上与硬链接大不相同。==

The first difference is that a symbolic link is actually a file itself, of a different type.
==第一个区别是，符号链接本身实际上是一个文件，属于不同的类型。==

The reason that **file2** is 4 bytes is because the way a symbolic link is formed is by holding the pathname of the linked-to file as the data of the link file.
==**file2** 为 4 字节的原因是，符号链接的形成方式是将链接目标的路径名作为链接文件的数据。==

Finally, because of the way symbolic links are created, they leave the possibility for what is known as a **dangling reference**:
==最后，由于符号链接的创建方式，它们留下了所谓**悬空引用**（dangling reference）的可能性：==

```
prompt> echo hello > file
prompt> ln -s file file2
prompt> cat file2
hello
prompt> rm file
prompt> cat file2
cat: file2: No such file or directory
```
```
prompt> echo hello > file
prompt> ln -s file file2
prompt> cat file2
hello
prompt> rm file
prompt> cat file2
cat: file2: No such file or directory
```

39.16 Permission Bits And Access Control Lists
==39.16 权限位与访问控制列表==

The file system also presents a virtual view of a disk, transforming it from a bunch of raw blocks into much more user-friendly files and directories.
==文件系统还呈现了磁盘的虚拟视图，将其从一堆原始块转换为更加用户友好的文件和目录。==

The first form of such mechanisms is the classic UNIX **permission bits**.
==这类机制的第一种形式是经典的 UNIX **权限位**。==

```
prompt> ls -l foo.txt
-rw-r--r-- 1 remzi wheel 0 Aug 24 16:29 foo.txt
```
```
prompt> ls -l foo.txt
-rw-r--r-- 1 remzi wheel 0 Aug 24 16:29 foo.txt
```

We are interested in the permission bits, which are represented by the next nine characters (**rw-r--r--**).
==我们感兴趣的是权限位，由接下来的九个字符（**rw-r--r--**）表示。==

These bits determine, for each regular file, directory, and other entities, exactly who can access it and how.
==这些位决定了对于每个常规文件、目录和其他实体，究竟谁可以访问它以及如何访问。==

The permissions consist of three groupings: what the **owner** of the file can do to it, what someone in a **group** can do to the file, and finally, what anyone (sometimes referred to as **other**) can do.
==权限由三组组成：文件的**所有者**（owner）可以对它做什么，**用户组**（group）中的成员可以对该文件做什么，以及最后，任何人（有时称为**其他人**，other）可以做什么。==

The abilities the owner, group member, or others can have include the ability to **read** the file, **write** it, or **execute** it.
==所有者、组成员或其他人可以拥有的能力包括**读取**（read）文件、**写入**（write）文件或**执行**（execute）文件的能力。==

The owner of the file can readily change these permissions, for example by using the **chmod** command.
==文件的所有者可以随时更改这些权限，例如通过使用 **chmod** 命令。==

For directories, the execute bit behaves a bit differently.
==对于目录，执行位的行为略有不同。==

Specifically, it enables a user (or group, or everyone) to do things like change directories (i.e., **cd**) into the given directory, and, in combination with the writable bit, create files therein.
==具体来说，它使授权用户（或组，或所有人）能够执行诸如进入（即 **cd**）给定目录之类的操作，并且结合写入位，可以在其中创建文件。==

Beyond permissions bits, some file systems include more sophisticated controls in the form of an **access control list (ACL)** per directory.
==除了权限位之外，一些文件系统还以每个目录的**访问控制列表（ACL）**的形式包含更复杂的控制。==

39.17 Making And Mounting A File System
==39.17 创建与挂载文件系统==

This task is accomplished via first **making** file systems, and then **mounting** them to make their contents accessible.
==这项任务通过首先**创建**文件系统，然后**挂载**它们以使其内容可访问来完成。==

To make a file system, most file systems provide a tool, usually referred to as **mkfs**.
==为了创建文件系统，大多数文件系统提供了一个工具，通常被称为 **mkfs**。==

However, once such a file system is created, it needs to be made accessible within the uniform file-system tree via the **mount** program.
==然而，一旦创建了这样的文件系统，就需要通过 **mount** 程序使其在统一的文件系统树中可被访问。==

What **mount** does, quite simply is take an existing directory as a target **mount point** and essentially paste a new file system onto the directory tree at that point.
==**mount** 的作用非常简单，就是将一个现有目录作为目标**挂载点**（mount point），并实质上在该点将一个新的文件系统“粘贴”到目录树上。==

```
prompt> mount -t ext3 /dev/sda1 /home/users
```
```
prompt> mount -t ext3 /dev/sda1 /home/users
```

TIP: BE WARY OF TOCTTOU
==提示：警惕 TOCTTOU==

We today call this the **Time Of Check To Time Of Use (TOCTTOU)** problem.
==我们今天称之为**检查时间与使用时间（TOCTTOU）**问题。==

The **O_NOFOLLOW** flag makes it so that **open()** will fail if the target is a symbolic link, thus avoiding attacks that require said links.
==**O_NOFOLLOW** 标志使得如果目标是符号链接，**open()** 将失败，从而避免了需要利用此类链接的攻击。==

40.2 Overall Organization
==40.2 整体组织==

The first thing we'll need to do is divide the disk into **blocks**; simple file systems use just one block size, and that’s exactly what we’ll do here.
==我们首先需要做的是将磁盘划分为**块**（blocks）；简单文件系统只使用一种块大小，这正是我们在这里要做的。==

Let’s choose a commonly-used size of 4 KB.
==让我们选择一个常用的 4 KB 大小。==

Assume we have a really small disk, with just 64 blocks.
==假设我们有一个非常小的磁盘，只有 64 个块。==

Let’s now think about what we need to store in these blocks to build a file system.
==现在让我们思考一下，为了构建一个文件系统，我们需要在这些块中存储什么。==

Of course, the first thing that comes to mind is user data.
==当然，首先想到的是用户数据。==

Let’s call the region of the disk we use for user data the **data region**.
==我们将磁盘上用于用户数据的区域称为**数据区域**（data region）。==

To store metadata, file systems usually have a structure called an **inode**.
==为了存储元数据，文件系统通常有一个称为 **inode**（i 节点）的结构。==

To accommodate inodes, we’ll need to reserve some space on the disk for them as well.
==为了容纳 i 节点，我们还需要在磁盘上为它们预留一些空间。==

Let’s call this portion of the disk the **inode table**, which simply holds an array of on-disk inodes.
==我们将磁盘的这部分称为 **inode 表**，它只是保存了一个磁盘上 i 节点的数组。==

One primary component that is still needed is some way to track whether inodes or data blocks are free or allocated.
==仍然需要的一个主要组件是某种跟踪 i 节点或数据块是空闲还是已分配的方法。==

We instead choose a simple and popular structure known as a **bitmap**, one for the data region (the **data bitmap**), and one for the inode table (the **inode bitmap**).
==相反，我们选择了一种简单且流行的结构，称为**位图**（bitmap），一个用于数据区域（**数据位图**），一个用于 i 节点表（**i 节点位图**）。==

A bitmap is a simple structure: each bit is used to indicate whether the corresponding object/block is free (0) or in-use (1).
==位图是一个简单的结构：每个位用于指示相应的对象/块是空闲（0）还是在使用中（1）。==

The **superblock** contains information about this particular file system, including, for example, how many inodes and data blocks are in the file system.
==**超级块**（superblock）包含有关此特定文件系统的信息，例如文件系统中有多少个 i 节点和数据块。==

40.3 File Organization: The Inode
==40.3 文件组织：i 节点==

One of the most important on-disk structures of a file system is the **inode**; virtually all file systems have a structure similar to this.
==文件系统最重要的磁盘结构之一是 **inode**；几乎所有的文件系统都有类似的结构。==

The name inode is short for **index node**.
==inode 这个名字是**索引节点**（index node）的缩写。==

Each inode is implicitly referred to by a number (called the **i-number**), which we’ve earlier called the **low-level name** of the file.
==每个 i 节点都通过一个数字（称为 **i 节点号**）隐式引用，我们之前称之为文件的**低级名称**。==

The sector address $sector$ of the inode block can be calculated as follows:
==i 节点块的扇区地址 $sector$ 可以如下计算：==

$blk = (inumber \times sizeof(inode\_t)) / blockSize$
$blk = (inumber \times sizeof(inode\_t)) / blockSize$

$sector = ((blk \times blockSize) + inodeStartAddr) / sectorSize$
$sector = ((blk \times blockSize) + inodeStartAddr) / sectorSize$

One simple approach would be to have one or more **direct pointers** (disk addresses) inside the inode; each pointer refers to one disk block that belongs to the file.
==一种简单的方法是在 i 节点内部设有一个或多个**直接指针**（direct pointers，磁盘地址）；每个指针指向属于该文件的一个磁盘块。==

To support bigger files, file system designers have had to introduce different structures within inodes.
==为了支持更大的文件，文件系统设计者不得不在 i 节点内引入不同的结构。==

One common idea is to have a special pointer known as an **indirect pointer**.
==一个常见的想法是使用一种特殊的指针，称为**间接指针**（indirect pointer）。==

Assuming 4-KB blocks and 4-byte disk addresses, the file can grow to be $(12 + 1024) \times 4\text{KB}$ or $4144\text{KB}$.
==假设块大小为 4 KB，磁盘地址为 4 字节，文件大小可以增长到 $(12 + 1024) \times 4\text{KB}$，即 $4144\text{KB}$。==

A different approach is to use **extents** instead of pointers.
==另一种方法是使用**范围**（extents）而不是指针。==

An extent is simply a disk pointer plus a length (in blocks).
==一个范围（extent）仅仅是一个磁盘指针加上一个长度（以块为单位）。==

Overall, this imbalanced tree is referred to as the **multi-level index** approach to pointing to file blocks.
==总的来说，这种不平衡树被称为指向文件块的**多级索引**（multi-level index）方法。==

40.4 Directory Organization
==40.4 目录组织==

In **vsfs** (as in many file systems), directories have a simple organization; a directory basically just contains a list of (entry name, inode number) pairs.
==在 **vsfs** 中（如在许多文件系统中），目录具有简单的组织方式；目录基本上只包含一组（条目名称，i 节点号）对。==




ASIDE: LINKED-BASED APPROACHES
==旁注：基于链接的方法==

Another simpler approach in designing inodes is to use a **linked list**.
==设计 inode 的另一种更简单的方法是使用**链表**。==

Thus, inside an inode, instead of having multiple pointers, you just need one, to point to the first block of the file.
==因此，在 inode 内部，你不需要多个指针，只需要一个指针指向文件的第一个块。==

To handle larger files, add another pointer at the end of that data block, and so on, and thus you can support large files.
==为了处理更大的文件，在那个数据块的末尾添加另一个指针，依此类推，从而支持大文件。==

As you might have guessed, linked file allocation performs poorly for some workloads; think about reading the last block of a file, for example, or just doing random access.
==正如你可能猜到的，链接文件分配在某些工作负载下表现不佳；例如，考虑读取文件的最后一个块，或者仅仅是进行随机访问。==

Thus, to make linked allocation work better, some systems will keep an in-memory table of link information, instead of storing the next pointers with the data blocks themselves.
==因此，为了使链接分配更好地工作，一些系统会在内存中保留一张链接信息表，而不是将下一个指针与数据块本身存储在一起。==

The table is indexed by the address of a data block $D$; the content of an entry is simply $D$’s next pointer, i.e., the address of the next block in a file which follows $D$.
==该表以数据块 $D$ 的地址为索引；条目的内容仅仅是 $D$ 的下一个指针，即紧随 $D$ 之后的文件中下一个块的地址。==

A null-value could be there too (indicating an end-of-file), or some other marker to indicate that a particular block is free.
==那里也可以有一个空值（表示文件结束），或者其他一些标记来表示某个特定块是空闲的。==

Having such a table of next pointers makes it so that a linked allocation scheme can effectively do random file accesses, simply by first scanning through the (in memory) table to find the desired block, and then accessing (on disk) it directly.
==拥有这样一张“下一指针表”，使得链接分配方案能够有效地进行随机文件访问，只需首先扫描（内存中的）表以找到所需的块，然后直接访问（磁盘上的）该块。==

Does such a table sound familiar?
==这样的表听起来熟悉吗？==

What we have described is the basic structure of what is known as the **file allocation table**, or **FAT** file system.
==我们所描述的是所谓的**文件分配表**（**FAT**）文件系统的基本结构。==

Yes, this classic old Windows file system, before NTFS [C94], is based on a simple linked-based allocation scheme.
==是的，这个在 NTFS [C94] 之前的经典老式 Windows 文件系统，就是基于一种简单的链接分配方案。==

There are other differences from a standard UNIX file system too; for example, there are no inodes per se, but rather directory entries which store metadata about a file and refer directly to the first block of said file, which makes creating hard links impossible.
==与标准 UNIX 文件系统还有其他不同之处；例如，它本身没有 inode，而是通过目录项存储文件的元数据并直接引用该文件的第一个块，这使得创建硬链接变得不可能。==

See Brouwer [B02] for more of the inelegant details.
==有关更多不雅的细节，请参阅 Brouwer [B02]。==

You might be wondering where exactly directories are stored.
==你可能想知道目录到底存储在哪里。==

Often, file systems treat directories as a special type of file.
==通常，文件系统将目录视为一种特殊类型的文件。==

Thus, a directory has an inode, somewhere in the inode table (with the type field of the inode marked as “directory” instead of “regular file”).
==因此，目录有一个 inode，位于 inode 表中的某个位置（其 inode 的类型字段被标记为“目录”而不是“普通文件”）。==

The directory has data blocks pointed to by the inode (and perhaps, indirect blocks); these data blocks live in the data block region of our simple file system.
==该目录具有由 inode 指向的数据块（可能还有间接块）；这些数据块位于我们简单文件系统的数据块区域中。==

Our on-disk structure thus remains unchanged.
==因此，我们的磁盘结构保持不变。==

We should also note again that this simple linear list of directory entries is not the only way to store such information.
==我们还应该再次注意，这种简单的目录项线性列表并不是存储此类信息的唯一方法。==

As before, any data structure is possible.
==和以前一样，任何数据结构都是可能的。==

For example, XFS [S+96] stores directories in **B-tree** form, making file create operations (which have to ensure that a file name has not been used before creating it) faster than systems with simple lists that must be scanned in their entirety.
==例如，XFS [S+96] 以 **B 树**形式存储目录，这使得文件创建操作（必须在创建之前确保文件名未被使用）比必须完整扫描的简单列表系统更快。==

ASIDE: FREE SPACE MANAGEMENT
==旁注：空闲空间管理==

There are many ways to manage free space; **bitmaps** are just one way.
==管理空闲空间有很多方法；**位图**只是其中一种。==

Some early file systems used **free lists**, where a single pointer in the super block was kept to point to the first free block; inside that block the next free pointer was kept, thus forming a list through the free blocks of the system.
==一些早期的文件系统使用**空闲列表**，在超级块中保留一个指针指向第一个空闲块；在该块内部保留下一个空闲指针，从而形成贯穿系统空闲块的列表。==

When a block was needed, the head block was used and the list updated accordingly.
==当需要一个块时，使用头块并相应地更新列表。==

Modern file systems use more sophisticated data structures.
==现代文件系统使用更复杂的数据结构。==

For example, SGI’s XFS [S+96] uses some form of a **B-tree** to compactly represent which chunks of the disk are free.
==例如，SGI 的 XFS [S+96] 使用某种形式的 **B 树**来紧凑地表示磁盘的哪些部分是空闲的。==

As with any data structure, different time-space trade-offs are possible.
==与任何数据结构一样，不同的时空折衷是可能的。==

40.5 Free Space Management
==40.5 空闲空间管理==

A file system must track which inodes and data blocks are free, and which are not, so that when a new file or directory is allocated, it can find space for it.
==文件系统必须追踪哪些 inode 和数据块是空闲的，哪些不是，以便在分配新文件或目录时，它能为其找到空间。==

Thus **free space management** is important for all file systems.
==因此，**空闲空间管理**对所有文件系统都很重要。==

In vsfs, we have two simple **bitmaps** for this task.
==在 vsfs 中，我们有两个简单的**位图**来完成这项任务。==

For example, when we create a file, we will have to allocate an inode for that file.
==例如，当我们创建一个文件时，我们必须为该文件分配一个 inode。==

The file system will thus search through the bitmap for an inode that is free, and allocate it to the file; the file system will have to mark the inode as used (with a 1) and eventually update the on-disk bitmap with the correct information.
==因此，文件系统将在位图中搜索空闲的 inode，并将其分配给文件；文件系统必须将该 inode 标记为已使用（用 1 表示），并最终用正确的信息更新磁盘上的位图。==

A similar set of activities take place when a data block is allocated.
==当分配数据块时，会发生一系列类似的活动。==

Some other considerations might also come into play when allocating data blocks for a new file.
==在为新文件分配数据块时，还可能涉及其他一些考虑因素。==

For example, some Linux file systems, such as ext2 and ext3, will look for a sequence of blocks (say 8) that are free when a new file is created and needs data blocks; by finding such a sequence of free blocks, and then allocating them to the newly-created file, the file system guarantees that a portion of the file will be contiguous on the disk, thus improving performance.
==例如，一些 Linux 文件系统（如 ext2 和 ext3）在创建新文件并需要数据块时，会寻找一系列空闲块（例如 8 个）；通过寻找这样一系列空闲块并将其分配给新创建的文件，文件系统保证了文件的一部分在磁盘上是连续的，从而提高了性能。==

Such a **pre-allocation** policy is thus a commonly-used heuristic when allocating space for data blocks.
==因此，这种**预分配**策略是分配数据块空间时常用的一种启发式方法。==

40.6 Access Paths: Reading and Writing
==40.6 访问路径：读取和写入==

Now that we have some idea of how files and directories are stored on disk, we should be able to follow the flow of operation during the activity of reading or writing a file.
==既然我们已经了解了文件和目录在磁盘上的存储方式，我们应该能够跟踪读取或写入文件活动期间的操作流程。==

Understanding what happens on this **access path** is thus the second key in developing an understanding of how a file system works; pay attention!
==因此，了解这条**访问路径**上发生的事情，是深入理解文件系统如何工作的第二个关键；请注意！==

For the following examples, let us assume that the file system has been mounted and thus that the superblock is already in memory.
==对于以下示例，让我们假设文件系统已经挂载，因此超级块已经在内存中。==

Everything else (i.e., inodes, directories) is still on the disk.
==其他所有内容（即 inode、目录）仍位于磁盘上。==

Reading A File From Disk
==从磁盘读取文件==

In this simple example, let us first assume that you want to simply open a file (e.g., `/foo/bar`), read it, and then close it.
==在这个简单的示例中，让我们首先假设你只想打开一个文件（例如 `/foo/bar`），读取它，然后关闭它。==

For this simple example, let’s assume the file is just 12KB in size (i.e., 3 blocks).
==对于这个简单的例子，假设文件的大小只有 12KB（即 3 个块）。==

When you issue an `open("/foo/bar", O_RDONLY)` call, the file system first needs to find the inode for the file `bar`, to obtain some basic information about the file (permissions information, file size, etc.).
==当你发出 `open("/foo/bar", O_RDONLY)` 调用时，文件系统首先需要找到文件 `bar` 的 inode，以获取有关该文件的一些基本信息（权限信息、文件大小等）。==

To do so, the file system must be able to find the inode, but all it has right now is the full pathname.
==为此，文件系统必须能够找到 inode，但它现在只有完整的路径名。==

The file system must **traverse** the pathname and thus locate the desired inode.
==文件系统必须**遍历**路径名，从而定位所需的 inode。==

All traversals begin at the root of the file system, in the **root directory** which is simply called `/`.
==所有遍历都从文件系统的根目录开始，即**根目录**，简称为 `/`。==

Thus, the first thing the FS will read from disk is the inode of the root directory.
==因此，文件系统（FS）从磁盘读取的第一件事就是根目录的 inode。==

But where is this inode?
==但是这个 inode 在哪里呢？==

To find an inode, we must know its i-number.
==要找到一个 inode，我们必须知道它的 i-number（索引节点号）。==

Usually, we find the i-number of a file or directory in its parent directory; the root has no parent (by definition).
==通常，我们在文件或目录的父目录中找到其 i-number；而根目录没有父目录（根据定义）。==

Thus, the root inode number must be “well known”; the FS must know what it is when the file system is mounted.
==因此，根 inode 编号必须是“众所周知”的；文件系统必须在挂载时知道它是什么。==

In most UNIX file systems, the root inode number is 2.
==在大多数 UNIX 文件系统中，根 inode 编号是 2。==

Thus, to begin the process, the FS reads in the block that contains inode number 2 (the first inode block).
==因此，为了开始这个过程，文件系统读入包含 inode 编号 2 的块（第一个 inode 块）。==

Once the inode is read in, the FS can look inside of it to find pointers to data blocks, which contain the contents of the root directory.
==一旦读入 inode，文件系统就可以查看其内部以找到指向数据块的指针，这些数据块包含根目录的内容。==

The FS will thus use these on-disk pointers to read through the directory, in this case looking for an entry for `foo`.
==因此，文件系统将使用这些磁盘上的指针来读取目录，在本例中是寻找 `foo` 的条目。==

By reading in one or more directory data blocks, it will find the entry for `foo`; once found, the FS will also have found the inode number of `foo` (say it is 44) which it will need next.
==通过读入一个或多个目录数据块，它将找到 `foo` 的条目；一旦找到，文件系统也将找到 `foo` 的 inode 编号（假设为 44），这是它下一步需要的。==

The next step is to recursively traverse the pathname until the desired inode is found.
==下一步是递归遍历路径名，直到找到所需的 inode。==

In this example, the FS reads the block containing the inode of `foo` and then its directory data, finally finding the inode number of `bar`.
==在此示例中，文件系统读取包含 `foo` 的 inode 的块，然后读取其目录数据，最终找到 `bar` 的 inode 编号。==

The final step of `open()` is to read `bar`’s inode into memory; the FS then does a final permissions check, allocates a file descriptor for this process in the per-process open-file table, and returns it to the user.
==`open()` 的最后一步是将 `bar` 的 inode 读入内存；然后，文件系统进行最终的权限检查，在每个进程的打开文件表中为此进程分配一个文件描述符，并将其返回给用户。==

Once open, the program can then issue a `read()` system call to read from the file.
==一旦打开，程序就可以发出 `read()` 系统调用来从文件中读取。==

The first read (at offset 0 unless `lseek()` has been called) will thus read in the first block of the file, consulting the inode to find the location of such a block; it may also update the inode with a new last-accessed time.
==因此，第一次读取（偏移量为 0，除非调用了 `lseek()`）将读入文件的第一个块，并咨询 inode 以查找该块的位置；它还可能用新的最后访问时间更新 inode。==

The read will further update the in-memory open file table for this file descriptor, updating the file offset such that the next read will read the second file block, etc.
==读取操作将进一步更新此文件描述符的内存中打开文件表，更新文件偏移量，以便下一次读取将读取第二个文件块，依此类推。==

At some point, the file will be closed.
==在某个时刻，文件将被关闭。==

There is much less work to be done here; clearly, the file descriptor should be deallocated, but for now, that is all the FS really needs to do.
==这里要做的工作少得多；显然，文件描述符应该被释放，但目前，这就是文件系统真正需要做的全部工作。==

No disk I/Os take place.
==不发生磁盘 I/O。==

A depiction of this entire process is found in Figure 40.3 (page 11); time increases downward in the figure.
==图 40.3（第 11 页）描绘了整个过程；图中时间向下递增。==

In the figure, the open causes numerous reads to take place in order to finally locate the inode of the file.
==在图中，open 导致发生了多次读取，以便最终定位文件的 inode。==

Afterwards, reading each block requires the file system to first consult the inode, then read the block, and then update the inode’s last-accessed-time field with a write.
==随后，读取每个块都需要文件系统首先咨询 inode，然后读取该块，最后通过写入更新 inode 的最后访问时间字段。==

Spend some time and understand what is going on.
==花点时间理解发生了什么。==

Also note that the amount of I/O generated by the open is proportional to the length of the pathname.
==还请注意，open 生成的 I/O 量与路径名的长度成正比。==

For each additional directory in the path, we have to read its inode as well as its data.
==路径中每增加一个目录，我们都必须读取其 inode 及其数据。==

Making this worse would be the presence of large directories; here, we only have to read one block to get the contents of a directory, whereas with a large directory, we might have to read many data blocks to find the desired entry.
==大目录的存在会使情况变得更糟；在这里，我们只需要读取一个块就能获得一个目录的内容，而对于大目录，我们可能需要读取许多数据块才能找到所需的条目。==

Yes, life can get pretty bad when reading a file; as you’re about to find out, writing out a file (and especially, creating a new one) is even worse.
==是的，读取文件时情况可能会变得非常糟糕；正如你即将发现的那样，写出文件（尤其是创建一个新文件）的情况甚至更糟。==

Writing A File To Disk
==将文件写入磁盘==

Writing to a file is a similar process.
==写入文件是一个类似的过程。==

First, the file must be opened (as above).
==首先，必须打开文件（如上所述）。==

Then, the application can issue `write()` calls to update the file with new contents.
==然后，应用程序可以发出 `write()` 调用来使用新内容更新文件。==

Finally, the file is closed.
==最后，关闭文件。==

Unlike reading, writing to the file may also allocate a block (unless the block is being overwritten, for example).
==与读取不同，写入文件还可能分配一个块（例如，除非该块正在被覆盖）。==

When writing out a new file, each write not only has to write data to disk but has to first decide which block to allocate to the file and thus update other structures of the disk accordingly (e.g., the data bitmap and inode).
==写出一个新文件时，每次写入不仅必须将数据写入磁盘，还必须首先决定将哪个块分配给该文件，并相应地更新磁盘的其他结构（例如，数据位图和 inode）。==

Thus, each write to a file logically generates five I/Os: one to read the data bitmap (which is then updated to mark the newly-allocated block as used), one to write the bitmap (to reflect its new state to disk), two more to read and then write the inode (which is updated with the new block’s location), and finally one to write the actual block itself.
==因此，逻辑上对文件的每次写入都会产生五个 I/O：一个用于读取数据位图（然后更新该位图以将新分配的块标记为已使用），一个用于写入位图（以向磁盘反映其新状态），另外两个用于读取然后写入 inode（更新新块的位置），最后是一个用于写入实际块本身。==

The amount of write traffic is even worse when one considers a simple and common operation such as file creation.
==考虑到文件创建等简单且常见的操作时，写入流量的情况甚至更糟。==

To create a file, the file system must not only allocate an inode, but also allocate space within the directory containing the new file.
==为了创建文件，文件系统不仅必须分配一个 inode，还必须在包含新文件的目录中分配空间。==

The total amount of I/O traffic to do so is quite high: one read to the inode bitmap (to find a free inode), one write to the inode bitmap (to mark it allocated), one write to the new inode itself (to initialize it), one to the data of the directory (to link the high-level name of the file to its inode number), and one read and write to the directory inode to update it.
==这样做产生的总 I/O 流量相当高：一次读取 inode 位图（以查找空闲 inode），一次写入 inode 位图（将其标记为已分配），一次写入新 inode 本身（对其进行初始化），一次写入目录数据（将文件的高级名称与其 inode 编号链接），以及一次读取和写入目录 inode 以更新它。==

If the directory needs to grow to accommodate the new entry, additional I/Os (i.e., to the data bitmap, and the new directory block) will be needed too.
==如果目录需要增长以容纳新条目，则还需要额外的 I/O（即，对数据位图和新目录块的操作）。==

All that just to create a file!
==这一切仅仅是为了创建一个文件！==

40.7 Caching and Buffering
==40.7 缓存与缓冲==

As the examples above show, reading and writing files can be expensive, incurring many I/Os to the (slow) disk.
==正如上面的例子所示，读取和写入文件可能非常昂贵，会对（缓慢的）磁盘产生多次 I/O。==

To remedy what would clearly be a huge performance problem, most file systems aggressively use system memory (DRAM) to cache important blocks.
==为了补救显然巨大的性能问题，大多数文件系统会积极使用系统内存（DRAM）来缓存重要块。==

Imagine the open example above: without caching, every file open would require at least two reads for every level in the directory hierarchy (one to read the inode of the directory in question, and at least one to read its data).
==想象一下上面的打开示例：如果没有缓存，每次打开文件都需要对目录层次结构中的每一层至少进行两次读取（一次读取相关目录的 inode，至少一次读取其数据）。==

With a long pathname (e.g., `/1/2/3/ ... /100/file.txt`), the file system would literally perform hundreds of reads just to open the file!
==对于长路径名（例如 `/1/2/3/ ... /100/file.txt`），文件系统为了打开文件确实会执行数百次读取！==

Early file systems thus introduced a **fixed-size cache** to hold popular blocks.
==因此，早期的文件系统引入了**固定大小的缓存**来保存常用块。==

As in our discussion of virtual memory, strategies such as **LRU** and different variants would decide which blocks to keep in cache.
==正如我们在虚拟内存讨论中所述，**LRU** 等策略及其各种变体会决定哪些块保留在缓存中。==

This fixed-size cache would usually be allocated at boot time to be roughly 10% of total memory.
==这种固定大小的缓存通常在引导时分配，约占总内存的 10%。==

This **static partitioning** of memory, however, can be wasteful; what if the file system doesn’t need 10% of memory at a given point in time?
==然而，这种内存的**静态分区**可能会造成浪费；如果文件系统在特定时间点不需要 10% 的内存怎么办？==

With the fixed-size approach described above, unused pages in the file cache cannot be re-purposed for some other use, and thus go to waste.
==使用上述固定大小的方法，文件缓存中未使用的页面不能重新用于其他用途，从而造成浪费。==

Modern systems, in contrast, employ a **dynamic partitioning** approach.
==相比之下，现代系统采用**动态分区**方法。==

Specifically, many modern operating systems integrate virtual memory pages and file system pages into a **unified page cache** [S00].
==具体而言，许多现代操作系统将虚拟内存页和文件系统页集成到一个**统一页面缓存** [S00] 中。==

In this way, memory can be allocated more flexibly across virtual memory and file system, depending on which needs more memory at a given time.
==通过这种方式，内存可以更灵活地在虚拟内存和文件系统之间分配，具体取决于给定时间哪方需要更多内存。==

Now imagine the file open example with caching.
==现在想象一下带缓存的文件打开示例。==

The first open may generate a lot of I/O traffic to read in directory inode and data, but subsequent file opens of that same file (or files in the same directory) will mostly hit in the cache and thus no I/O is needed.
==第一次打开可能会产生大量的 I/O 流量来读入目录 inode 和数据，但随后对同一文件（或同一目录下的文件）的打开将大部分命中缓存，因此不需要 I/O。==

Let us also consider the effect of caching on writes.
==让我们也考虑一下缓存对写入的影响。==

Whereas read I/O can be avoided altogether with a sufficiently large cache, write traffic has to go to disk in order to become persistent.
==虽然通过足够大的缓存可以完全避免读取 I/O，但写入流量必须进入磁盘才能实现持久化。==

Thus, a cache does not serve as the same kind of filter on write traffic that it does for reads.
==因此，缓存对写入流量的过滤作用与对读取流量的不同。==

That said, **write buffering** (as it is sometimes called) certainly has a number of performance benefits.
==即便如此，**写入缓冲**（有时也这么叫）确实有很多性能上的好处。==

First, by delaying writes, the file system can **batch** some updates into a smaller set of I/Os; for example, if an inode bitmap is updated when one file is created and then updated moments later as another file is created, the file system saves an I/O by delaying the write after the first update.
==首先，通过延迟写入，文件系统可以将一些更新**批量处理**成更少的 I/O；例如，如果在创建一个文件时更新了 inode 位图，稍后在创建另一个文件时又更新了它，文件系统通过在第一次更新后延迟写入，节省了一次 I/O。==

Second, by buffering a number of writes in memory, the system can then **schedule** the subsequent I/Os and thus increase performance.
==其次，通过在内存中缓冲多次写入，系统随后可以对后续 I/O 进行**调度**，从而提高性能。==

Finally, some writes are avoided altogether by delaying them; for example, if an application creates a file and then deletes it, delaying the writes to reflect the file creation to disk avoids them entirely.
==最后，通过延迟写入，可以完全避免某些写入；例如，如果应用程序创建了一个文件然后又将其删除，通过延迟反映文件创建到磁盘的写入，可以完全避免这些写入。==

In this case, laziness (in writing blocks to disk) is a virtue.
==在这种情况下，（在将块写入磁盘方面的）懒惰是一种美德。==

For the reasons above, most modern file systems buffer writes in memory for anywhere between five and thirty seconds, representing yet another trade-off: if the system crashes before the updates have been propagated to disk, the updates are lost; however, by keeping writes in memory longer, performance can be improved by batching, scheduling, and even avoiding writes.
==由于上述原因，大多数现代文件系统在内存中缓冲写入的时间在 5 到 30 秒之间，这代表了另一种折衷：如果系统在更新传播到磁盘之前崩溃，更新就会丢失；然而，通过将写入在内存中保留更长时间，可以通过批量处理、调度甚至避免写入来提高性能。==

TIP: UNDERSTAND THE DURABILITY/PERFORMANCE TRADE-OFF
==提示：理解持久性/性能的折衷==

Storage systems often present a durability/performance trade-off to users.
==存储系统经常向用户呈现持久性与性能之间的折衷。==

If the user wishes data that is written to be immediately durable, the system must go through the full effort of committing the newly-written data to disk, and thus the write is slow (but safe).
==如果用户希望写入的数据立即持久化，系统必须全力将新写入的数据提交到磁盘，因此写入速度慢（但安全）。==

However, if the user can tolerate the loss of a little data, the system can buffer writes in memory for some time and write them later to the disk (in the background).
==然而，如果用户可以容忍丢失少量数据，系统可以在内存中缓冲写入一段时间，稍后再将其写入磁盘（在后台进行）。==

Doing so makes writes appear to complete quickly, thus improving perceived performance; however, if a crash occurs, writes not yet committed to disk will be lost, and hence the trade-off.
==这样做使写入看起来很快完成，从而提高了感知性能；然而，如果发生崩溃，尚未提交到磁盘的写入将会丢失，这就是折衷所在。==

To understand how to make this trade-off properly, it is best to understand what the application using the storage system requires; for example, while it may be tolerable to lose the last few images downloaded by your web browser, losing part of a database transaction that is adding money to your bank account may be less tolerable.
==为了理解如何妥善处理这种折衷，最好了解使用存储系统的应用程序的需求；例如，虽然丢失网页浏览器下载的最后几张图片可能是可以忍受的，但丢失向你的银行账户加钱的部分数据库事务可能就不那么可以忍受了。==

Unless you’re rich, of course; in that case, why do you care so much about hoarding every last penny?
==当然，除非你很有钱；在那这种情况下，你为什么要如此在意囤积每一分钱呢？==

Some applications (such as databases) don’t enjoy this trade-off.
==某些应用程序（如数据库）并不喜欢这种折衷。==

Thus, to avoid unexpected data loss due to write buffering, they simply force writes to disk, by calling `fsync()`, by using **direct I/O** interfaces that work around the cache, or by using the **raw disk** interface and avoiding the file system altogether.
==因此，为了避免由于写入缓冲导致的意外数据丢失，它们通过调用 `fsync()`、使用绕过缓存的**直接 I/O** 接口，或者使用**原始磁盘**接口并完全避开文件系统，简单地强制将数据写入磁盘。==

While most applications live with the trade-offs made by the file system, there are enough controls in place to get the system to do what you want it to, should the default not be satisfying.
==虽然大多数应用程序接受文件系统所做的折衷，但如果有足够多的控制措施，如果默认设置不令人满意，也可以让系统执行你想要的操作。==

40.8 Summary
==40.8 总结==

We have seen the basic machinery required in building a file system.
==我们已经看到了构建文件系统所需的基本机制。==

There needs to be some information about each file (metadata), usually stored in a structure called an **inode**.
==需要有关于每个文件的一些信息（元数据），通常存储在一个称为 **inode** 的结构中。==

Directories are just a specific type of file that store name $\rightarrow$ inode-number mappings.
==目录只是一种特殊类型的文件，存储“名称 $\rightarrow$ inode 编号”的映射。==

And other structures are needed too; for example, file systems often use a structure such as a **bitmap** to track which inodes or data blocks are free or allocated.
==还需要其他结构；例如，文件系统经常使用诸如**位图**之类的结构来追踪哪些 inode 或数据块是空闲的或已分配的。==

Locality and The Fast File System
==局部性与快速文件系统==

When the UNIX operating system was first introduced, the UNIX wizard himself Ken Thompson wrote the first file system.
==当 UNIX 操作系统首次推出时，UNIX 奇才肯·汤普森亲自编写了第一个文件系统。==

Let’s call that the “old UNIX file system”, and it was really simple.
==让我们称之为“旧 UNIX 文件系统”，它非常简单。==

Basically, its data structures looked like this on the disk:
==基本上，它在磁盘上的数据结构如下所示：==

`S | Inodes | Data`
`S | Inodes | Data`

The super block (S) contained information about the entire file system: how big the volume is, how many inodes there are, a pointer to the head of a free list of blocks, and so forth.
==超级块 (S) 包含有关整个文件系统的信息：卷有多大、有多少个 inode、指向块空闲列表头部的指针，等等。==

The inode region of the disk contained all the inodes for the file system.
==磁盘的 inode 区域包含了文件系统的所有 inode。==

Finally, most of the disk was taken up by data blocks.
==最后，磁盘的大部分空间被数据块占用。==

The good thing about the old file system was that it was simple, and supported the basic abstractions the file system was trying to deliver: files and the directory hierarchy.
==旧文件系统的优点在于它很简单，并且支持文件系统试图提供的基本抽象：文件和目录层次结构。==

This easy-to-use system was a real step forward from the clumsy, record-based storage systems of the past, and the directory hierarchy was a true advance over simpler, one-level hierarchies provided by earlier systems.
==这个易于使用的系统是对过去笨拙的、基于记录的存储系统的真正进步，而且目录层次结构比早期系统提供的更简单的单层层次结构更是一种真正的进步。==

41.1 The Problem: Poor Performance
==41.1 问题：性能低下==

The problem: performance was terrible.
==问题在于：性能非常糟糕。==

As measured by Kirk McKusick and his colleagues at Berkeley [MJLF84], performance started off bad and got worse over time, to the point where the file system was delivering only 2% of overall disk bandwidth!
==根据 Kirk McKusick 及其在伯克利的同事 [MJLF84] 的测量，性能起初就很差，并随着时间的推移而恶化，甚至到了文件系统仅能提供 2% 的总磁盘带宽的程度！==

The main issue was that the old UNIX file system treated the disk like it was a random-access memory; data was spread all over the place without regard to the fact that the medium holding the data was a disk, and thus had real and expensive positioning costs.
==主要问题在于，旧的 UNIX 文件系统将磁盘视为随机存取存储器；数据遍布各处，而无视承载数据的介质是磁盘这一事实，因此具有真实且昂贵的定位成本。==

For example, the data blocks of a file were often very far away from its inode, thus inducing an expensive seek whenever one first read the inode and then the data blocks of a file (a pretty common operation).
==例如，文件的数据块通常与其 inode 相距甚远，因此每当先读取 inode 然后读取文件数据块（这是一种非常常见的操作）时，都会引发昂贵的寻道。==

Worse, the file system would end up getting quite fragmented, as the free space was not carefully managed.
==更糟糕的是，由于空闲空间没有得到仔细管理，文件系统最终会变得非常碎片化。==

The free list would end up pointing to a bunch of blocks spread across the disk, and as files got allocated, they would simply take the next free block.
==空闲列表最终会指向分布在磁盘上的一堆块，当文件被分配时，它们只会占用下一个空闲块。==

The result was that a logically contiguous file would be accessed by going back and forth across the disk, thus reducing performance dramatically.
==结果是，在访问逻辑上连续的文件时，会在磁盘上前后移动，从而极大地降低性能。==

One other problem: the original block size was too small (512 bytes).
==另一个问题：原始块大小太小（512 字节）。==

Thus, transferring data from the disk was inherently inefficient.
==因此，从磁盘传输数据本质上是低效的。==

Smaller blocks were good because they minimized **internal fragmentation** (waste within the block), but bad for transfer as each block might require a positioning overhead to reach it.
==较小的块虽好，因为它们最小化了**内部碎片**（块内的浪费），但对于传输却不利，因为每个块都可能需要定位开销才能触达。==

41.2 FFS: Disk Awareness Is The Solution
==41.2 FFS：磁盘感知是解决方案==

A group at Berkeley decided to build a better, faster file system, which they cleverly called the **Fast File System (FFS)**.
==伯克利的一个小组决定构建一个更好、更快捷的文件系统，他们聪明地称之为**快速文件系统 (FFS)**。==

The idea was to design the file system structures and allocation policies to be “disk aware” and thus improve performance, which is exactly what they did.
==其想法是设计“磁盘感知”的文件系统结构和分配策略，从而提高性能，而这正是他们所做的。==

41.3 Organizing Structure: The Cylinder Group
==41.3 组织结构：柱面组==

The first step was to change the on-disk structures.
==第一步是更改磁盘上的结构。==

FFS divides the disk into a number of **cylinder groups**.
==FFS 将磁盘分成若干个**柱面组**。==

A single **cylinder** is a set of tracks on different surfaces of a hard drive that are the same distance from the center of the drive; it is called a cylinder because of its clear resemblance to the so-called geometrical shape.
==单个**柱面**是硬盘不同盘面上距离驱动器中心相同距离的一组磁道；它之所以被称为柱面，是因为它与所谓的几何形状非常相似。==

FFS aggregates $N$ consecutive cylinders into a group, and thus the entire disk can thus be viewed as a collection of cylinder groups.
==FFS 将 $N$ 个连续的柱面聚合为一个组，因此整个磁盘可以被视为柱面组的集合。==

Within each group, FFS needs to track whether the inodes and data blocks of the group are allocated.
==在每个组中，FFS 需要追踪该组的 inode 和数据块是否已分配。==

A per-group **inode bitmap (ib)** and **data bitmap (db)** serve this role for inodes and data blocks in each group.
==每个组中的**索引节点位图 (ib)** 和**数据位图 (db)** 分别负责追踪每个组中的 inode 和数据块。==

Finally, the **inode** and **data block** regions are just like those in the previous very-simple file system (VSFS).
==最后，**inode** 和**数据块**区域就像之前的极简文件系统 (VSFS) 中的一样。==

41.4 Policies: How To Allocate Files and Directories
==41.4 策略：如何分配文件和目录==

The basic mantra is simple: *keep related stuff together* (and its corollary, *keep unrelated stuff far apart*).
==基本原则很简单：*将相关的东西放在一起*（及其推论，*将不相关的东西分开放*）。==

Thus, to obey the mantra, FFS has to decide what is “related” and place it within the same block group; conversely, unrelated items should be placed into different block groups.
==因此，为了遵循该原则，FFS 必须决定什么是“相关的”并将其放在同一个块组中；相反，不相关的项应该放在不同的块组中。==

To achieve this end, FFS makes use of a few simple placement heuristics.
==为了达到这个目的，FFS 使用了一些简单的放置启发式方法。==

The first is the placement of directories.
==第一个是目录的放置。==

FFS employs a simple approach: find the cylinder group with a low number of allocated directories (to balance directories across groups) and a high number of free inodes (to subsequently be able to allocate a bunch of files), and put the directory data and inode in that group.
==FFS 采用了一种简单的方法：寻找已分配目录数量较少（为了在组之间平衡目录）且空闲 inode 数量较多（以便随后能够分配一堆文件）的柱面组，并将目录数据和 inode 放在该组中。==

For files, FFS does two things.
==对于文件，FFS 做两件事。==

First, it makes sure (in the general case) to allocate the data blocks of a file in the same group as its inode, thus preventing long seeks between inode and data (as in the old file system).
==首先，它确保（在一般情况下）将文件的数据块分配在与其 inode 相同的组中，从而防止 inode 和数据之间长距离寻道（如在旧文件系统中那样）。==

Second, it places all files that are in the same directory in the cylinder group of the directory they are in.
==其次，它将同一目录中的所有文件都放在该目录所在的柱面组中。==

41.6 The Large-File Exception
==41.6 大文件例外情况==

In FFS, there is one important exception to the general policy of file placement, and it arises for large files.
==在 FFS 中，文件放置的一般策略有一个重要的例外，即针对大文件。==

Without a different rule, a large file would entirely fill the block group it is first placed within (and maybe others).
==如果没有不同的规则，一个大文件会完全填满它最初被放置的块组（可能还有其他块组）。==

Filling a block group in this manner is undesirable, as it prevents subsequent “related” files from being placed within this block group, and thus may hurt file-access locality.
==以这种方式填满一个块组是不可取的，因为它会阻止后续的“相关”文件被放置在该块组中，从而可能损害文件访问的局部性。==

Thus, for large files, FFS does the following.
==因此，对于大文件，FFS 会执行以下操作。==

After some number of blocks are allocated into the first block group (e.g., 12 blocks, or the number of direct pointers available within an inode), FFS places the next “large” chunk of the file (e.g., those pointed to by the first indirect block) in another block group (perhaps chosen for its low utilization).
==在第一个块组中分配了一定数量的块（例如 12 个块，或 inode 中可用的直接指针数量）之后，FFS 将文件的下一个“大”块（例如，由第一个间接块指向的块）放在另一个块组中（可能是根据低利用率选择的）。==

Then, the next chunk of the file is placed in yet another different block group, and so on.
==然后，文件的下一个块被放置在另一个不同的块组中，依此类推。==

Specifically, if the chunk size is large enough, the file system will spend most of its time transferring data from disk and just a (relatively) little time seeking between chunks of the block.
==具体来说，如果块大小足够大，文件系统将花费大部分时间从磁盘传输数据，而只花费（相对）很少的时间在块的各个分块之间寻道。==

This process of reducing an overhead by doing more work per overhead paid is called **amortization** and is a common technique in computer systems.
==这种通过在每次支付的开销中做更多工作来减少开销的过程被称为**摊销**（amortization），是计算机系统中的一种常用技术。==

Let’s do an example: assume that the average positioning time (i.e., seek and rotation) for a disk is 10 ms.
==让我们举个例子：假设磁盘的平均定位时间（即寻道和旋转）为 10 毫秒。==

Assume further that the disk transfers data at 40 MB/s.
==进一步假设磁盘传输数据的速度为 40 MB/s。==

If your goal was to spend half our time seeking between chunks and half our time transferring data (and thus achieve 50% of peak disk performance), you would thus need to spend 10 ms transferring data for every 10 ms positioning.
==如果你的目标是花一半时间在块之间寻道，另一半时间传输数据（从而达到峰值磁盘性能的 50%），那么每定位 10 毫秒你就需要花费 10 毫秒来传输数据。==

So the question becomes: how big does a chunk have to be in order to spend 10 ms in transfer?
==所以问题变成了：为了在传输上花费 10 毫秒，一个块需要多大？==

$\frac{40 MB}{sec} \cdot \frac{1024 KB}{1 MB} \cdot \frac{1 sec}{1000 ms} \cdot 10 ms = 409.6 KB$
$\frac{40 MB}{sec} \cdot \frac{1024 KB}{1 MB} \cdot \frac{1 sec}{1000 ms} \cdot 10 ms = 409.6 KB$

Basically, what this equation says is this: if you transfer data at 40 MB/s, you need to transfer only 409.6KB every time you seek in order to spend half your time seeking and half your time transferring.
==基本上，这个方程说的是：如果你以 40 MB/s 的速度传输数据，每次寻道只需要传输 409.6KB，就可以让寻道和传输各占一半时间。==




Figure 41.3: FFS: Standard Versus Parameterized Placement
==图 41.3：FFS：标准布局与参数化布局的对比==

waste an entire 4KB block.
==浪费掉整整一个 4KB 数据块。==

As the file grew, the file system will continue allocating 512-byte blocks to it until it acquires a full 4KB of data.
==随着文件的增长，文件系统将继续为其分配 512 字节的块，直到它获得整整 4KB 的数据。==

At that point, FFS will find a 4KB block, copy the sub-blocks into it, and free the sub-blocks for future use.
==此时，FFS 将寻找一个 4KB 的块，将子块内容复制到其中，并释放这些子块以便将来使用。==

You might observe that this process is inefficient, requiring a lot of extra work for the file system (in particular, a lot of extra I/O to perform the copy).
==你可能会注意到这个过程是低效的，需要文件系统做大量的额外工作（特别是执行复制时的大量额外 I/O）。==

And you’d be right again!
==你又说对了！==

Thus, FFS generally avoided this pessimal behavior by modifying the `libc` library; the library would buffer writes and then issue them in 4KB chunks to the file system, thus avoiding the sub-block specialization entirely in most cases.
==因此，FFS 通常通过修改 `libc` 库来避免这种最差情况；该库会缓冲写入，然后以 4KB 为单位发给文件系统，从而在大多数情况下完全避免了子块细化的开销。==

A second neat thing that FFS introduced was a disk layout that was optimized for performance.
==FFS 引入的第二个巧妙之处是针对性能进行了优化的磁盘布局。==

In those times (before SCSI and other more modern device interfaces), disks were much less sophisticated and required the host CPU to control their operation in a more hands-on way.
==在那个时代（SCSI 和其他更现代的设备接口出现之前），磁盘远没有现在这么复杂，需要主机 CPU 以更直接的方式控制其运行。==

A problem arose in FFS when a file was placed on consecutive sectors of the disk, as on the left in Figure 41.3.
==当文件被放置在磁盘的连续扇区上时（如图 41.3 左侧所示），FFS 中出现了一个问题。==

In particular, the problem arose during sequential reads.
==特别是在顺序读取过程中会出现这个问题。==

FFS would first issue a read to block 0; by the time the read was complete, and FFS issued a read to block 1, it was too late: block 1 had rotated under the head and now the read to block 1 would incur a full rotation.
==FFS 会首先发出读取块 0 的指令；等到读取完成且 FFS 发出读取块 1 的指令时，已经太晚了：块 1 已经旋转过了磁头，现在读取块 1 将导致一个完整的旋转延迟。==

FFS solved this problem with a different layout, as you can see on the right in Figure 41.3.
==FFS 通过一种不同的布局解决了这个问题，正如你在图 41.3 右侧所看到的。==

By skipping over every other block (in the example), FFS has enough time to request the next block before it went past the disk head.
==通过跳过每一个间隔块（在示例中），FFS 就有足够的时间在块经过磁头之前请求下一个块。==

In fact, FFS was smart enough to figure out for a particular disk how many blocks it should skip in doing layout in order to avoid the extra rotations; this technique was called **parameterization**, as FFS would figure out the specific performance parameters of the disk and use those to decide on the exact staggered layout scheme.
==事实上，FFS 非常聪明，能够针对特定磁盘计算出在布局时应该跳过多少个块，以避免额外的旋转；这种技术被称为**参数化**（parameterization），因为 FFS 会计算出磁盘的具体性能参数，并利用这些参数来决定精确的交错布局方案。==

You might be thinking: this scheme isn’t so great after all.
==你可能会想：这个方案毕竟也不怎么样。==

In fact, you will only get 50% of peak bandwidth with this type of layout, because you have to go around each track twice just to read each block once.
==事实上，使用这种类型的布局，你只能获得峰值带宽的 50%，因为你必须绕磁道转两圈才能读完每个块一次。==

Fortunately, modern disks are much smarter: they internally read the entire track in and buffer it in an internal disk cache (often called a **track buffer** for this very reason).
==幸运的是，现代磁盘要聪明得多：它们会在内部读入整个磁道，并将其缓冲在内部磁盘缓存中（正因如此，这通常被称为**磁道缓冲器**）。==

Then, on subsequent reads to the track, the disk will just return the desired data from its cache.
==然后，在随后对该磁道的读取中，磁盘只需从其缓存中返回所需的数据。==

File systems thus no longer have to worry about these incredibly low-level details.
==因此，文件系统不再需要担心这些极低层级的细节。==

Abstraction and higher-level interfaces can be a good thing, when designed properly.
==当设计得当时，抽象和更高级别的接口可能是一件好事。==

Some other usability improvements were added as well.
==此外还增加了一些其他的易用性改进。==

FFS was one of the first file systems to allow for **long file names**, thus enabling more expressive names in the file system instead of the traditional fixed-size approach (e.g., 8 characters).
==FFS 是最早允许**长文件名**的文件系统之一，从而在文件系统中实现了更具表现力的命名，而不是传统的固定长度方式（例如 8 个字符）。==

Further, a new concept was introduced called a **symbolic link**.
==此外，还引入了一个名为**符号链接**（symbolic link）的新概念。==

As discussed in a previous chapter [AD14b], hard links are limited in that they both could not point to directories (for fear of introducing loops in the file system hierarchy) and that they can only point to files within the same volume (i.e., the inode number must still be meaningful).
==正如在之前的章节 [AD14b] 中讨论的那样，硬链接受到限制，因为它们既不能指向目录（担心在文件系统层级结构中引入环路），也只能指向同一卷内的文件（即 inode 编号必须仍然有意义）。==

Symbolic links allow the user to create an “alias” to any other file or directory on a system and thus are much more flexible.
==符号链接允许用户为系统上的任何其他文件或目录创建“别名”，因此更加灵活。==

FFS also introduced an atomic `rename()` operation for renaming files.
==FFS 还引入了一个用于重命名文件的原子 `rename()` 操作。==

Usability improvements, beyond the basic technology, also likely gained FFS a stronger user base.
==除了基础技术之外，易用性的改进也可能为 FFS 赢得了更强大的用户群。==

**41.8 Summary**
==**41.8 总结**==

The introduction of FFS was a watershed moment in file system history, as it made clear that the problem of file management was one of the most interesting issues within an operating system, and showed how one might begin to deal with that most important of devices, the hard disk.
==FFS 的引入是文件系统历史上的一个分水岭，因为它明确了文件管理问题是操作系统中最有趣的问题之一，并展示了人们该如何着手处理最重要的设备——硬盘。==

Since that time, hundreds of new file systems have developed, but still today many file systems take cues from FFS (e.g., Linux ext2 and ext3 are obvious intellectual descendants).
==从那时起，已经开发了数百种新的文件系统，但直到今天，许多文件系统仍从 FFS 中汲取灵感（例如，Linux ext2 和 ext3 是明显的智力后代）。==

Certainly all modern systems account for the main lesson of FFS: treat the disk like it’s a disk.
==当然，所有现代系统都考虑了 FFS 的主要教训：像对待磁盘一样对待磁盘。==

**References**
==**参考文献**==

[AD14a] “Operating Systems: Three Easy Pieces” (Chapter: Hard Disk Drives) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau.
==[AD14a] 《操作系统：三部曲》（章节：硬盘驱动器），作者：Remzi Arpaci-Dusseau 和 Andrea Arpaci-Dusseau。==

Arpaci-Dusseau Books, 2014.
==Arpaci-Dusseau 出版社，2014 年。==

There is no way you should be reading about FFS without having first understood hard drives in some detail.
==在没有详细了解硬盘驱动器之前，你不应该阅读有关 FFS 的内容。==

If you try to do so, please instead go directly to jail; do not pass go, and, critically, do not collect 200 much-needed simoleons.
==如果你尝试这样做，请直接去坐牢；不要经过起点，更重要的是，不要领取那 200 个急需的模拟币。==

[AD14b] “Operating Systems: Three Easy Pieces” (Chapter: File System Implementation) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau.
==[AD14b] 《操作系统：三部曲》（章节：文件系统实现），作者：Remzi Arpaci-Dusseau 和 Andrea Arpaci-Dusseau。==

Arpaci-Dusseau Books, 2014.
==Arpaci-Dusseau 出版社，2014 年。==

As above, it makes little sense to read this chapter unless you have read (and understood) the chapter on file system implementation.
==同上，除非你已经阅读（并理解）了关于文件系统实现的章节，否则阅读本章没有太大意义。==

Otherwise, we’ll be throwing around terms like “inode” and “indirect block” and you’ll be like “huh?” and that is no fun for either of us.
==否则，我们会到处抛出“inode”和“间接块”之类的术语，而你只会一脸茫然，这对我们双方都没有好处。==

[K94] “The Design of the SEER Predictive Caching System” by G. H. Kuenning.
==[K94] 《SEER 预测缓存系统的设计》，作者：G. H. Kuenning。==

MOBICOMM ’94, Santa Cruz, California, December 1994.
==MOBICOMM ’94，加利福尼亚州圣克鲁斯，1994 年 12 月。==

According to Kuenning, this is the best overview of the SEER project, which led to (among other things) the collection of these traces.
==根据 Kuenning 的说法，这是对 SEER 项目最好的概述，该项目促成了（除其他事项外）这些追踪数据的收集。==

[MJLF84] “A Fast File System for UNIX” by Marshall K. McKusick, William N. Joy, Sam J. Leffler, Robert S. Fabry.
==[MJLF84] 《一种用于 UNIX 的快速文件系统》，作者：Marshall K. McKusick，William N. Joy，Sam J. Leffler，Robert S. Fabry。==

ACM TOCS, 2:3, August 1984.
==ACM TOCS，2:3，1984 年 8 月。==

McKusick was recently honored with the IEEE Reynold B. Johnson award for his contributions to file systems, much of which was based on his work building FFS.
==McKusick 最近因其对文件系统的贡献被授予 IEEE Reynold B. Johnson 奖，其中大部分贡献是基于他构建 FFS 的工作。==

In his acceptance speech, he discussed the original FFS software: only 1200 lines of code!
==在获奖感言中，他谈到了最初的 FFS 软件：只有 1200 行代码！==

Modern versions are a little more complex, e.g., the BSD FFS descendant now is in the 50-thousand lines-of-code range.
==现代版本要复杂一些，例如，BSD FFS 的后代现在处于 5 万行代码的范围。==

[P98] “Hardware Technology Trends and Database Opportunities” by David A. Patterson.
==[P98] 《硬件技术趋势与数据库机遇》，作者：David A. Patterson。==

Keynote Lecture at SIGMOD ’98, June 1998.
==SIGMOD ’98 主旨演讲，1998 年 6 月。==

A great and simple overview of disk technology trends and how they change over time.
==对磁盘技术趋势及其随时间演变的一个极好且简单的概述。==

**Homework (Simulation)**
==**课后作业（模拟）**==

This section introduces `ffs.py`, a simple FFS simulator you can use to understand better how FFS-based file and directory allocation work.
==本节介绍了 `ffs.py`，这是一个简单的 FFS 模拟器，你可以用它来更好地理解基于 FFS 的文件和目录分配是如何工作的。==

See the README for details on how to run the simulator.
==有关如何运行模拟器的详细信息，请参阅 README。==

**Questions**
==**问题**==

1. Examine the file `in.largefile`, and then run the simulator with flag `-f in.largefile` and `-L 4`.
==1. 检查文件 `in.largefile`，然后使用标志 `-f in.largefile` 和 `-L 4` 运行模拟器。==

The latter sets the large-file exception to 4 blocks.
==后者将大文件例外设置为 4 个块。==

What will the resulting allocation look like?
==最终的分配会是什么样子的？==

Run with `-c` to check.
==使用 `-c` 运行以检查。==

2. Now run with `-L 30`.
==2. 现在使用 `-L 30` 运行。==

What do you expect to see?
==你期望看到什么？==

Once again, turn on `-c` to see if you were right.
==再次打开 `-c` 看看你是否正确。==

You can also use `-S` to see exactly which blocks were allocated to the file `/a`.
==你还可以使用 `-S` 来查看具体哪些块被分配给了文件 `/a`。==

3. Now we will compute some statistics about the file.
==3. 现在我们将计算一些关于文件的统计信息。==

The first is something we call **filespan**, which is the max distance between any two data blocks of the file or between the inode and any data block.
==第一个是我们称之为 **filespan** 的指标，它是文件的任何两个数据块之间，或者 inode 与任何数据块之间的最大距离。==

Calculate the filespan of `/a`.
==计算 `/a` 的 filespan。==

Run `ffs.py -f in.largefile -L 4 -T -c` to see what it is.
==运行 `ffs.py -f in.largefile -L 4 -T -c` 查看结果。==

Do the same with `-L 100`.
==对 `-L 100` 执行相同的操作。==

What difference do you expect in filespan as the large-file exception parameter changes from low values to high values?
==当大文件例外参数从低值变为高值时，你预期的 filespan 会有什么变化？==

4. Now let’s look at a new input file, `in.manyfiles`.
==4. 现在让我们看一个新的输入文件 `in.manyfiles`。==

How do you think the FFS policy will lay these files out across groups?
==你认为 FFS 策略将如何跨组布局这些文件？==

(you can run with `-v` to see what files and directories are created, or just `cat in.manyfiles`).
==（你可以使用 `-v` 运行来查看创建了哪些文件和目录，或者直接 `cat in.manyfiles`）。==

Run the simulator with `-c` to see if you were right.
==使用 `-c` 运行模拟器，看看你是否正确。==

5. A metric to evaluate FFS is called **dirspan**.
==5. 评估 FFS 的一个指标称为 **dirspan**。==

This metric calculates the spread of files within a particular directory, specifically the max distance between the inodes and data blocks of all files in the directory and the inode and data block of the directory itself.
==该指标计算特定目录内文件的分布情况，具体而言，是该目录中所有文件的 inode 和数据块，与目录自身的 inode 和数据块之间的最大距离。==

Run with `in.manyfiles` and the `-T` flag, and calculate the dirspan of the three directories.
==使用 `in.manyfiles` 和 `-T` 标志运行，并计算三个目录的 dirspan。==

Run with `-c` to check.
==使用 `-c` 运行以检查。==

How good of a job does FFS do in minimizing dirspan?
==FFS 在最小化 dirspan 方面做得如何？==

6. Now change the size of the inode table per group to 5 (`-i 5`).
==6. 现在将每个组的 inode 表大小更改为 5 (`-i 5`)。==

How do you think this will change the layout of the files?
==你认为这将如何改变文件的布局？==

Run with `-c` to see if you were right.
==使用 `-c` 运行，看看你是否正确。==

How does it affect the dirspan?
==它对 dirspan 有什么影响？==

7. Which group should FFS place inode of a new directory in?
==7. FFS 应该将新目录的 inode 放在哪个组中？==

The default (simulator) policy looks for the group with the most free inodes.
==默认（模拟器）策略是寻找拥有最多空闲 inode 的组。==

A different policy looks for a set of groups with the most free inodes.
==另一种策略是寻找一组拥有最多空闲 inode 的组。==

For example, if you run with `-A 2`, when allocating a new directory, the simulator will look at groups in pairs and pick the best pair for the allocation.
==例如，如果使用 `-A 2` 运行，在分配新目录时，模拟器将成对查看组，并选择最佳的一对进行分配。==

Run `./ffs.py -f in.manyfiles -i 5 -A 2 -c` to see how allocation changes with this strategy.
==运行 `./ffs.py -f in.manyfiles -i 5 -A 2 -c` 查看该策略下分配的变化。==

How does it affect dirspan?
==它对 dirspan 有什么影响？==

Why might this policy be good?
==为什么这个策略可能比较好？==

8. One last policy change we will explore relates to file fragmentation.
==8. 我们将探讨的最后一个策略更改与文件碎片有关。==

Run `./ffs.py -f in.fragmented -v` and see if you can predict how the files that remain are allocated.
==运行 `./ffs.py -f in.fragmented -v`，看看你是否能预测剩余文件的分配方式。==

Run with `-c` to confirm your answer.
==使用 `-c` 运行以确认你的答案。==

What is interesting about the data layout of file `/i`?
==文件 `/i` 的数据布局有什么有趣之处？==

Why is it problematic?
==为什么它是有问题的？==

9. A new policy, which we call **contiguous allocation** (`-C`), tries to ensure that each file is allocated contiguously.
==9. 一种被称为**连续分配** (`-C`) 的新策略，试图确保每个文件都被连续分配。==

Specifically, with `-C n`, the file system tries to ensure that $n$ contiguous blocks are free within a group before allocating a block.
==具体而言，使用 `-C n`，文件系统会尝试在分配一个块之前，确保组内有 $n$ 个连续的空闲块。==

Run `./ffs.py -f in.fragmented -v -C 2 -c` to see the difference.
==运行 `./ffs.py -f in.fragmented -v -C 2 -c` 查看区别。==

How does layout change as the parameter passed to `-C` increases?
==随着传递给 `-C` 的参数增加，布局会如何变化？==

Finally, how does `-C` affect filespan and dirspan?
==最后，`-C` 是如何影响 filespan 和 dirspan 的？==

**42**
**42**

**Crash Consistency: FSCK and Journaling**
==**崩溃一致性：FSCK 与日志**==

As we’ve seen thus far, the file system manages a set of data structures to implement the expected abstractions: files, directories, and all of the other metadata needed to support the basic abstraction that we expect from a file system.
==正如我们目前所见，文件系统管理着一组数据结构来实现预期的抽象：文件、目录以及支持我们对文件系统所期望的基本抽象所需的所有其他元数据。==

Unlike most data structures (for example, those found in memory of a running program), file system data structures must **persist**, i.e., they must survive over the long haul, stored on devices that retain data despite power loss (such as hard disks or flash-based SSDs).
==与大多数数据结构（例如运行程序内存中的数据结构）不同，文件系统的数据结构必须**持久化**（persist），即它们必须能够长期保存，存储在断电后仍能保留数据的设备上（如硬盘或基于闪存的 SSD）。==

One major challenge faced by a file system is how to update persistent data structures despite the presence of a **power loss** or **system crash**.
==文件系统面临的一个主要挑战是，在存在**断电**或**系统崩溃**的情况下，如何更新持久性数据结构。==

Specifically, what happens if, right in the middle of updating on-disk structures, someone trips over the power cord and the machine loses power?
==具体来说，如果在更新磁盘结构的途中，有人绊到了电源线导致机器断电，会发生什么？==

Or the operating system encounters a bug and crashes?
==或者操作系统遇到了 bug 并崩溃了？==

Because of power losses and crashes, updating a persistent data structure can be quite tricky, and leads to a new and interesting problem in file system implementation, known as the **crash-consistency problem**.
==由于断电和崩溃，更新持久性数据结构可能会非常棘手，并导致文件系统实现中的一个新而有趣的问题，即**崩溃一致性问题**（crash-consistency problem）。==

This problem is quite simple to understand.
==这个问题很容易理解。==

Imagine you have to update two on-disk structures, $A$ and $B$, in order to complete a particular operation.
==假设你必须更新两个磁盘结构 $A$ 和 $B$ 才能完成特定操作。==

Because the disk only services a single request at a time, one of these requests will reach the disk first (either $A$ or $B$).
==因为磁盘一次只能处理一个请求，所以这些请求中的一个会首先到达磁盘（要么是 $A$，要么是 $B$）。==

If the system crashes or loses power after one write completes, the on-disk structure will be left in an **inconsistent** state.
==如果系统在一次写入完成后崩溃或断电，磁盘结构将处于**不一致**的状态。==

And thus, we have a problem that all file systems need to solve:
==因此，我们遇到了一个所有文件系统都需要解决的问题：==

**THE CRUX: HOW TO UPDATE THE DISK DESPITE CRASHES**
==**核心问题：如何在崩溃的情况下更新磁盘**==

The system may crash or lose power between any two writes, and thus the on-disk state may only partially get updated.
==系统可能会在任何两次写入之间崩溃或断电，因此磁盘状态可能只得到了部分更新。==

After the crash, the system boots and wishes to mount the file system again (in order to access files and such).
==崩溃后，系统启动并希望再次挂载文件系统（以便访问文件等）。==

Given that crashes can occur at arbitrary points in time, how do we ensure the file system keeps the on-disk image in a reasonable state?
==鉴于崩溃可能发生在任何时间点，我们如何确保文件系统将磁盘镜像保持在合理的状态？==

In this chapter, we’ll describe this problem in more detail, and look at some methods file systems have used to overcome it.
==在本章中，我们将更详细地描述这个问题，并研究文件系统用来克服它的一些方法。==

We’ll begin by examining the approach taken by older file systems, known as **fsck** or the **file system checker**.
==我们将首先研究旧文件系统采取的方法，即 **fsck** 或**文件系统检查器**。==

We’ll then turn our attention to another approach, known as **journaling** (also known as **write-ahead logging**), a technique which adds a little bit of overhead to each write but recovers more quickly from crashes or power losses.
==然后，我们将注意力转向另一种方法，即**日志**（journaling，也称为**写前日志** write-ahead logging），这种技术给每次写入增加了一点点开销，但从崩溃或断电中恢复的速度更快。==

We will discuss the basic machinery of journaling, including a few different flavors of journaling that Linux ext3 [T98, PAA05] (a relatively modern journaling file system) implements.
==我们将讨论日志的基本机制，包括 Linux ext3 [T98, PAA05]（一种相对现代的日志文件系统）实现的几种不同类型的日志。==

**42.1 A Detailed Example**
==**42.1 一个详细的示例**==

To kick off our investigation of journaling, let’s look at an example.
==为了开始我们对日志的研究，让我们来看一个例子。==

We’ll need to use a workload that updates on-disk structures in some way.
==我们需要使用一种以某种方式更新磁盘结构的工作负载。==

Assume here that the workload is simple: the append of a single data block to an existing file.
==这里假设工作负载很简单：向现有文件追加一个数据块。==

The append is accomplished by opening the file, calling `lseek()` to move the file offset to the end of the file, and then issuing a single 4KB write to the file before closing it.
==追加操作是通过打开文件，调用 `lseek()` 将文件偏移量移动到文件末尾，然后在关闭文件之前向文件发出单个 4KB 写入来完成的。==

Let’s also assume we are using standard simple file system structures on the disk, similar to file systems we have seen before.
==我们还假设磁盘上使用的是标准的简单文件系统结构，类似于我们之前看到的文件系统。==

This tiny example includes an **inode bitmap** (with just 8 bits, one per inode), a **data bitmap** (also 8 bits, one per data block), **inodes** (8 total, numbered 0 to 7, and spread across four blocks), and **data blocks** (8 total, numbered 0 to 7).
==这个小例子包括一个 **inode 位图**（只有 8 位，每个 inode 一位）、一个**数据位图**（也是 8 位，每个数据块一位）、**inodes**（共 8 个，编号 0 到 7，分布在四个块中）和**数据块**（共 8 个，编号 0 到 7）。==

Here is a diagram of this file system:
==这是该文件系统的示意图：==

Bitmaps Inode Data Inodes Data Blocks I[v1] Da
==位图 Inode Data Inodes Data Blocks I[v1] Da==

0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7

If you look at the structures in the picture, you can see that a single inode is allocated (inode number 2), which is marked in the inode bitmap, and a single allocated data block (data block 4), also marked in the data bitmap.
==如果你看图中的结构，你可以看到分配了一个 inode（inode 编号 2），它在 inode 位图中标记；并且分配了一个数据块（数据块 4），它也在数据位图中标记。==

The inode is denoted I[v1], as it is the first version of this inode; it will soon be updated (due to the workload described above).
==该 inode 被表示为 I[v1]，因为它是该 inode 的第一个版本；由于上述工作负载，它很快就会被更新。==

Let’s peek inside this simplified inode too.
==我们也来看看这个简化版 inode 的内部。==

Inside of I[v1], we see:
==在 I[v1] 内部，我们看到：==

```
owner       : remzi
permissions : read-write
size        : 1
pointer     : 4
pointer     : null
pointer     : null
pointer     : null
```

In this simplified inode, the `size` of the file is 1 (it has one block allocated), the first direct pointer points to block 4 (the first data block of the file, Da), and all three other direct pointers are set to `null` (indicating that they are not used).
==在这个简化的 inode 中，文件的 `size` 为 1（它分配了一个块），第一个直接指针指向块 4（文件的第一个数据块 Da），而所有其他三个直接指针都设置为 `null`（表示它们未被使用）。==

Of course, real inodes have many more fields; see previous chapters for more information.
==当然，真实的 inode 有更多的字段；更多信息请参见之前的章节。==

When we append to the file, we are adding a new data block to it, and thus must update three on-disk structures: the inode (which must point to the new block and record the new larger size due to the append), the new data block $Db$, and a new version of the data bitmap (call it $B[v2]$) to indicate that the new data block has been allocated.
==当我们向文件追加内容时，我们要给它添加一个新的数据块，因此必须更新三个磁盘结构：inode（它必须指向新块并记录由于追加而产生的新文件大小）、新数据块 $Db$，以及数据位图的新版本（称之为 $B[v2]$）以指示新数据块已被分配。==

Thus, in the memory of the system, we have three blocks which we must write to disk.
==因此，在系统的内存中，我们有三个必须写入磁盘的块。==

The updated inode (inode version 2, or I[v2] for short) now looks like this:
==更新后的 inode（inode 版本 2，简称 I[v2]）现在看起来像这样：==

```
owner       : remzi
permissions : read-write
size        : 2
pointer     : 4
pointer     : 5
pointer     : null
pointer     : null
```

The updated data bitmap ($B[v2]$) now looks like this: `00001100`.
==更新后的数据位图 ($B[v2]$) 现在看起来像这样：`00001100`。==

Finally, there is the data block ($Db$), which is just filled with whatever it is users put into files.
==最后是数据块 ($Db$)，它只是填充了用户放入文件中的任何内容。==

Stolen music, perhaps?
==也许是盗版音乐？==

What we would like is for the final on-disk image of the file system to look like this:
==我们希望文件系统最终的磁盘镜像看起来像这样：==

Bitmaps Inode Data Inodes Data Blocks I[v2] Da Db
==位图 Inode Data Inodes Data Blocks I[v2] Da Db==

0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7

To achieve this transition, the file system must perform three separate writes to the disk, one each for the inode ($I[v2]$), bitmap ($B[v2]$), and data block ($Db$).
==为了实现这种转变，文件系统必须对磁盘执行三次独立的写入，分别针对 inode ($I[v2]$)、位图 ($B[v2]$) 和数据块 ($Db$)。==

Note that these writes usually don’t happen immediately when the user issues a `write()` system call; rather, the dirty inode, bitmap, and new data will sit in main memory (in the **page cache** or **buffer cache**) for some time first; then, when the file system finally decides to write them to disk (after say 5 seconds or 30 seconds), the file system will issue the requisite write requests to the disk.
==请注意，这些写入通常不会在用户发出 `write()` 系统调用时立即发生；相反，脏 inode、位图和新数据会先在主存（**页缓存**或**缓冲缓存**）中存放一段时间；然后，当文件系统最终决定将它们写入磁盘时（比如 5 秒或 30 秒后），文件系统将向磁盘发出必要的写入请求。==

Unfortunately, a crash may occur and thus interfere with these updates to the disk.
==不幸的是，可能会发生崩溃，从而干扰这些对磁盘的更新。==

In particular, if a crash happens after one or two of these writes have taken place, but not all three, the file system could be left in a funny state.
==特别是，如果崩溃发生在完成一两次写入之后，但还没完成全部三次，文件系统可能会处于一种奇怪的状态。==

**Crash Scenarios**
==**崩溃场景**==

To understand the problem better, let’s look at some example crash scenarios.
==为了更好地理解这个问题，让我们看看一些示例崩溃场景。==

Imagine only a single write succeeds; there are thus three possible outcomes, which we list here:
==假设只有一次写入成功；因此有三种可能的结果，我们在此列出：==

* **Just the data block ($Db$) is written to disk.**
==* **只有数据块 ($Db$) 写入了磁盘。**==

In this case, the data is on disk, but there is no inode that points to it and no bitmap that even says the block is allocated.
==在这种情况下，数据在磁盘上，但没有指向它的 inode，甚至没有位图指示该块已被分配。==

Thus, it is as if the write never occurred.
==因此，这就好像写入从未发生过一样。==

This case is not a problem at all, from the perspective of file-system crash consistency.
==从文件系统崩溃一致性的角度来看，这种情况根本不是问题。==

* **Just the updated inode ($I[v2]$) is written to disk.**
==* **只有更新后的 inode ($I[v2]$) 写入了磁盘。**==

In this case, the inode points to the disk address (5) where $Db$ was about to be written, but $Db$ has not yet been written there.
==在这种情况下，inode 指向了 $Db$ 即将写入的磁盘地址 (5)，但 $Db$ 尚未写入那里。==

Thus, if we trust that pointer, we will read **garbage data** from the disk (the old contents of disk address 5).
==因此，如果我们信任该指针，我们将从磁盘读取到**垃圾数据**（磁盘地址 5 的旧内容）。==

Further, we have a new problem, which we call a **file-system inconsistency**.
==此外，我们遇到了一个新问题，我们称之为**文件系统不一致性**。==

The on-disk bitmap is telling us that data block 5 has not been allocated, but the inode is saying that it has.
==磁盘上的位图告诉我们数据块 5 尚未分配，但 inode 却说它已经分配了。==

The disagreement between the bitmap and the inode is an inconsistency in the data structures of the file system; to use the file system, we must somehow resolve this problem (more on that below).
==位图和 inode 之间的分歧是文件系统数据结构中的一种不一致性；为了使用文件系统，我们必须设法解决这个问题（详见下文）。==

* **Just the updated bitmap ($B[v2]$) is written to disk.**
==* **只有更新后的位图 ($B[v2]$) 写入了磁盘。**==

In this case, the bitmap indicates that block 5 is allocated, but there is no inode that points to it.
==在这种情况下，位图指示块 5 已分配，但没有 inode 指向它。==

Thus the file system is inconsistent again; if left unresolved, this write would result in a **space leak**, as block 5 would never be used by the file system.
==因此文件系统再次不一致；如果不解决，这次写入将导致**空间泄漏**，因为块 5 永远不会被文件系统使用。==

There are also three more crash scenarios in this attempt to write three blocks to disk.
==在这次尝试将三个块写入磁盘的过程中，还有另外三种崩溃场景。==

In these cases, two writes succeed and the last one fails:
==在这些情况下，两次写入成功，最后一次失败：==

* **The inode ($I[v2]$) and bitmap ($B[v2]$) are written to disk, but not data ($Db$).**
==* **inode ($I[v2]$) 和位图 ($B[v2]$) 写入了磁盘，但数据 ($Db$) 没有。**==

In this case, the file system metadata is completely consistent: the inode has a pointer to block 5, the bitmap indicates that 5 is in use, and thus everything looks OK from the perspective of the file system’s metadata.
==在这种情况下，文件系统元数据是完全一致的：inode 有一个指向块 5 的指针，位图指示块 5 正在使用中，因此从文件系统元数据的角度来看，一切都显得正常。==

But there is one problem: 5 has garbage in it again.
==但有一个问题：块 5 中又是垃圾数据。==

* **The inode ($I[v2]$) and the data block ($Db$) are written, but not the bitmap ($B[v2]$).**
==* **inode ($I[v2]$) 和数据块 ($Db$) 写入了，但位图 ($B[v2]$) 没有。**==

In this case, we have the inode pointing to the correct data on disk, but again have an inconsistency between the inode and the old version of the bitmap (B1).
==在这种情况下，我们的 inode 指向了磁盘上正确的数据，但 inode 与旧版本的位图 (B1) 之间再次出现了不一致。==

Thus, we once again need to resolve the problem before using the file system.
==因此，在处理文件系统之前，我们再次需要解决这个问题。==

* **The bitmap ($B[v2]$) and data block ($Db$) are written, but not the inode ($I[v2]$).**
==* **位图 ($B[v2]$) 和数据块 ($Db$) 写入了，但 inode ($I[v2]$) 没有。**==

In this case, we again have an inconsistency between the inode and the data bitmap.
==在这种情况下，inode 和数据位图之间再次出现了不一致。==

However, even though the block was written and the bitmap indicates its usage, we have no idea which file it belongs to, as no inode points to the file.
==然而，尽管该块已写入且位图指示其已被使用，我们却不知道它属于哪个文件，因为没有 inode 指向该文件。==

**The Crash Consistency Problem**
==**崩溃一致性问题**==

Hopefully, from these crash scenarios, you can see the many problems that can occur to our on-disk file system image because of crashes: we can have inconsistency in file system data structures; we can have space leaks; we can return garbage data to a user; and so forth.
==希望通过这些崩溃场景，你可以看到崩溃会对我们的磁盘文件系统镜像造成的许多问题：文件系统数据结构可能不一致；可能会出现空间泄漏；可能会向用户返回垃圾数据；等等。==

What we’d like to do ideally is move the file system from one consistent state (e.g., before the file got appended to) to another **atomically** (e.g., after the inode, bitmap, and new data block have been written to disk).
==理想情况下，我们希望将文件系统**原子地**（atomically）从一个一致状态（例如，文件被追加之前）转换到另一个一致状态（例如，inode、位图和新数据块都已写入磁盘之后）。==

Unfortunately, we can’t do this easily because the disk only commits one write at a time, and crashes or power loss may occur between these updates.
==不幸的是，我们无法轻易做到这一点，因为磁盘一次只能提交一个写入，而这些更新之间可能会发生崩溃或断电。==

We call this general problem the **crash-consistency problem** (we could also call it the **consistent-update problem**).
==我们称这个普遍问题为**崩溃一致性问题**（也可以称之为**一致性更新问题**）。==

**42.2 Solution #1: The File System Checker**
==**42.2 解决方案 #1：文件系统检查器**==

Early file systems took a simple approach to crash consistency.
==早期的文件系统对崩溃一致性采取了一种简单的方法。==

Basically, they decided to let inconsistencies happen and then fix them later (when rebooting).
==基本上，它们决定让不一致性发生，然后稍后（重启时）再修复它们。==

A classic example of this lazy approach is found in a tool that does this: `fsck`.
==这种惰性方法的一个经典例子见于一个专门做这件事的工具：`fsck`。==

`fsck` is a UNIX tool for finding such inconsistencies and repairing them [MK96]; similar tools to check and repair a disk partition exist on different systems.
==`fsck` 是一个用于查找并修复此类不一致性的 UNIX 工具 [MK96]；不同系统上都存在类似的用于检查和修复磁盘分区的工具。==

Note that such an approach can’t fix all problems; consider, for example, the case above where the file system looks consistent but the inode points to garbage data.
==注意，这种方法不能解决所有问题；例如，考虑上述情况，文件系统看起来一致，但 inode 指向的是垃圾数据。==

The only real goal is to make sure the file system metadata is internally consistent.
==唯一的真正目标是确保文件系统元数据内部一致。==

The tool `fsck` operates in a number of phases, as summarized in McKusick and Kowalski’s paper [MK96].
==工具 `fsck` 在多个阶段运行，如 McKusick 和 Kowalski 的论文 [MK96] 中所总结的。==

It is run **before** the file system is mounted and made available (`fsck` assumes that no other file-system activity is on-going while it runs); once finished, the on-disk file system should be consistent and thus can be made accessible to users.
==它在文件系统被挂载并变为可用**之前**运行（`fsck` 假设在其运行时没有其他文件系统活动正在进行）；一旦完成，磁盘上的文件系统应该是一致的，从而可以被用户访问。==

Here is a basic summary of what `fsck` does:
==以下是 `fsck` 所做工作的基本总结：==

* **Superblock:** `fsck` first checks if the superblock looks reasonable, mostly doing sanity checks such as making sure the file system size is greater than the number of blocks that have been allocated.
==* **超级块：** `fsck` 首先检查超级块是否看起来合理，主要是进行完整性检查，例如确保文件系统大小大于已分配的块数。==

Usually the goal of these sanity checks is to find a suspect (corrupt) superblock; in this case, the system (or administrator) may decide to use an alternate copy of the superblock.
==通常这些完整性检查的目标是找到一个可疑（损坏）的超级块；在这种情况下，系统（或管理员）可能会决定使用超级块的备选副本。==

* **Free blocks:** Next, `fsck` scans the inodes, indirect blocks, double indirect blocks, etc., to build an understanding of which blocks are currently allocated within the file system.
==* **空闲块：** 接下来，`fsck` 扫描 inode、间接块、双重间接块等，以了解文件系统内当前分配了哪些块。==

It uses this knowledge to produce a correct version of the allocation bitmaps; thus, if there is any inconsistency between bitmaps and inodes, it is resolved by trusting the information within the inodes.
==它利用这些信息来生成分配位图的正确版本；因此，如果位图和 inode 之间存在任何不一致，将通过信任 inode 中的信息来解决。==

The same type of check is performed for all the inodes, making sure that all inodes that look like they are in use are marked as such in the inode bitmaps.
==对所有 inode 执行同类型的检查，确保所有看起来正在使用的 inode 在 inode 位图中都被标记为已使用。==

* **Inode state:** Each inode is checked for corruption or other problems.
==* **Inode 状态：** 检查每个 inode 是否损坏或其他问题。==

For example, `fsck` makes sure that each allocated inode has a valid type field (e.g., regular file, directory, symbolic link, etc.).
==例如，`fsck` 确保每个分配的 inode 都有一个有效的类型字段（例如，普通文件、目录、符号链接等）。==

If there are problems with the inode fields that are not easily fixed, the inode is considered suspect and cleared by `fsck`; the inode bitmap is correspondingly updated.
==如果 inode 字段中存在不容易修复的问题，该 inode 将被视为可疑并由 `fsck` 清除；inode 位图也会相应更新。==

* **Inode links:** `fsck` also verifies the link count of each allocated inode.
==* **Inode 链接：** `fsck` 还验证每个已分配 inode 的链接计数。==

As you may recall, the link count indicates the number of different directories that contain a reference (i.e., a link) to this particular file.
==你可能还记得，链接计数表示包含对该特定文件引用（即链接）的不同目录的数量。==

To verify the link count, `fsck` scans through the entire directory tree, starting at the root directory, and builds its own link counts for every file and directory in the file system.
==为了验证链接计数，`fsck` 从根目录开始扫描整个目录树，并为文件系统中的每个文件和目录构建自己的链接计数。==

If there is a mismatch between the newly-calculated count and that found within an inode, corrective action must be taken, usually by fixing the count within the inode.
==如果新计算的计数与 inode 中的计数不匹配，必须采取纠正措施，通常是修正 inode 内部的计数。==

If an allocated inode is discovered but no directory refers to it, it is moved to the `lost+found` directory.
==如果发现一个已分配的 inode 但没有目录引用它，它将被移动到 `lost+found` 目录中。==

* **Duplicates:** `fsck` also checks for duplicate pointers, i.e., cases where two different inodes refer to the same block.
==* **重复项：** `fsck` 还会检查重复指针，即两个不同的 inode 指向同一个块的情况。==

If one inode is obviously bad, it may be cleared.
==如果一个 inode 明显是坏的，它可能会被清除。==

Alternately, the pointed-to block could be copied, thus giving each inode its own copy as desired.
==或者，可以复制所指向的块，从而根据需要为每个 inode 提供其自己的副本。==

* **Bad blocks:** A check for bad block pointers is also performed while scanning through the list of all pointers.
==* **坏块：** 在扫描所有指针列表时，还会执行坏块指针检查。==

A pointer is considered “bad” if it obviously points to something outside its valid range, e.g., it has an address that refers to a block greater than the partition size.
==如果一个指针明显指向其有效范围之外的内容，则被认为是“坏”的，例如，它的地址指向大于分区大小的块。==

In this case, `fsck` can’t do anything too intelligent; it just removes (clears) the pointer from the inode or indirect block.
==在这种情况下，`fsck` 做不了太聪明的事情；它只是从 inode 或间接块中移除（清除）该指针。==

* **Directory checks:** `fsck` does not understand the contents of user files; however, directories hold specifically formatted information created by the file system itself.
==* **目录检查：** `fsck` 不理解用户文件的内容；然而，目录保存了由文件系统自身创建的特定格式信息。==

Thus, `fsck` performs additional integrity checks on the contents of each directory, making sure that “.” and “..” are the first entries, that each inode referred to in a directory entry is allocated, and ensuring that no directory is linked to more than once in the entire hierarchy.
==因此，`fsck` 对每个目录的内容执行额外的完整性检查，确保“.”和“..”是前两个条目，目录条目中引用的每个 inode 都是已分配的，并确保整个层级结构中没有目录被链接超过一次。==

As you can see, building a working `fsck` requires intricate knowledge of the file system; making sure such a piece of code works correctly in all cases can be challenging [G+08].
==如你所见，构建一个可运行的 `fsck` 需要对文件系统有深入的了解；确保这样一段代码在所有情况下都能正确运行是很有挑战性的 [G+08]。==

However, `fsck` (and similar approaches) have a bigger and perhaps more fundamental problem: they are **too slow**.
==然而，`fsck`（及类似方法）有一个更大、或许更根本的问题：它们**太慢了**。==

With a very large disk volume, scanning the entire disk to find all the allocated blocks and read the entire directory tree may take many minutes or hours.
==对于非常大的磁盘卷，扫描整个磁盘以找到所有分配的块并读取整个目录树可能需要花费数分钟或数小时。==

Performance of `fsck`, as disks grew in capacity and RAIDs grew in popularity, became prohibitive (despite recent advances [M+13]).
==随着磁盘容量的增加和 RAID 的普及，`fsck` 的性能变得令人望而却步（尽管最近有所进展 [M+13]）。==

At a higher level, the basic premise of `fsck` seems just a tad irrational.
==在更高层面上，`fsck` 的基本前提似乎有一点点不合理。==

Consider our example above, where just three blocks are written to the disk; it is incredibly expensive to scan the entire disk to fix problems that occurred during an update of just three blocks.
==考虑我们上面的例子，其中只向磁盘写入了三个块；扫描整个磁盘来修复仅在三个块的更新过程中发生的问题是极其昂贵的。==

This situation is akin to dropping your keys on the floor in your bedroom, and then commencing a **search-the-entire-house-for-keys** recovery algorithm, starting in the basement and working your way through every room.
==这种情况类似于你把钥匙掉在卧室的地板上，然后开始执行一个“搜遍全家找钥匙”的恢复算法，从地下室开始，挨个房间搜寻。==

It works but is wasteful.
==它有效，但很浪费。==

Thus, as disks (and RAIDs) grew, researchers and practitioners started to look for other solutions.
==因此，随着磁盘（和 RAID）的发展，研究人员和从业者开始寻找其他解决方案。==

**42.3 Solution #2: Journaling (or Write-Ahead Logging)**
==**42.3 解决方案 #2：日志（或写前日志）**==

Probably the most popular solution to the consistent update problem is to steal an idea from the world of database management systems.
==解决一致性更新问题最流行的方法可能是从数据库管理系统领域借鉴一个想法。==

That idea, known as **write-ahead logging**, was invented to address exactly this type of problem.
==这个被称为**写前日志**的想法正是为了解决这类问题而发明的。==

In file systems, we usually call write-ahead logging **journaling** for historical reasons.
==在文件系统中，由于历史原因，我们通常将写前日志称为**日志**（journaling）。==

The first file system to do this was Cedar [H87], though many modern file systems use the idea, including Linux ext3 and ext4, reiserfs, IBM’s JFS, SGI’s XFS, and Windows NTFS.
==第一个这样做的文件系统是 Cedar [H87]，尽管许多现代文件系统都使用了这个想法，包括 Linux ext3 和 ext4、reiserfs、IBM 的 JFS、SGI 的 XFS 和 Windows NTFS。==

The basic idea is as follows.
==基本思想如下。==

When updating the disk, before overwriting the structures in place, first write down a little note (somewhere else on the disk, in a well-known location) describing what you are about to do.
==在更新磁盘时，在就地覆盖结构之前，先写下一个小笔记（在磁盘的其他地方，一个众所周知的位置），描述你即将要做什么。==

Writing this note is the “write ahead” part, and we write it to a structure that we organize as a “log”; hence, write-ahead logging.
==写下这个笔记就是“写前”部分，我们将其写入一个被组织为“日志”的结构中；因此得名写前日志。==

By writing the note to disk, you are guaranteeing that if a crash takes places during the update (overwrite) of the structures you are updating, you can go back and look at the note you made and try again; thus, you will know exactly what to fix (and how to fix it) after a crash, instead of having to scan the entire disk.
==通过将笔记写入磁盘，你可以保证如果在更新（覆盖）你正在更新的结构期间发生崩溃，你可以回头查看你做的笔记并重试；因此，在崩溃后你将确切知道要修复什么（以及如何修复），而不必扫描整个磁盘。==

By design, journaling thus adds a bit of work during updates to greatly reduce the amount of work required during recovery.
==通过这种设计，日志在更新期间增加了一点工作量，以大幅减少恢复期间所需的工作量。==

We’ll now describe how **Linux ext3**, a popular journaling file system, incorporates journaling into the file system.
==我们现在将描述流行的日志文件系统 **Linux ext3** 是如何将日志融入文件系统的。==

Most of the on-disk structures are identical to **Linux ext2**, e.g., the disk is divided into block groups, and each block group contains an inode bitmap, data bitmap, inodes, and data blocks.
==大多数磁盘结构与 **Linux ext2** 相同，例如，磁盘被划分为块组，每个块组包含 inode 位图、数据位图、inode 和数据块。==

The new key structure is the **journal** itself, which occupies some small amount of space within the partition or on another device.
==新的关键结构是**日志**本身，它在分区内或另一个设备上占据少量的空间。==

Thus, an ext2 file system (without journaling) looks like this:
==因此，一个 ext2 文件系统（没有日志）看起来像这样：==

`Super | Group 0 | Group 1 | . . . | Group N`
==`超级块 | 组 0 | 组 1 | . . . | 组 N`==

Assuming the journal is placed within the same file system image (though sometimes it is placed on a separate device, or as a file within the file system), an ext3 file system with a journal looks like this:
==假设日志放置在同一个文件系统镜像中（尽管有时它被放置在独立的设备上，或者作为文件系统内的一个文件），带日志的 ext3 文件系统看起来像这样：==

`Super | Journal | Group 0 | Group 1 | . . . | Group N`
==`超级块 | 日志 | 组 0 | 组 1 | . . . | 组 N`==

The real difference is just the presence of the journal, and of course, how it is used.
==真正的区别仅在于日志的存在，当然，还有它是如何被使用的。==

**Data Journaling**
==**数据日志**==

Let’s look at a simple example to understand how **data journaling** works.
==让我们看一个简单的例子来理解**数据日志**（data journaling）是如何工作的。==

Data journaling is available as a mode with the Linux ext3 file system, from which much of this discussion is based.
==数据日志是 Linux ext3 文件系统的一种可选模式，本次讨论的大部分内容都基于此。==

Say we have our canonical update again, where we wish to write the inode ($I[v2]$), bitmap ($B[v2]$), and data block ($Db$) to disk again.
==假设我们再次遇到经典的更新情况，我们希望再次将 inode ($I[v2]$)、位图 ($B[v2]$) 和数据块 ($Db$) 写入磁盘。==

Before writing them to their final disk locations, we are now first going to write them to the log (a.k.a. journal).
==在将它们写入最终磁盘位置之前，我们现在首先将它们写入日志（又称 journal）。==

This is what this will look like in the log:
==这就是它在日志中的样子：==

`Journal: | TxB | I[v2] | B[v2] | Db | TxE |`
==`日志： | 事务开始 | I[v2] | B[v2] | Db | 事务结束 |`==

You can see we have written five blocks here.
==你可以看到我们在这里写了五个块。==

The **transaction begin** (TxB) tells us about this update, including information about the pending update to the file system (e.g., the final addresses of the blocks $I[v2]$, $B[v2]$, and $Db$), and some kind of **transaction identifier** (TID).
==**事务开始** (TxB) 告知我们这次更新的信息，包括文件系统待处理更新的信息（例如块 $I[v2]$、$B[v2]$ 和 $Db$ 的最终地址），以及某种**事务标识符** (TID)。==

The middle three blocks just contain the exact contents of the blocks themselves; this is known as **physical logging** as we are putting the exact physical contents of the update in the journal (an alternate idea, **logical logging**, puts a more compact logical representation of the update in the journal, e.g., “this update wishes to append data block Db to file X”, which is a little more complex but can save space in the log and perhaps improve performance).
==中间的三个块只包含块本身的精确内容；这被称为**物理日志**（physical logging），因为我们将更新的精确物理内容放入日志中（另一种想法是**逻辑日志** logical logging，它将更新的更紧凑的逻辑表示放入日志中，例如“此更新希望将数据块 Db 追加到文件 X”，这稍微复杂一些，但可以节省日志空间并可能提高性能）。==

The final block (TxE) is a marker of the end of this transaction, and will also contain the TID.
==最后一个块 (TxE) 是该事务结束的标记，也将包含 TID。==

Once this transaction is safely on disk, we are ready to overwrite the old structures in the file system; this process is called **checkpointing**.
==一旦该事务安全地存入磁盘，我们就准备好覆盖文件系统中的旧结构了；这个过程被称为**检查点**（checkpointing）。==

Thus, to **checkpoint** the file system (i.e., bring it up to date with the pending update in the journal), we issue the writes $I[v2]$, $B[v2]$, and $Db$ to their disk locations as seen above; if these writes complete successfully, we have successfully checkpointed the file system and are basically done.
==因此，为了对文件系统执行**检查点**（即，使其与日志中待处理的更新保持一致），我们向上述磁盘位置发出 $I[v2]$、$B[v2]$ 和 $Db$ 的写入请求；如果这些写入成功完成，我们就成功地对文件系统执行了检查点，基本上就大功告成了。==

Thus, our initial sequence of operations:
==因此，我们最初的操作序列如下：==

1. **Journal write:** Write the transaction, including a transaction-begin block, all pending data and metadata updates, and a transaction-end block, to the log; wait for these writes to complete.
==1. **日志写入：** 将事务（包括事务开始块、所有待处理的数据和元数据更新以及事务结束块）写入日志；等待这些写入完成。==

2. **Checkpoint:** Write the pending metadata and data updates to their final locations in the file system.
==2. **检查点：** 将待处理的元数据和数据更新写入它们在文件系统中的最终位置。==

In our example, we would write TxB, $I[v2]$, $B[v2]$, $Db$, and TxE to the journal first.
==在我们的例子中，我们会先将 TxB、$I[v2]$、$B[v2]$、$Db$ 和 TxE 写入日志。==

When these writes complete, we would complete the update by checkpointing $I[v2]$, $B[v2]$, and $Db$, to their final locations on disk.
==当这些写入完成时，我们将通过把 $I[v2]$、$B[v2]$ 和 $Db$ 检查点到它们在磁盘上的最终位置来完成更新。==

Things get a little trickier when a crash occurs during the writes to the journal.
==当在写入日志期间发生崩溃时，事情会变得有点复杂。==

Here, we are trying to write the set of blocks in the transaction (e.g., TxB, $I[v2]$, $B[v2]$, $Db$, TxE) to disk.
==在这里，我们试图将事务中的一组块（例如 TxB、$I[v2]$、$B[v2]$、$Db$、TxE）写入磁盘。==

One simple way to do this would be to issue each one at a time, waiting for each to complete, and then issuing the next.
==一种简单的方法是逐个发出请求，等待每个请求完成，然后再发出下一个。==

However, this is slow.
==然而，这很慢。==

Ideally, we’d like to issue all five block writes at once, as this would turn five writes into a single sequential write and thus be faster.
==理想情况下，我们希望一次性发出所有五个块的写入，因为这会将五次写入变成一次顺序写入，从而更快。==

However, this is unsafe, for the following reason: given such a big write, the disk internally may perform scheduling and complete small pieces of the big write in any order.
==然而，这是不安全的，原因如下：考虑到这么大的写入量，磁盘内部可能会进行调度，并以任何顺序完成大写入中的小片段。==

Thus, the disk internally may (1) write TxB, $I[v2]$, $B[v2]$, and TxE and only later (2) write $Db$.
==因此，磁盘内部可能会 (1) 写入 TxB、$I[v2]$、$B[v2]$ 和 TxE，而稍后才 (2) 写入 $Db$。==

Unfortunately, if the disk loses power between (1) and (2), this is what ends up on disk:
==不幸的是，如果磁盘在 (1) 和 (2) 之间失去动力，磁盘上的最终结果如下：==

`Journal: | TxB id=1 | I[v2] | B[v2] | ?? | TxE id=1 |`
==`日志： | TxB id=1 | I[v2] | B[v2] | ?? | TxE id=1 |`==

Why is this a problem?
==为什么这是一个问题？==

Well, the transaction looks like a valid transaction (it has a begin and an end with matching sequence numbers).
==嗯，这个事务看起来像一个有效的事务（它有一个开始和结束，且序列号匹配）。==

Further, the file system can’t look at that fourth block and know it is wrong; after all, it is arbitrary user data.
==此外，文件系统无法通过查看第四个块来判断它是错误的；毕竟，它是任意的用户数据。==

Thus, if the system now reboots and runs recovery, it will replay this transaction, and ignorantly copy the contents of the garbage block ‘??’ to the location where $Db$ is supposed to live.
==因此，如果系统现在重启并运行恢复，它将重放该事务，并无知地将垃圾块 ‘??’ 的内容复制到 $Db$ 应该存放的位置。==

This is bad for arbitrary user data in a file; it is much worse if it happens to a critical piece of file system, such as the superblock, which could render the file system unmountable.
==这对于文件中的任意用户数据来说是很糟糕的；如果它发生在文件系统的关键部分（如超级块），情况会糟糕得多，可能会导致文件系统无法挂载。==

**ASIDE: FORCING WRITES TO DISK**
==**旁注：强制写入磁盘**==

To enforce ordering between two disk writes, modern file systems have to take a few extra precautions.
==为了强制执行两次磁盘写入之间的顺序，现代文件系统必须采取一些额外的预防措施。==

In olden times, forcing ordering between two writes, $A$ and $B$, was easy: just issue the write of $A$ to the disk, wait for the disk to interrupt the OS when the write is complete, and then issue the write of $B$.
==在过去，强制两次写入 $A$ 和 $B$ 之间的顺序很容易：只需向磁盘发出 $A$ 的写入，等待磁盘在写入完成时中断操作系统，然后发出 $B$ 的写入。==

Things got slightly more complex due to the increased use of write caches within disks.
==由于磁盘内写入缓存的使用增加，情况变得稍微复杂了一些。==

With write buffering enabled (sometimes called **immediate reporting**), a disk will inform the OS the write is complete when it simply has been placed in the disk’s memory cache, and has not yet reached disk.
==当启用写入缓冲（有时称为**立即报告**）时，磁盘会在写入仅被放入磁盘内存缓存而尚未到达磁盘时，就通知操作系统写入已完成。==

If the OS then issues a subsequent write, it is not guaranteed to reach the disk after previous writes; thus ordering between writes is not preserved.
==如果操作系统随后发出后续写入，则不能保证它会在先前的写入之后到达磁盘；因此，写入之间的顺序无法得到保持。==

One solution is to disable write buffering.
==一种解决方案是禁用写入缓冲。==

However, more modern systems take extra precautions and issue explicit **write barriers**; such a barrier, when it completes, guarantees that all writes issued before the barrier will reach disk before any writes issued after the barrier.
==然而，更现代的系统会采取额外的预防措施，并发出显式的**写入屏障**（write barriers）；这种屏障在完成时保证，在屏障之前发出的所有写入都将在屏障之后发出的任何写入之前到达磁盘。==

All of this machinery requires a great deal of trust in the correct operation of the disk.
==所有这些机制都需要对磁盘的正确运行抱有极大的信任。==

Unfortunately, recent research shows that some disk manufacturers, in an effort to deliver “higher performing” disks, explicitly ignore write-barrier requests, thus making the disks seemingly run faster but at the risk of incorrect operation [C+13, R+11].
==不幸的是，最近的研究表明，一些磁盘制造商为了提供“更高性能”的磁盘，明确忽略了写入屏障请求，从而使磁盘看起来运行得更快，但却冒着运行错误的风险 [C+13, R+11]。==

As Kahan said, the fast almost always beats out the slow, even if the fast is wrong.
==正如 Kahan 所说，快几乎总是胜过慢，即使快是错误的。==

**ASIDE: OPTIMIZING LOG WRITES**
==**旁注：优化日志写入**==

You may have noticed a particular inefficiency of writing to the log.
==你可能已经注意到写入日志的一种特定低效性。==

Namely, the file system first has to write out the transaction-begin block and contents of the transaction; only after these writes complete can the file system send the transaction-end block to disk.
==也就是说，文件系统首先必须写出事务开始块和事务内容；只有在这些写入完成后，文件系统才能将事务结束块发送到磁盘。==

The performance impact is clear, if you think about how a disk works: usually an extra rotation is incurred (think about why).
==如果你思考一下磁盘是如何工作的，性能影响就很清楚了：通常会产生一次额外的旋转延迟（想想为什么）。==

One of our former graduate students, Vijayan Prabhakaran, had a simple idea to fix this problem [P+05].
==我们以前的一位研究生 Vijayan Prabhakaran 提出了一个简单的想法来解决这个问题 [P+05]。==

When writing a transaction to the journal, include a checksum of the contents of the journal in the begin and end blocks.
==在向日志写入事务时，在开始块和结束块中包含日志内容的校验和。==

Doing so enables the file system to write the entire transaction at once, without incurring a wait; if, during recovery, the file system sees a mismatch in the computed checksum versus the stored checksum in the transaction, it can conclude that a crash occurred during the write of the transaction and thus discard the file-system update.
==这样做使文件系统能够一次性写入整个事务，而无需等待；如果在恢复期间，文件系统发现计算出的校验和与事务中存储的校验和不匹配，它可以断定在事务写入过程中发生了崩溃，从而放弃该文件系统更新。==

Thus, with a small tweak in the write protocol and recovery system, a file system can achieve faster common-case performance; on top of that, the system is slightly more reliable, as any reads from the journal are now protected by a checksum.
==因此，通过对写入协议和恢复系统进行微调，文件系统可以实现更快的常见情况性能；除此之外，系统稍微更可靠一些，因为现在从日志中读取的任何内容都受到校验和的保护。==

This simple fix was attractive enough to gain the notice of Linux file system developers, who then incorporated it into the next generation Linux file system, called (you guessed it!) **Linux ext4**.
==这个简单的修复方案非常有吸引力，引起了 Linux 文件系统开发人员的注意，随后他们将其纳入了下一代 Linux 文件系统，即（你猜对了！）**Linux ext4**。==

It now ships on millions of machines worldwide, including the Android handheld platform.
==它现在已安装在全球数百万台机器上，包括 Android 手持平台。==

Thus, every time you write to disk on many Linux-based systems, a little code developed at Wisconsin makes your system a little faster and more reliable.
==因此，在许多基于 Linux 的系统上，每当你向磁盘写入数据时，威斯康星大学开发的一点代码都会让你的系统变得更快、更可靠一些。==

To avoid this problem, the file system issues the transactional write in two steps.
==为了避免这个问题，文件系统分两步发出事务性写入。==

First, it writes all blocks except the TxE block to the journal, issuing these writes all at once.
==首先，它将除 TxE 块之外的所有块写入日志，一次性发出这些写入。==

When these writes complete, the journal will look something like this (assuming our append workload again):
==当这些写入完成时，日志看起来就像这样（再次假设我们的追加工作负载）：==

`Journal: | TxB id=1 | I[v2] | B[v2] | Db |`
==`日志： | TxB id=1 | I[v2] | B[v2] | Db |`==

When those writes complete, the file system issues the write of the TxE block, thus leaving the journal in this final, safe state:
==当这些写入完成时，文件系统发出 TxE 块的写入，从而使日志处于最终的安全状态：==

`Journal: | TxB id=1 | I[v2] | B[v2] | Db | TxE id=1 |`
==`日志： | TxB id=1 | I[v2] | B[v2] | Db | TxE id=1 |`==

An important aspect of this process is the **atomicity** guarantee provided by the disk.
==这个过程的一个重要方面是磁盘提供的**原子性**（atomicity）保证。==

It turns out that the disk guarantees that any 512-byte write will either happen or not (and never be half-written); thus, to make sure the write of TxE is atomic, one should make it a single 512-byte block.
==事实证明，磁盘保证任何 512 字节的写入要么发生，要么不发生（永远不会只写一半）；因此，为了确保 TxE 的写入是原子的，应该将其设为单个 512 字节的块。==

Thus, our current protocol to update the file system, with each of its three phases labeled:
==因此，我们当前的更新文件系统的协议如下，其三个阶段分别标记为：==

1. **Journal write:** Write the contents of the transaction (including TxB, metadata, and data) to the log; wait for these writes to complete.
==1. **日志写入：** 将事务内容（包括 TxB、元数据和数据）写入日志；等待这些写入完成。==

2. **Journal commit:** Write the transaction commit block (containing TxE) to the log; wait for write to complete; transaction is said to be **committed**.
==2. **日志提交：** 将事务提交块（包含 TxE）写入日志；等待写入完成；此时事务被称为已**提交**（committed）。==

3. **Checkpoint:** Write the contents of the update (metadata and data) to their final on-disk locations.
==3. **检查点：** 将更新内容（元数据和数据）写入它们的最终磁盘位置。==

**Recovery**
==**恢复**==

Let’s now understand how a file system can use the contents of the journal to **recover** from a crash.
==现在让我们了解文件系统如何利用日志内容从崩溃中**恢复**（recover）。==

A crash may happen at any time during this sequence of updates.
==崩溃可能在这一系列更新过程中的任何时刻发生。==

If the crash happens before the transaction is written safely to the log (i.e., before Step 2 above completes), then our job is easy: the pending update is simply skipped.
==如果崩溃发生在事务安全地写入日志之前（即在上述步骤 2 完成之前），那么我们的工作很简单：只需跳过待处理的更新。==

If the crash happens after the transaction has committed to the log, but before the checkpoint is complete, the file system can **recover** the update as follows.
==如果崩溃发生在事务已提交到日志之后，但在检查点完成之前，文件系统可以按如下方式**恢复**更新。==

When the system boots, the file system recovery process will scan the log and look for transactions that have committed to the disk; these transactions are thus **replayed** (in order), with the file system again attempting to write out the blocks in the transaction to their final on-disk locations.
==当系统启动时，文件系统恢复过程将扫描日志并寻找已提交到磁盘的事务；因此，这些事务将被**重放**（replayed，按顺序执行），文件系统再次尝试将事务中的块写入它们的最终磁盘位置。==

This form of logging is one of the simplest forms there is, and is called **redo logging**.
==这种形式的日志是现存最简单的形式之一，被称为**重做日志**（redo logging）。==

By recovering the committed transactions in the journal, the file system ensures that the on-disk structures are consistent, and thus can proceed by mounting the file system and readying itself for new requests.
==通过恢复日志中已提交的事务，文件系统确保磁盘结构是一致的，从而可以继续挂载文件系统并准备好处理新请求。==

Note that it is fine for a crash to happen at any point during checkpointing, even after some of the updates to the final locations of the blocks have completed.
==注意，在检查点过程中的任何一点发生崩溃都是可以的，即使在某些块到最终位置的更新已经完成后。==

In the worst case, some of these updates are simply performed again during recovery.
==在最坏的情况下，其中一些更新在恢复期间只是简单地再次执行。==

Because recovery is a rare operation (only taking place after an unexpected system crash), a few redundant writes are nothing to worry about.
==因为恢复是一种罕见的操作（仅在意外系统崩溃后发生），所以少量冗余写入无需担心。==

**Batching Log Updates**
==**批量日志更新**==

You might have noticed that the basic protocol could add a lot of extra disk traffic.
==你可能已经注意到，基本协议可能会增加大量的额外磁盘流量。==

For example, imagine we create two files in a row, called `file1` and `file2`, in the same directory.
==例如，假设我们在同一个目录中连续创建两个文件，分别称为 `file1` 和 `file2`。==

To create one file, one has to update a number of on-disk structures, minimally including: the inode bitmap (to allocate a new inode), the newly-created inode of the file, the data block of the parent directory containing the new directory entry, and the parent directory inode (which now has a new modification time).
==为了创建一个文件，必须更新许多磁盘结构，至少包括：inode 位图（用于分配新的 inode）、新创建的文件 inode、包含新目录项的父目录数据块，以及父目录 inode（它现在有了新的修改时间）。==

With journaling, we logically commit all of this information to the journal for each of our two file creations; because the files are in the same directory, and assuming they even have inodes within the same inode block, this means that if we’re not careful, we’ll end up writing these same blocks over and over.
==通过日志，我们逻辑上为两次文件创建分别向日志提交所有这些信息；因为文件在同一个目录中，并且假设它们的 inode 甚至在同一个 inode 块内，这意味着如果我们不小心，最终会一遍又一遍地写入这些相同的块。==

To remedy this problem, some file systems do not commit each update to disk one at a time (e.g., Linux ext3); rather, one can buffer all updates into a global transaction.
==为了解决这个问题，一些文件系统并不一次向磁盘提交一个更新（例如 Linux ext3）；相反，人们可以将所有更新缓冲到一个全局事务中。==

In our example above, when the two files are created, the file system just marks the in-memory inode bitmap, inodes of the files, directory data, and directory inode as dirty, and adds them to the list of blocks that form the current transaction.
==在上面的例子中，当创建两个文件时，文件系统只是将内存中的 inode 位图、文件的 inode、目录数据和目录 inode 标记为脏，并将它们添加到构成当前事务的块列表中。==

When it is finally time to write these blocks to disk (say, after a timeout of 5 seconds), this single global transaction is committed containing all of the updates described above.
==当最终需要将这些块写入磁盘时（比如在 5 秒超时后），这个包含上述所有更新的单一全局事务被提交。==

Thus, by buffering updates, a file system can avoid excessive write traffic to disk in many cases.
==因此，通过缓冲更新，文件系统在许多情况下可以避免过多的磁盘写入流量。==

**Making The Log Finite**
==**使日志有限**==

We thus have arrived at a basic protocol for updating file-system on-disk structures.
==至此，我们得出了更新文件系统磁盘结构的基本协议。==

The file system buffers updates in memory for some time; when it is finally time to write to disk, the file system first carefully writes out the details of the transaction to the journal (a.k.a. write-ahead log); after the transaction is complete, the file system checkpoints those blocks to their final locations on disk.
==文件系统将更新在内存中缓冲一段时间；当最终需要写入磁盘时，文件系统首先小心地将事务详情写入日志（又称写前日志）；事务完成后，文件系统将这些块检查点到它们在磁盘上的最终位置。==

However, the log is of a finite size.
==然而，日志的大小是有限的。==

If we keep adding transactions to it (as in this figure), it will soon fill.
==如果我们不断地向其中添加事务（如图所示），它很快就会填满。==

What do you think happens then?
==你认为那时会发生什么？==

`Journal: | Tx1 | Tx2 | Tx3 | Tx4 | Tx5 | ... |`
==`日志： | Tx1 | Tx2 | Tx3 | Tx4 | Tx5 | ... |`==

Two problems arise when the log becomes full.
==当日志变满时，会出现两个问题。==

The first is simpler, but less critical: the larger the log, the longer recovery will take, as the recovery process must replay all the transactions within the log (in order) to recover.
==第一个问题较简单，但不那么关键：日志越大，恢复所需的时间就越长，因为恢复过程必须（按顺序）重放日志中的所有事务才能恢复。==

The second is more of an issue: when the log is full (or nearly full), no further transactions can be committed to the disk, thus making the file system “less than useful” (i.e., useless).
==第二个问题更为严重：当日志已满（或接近已满）时，无法再向磁盘提交更多事务，从而使文件系统变得“不太有用”（即无用）。==

To address these problems, journaling file systems treat the log as a circular data structure, re-using it over and over; this is why the journal is sometimes referred to as a **circular log**.
==为了解决这些问题，日志文件系统将日志视为循环数据结构，反复重用；这就是为什么日志有时被称为**循环日志**（circular log）。==

To do so, the file system must take action some time after a checkpoint.
==为此，文件系统必须在检查点之后的某个时间采取行动。==

Specifically, once a transaction has been checkpointed, the file system should free the space it was occupying within the journal, allowing the log space to be reused.
==具体来说，一旦一个事务被检查点，文件系统就应该释放它在日志中占据的空间，从而允许重用日志空间。==

There are many ways to achieve this end; for example, you could simply mark the oldest and newest non-checkpointed transactions in the log in a **journal superblock**; all other space is free.
==有很多方法可以达到这个目的；例如，你只需在**日志超级块**中标记日志中最旧和最新的未检查点事务；所有其他空间都是空闲的。==

Here is a graphical depiction:
==这是一个图形化描述：==

`Journal: | Journal Super | Tx1 | Tx2 | Tx3 | Tx4 | Tx5 | ... |`
==`日志： | 日志超级块 | Tx1 | Tx2 | Tx3 | Tx4 | Tx5 | ... |`==

In the journal superblock (not to be confused with the main file system superblock), the journaling system records enough information to know which transactions have not yet been checkpointed, and thus reduces recovery time as well as enables re-use of the log in a circular fashion.
==在日志超级块（不要与主文件系统超级块混淆）中，日志系统记录了足够的信息，以了解哪些事务尚未被检查点，从而缩短了恢复时间，并实现了日志的循环重用。==

And thus we add another step to our basic protocol:
==因此，我们在基本协议中增加了另一个步骤：==

1. **Journal write:** Write the contents of the transaction (containing TxB and the contents of the update) to the log; wait for these writes to complete.
==1. **日志写入：** 将事务内容（包含 TxB 和更新内容）写入日志；等待这些写入完成。==

2. **Journal commit:** Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction is now **committed**.
==2. **日志提交：** 将事务提交块（包含 TxE）写入日志；等待写入完成；事务现在已**提交**。==

3. **Checkpoint:** Write the contents of the update to their final locations within the file system.
==3. **检查点：** 将更新内容写入文件系统中的最终位置。==

4. **Free:** Some time later, mark the transaction free in the journal by updating the journal superblock.
==4. **释放：** 一段时间后，通过更新日志超级块，将该事务在日志中标记为释放。==

Thus we have our final data journaling protocol.
==至此，我们得到了最终的数据日志协议。==

But there is still a problem: we are writing each data block to the disk **twice**, which is a heavy cost to pay, especially for something as rare as a system crash.
==但仍然存在一个问题：我们将每个数据块向磁盘写入了**两次**，这是一个沉重的代价，特别是对于像系统崩溃这样罕见的事情。==

Can you figure out a way to retain consistency without writing data twice?
==你能想出一种在不写入两次数据的情况下保持一致性的方法吗？==

**Metadata Journaling**
==**元数据日志**==

Although recovery is now fast (scanning the journal and replaying a few transactions as opposed to scanning the entire disk), normal operation of the file system is slower than we might desire.
==尽管现在的恢复速度很快（扫描日志并重放少量事务，而不是扫描整个磁盘），但文件系统的正常运行速度可能比我们预期的要慢。==

In particular, for each write to disk, we are now also writing to the journal first, thus doubling write traffic; this doubling is especially painful during sequential write workloads, which now will proceed at half the peak write bandwidth of the drive.
==特别是对于每次磁盘写入，我们现在还要先写入日志，从而使写入流量翻倍；这种翻倍在顺序写入工作负载中尤其痛苦，因为现在的写入速度只有驱动器峰值带宽的一半。==

Further, between writes to the journal and writes to the main file system, there is a costly seek, which adds noticeable overhead for some workloads.
==此外，在写入日志和写入主文件系统之间存在昂贵的寻道，这为某些工作负载增加了明显的开销。==

Because of the high cost of writing every data block to disk twice, people have tried a few different things in order to speed up performance.
==由于将每个数据块写入磁盘两次的成本很高，人们尝试了几种不同的方法来提高性能。==

For example, the mode of journaling we described above is often called **data journaling** (as in Linux ext3), as it journals all user data (in addition to the metadata of the file system).
==例如，我们上面描述的日志模式通常被称为**数据日志**（data journaling，如 Linux ext3 中），因为它记录了所有用户数据（以及文件系统的元数据）。==

A simpler (and more common) form of journaling is sometimes called **ordered journaling** (or just **metadata journaling**), and it is nearly the same, except that user data is **not** written to the journal.
==一种更简单（且更常见）的日志形式有时被称为**有序日志**（ordered journaling，或简称为**元数据日志** metadata journaling），它几乎是一样的，只是用户数据**不**写入日志。==

Thus, when performing the same update as above, the following information would be written to the journal:
==因此，在执行与上述相同的更新时，以下信息将被写入日志：==

`Journal: | TxB | I[v2] | B[v2] | TxE |`
==`日志： | TxB | I[v2] | B[v2] | TxE |`==

The data block $Db$, previously written to the log, would instead be written to the file system proper, avoiding the extra write; given that most I/O traffic to the disk is data, not writing data twice substantially reduces the I/O load of journaling.
==之前被写入日志的数据块 $Db$ 现在将直接写入文件系统本身，从而避免了额外的写入；鉴于磁盘的大部分 I/O 流量是数据，不写入两次数据可大幅减轻日志的 I/O 负载。==

The modification does raise an interesting question, though: when should we write data blocks to disk?
==不过，这种修改确实提出了一个有趣的问题：我们应该在什么时候将数据块写入磁盘？==

Let’s again consider our example append of a file to understand the problem better.
==让我们再次考虑追加文件的例子，以便更好地理解这个问题。==

The update consists of three blocks: $I[v2]$, $B[v2]$, and $Db$.
==更新由三个块组成：$I[v2]$、$B[v2]$ 和 $Db$。==

The first two are both metadata and will be logged and then checkpointed; the latter will only be written once to the file system.
==前两个都是元数据，将被记录日志并进行检查点；后者将只向文件系统写入一次。==

When should we write $Db$ to disk? Does it matter?
==我们应该在什么时候将 $Db$ 写入磁盘？这有关系吗？==

As it turns out, the ordering of the data write does matter for metadata-only journaling.
==事实证明，对于仅元数据日志，数据写入的顺序确实很重要。==

For example, what if we write $Db$ to disk **after** the transaction (containing $I[v2]$ and $B[v2]$) completes?
==例如，如果我们是在事务（包含 $I[v2]$ 和 $B[v2]$）完成后才将 $Db$ 写入磁盘，会发生什么？==

Unfortunately, this approach has a problem: the file system is consistent but $I[v2]$ may end up pointing to garbage data.
==不幸的是，这种方法有一个问题：文件系统是一致的，但 $I[v2]$ 最终可能会指向垃圾数据。==

Specifically, consider the case where $I[v2]$ and $B[v2]$ are written but $Db$ did not make it to disk.
==具体来说，考虑这样一种情况：$I[v2]$ 和 $B[v2]$ 已写入，但 $Db$ 未能写入磁盘。==

The file system will then try to recover.
==文件系统随后将尝试恢复。==

Because $Db$ is **not** in the log, the file system will replay writes to $I[v2]$ and $B[v2]$, and produce a consistent file system (from the perspective of file-system metadata).
==因为 $Db$ **不在**日志中，文件系统将重放对 $I[v2]$ 和 $B[v2]$ 的写入，并产生一个一致的文件系统（从文件系统元数据的角度来看）。==

However, $I[v2]$ will be pointing to garbage data, i.e., at whatever was in the slot where $Db$ was headed.
==然而，$I[v2]$ 将指向垃圾数据，即指向 $Db$ 原本要存放的位置中的任何内容。==

To ensure this situation does not arise, some file systems (e.g., Linux ext3) write data blocks (of regular files) to the disk **first**, before related metadata is written to disk.
==为了确保不出现这种情况，一些文件系统（例如 Linux ext3）会**首先**将（普通文件的）数据块写入磁盘，然后再将相关的元数据写入磁盘。==

Specifically, the protocol is as follows:
==具体来说，协议如下：==

1. **Data write:** Write data to final location; wait for completion (the wait is optional; see below for details).
==1. **数据写入：** 将数据写入最终位置；等待完成（等待是可选的；详见下文）。==

2. **Journal metadata write:** Write the begin block and metadata to the log; wait for writes to complete.
==2. **日志元数据写入：** 将开始块和元数据写入日志；等待写入完成。==

3. **Journal commit:** Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction (including data) is now **committed**.
==3. **日志提交：** 将事务提交块（包含 TxE）写入日志；等待写入完成；此时事务（包括数据）已**提交**。==

4. **Checkpoint metadata:** Write the contents of the metadata update to their final locations within the file system.
==4. **元数据检查点：** 将元数据更新内容写入文件系统中的最终位置。==

5. **Free:** Later, mark the transaction free in journal superblock.
==5. **释放：** 稍后，在日志超级块中将事务标记为释放。==

By forcing the data write first, a file system can guarantee that a pointer will never point to garbage.
==通过强制首先写入数据，文件系统可以保证指针永远不会指向垃圾。==

Indeed, this rule of “write the pointed-to object before the object that points to it” is at the core of crash consistency, and is exploited even further by other crash consistency schemes [GP94] (see below for details).
==事实上，这种“在指向某个对象的对象之前写入被指向的对象”的规则是崩溃一致性的核心，并被其他崩溃一致性方案 [GP94] 进一步利用（详见下文）。==

In most systems, metadata journaling (akin to ordered journaling of ext3) is more popular than full data journaling.
==在大多数系统中，元数据日志（类似于 ext3 的有序日志）比完整数据日志更受欢迎。==

For example, Windows NTFS and SGI’s XFS both use some form of metadata journaling.
==例如，Windows NTFS 和 SGI 的 XFS 都使用某种形式的元数据日志。==

Linux ext3 gives you the option of choosing either data, ordered, or unordered modes (in unordered mode, data can be written at any time).
==Linux ext3 允许你选择数据模式、有序模式或无序模式（在无序模式下，数据可以在任何时间写入）。==

All of these modes keep metadata consistent; they vary in their semantics for data.
==所有这些模式都能保持元数据一致；它们在数据的语义上有所不同。==

Finally, note that forcing the data write to complete (Step 1) before issuing writes to the journal (Step 2) is not required for correctness, as indicated in the protocol above.
==最后，请注意，如上述协议所示，在向日志发出写入（步骤 2）之前强制完成数据写入（步骤 1）并不是正确性所必需的。==

Specifically, it would be fine to concurrently issue writes to data, the transaction-begin block, and journaled metadata; the only real requirement is that Steps 1 and 2 complete before the issuing of the journal commit block (Step 3).
==具体而言，同时发出对数据、事务开始块和日志元数据的写入是可以的；唯一真正的要求是步骤 1 和 2 在发出日志提交块（步骤 3）之前完成。==

**Tricky Case: Block Reuse**
==**棘手案例：块重用**==

There are some interesting corner cases that make journaling more tricky, and thus are worth discussing.
==有一些有趣的边缘情况使日志变得更加复杂，因此值得讨论。==

A number of them revolve around block reuse; as Stephen Tweedie (one of the main forces behind ext3) said:
==其中许多情况围绕着块重用展开；正如 Stephen Tweedie（ext3 背后的主要推动力之一）所说：==

“What’s the hideous part of the entire system? ... It’s deleting files. Everything to do with delete is hairy. Everything to do with delete... you have nightmares around what happens if blocks get deleted and then reallocated.” [T00]
==“整个系统最可怕的部分是什么？……是删除文件。所有与删除有关的事情都很棘手。所有与删除有关的事情……如果你做噩梦，梦见的准是如果块被删除然后又重新分配会发生什么。”[T00]==

The particular example Tweedie gives is as follows.
==Tweedie 给出的具体例子如下。==

Suppose you are using some form of metadata journaling (and thus data blocks for files are **not** journaled).
==假设你正在使用某种形式的元数据日志（因此文件的数据块**不**记录日志）。==

Let’s say you have a directory called `foo`.
==假设你有一个名为 `foo` 的目录。==

The user adds an entry to `foo` (say by creating a file), and thus the contents of `foo` (because directories are considered metadata) are written to the log; assume the location of the `foo` directory data is block 1000.
==用户向 `foo` 中添加了一个条目（例如通过创建文件），因此 `foo` 的内容（因为目录被视为元数据）被写入日志；假设 `foo` 目录数据的位置是块 1000。==

The log thus contains something like this:
==因此，日志中包含类似这样的内容：==

`Journal: | TxB id=1 I[foo] ptr:1000 | D[foo] [final addr:1000] | TxE id=1 |`
==`日志： | TxB id=1 I[foo] ptr:1000 | D[foo] [最终地址:1000] | TxE id=1 |`==

At this point, the user deletes everything in the directory and the directory itself, freeing up block 1000 for reuse.
==此时，用户删除了目录中的所有内容以及目录本身，释放了块 1000 以供重用。==

Finally, the user creates a new file (say `bar`), which ends up reusing the same block (1000) that used to belong to `foo`.
==最后，用户创建了一个新文件（比如 `bar`），它最终重用了曾经属于 `foo` 的同一个块 (1000)。==

The inode of `bar` is committed to disk, as is its data; note, however, because metadata journaling is in use, only the inode of `bar` is committed to the journal; the newly-written data in block 1000 in the file `bar` is **not** journaled.
==`bar` 的 inode 及其数据都已提交到磁盘；但请注意，由于使用了元数据日志，只有 `bar` 的 inode 被提交到了日志；文件 `bar` 中块 1000 的新写入数据**没有**记录日志。==

`Journal: | TxB id=1 ... | D[foo] ... | TxE id=1 | TxB id=2 I[bar] ptr:1000 | TxE id=2 |`
==`日志： | TxB id=1 ... | D[foo] ... | TxE id=1 | TxB id=2 I[bar] ptr:1000 | TxE id=2 |`==

Now assume a crash occurs and all of this information is still in the log.
==现在假设发生了崩溃，而所有这些信息仍保存在日志中。==

During replay, the recovery process simply replays everything in the log, including the write of directory data in block 1000; the replay thus overwrites the user data of current file `bar` with old directory contents!
==在重放期间，恢复过程只需重放日志中的所有内容，包括块 1000 中目录数据的写入；因此，重放操作会用旧的目录内容覆盖当前文件 `bar` 的用户数据！==

Clearly this is not a correct recovery action, and certainly it will be a surprise to the user when reading the file `bar`.
==显然这不是正确的恢复操作，而且用户在读取文件 `bar` 时肯定会大吃一惊。==

There are a number of solutions to this problem.
==针对这个问题有多种解决方案。==

One could, for example, never reuse blocks until the delete of said blocks is checkpointed out of the journal.
==例如，可以在该块的删除操作从日志中检查点清除之前，永远不重用该块。==

What Linux ext3 does instead is to add a new type of record to the journal, known as a **revoke** record.
==Linux ext3 采取的方法是向日志添加一种新类型的记录，称为**撤销**（revoke）记录。==

In the case above, deleting the directory would cause a revoke record to be written to the journal.
==在上述情况下，删除目录会导致向日志写入一条撤销记录。==

When replaying the journal, the system first scans for such revoke records; any such revoked data is never replayed, thus avoiding the problem mentioned above.
==在重放日志时，系统首先扫描此类撤销记录；任何此类撤销的数据都不会被重写，从而避免了上述问题。==

**Wrapping Up Journaling: A Timeline**
==**总结日志：时间线**==

Before ending our discussion of journaling, we summarize the protocols we have discussed with timelines depicting each of them.
==在结束对日志的讨论之前，我们用描绘每种协议的时间线来总结我们讨论过的协议。==

Figure 42.1 shows the protocol when journaling data and metadata, whereas Figure 42.2 shows the protocol when journaling only metadata.
==图 42.1 显示了同时记录数据和元数据日志时的协议，而图 42.2 显示了仅记录元数据日志时的协议。==

In each figure, time increases in the downward direction, and each row in the figure shows the logical time that a write can be issued or might complete.
==在每张图中，时间按向下方向递增，图中的每一行显示了写入可能发出或可能完成的逻辑时间。==

For example, in the data journaling protocol (Figure 42.1), the writes of the transaction begin block (TxB) and the contents of the transaction can logically be issued at the same time, and thus can be completed in any order; however, the write to the transaction end block (TxE) must not be issued until said previous writes complete.
==例如，在数据日志协议（图 42.1）中，事务开始块 (TxB) 和事务内容的写入可以在逻辑上同时发出，因此可以以任何顺序完成；然而，在上述先前写入完成之前，不得发出事务结束块 (TxE) 的写入。==

Similarly, the checkpointing writes to data and metadata blocks cannot begin until the transaction end block has committed.
==同样，在事务结束块提交之前，不能开始对数据和元数据块的检查点写入。==

Horizontal dashed lines show where write-ordering requirements must be obeyed.
==水平虚线表示必须遵守写入顺序要求的地方。==

A similar timeline is shown for the metadata journaling protocol.
==元数据日志协议也显示了类似的时间线。==

Figure 42.1: Data Journaling Timeline
==图 42.1：数据日志时间线==

Journal | File System
==日志 | 文件系统==

TxB | Contents | TxE | Metadata | Data
==TxB | 内容 | TxE | 元数据 | 数据==

(metadata) (data)
==(元数据) (数据)==

issue | issue | issue
==发出 | 发出 | 发出==

complete | complete | complete
==完成 | 完成 | 完成==

----------------------------------
----------------------------------

issue
==发出==

complete
==完成==

----------------------------------
----------------------------------

issue | issue
==发出 | 发出==

complete | complete
==完成 | 完成==




==### 第 42 章：崩溃一致性（续）==

Figure 42.2: Metadata Journaling Timeline
==图 42.2：元数据日志时间线==

that the data write can logically be issued at the same time as the writes to the transaction begin and the contents of the journal; however, it must be issued and complete before the transaction end has been issued.
==数据写入在逻辑上可以与事务开始及日志内容写入同时发布；但是，它必须在事务结束发布之前完成发布并执行完毕。==

Finally, note that the time of completion marked for each write in the timelines is arbitrary.
==最后，请注意，时间线中标记的每次写入的完成时间是任意的。==

In a real system, completion time is determined by the I/O subsystem, which may reorder writes to improve performance.
==在实际系统中，完成时间由 I/O 子系统决定，子系统可能会为了提高性能而对写入进行重排序。==

The only guarantees about ordering that we have are those that must be enforced for protocol correctness (and are shown via the horizontal dashed lines in the figures).
==我们拥有的关于顺序的唯一保证是那些为了协议正确性而必须强制执行的保证（并在图中通过水平虚线显示）。==

#### 42.4 Solution #3: Other Approaches
==#### 42.4 解决方案 #3：其他方法==

We’ve thus far described two options in keeping file system metadata consistent: a lazy approach based on `fsck`, and a more active approach known as journaling.
==到目前为止，我们已经描述了保持文件系统元数据一致性的两种选择：一种是基于 `fsck` 的惰性方法，另一种是被称为日志记录（journaling）的更主动的方法。==

However, these are not the only two approaches.
==然而，这两者并非仅有的方法。==

One such approach, known as **Soft Updates** [GP94], was introduced by Ganger and Patt.
==其中一种被称为 **软更新**（Soft Updates）[GP94] 的方法，是由 Ganger 和 Patt 提出的。==

This approach carefully orders all writes to the file system to ensure that the on-disk structures are never left in an inconsistent state.
==这种方法仔细地安排所有写入文件系统的顺序，以确保磁盘上的结构永远不会处于不一致的状态。==

For example, by writing a pointed-to data block to disk before the inode that points to it, we can ensure that the inode never points to garbage; similar rules can be derived for all the structures of the file system.
==例如，通过在指向数据块的 inode 之前先将该数据块写入磁盘，我们可以确保 inode 永远不会指向垃圾数据；对于文件系统的所有结构，都可以推导出类似的规则。==

Implementing **Soft Updates** can be a challenge, however; whereas the journaling layer described above can be implemented with relatively little knowledge of the exact file system structures, **Soft Updates** requires intricate knowledge of each file system data structure and thus adds a fair amount of complexity to the system.
==然而，实现 **软更新** 可能是一个挑战；上述的日志层可以在对具体文件系统结构了解相对较少的情况下实现，而 **软更新** 则需要对每个文件系统数据结构有精细的了解，因此给系统增加了相当大的复杂性。==

Another approach is known as **copy-on-write** (yes, **COW**), and is used in a number of popular file systems, including Sun’s ZFS [B07].
==另一种方法被称为 **写时复制**（没错，就是 **COW**），并被用于许多流行的文件系统，包括 Sun 的 ZFS [B07]。==

This technique never overwrites files or directories in place; rather, it places new updates to previously unused locations on disk.
==这项技术从不原位覆盖文件或目录；相反，它将新的更新放在磁盘上之前未使用的位置。==

After a number of updates are completed, **COW** file systems flip the root structure of the file system to include pointers to the newly updated structures.
==在完成一定数量的更新后，**COW** 文件系统会翻转文件系统的根结构，使其包含指向新更新结构的指针。==

Doing so makes keeping the file system consistent straightforward.
==这样做使得保持文件系统一致性变得简单直接。==

We’ll be learning more about this technique when we discuss the log-structured file system (LFS) in a future chapter; LFS is an early example of a **COW**.
==在未来的章节中讨论日志结构文件系统（LFS）时，我们将学到更多关于这项技术的知识；LFS 是 **COW** 的一个早期范例。==

Another approach is one we just developed here at Wisconsin.
==另一种方法是我们刚刚在威斯康星大学开发的。==

In this technique, entitled **backpointer-based consistency** (or **BBC**), no ordering is enforced between writes.
==在这项名为 **基于后向指针的一致性**（或 **BBC**）的技术中，写入之间不强制执行任何顺序。==

To achieve consistency, an additional **back pointer** is added to every block in the system; for example, each data block has a reference to the inode to which it belongs.
==为了实现一致性，系统中的每个块都增加了一个额外的 **后向指针**；例如，每个数据块都有一个指向它所属 inode 的引用。==

When accessing a file, the file system can determine if the file is consistent by checking if the forward pointer (e.g., the address in the inode or direct block) points to a block that refers back to it.
==访问文件时，文件系统可以通过检查前向指针（例如 inode 或直接块中的地址）是否指向一个引用回它的块，来确定文件是否一致。==

If so, everything must have safely reached disk and thus the file is consistent; if not, the file is inconsistent, and an error is returned.
==如果是这样，那么所有内容肯定都已安全到达磁盘，因此文件是一致的；如果不是，文件就是不一致的，并返回错误。==

By adding back pointers to the file system, a new form of lazy crash consistency can be attained [C+12].
==通过向文件系统添加后向指针，可以实现一种新型的惰性崩溃一致性 [C+12]。==

Finally, we also have explored techniques to reduce the number of times a journal protocol has to wait for disk writes to complete.
==最后，我们还探索了一些技术，用以减少日志协议必须等待磁盘写入完成的次数。==

Entitled **optimistic crash consistency** [C+13], this new approach issues as many writes to disk as possible by using a generalized form of the **transaction checksum** [P+05], and includes a few other techniques to detect inconsistencies should they arise.
==这种被称为 **乐观崩溃一致性** [C+13] 的新方法，通过使用 **事务校验和** [P+05] 的通用形式，尽可能多地向磁盘发布写入，并包含其他几种技术来检测可能出现的不一致。==

For some workloads, these optimistic techniques can improve performance by an order of magnitude.
==对于某些工作负载，这些乐观技术可以将性能提高一个数量级。==

However, to truly function well, a slightly different disk interface is required [C+13].
==然而，为了真正发挥作用，需要一种略有不同的磁盘接口 [C+13]。==

==### 第 43 章：日志结构文件系统 (Log-structured File Systems)==

In the early 90’s, a group at Berkeley led by Professor John Ousterhout and graduate student Mendel Rosenblum developed a new file system known as the **log-structured file system** [RO91].
==20 世纪 90 年代初，伯克利大学由 John Ousterhout 教授和研究生 Mendel Rosenblum 领导的一个小组开发了一种新型文件系统，称为 **日志结构文件系统** [RO91]。==

Their motivation to do so was based on the following observations:
==他们这样做的动机基于以下观察：==

**System memories are growing:** As memory gets bigger, more data can be cached in memory.
==**系统内存正在增长：** 随着内存变大，更多数据可以缓存在内存中。==

As more data is cached, disk traffic increasingly consists of writes, as reads are serviced by the cache.
==随着缓存的数据增多，磁盘流量越来越多地由写入组成，因为读取由缓存提供服务。==

Thus, file system performance is largely determined by its write performance.
==因此，文件系统的性能在很大程度上取决于其写入性能。==

**There is a large gap between random I/O performance and sequential I/O performance:** Hard-drive transfer bandwidth has increased a great deal over the years [P98]; as more bits are packed into the surface of a drive, the bandwidth when accessing said bits increases.
==**随机 I/O 性能与顺序 I/O 性能之间存在巨大差距：** 多年来，硬盘的传输带宽大幅增加 [P98]；随着更多位被封装到驱动器表面，访问这些位时的带宽也会增加。==

Seek and rotational delay costs, however, have decreased slowly; it is challenging to make cheap and small motors spin the platters faster or move the disk arm more quickly.
==然而，寻道和旋转延迟成本下降缓慢；让廉价且微小的电机使盘片转动得更快或使磁盘臂移动得更迅速极具挑战。==

Thus, if you are able to use disks in a sequential manner, you gain a sizeable performance advantage over approaches that cause seeks and rotations.
==因此，如果你能够以顺序方式使用磁盘，那么与导致寻道和旋转的方法相比，你将获得显著的性能优势。==

**Existing file systems perform poorly on many common workloads:** For example, FFS [MJLF84] would perform a large number of writes to create a new file of size one block: one for a new inode, one to update the inode bitmap, one to the directory data block that the file is in, one to the directory inode to update it, one to the new data block that is a part of the new file, and one to the data bitmap to mark the data block as allocated.
==**现有文件系统在许多常见工作负载下表现不佳：** 例如，FFS [MJLF84] 在创建一个大小为一个块的新文件时，会执行大量写入：一个用于新 inode，一个用于更新 inode 位图，一个写入该文件所在的目录数据块，一个写入目录 inode 以更新它，一个写入作为新文件一部分的新数据块，还有一个写入数据位图以将数据块标记为已分配。==

Thus, although FFS places all of these blocks within the same block group, FFS incurs many short seeks and subsequent rotational delays and thus performance falls far short of peak sequential bandwidth.
==因此，尽管 FFS 将所有这些块放在同一个块组内，但 FFS 仍会产生许多短寻道和随后的旋转延迟，因此性能远低于峰值顺序带宽。==

**File systems are not RAID-aware:** For example, both RAID-4 and RAID-5 have the **small-write problem** where a logical write to a single block causes 4 physical I/Os to take place.
==**文件系统不具备 RAID 感知能力：** 例如，RAID-4 和 RAID-5 都有 **小写问题**，即对单个块的逻辑写入会导致 4 次物理 I/O。==

Existing file systems do not try to avoid this worst-case RAID writing behavior.
==现有文件系统并不试图避免这种最糟糕的 RAID 写入行为。==

#### 43.1 Writing To Disk Sequentially
==#### 43.1 顺序写入磁盘==

We thus have our first challenge: how do we transform all updates to file-system state into a series of sequential writes to disk?
==因此，我们面临第一个挑战：如何将对文件系统状态的所有更新转化为磁盘上的一系列顺序写入？==

To understand this better, let’s use a simple example.
==为了更好地理解这一点，让我们使用一个简单的例子。==

Imagine we are writing a data block $D$ to a file.
==假设我们要将一个数据块 $D$ 写入文件。==

Writing the data block to disk might result in the following on-disk layout, with $D$ written at disk address $A0$:
==将该数据块写入磁盘可能会导致以下磁盘布局，其中 $D$ 写入磁盘地址 $A0$：==

However, when a user writes a data block, it is not only data that gets written to disk; there is also other **metadata** that needs to be updated.
==然而，当用户写入一个数据块时，不仅是数据被写入磁盘；还有其他 **元数据** 需要更新。==

In this case, let’s also write the **inode** ($I$) of the file to disk, and have it point to the data block $D$.
==在这种情况下，让我们也将文件的 **inode** ($I$) 写入磁盘，并使其指向数据块 $D$。==

This basic idea, of simply writing all updates (such as data blocks, inodes, etc.) to the disk sequentially, sits at the heart of LFS.
==这种简单地将所有更新（如数据块、inode 等）按顺序写入磁盘的基本思想，是 LFS 的核心。==

#### 43.2 Writing Sequentially And Effectively
==#### 43.2 有效且顺序地写入==

To achieve this end, LFS uses an ancient technique known as **write buffering**.
==为了达到这个目的，LFS 使用了一种古老的技术，称为 **写入缓冲**。==

Before writing to the disk, LFS keeps track of updates in memory; when it has received a sufficient number of updates, it writes them to disk all at once, thus ensuring efficient use of the disk.
==在写入磁盘之前，LFS 在内存中跟踪更新；当收到足够数量的更新时，它会一次性将它们写入磁盘，从而确保磁盘的高效利用。==

The large chunk of updates LFS writes at one time is referred to by the name of a **segment**.
==LFS 一次写入的大块更新被称为 **段**（segment）。==

#### 43.3 How Much To Buffer?
==#### 43.3 需要缓冲多少？==

To obtain a concrete answer, let’s assume we are writing out $D$ MB.
==为了得到一个具体的答案，假设我们要写出 $D$ MB 的数据。==

The time to write out this chunk of data ($T_{write}$) is the positioning time $T_{position}$ plus the time to transfer $D$ ($\frac{D}{R_{peak}}$), or:
==写出这块数据的时间（$T_{write}$）是定位时间 $T_{position}$ 加上传输 $D$ 所需的时间（$\frac{D}{R_{peak}}$），即：==

$T_{write} = T_{position} + \frac{D}{R_{peak}}$ (43.1)

And thus the effective rate of writing ($R_{effective}$), which is just the amount of data written divided by the total time to write it, is:
==因此，有效写入速率（$R_{effective}$）即写入的数据量除以写入的总时间，为：==

$R_{effective} = \frac{D}{T_{write}} = \frac{D}{T_{position} + \frac{D}{R_{peak}}}$ (43.2)

In mathematical form, this means we want $R_{effective} = F \times R_{peak}$.
==以数学形式表示，这意味着我们希望 $R_{effective} = F \times R_{peak}$。==

$D = \frac{F}{1 - F} \times R_{peak} \times T_{position}$ (43.6)

Let’s do an example, with a disk with a positioning time of 10 milliseconds and peak transfer rate of 100 MB/s; assume we want an effective bandwidth of 90% of peak ($F = 0.9$).
==让我们举个例子，假设磁盘定位时间为 10 毫秒，峰值传输速率为 100 MB/s；假设我们想要达到峰值 90% 的有效带宽（$F = 0.9$）。==

In this case, $D = \frac{0.9}{0.1} \times 100\text{ MB/s} \times 0.01\text{ seconds} = 9\text{ MB}$.
==在这种情况下，$D = \frac{0.9}{0.1} \times 100\text{ MB/s} \times 0.01\text{ 秒} = 9\text{ MB}$。==

#### 43.4 Problem: Finding Inodes
==#### 43.4 问题：查找 Inode==

In LFS, life is more difficult. Why? Well, we’ve managed to scatter the inodes all throughout the disk!
==在 LFS 中，情况变得更加困难。为什么呢？因为我们设法将 inode 散布在整个磁盘上！==

Worse, we never overwrite in place, and thus the latest version of an inode (i.e., the one we want) keeps moving.
==更糟糕的是，我们从不原位覆盖，因此 inode 的最新版本（即我们想要的版本）一直在移动位置。==

#### 43.5 Solution Through Indirection: The Inode Map
==#### 43.5 通过间接层解决：Inode 映射表==

To remedy this, the designers of LFS introduced a level of indirection between inode numbers and the inodes through a data structure called the **inode map** (**imap**).
==为了补救这一点，LFS 的设计者通过一种名为 **inode 映射表**（**imap**）的数据结构，在 inode 编号和 inode 之间引入了一个间接层。==

The **imap** is a structure that takes an inode number as input and produces the disk address of the most recent version of the inode.
==**imap** 是一种将 inode 编号作为输入并产生该 inode 最新版本磁盘地址的结构。==

#### 43.6 Completing The Solution: The Checkpoint Region
==#### 43.6 完善方案：检查点区域==

LFS has just such a fixed place on disk for this, known as the **checkpoint region** (**CR**).
==LFS 在磁盘上有一个固定的位置来实现这一点，被称为 **检查点区域**（**CR**）。==

The **checkpoint region** contains pointers to (i.e., addresses of) the latest pieces of the inode map, and thus the inode map pieces can be found by reading the CR first.
==**检查点区域** 包含指向（即地址）inode 映射表最新部分的指针，因此通过首先读取 CR 就可以找到 inode 映射表的分块。==

#### 43.9 A New Problem: Garbage Collection
==#### 43.9 新问题：垃圾回收==

LFS leaves old versions of file structures scattered throughout the disk. We (rather unceremoniously) call these old versions **garbage**.
==LFS 将文件结构的旧版本散布在整个磁盘上。我们（相当不客气地）将这些旧版本称为 **垃圾**（garbage）。==

However, LFS instead keeps only the latest live version of a file; thus (in the background), LFS must periodically find these old dead versions of file data, inodes, and other structures, and **clean** them.
==然而，LFS 相反只保留文件的最新活动版本；因此（在后台），LFS 必须定期查找这些过时的文件数据、inode 和其他结构的旧版本，并 **清理** 它们。==

The LFS cleaner works on a segment-by-segment basis, thus clearing up large chunks of space for subsequent writing.
==LFS 清理器以段为单位进行工作，从而为后续写入腾出大块空间。==

#### 43.12 Crash Recovery And The Log
==#### 43.12 崩溃恢复与日志==

To ensure that the CR update happens atomically, LFS actually keeps two CRs, one at either end of the disk, and writes to them alternately.
==为了确保 CR 更新原子化地发生，LFS 实际上保留了两个 CR，分别位于磁盘的两端，并轮流对它们进行写入。==

If the system crashes during a CR update, LFS can detect this by seeing an inconsistent pair of timestamps.
==如果系统在 CR 更新期间崩溃，LFS 可以通过看到不一致的时间戳对来检测到这一点。==

LFS will always choose to use the most recent CR that has consistent timestamps, and thus consistent update of the CR is achieved.
==LFS 将始终选择使用具有一致时间戳的最新 CR，从而实现 CR 的一致性更新。==




Compute the liveness again, and check if you are right with -c.
==重新计算活跃度，并使用 -c 检查你的结果是否正确。==

What is the main difference between writing a file all at once (as we do here) versus doing it one block at a time (as above)?
==一次性写入整个文件（如我们在此所做的）与一次写入一个块（如上所述）之间的主要区别是什么？==

What does this tell you about the importance of buffering updates in main memory as the real LFS does?
==关于像真实的 LFS 那样在主内存中缓冲更新的重要性，这告诉了你什么？==

7. Let’s do another specific example.
==7. 让我们看另一个具体的例子。==

First, run the following: `./lfs.py -L c,/foo:w,/foo,0,1`.
==首先，运行以下命令：`./lfs.py -L c,/foo:w,/foo,0,1`。==

What does this set of commands do?
==这组命令的作用是什么？==

Now, run `./lfs.py -L c,/foo:w,/foo,7,1`.
==现在，运行 `./lfs.py -L c,/foo:w,/foo,7,1`。==

What does this set of commands do?
==这组命令的作用是什么？==

How are the two different?
==这两者有何不同？==

What can you tell about the size field in the inode from these two sets of commands?
==从这两组命令中，你能发现 inode 中的 size（大小）字段有什么特点？==

8. Now let’s look explicitly at file creation versus directory creation.
==8. 现在让我们明确地对比一下文件创建与目录创建。==

Run simulations `./lfs.py -L c,/foo` and `./lfs.py -L d,/foo` to create a file and then a directory.
==运行模拟命令 `./lfs.py -L c,/foo` 和 `./lfs.py -L d,/foo` 来分别创建一个文件和一个目录。==

What is similar about these runs, and what is different?
==这些运行过程有哪些相似之处，又有哪些不同之处？==

9. The LFS simulator supports hard links as well.
==9. LFS 模拟器也支持硬链接。==

Run the following to study how they work: `./lfs.py -L c,/foo:l,/foo,/bar:l,/foo,/goo -o -i`.
==运行以下命令来研究它们是如何工作的：`./lfs.py -L c,/foo:l,/foo,/bar:l,/foo,/goo -o -i`。==

What blocks are written out when a hard link is created?
==创建硬链接时会写出哪些块？==

How is this similar to just creating a new file, and how is it different?
==这与仅创建一个新文件有何相似之处，又有何不同？==

How does the reference count field change as links are created?
==随着链接的创建，引用计数（reference count）字段是如何变化的？==

10. LFS makes many different policy decisions.
==10. LFS 会做出许多不同的策略决策。==

We do not explore many of them here – perhaps something left for the future – but here is a simple one we do explore: the choice of inode number.
==我们在这里不会探讨其中的大部分策略——或许留给以后——但这里有一个我们将要探讨的简单策略：inode 编号的选择。==

First, run `./lfs.py -p c100 -n 10 -o -a s` to show the usual behavior with the "sequential" allocation policy, which tries to use free inode numbers nearest to zero.
==首先，运行 `./lfs.py -p c100 -n 10 -o -a s` 以展示“顺序”分配策略的通常行为，该策略尝试使用最接近零的空闲 inode 编号。==

Then, change to a "random" policy by running `./lfs.py -p c100 -n 10 -o -a r` (the -p c100 flag ensures 100 percent of the random operations are file creations).
==然后，通过运行 `./lfs.py -p c100 -n 10 -o -a r` 切换到“随机”策略（-p c100 标志确保 100% 的随机操作都是文件创建）。==

What on-disk differences does a random policy versus a sequential policy result in?
==随机策略与顺序策略相比，会导致磁盘上出现哪些差异？==

What does this say about the importance of choosing inode numbers in a real LFS?
==关于在真实的 LFS 中选择 inode 编号的重要性，这说明了什么？==

11. One last thing we’ve been assuming is that the LFS simulator always updates the checkpoint region after each update.
==11. 我们一直假设的最后一件事是 LFS 模拟器在每次更新后都会更新检查点区域（checkpoint region）。==

In the real LFS, that isn’t the case: it is updated periodically to avoid long seeks.
==在真实的 LFS 中并非如此：它是定期更新的，以避免长时间的寻道。==

Run `./lfs.py -N -i -o -s 1000` to see some operations and the intermediate and final states of the file system when the checkpoint region isn’t forced to disk.
==运行 `./lfs.py -N -i -o -s 1000` 来观察在不强制将检查点区域写入磁盘时，文件系统的一些操作以及中间和最终状态。==

What would happen if the checkpoint region is never updated?
==如果检查点区域永远不更新会发生什么？==

What if it is updated periodically?
==如果是定期更新又会怎样？==

Could you figure out how to recover the file system to the latest state by rolling forward in the log?
==你能否想出如何通过在日志中“向前滚动”（rolling forward）来将文件系统恢复到最新状态？==

**44 Flash-based SSDs**
==**44 基于闪存的固态硬盘 (SSDs)**==

After decades of hard-disk drive dominance, a new form of persistent storage device has recently gained significance in the world.
==在硬盘驱动器统治了数十年之后，一种新型的持久化存储设备最近在全球范围内变得举足轻重。==

Generically referred to as **solid-state storage**, such devices have no mechanical or moving parts like hard drives; rather, they are simply built out of transistors, much like memory and processors.
==这类设备通称为**固态存储**，它们不像硬盘驱动器那样拥有机械或移动部件；相反，它们仅由晶体管构建而成，非常类似于内存和处理器。==

However, unlike typical random-access memory (e.g., DRAM), such a solid-state storage device (a.k.a., an **SSD**) retains information despite power loss, and thus is an ideal candidate for use in persistent storage of data.
==然而，与典型的随机存取存储器（如 DRAM）不同，这种固态存储设备（又称 **SSD**）在断电后仍能保留信息，因此是用于数据持久化存储的理想选择。==

The technology we’ll focus on is known as **flash** (more specifically, **NAND-based flash**), which was created by Fujio Masuoka in the 1980s [M+14].
==我们将重点关注的技术被称为**闪存**（更具体地说，是**基于 NAND 的闪存**），它是由舛冈富士雄（Fujio Masuoka）在 20 世纪 80 年代发明的 [M+14]。==

Flash, as we’ll see, has some unique properties.
==正如我们将看到的，闪存具有一些独特的属性。==

For example, to write to a given chunk of it (i.e., a **flash page**), you first have to erase a bigger chunk (i.e., a **flash block**), which can be quite expensive.
==例如，要写入其中的一个特定块（即**闪存页**），你必须先擦除一个更大的块（即**闪存块**），这可能代价非常高。==

In addition, writing too often to a page will cause it to **wear out**.
==此外，过于频繁地向某个页写入会导致其**磨损**。==

These two properties make construction of a flash-based SSD an interesting challenge:
==这两个属性使得构建基于闪存的 SSD 成为一个有趣的挑战：==

**CRUX: HOW TO BUILD A FLASH-BASED SSD**
==**关键问题：如何构建基于闪存的 SSD**==

How can we build a flash-based SSD?
==我们如何构建一个基于闪存的 SSD？==

How can we handle the expensive nature of erasing?
==我们如何处理擦除操作的高昂代价？==

How can we build a device that lasts a long time, given that repeated overwrite will wear the device out?
==考虑到重复覆写会磨损设备，我们如何构建一个寿命长久的设备？==

Will the march of progress in technology ever cease?
==技术的进步步伐会停止吗？==

Or cease to amaze?
==或者停止带给人惊喜？==

**44.1 Storing a Single Bit**
==**44.1 存储单个位**==

Flash chips are designed to store one or more bits in a single transistor; the level of charge trapped within the transistor is mapped to a binary value.
==闪存芯片设计用于在单个晶体管中存储一个或多个位；陷在晶体管内的电荷水平被映射为一个二进制值。==

In a **single-level cell (SLC)** flash, only a single bit is stored within a transistor (i.e., 1 or 0); with a **multi-level cell (MLC)** flash, two bits are encoded into different levels of charge, e.g., 00, 01, 10, and 11 are represented by low, somewhat low, somewhat high, and high levels.
==在**单层单元 (SLC)** 闪存中，晶体管内仅存储一个位（即 1 或 0）；而在**多层单元 (MLC)** 闪存中，两个位被编码为不同的电荷水平，例如，00、01、10 和 11 分别由低、较低、较高和高水平表示。==

There is even **triple-level cell (TLC)** flash, which encodes 3 bits per cell.
==甚至还有**三层单元 (TLC)** 闪存，每个单元编码 3 个位。==

Overall, SLC chips achieve higher performance and are more expensive.
==总的来说，SLC 芯片性能更高，但也更昂贵。==

**TIP: BE CAREFUL WITH TERMINOLOGY**
==**提示：注意术语的使用**==

You may have noticed that some terms we have used many times before (blocks, pages) are being used within the context of a flash, but in slightly different ways than before.
==你可能已经注意到，我们之前多次使用过的一些术语（块、页）在闪存的语境中也在使用，但其含义与之前略有不同。==

New terms are not created to make your life harder (although they may be doing just that), but arise because there is no central authority where terminology decisions are made.
==创建新术语并不是为了让你的生活变得更艰难（尽管它们确实起到了这种作用），而是因为没有一个中央机构来进行术语决策。==

What is a block to you may be a page to someone else, and vice versa, depending on the context.
==对你来说是一个“块”的东西，对别人来说可能是一个“页”，反之亦然，这取决于具体的上下文。==

Your job is simple: to know the appropriate terms within each domain, and use them such that people well-versed in the discipline can understand what you are talking about.
==你的任务很简单：了解每个领域中的适当术语，并正确使用它们，以便该领域的专业人士能够理解你在说什么。==

It’s one of those times where the only solution is simple but sometimes painful: use your memory.
==在这些情况下，唯一的解决办法既简单但有时又令人痛苦：动用你的记忆力。==

Of course, there are many details as to exactly how such bit-level storage operates, down at the level of device physics.
==当然，关于这种位级存储究竟如何运作，在设备物理层面上还有许多细节。==

While beyond the scope of this book, you can read more about it on your own [J10].
==虽然这超出了本书的范围，但你可以自行阅读更多相关资料 [J10]。==

**44.2 From Bits to Banks/Planes**
==**44.2 从位到银行/平面**==

As they say in ancient Greece, storing a single bit (or a few) does not a storage system make.
==正如古希腊人所说，仅存储一个位（或几个位）并不能构成一个存储系统。==

Hence, flash chips are organized into **banks** or **planes** which consist of a large number of cells.
==因此，闪存芯片被组织成**银行 (banks)** 或**平面 (planes)**，它们由大量的单元组成。==

A bank is accessed in two different sized units: **blocks** (sometimes called **erase blocks**), which are typically of size 128 KB or 256 KB, and **pages**, which are a few KB in size (e.g., 4KB).
==访问银行时有两种不同大小的单位：**块**（有时称为**擦除块**），其大小通常为 128 KB 或 256 KB；以及**页**，其大小为几 KB（例如 4KB）。==

Within each bank there are a large number of blocks; within each block, there are a large number of pages.
==在每个银行内部有大量的块；在每个块内部，有大量的页。==

When thinking about flash, you must remember this new terminology, which is different than the blocks we refer to in disks and RAIDs and the pages we refer to in virtual memory.
==在思考闪存时，你必须记住这一套新术语，它与我们在磁盘和 RAID 中提到的“块”以及在虚拟内存中提到的“页”是不同的。==

Figure 44.1 shows an example of a flash plane with blocks and pages; there are three blocks, each containing four pages, in this simple example.
==图 44.1 显示了一个包含块和页的闪存平面示例；在这个简单的例子中，有三个块，每个块包含四个页。==

We’ll see below why we distinguish between blocks and pages; it turns out this distinction is critical for flash operations such as reading and writing, and even more so for the overall performance of the device.
==我们将在下文中看到为什么要区分块和页；事实证明，这种区分对于闪存的读写操作至关重要，对于设备的整体性能更是如此。==

The most important (and weird) thing you will learn is that to write to a page within a block, you first have to **erase the entire block**; this tricky detail makes building a flash-based SSD an interesting and worthwhile challenge, and the subject of the second-half of the chapter.
==你将学到的最重要（也是最奇怪）的一点是，要写入块中的某个页，你必须先**擦除整个块**；这个棘手的细节使得构建基于闪存的 SSD 成为一个有趣且值得挑战的任务，也是本章后半部分的主题。==

**44.3 Basic Flash Operations**
==**44.3 闪存基础操作**==

Given this flash organization, there are three low-level operations that a flash chip supports.
==考虑到这种闪存结构，闪存芯片支持三种底层操作。==

The **read** command is used to read a page from the flash; **erase** and **program** are used in tandem to write.
==**read**（读取）命令用于从闪存中读取一个页；**erase**（擦除）和 **program**（编程）则配合使用来完成写入操作。==

The details:
==详细信息如下：==

• **Read (a page)**: A client of the flash chip can read any page (e.g., 2KB or 4KB), simply by specifying the read command and appropriate page number to the device.
==• **读取（一个页）**：闪存芯片的客户端可以读取任何页（例如 2KB 或 4KB），只需向设备指定读取命令和相应的页号即可。==

This operation is typically quite fast, 10s of microseconds or so, regardless of location on the device, and (more or less) regardless of the location of the previous request (quite unlike a disk).
==无论在设备的什么位置，也（或多或少）无论上一个请求的位置在哪里（这与磁盘完全不同），此操作通常都非常快，大约在几十微秒左右。==

Being able to access any location uniformly quickly means the device is a **random access** device.
==能够统一且快速地访问任何位置，意味着该设备是一个**随机存取**设备。==

• **Erase (a block)**: Before writing to a *page* within a flash, the nature of the device requires that you first **erase** the entire **block** the page lies within.
==• **擦除（一个块）**：在向闪存中的*页*写入之前，由于设备的物理特性，要求你必须先**擦除**该页所在的整个**块**。==

Erase, importantly, destroys the contents of the block (by setting each bit to the value 1); therefore, you must be sure that any data you care about in the block has been copied elsewhere (to memory, or perhaps to another flash block) *before* executing the erase.
==重要的是，擦除会破坏块的内容（通过将每个位设置为 1）；因此，你必须确保在执行擦除*之前*，块中任何你关心的数据都已复制到其他地方（内存，或者可能是另一个闪存块）。==

The erase command is quite expensive, taking a few milliseconds to complete.
==擦除命令代价相当高，需要几毫秒才能完成。==

Once finished, the entire block is reset and each page is ready to be programmed.
==一旦完成，整个块就会重置，每个页都准备好进行编程。==

• **Program (a page)**: Once a block has been erased, the **program** command can be used to change some of the 1’s within a page to 0’s, and write the desired contents of a page to the flash.
==• **编程（一个页）**：一旦一个块被擦除，**program** 命令就可以用来将页内的某些 1 改为 0，并将所需的页内容写入闪存。==

Programming a page is less expensive than erasing a block, but more costly than reading a page, usually taking around 100s of microseconds on modern flash chips.
==对页进行编程的代价比擦除块要小，但比读取页要高，在现代闪存芯片上通常需要 100 微秒左右。==

One way to think about flash chips is that each page has a **state** associated with it.
==思考闪存芯片的一种方式是，每个页都关联有一个**状态**。==

Pages start in an `INVALID` state.
==页的初始状态为 `INVALID`（无效）。==

By erasing the block that a page resides within, you set the state of the page (and all pages within that block) to `ERASED`, which resets the content of each page in the block but also (importantly) makes them programmable.
==通过擦除页所在的块，你可以将该页（以及该块内的所有页）的状态设置为 `ERASED`（已擦除），这会重置块中每个页的内容，而且（重要的是）使它们变得可编程。==

When you program a page, its state changes to `VALID`, meaning its contents have been set and can be read.
==当你对一个页进行编程时，它的状态会变为 `VALID`（有效），这意味着它的内容已经设置好并且可以被读取。==

Reads do not affect these states (although you should only read from pages that have been programmed).
==读取操作不会影响这些状态（尽管你只能读取已经编程过的页）。==

Once a page has been programmed, the only way to change its contents is to erase the entire block within which the page resides.
==一旦一个页被编程，更改其内容的唯一方法就是擦除该页所在的整个块。==

Here is an example of states transition after various erase and program operations within a 4-page block:
==以下是在一个 4 页块内经过各种擦除和编程操作后的状态转换示例：==

`iiii` Initial: pages in block are invalid (i)
==`iiii` 初始状态：块中的页是无效的 (i)==

`Erase() -> EEEE` State of pages in block set to erased (E)
==`Erase() -> EEEE` 块中页的状态设置为已擦除 (E)==

`Program(0) -> VEEE` Program page 0; state set to valid (V)
==`Program(0) -> VEEE` 对第 0 页进行编程；状态设置为有效 (V)==

`Program(0) -> error` Cannot re-program page after programming
==`Program(0) -> error` 编程后无法再次对该页进行编程==

`Program(1) -> VVEE` Program page 1
==`Program(1) -> VVEE` 对第 1 页进行编程==

`Erase() -> EEEE` Contents erased; all pages programmable
==`Erase() -> EEEE` 内容已擦除；所有页均可编程==

**A Detailed Example**
==**一个详细的例子**==

Because the process of writing (i.e., erasing and programming) is so unusual, let’s go through a detailed example to make sure it makes sense.
==由于写入过程（即擦除和编程）非常特殊，让我们通过一个详细的例子来确保你理解了它。==

In this example, imagine we have the following four 8-bit pages, within a 4-page block (both unrealistically small sizes, but useful within this example); each page is `VALID` as each has been previously programmed.
==在这个例子中，假设我们在一个 4 页的块中有以下四个 8 位页（这两个尺寸在现实中都小得不切实际，但在本例中很有用）；每个页都是 `VALID`（有效）的，因为每个页之前都已被编程。==

Now say we wish to write to page 0, filling it with new contents.
==现在假设我们希望写入第 0 页，为其填充新内容。==

To write any page, we must first erase the entire block.
==要写入任何页，我们必须先擦除整个块。==

Let’s assume we do so, thus leaving the block in this state:
==假设我们执行了此操作，从而使块处于这种状态：==

Good news! We could now go ahead and program page 0, for example with the contents `00000011`, overwriting the old page 0 (contents `00011000`) as desired.
==好消息！我们现在可以继续对第 0 页进行编程，例如使用内容 `00000011`，按预期覆盖旧的第 0 页（内容 `00011000`）。==

After doing so, our block looks like this:
==完成之后，我们的块看起来像这样：==

And now the bad news: the previous contents of pages 1, 2, and 3 are all gone!
==现在是坏消息：第 1、2 和 3 页之前的内容全都消失了！==

Thus, before overwriting any page *within* a block, we must first move any data we care about to another location (e.g., memory, or elsewhere on the flash).
==因此，在覆盖块*内*的任何页之前，我们必须首先将我们关心的任何数据移动到另一个位置（例如，内存或闪存的其他地方）。==

The nature of erase will have a strong impact on how we design flash-based SSDs, as we’ll soon learn about.
==擦除的特性将对我们如何设计基于闪存的 SSD 产生深远影响，我们很快就会了解到这一点。==

**Summary**
==**总结**==

To summarize, reading a page is easy: just read the page.
==总而言之，读取页很简单：直接读就行。==

Flash chips do this quite well, and quickly; in terms of performance, they offer the potential to greatly exceed the random read performance of modern disk drives, which are slow due to mechanical seek and rotation costs.
==闪存芯片在这方面表现出色且迅速；在性能方面，它们有可能大大超过现代磁盘驱动器的随机读取性能，而磁盘驱动器由于机械寻道和旋转成本而变得缓慢。==

Writing a page is trickier; the entire block must first be erased (taking care to first move any data we care about to another location), and then the desired page programmed.
==写入页则更为复杂；必须先擦除整个块（注意先将关心的任何数据移动到另一个位置），然后对所需的页进行编程。==

Not only is this expensive, but frequent repetitions of this program/erase cycle can lead to the biggest reliability problem flash chips have: **wear out**.
==这不仅代价高昂，而且频繁重复这种“编程/擦除”循环会导致闪存芯片面临最大的可靠性问题：**磨损**。==

When designing a storage system with flash, the performance and reliability of writing is a central focus.
==在设计使用闪存的存储系统时，写入的性能和可靠性是核心关注点。==

We’ll soon learn more about how modern SSDs attack these issues, delivering excellent performance and reliability despite these limitations.
==我们很快就会学到更多关于现代 SSD 如何解决这些问题的知识，从而在存在这些局限性的情况下依然提供卓越的性能和可靠性。==

**44.4 Flash Performance And Reliability**
==**44.4 闪存性能与可靠性**==

Because we’re interested in building a storage device out of raw flash chips, it is worthwhile to understand their basic performance characteristics.
==因为我们有兴趣利用原始闪存芯片构建存储设备，所以了解其基本性能特征是值得的。==

Figure 44.2 presents a rough summary of some numbers found in the popular press [V12].
==图 44.2 给出了大众媒体上常见的一些数字的粗略总结 [V12]。==

Therein, the author presents the basic operation latency of reads, programs, and erases across SLC, MLC, and TLC flash, which store 1, 2, and 3 bits of information per cell, respectively.
==在其中，作者展示了 SLC、MLC 和 TLC 闪存的读取、编程和擦除的基础操作延迟，这些闪存每个单元分别存储 1、2 和 3 位信息。==

As we can see from the table, read latencies are quite good, taking just 10s of microseconds to complete.
==从表中可以看出，读取延迟相当好，只需几十微秒即可完成。==

Program latency is higher and more variable, as low as 200 microseconds for SLC, but higher as you pack more bits into each cell; to get good write performance, you will have to make use of multiple flash chips in parallel.
==编程延迟更高且更具波动性，SLC 闪存最低为 200 微秒，但随着每个单元容纳更多位，延迟也会随之增加；为了获得良好的写入性能，你必须并行使用多个闪存芯片。==

Finally, erases are quite expensive, taking a few milliseconds typically.
==最后，擦除操作代价非常高，通常需要几毫秒。==

Dealing with this cost is central to modern flash storage design.
==处理这种成本是现代闪存存储设计的核心。==

Let’s now consider reliability of flash chips.
==现在让我们考虑闪存芯片的可靠性。==

Unlike mechanical disks, which can fail for a wide variety of reasons (including the gruesome and quite physical head crash, where the drive head actually makes contact with the recording surface), flash chips are pure silicon and in that sense have fewer reliability issues to worry about.
==机械磁盘可能由于各种原因失效（包括可怕且非常具有物理破坏性的磁头崩溃，即驱动器磁头实际上接触到了记录表面），与此不同，闪存芯片是纯硅制品，从这个意义上说，需要担心的可靠性问题较少。==

The primary concern is **wear out**; when a flash block is erased and programmed, it slowly accrues a little bit of extra charge.
==主要的担忧是**磨损**；当一个闪存块被擦除和编程时，它会慢慢积累一点额外的电荷。==

Over time, as that extra charge builds up, it becomes increasingly difficult to differentiate between a 0 and a 1.
==随着时间的推移，随着额外电荷的积累，区分 0 和 1 变得越来越困难。==

At the point where it becomes impossible, the block becomes unusable.
==当这种区分变得不可能时，该块就变得无法使用了。==

The typical lifetime of a block is currently not well known.
==块的典型寿命目前尚不十分清楚。==

Manufacturers rate MLC-based blocks as having a 10,000 P/E (Program/Erase) cycle lifetime; that is, each block can be erased and programmed 10,000 times before failing.
==制造商将基于 MLC 的块额定为 10,000 次 P/E（编程/擦除）循环寿命；也就是说，每个块在失效前可以被擦除和编程 10,000 次。==

SLC-based chips, because they store only a single bit per transistor, are rated with a longer lifetime, usually 100,000 P/E cycles.
==基于 SLC 的芯片由于每个晶体管只存储一位，其额定寿命更长，通常为 100,000 次 P/E 循环。==

However, recent research has shown that lifetimes are much longer than expected [BD10].
==然而，最近的研究表明，寿命比预期的要长得多 [BD10]。==

One other reliability problem within flash chips is known as **disturbance**.
==闪存芯片内的另一个可靠性问题被称为**干扰 (disturbance)**。==

When accessing a particular page within a flash, it is possible that some bits get flipped in neighboring pages; such bit flips are known as **read disturbs** or **program disturbs**, depending on whether the page is being read or programmed, respectively.
==当访问闪存中的特定页时，相邻页中的某些位可能会发生翻转；这种位翻转被称为**读取干扰**或**编程干扰**，分别取决于该页是在被读取还是被编程。==

**TIP: THE IMPORTANCE OF BACKWARDS COMPATIBILITY**
==**提示：向后兼容性的重要性**==

Backwards compatibility is always a concern in layered systems.
==在分层系统中，向后兼容性始终是一个值得关注的问题。==

By defining a stable interface between two systems, one enables innovation on each side of the interface while ensuring continued interoperability.
==通过在两个系统之间定义稳定的接口，可以在确保持续互操作性的同时，允许接口两侧各自进行创新。==

Such an approach has been quite successful in many domains: operating systems have relatively stable APIs for applications, disks provide the same block-based interface to file systems, and each layer in the IP networking stack provides a fixed unchanging interface to the layer above.
==这种方法在许多领域都非常成功：操作系统为应用程序提供相对稳定的 API，磁盘为文件系统提供相同的基于块的接口，IP 网络堆栈中的每一层都为上一层提供固定不变的接口。==

Not surprisingly, there can be a downside to such rigidity, as interfaces defined in one generation may not be appropriate in the next.
==不出所料，这种僵化也有其弊端，因为某一代定义的接口在下一代可能就不再适用。==

In some cases, it may be useful to think about redesigning the entire system entirely.
==在某些情况下，考虑彻底重新设计整个系统可能会很有用。==

An excellent example is found in the Sun ZFS file system [B07]; by reconsidering the interaction of file systems and RAID, the creators of ZFS envisioned (and then realized) a more effective integrated whole.
==Sun ZFS 文件系统 [B07] 就是一个绝佳的例子；通过重新考虑文件系统和 RAID 的交互，ZFS 的创造者设想（并随后实现了）一个更有效的集成整体。==

**44.5 From Raw Flash to Flash-Based SSDs**
==**44.5 从原始闪存到基于闪存的 SSD**==

Given our basic understanding of flash chips, we now face our next task: how to turn a basic set of flash chips into something that looks like a typical storage device.
==既然我们对闪存芯片有了基本的了解，现在面临下一个任务：如何将一组基础的闪存芯片转变为看起来像典型存储设备的东西。==

The standard storage interface is a simple block-based one, where blocks (sectors) of size 512 bytes (or larger) can be read or written, given a block address.
==标准的存储接口是简单的基于块的接口，在这种接口下，给定一个块地址，就可以读取或写入大小为 512 字节（或更大）的块（扇区）。==

The task of the flash-based SSD is to provide that standard block interface atop the raw flash chips inside it.
==基于闪存的 SSD 的任务就是在其内部的原始闪存芯片之上提供这种标准的块接口。==

Internally, an SSD consists of some number of flash chips (for persistent storage).
==在内部，一个 SSD 由若干闪存芯片组成（用于持久化存储）。==

An SSD also contains some amount of volatile (i.e., non-persistent) memory (e.g., SRAM); such memory is useful for caching and buffering of data as well as for mapping tables, which we’ll learn about below.
==SSD 还包含一定量的易失性（即非持久性）内存（例如 SRAM）；这种内存对于数据缓存、缓冲以及映射表（我们将在下文了解到）非常有用。==

Finally, an SSD contains control logic to orchestrate device operation.
==最后，SSD 包含控制逻辑来协调设备的运行。==

See Agrawal et. al for details [A+08]; a simplified block diagram is seen in Figure 44.3 (page 7).
==详情请参阅 Agrawal 等人的文章 [A+08]；简化的框图见图 44.3（第 7 页）。==

One of the essential functions of this control logic is to satisfy client reads and writes, turning them into internal flash operations as need be.
==这种控制逻辑的核心功能之一是满足客户端的读写请求，并根据需要将其转换为内部闪存操作。==

The **flash translation layer**, or **FTL**, provides exactly this functionality.
==**闪存转换层**（或 **FTL**）正是提供了这种功能。==

The FTL takes read and write requests on **logical blocks** (that comprise the device interface) and turns them into low-level read, erase, and program commands on the underlying **physical blocks** and **physical pages** (that comprise the actual flash device).
==FTL 接收对**逻辑块**（构成设备接口）的读写请求，并将其转换为对底层**物理块**和**物理页**（构成实际闪存设备）的底层读取、擦除和编程命令。==

The FTL should accomplish this task with the goal of delivering excellent performance and high reliability.
==FTL 应该以提供卓越性能和高可靠性为目标来完成这项任务。==

Excellent performance, as we’ll see, can be realized through a combination of techniques.
==正如我们将看到的，卓越的性能可以通过多种技术的结合来实现。==

One key will be to utilize multiple flash chips **in parallel**; although we won’t discuss this technique much further, suffice it to say that all modern SSDs use multiple chips internally to obtain higher performance.
==其中一个关键是**并行**利用多个闪存芯片；虽然我们不会进一步讨论这种技术，但可以肯定的是，所有现代 SSD 都在内部使用多个芯片以获得更高的性能。==

Another performance goal will be to reduce **write amplification**, which is defined as the total write traffic (in bytes) issued to the flash chips by the FTL divided by the total write traffic (in bytes) issued by the client to the SSD.
==另一个性能目标是减少**写入放大**，其定义为 FTL 发送到闪存芯片的总写入流量（以字节为单位）除以客户端发送到 SSD 的总写入流量（以字节为单位）。==

As we’ll see below, naive approaches to FTL construction will lead to high write amplification and low performance.
==正如我们将在下文中看到的，幼稚的 FTL 构建方法会导致高写入放大和低性能。==

High reliability will be achieved through the combination of a few different approaches.
==高可靠性将通过几种不同方法的结合来实现。==

One main concern, as discussed above, is **wear out**.
==如上所述，一个主要问题是**磨损**。==

If a single block is erased and programmed too often, it will become unusable; as a result, the FTL should try to spread writes across the blocks of the flash as evenly as possible, ensuring that all of the blocks of the device wear out at roughly the same time; doing so is called **wear leveling** and is an essential part of any modern FTL.
==如果单个块被擦除和编程过于频繁，它将变得不可用；因此，FTL 应尝试尽可能均匀地将写入分布在闪存的各个块上，确保设备的所有块大致在同一时间磨损；这样做被称为**磨损均衡**，是任何现代 FTL 的重要组成部分。==

Another reliability concern is program disturbance.
==另一个可靠性问题是编程干扰。==

To minimize such disturbance, FTLs will commonly program pages within an erased block in order, from low page to high page.
==为了尽量减少这种干扰，FTL 通常会按照从低页到高页的顺序对已擦除块内的页进行编程。==

This sequential-programming approach minimizes disturbance and is widely utilized.
==这种顺序编程方法可以最大限度地减少干扰，并被广泛采用。==

**44.6 FTL Organization: A Bad Approach**
==**44.6 FTL 组织：一种糟糕的方法**==

The simplest organization of an FTL would be something we call **direct mapped**.
==FTL 最简单的组织形式是我们所谓的**直接映射**。==

In this approach, a read to logical page $N$ is mapped directly to a read of physical page $N$.
==在这种方法中，对逻辑页 $N$ 的读取被直接映射到对物理页 $N$ 的读取。==

A write to logical page $N$ is more complicated; the FTL first has to read in the entire block that page $N$ is contained within; it then has to erase the block; finally, the FTL programs the old pages as well as the new one.
==对逻辑页 $N$ 的写入则更为复杂；FTL 首先必须读入包含页 $N$ 的整个块；然后必须擦除该块；最后，FTL 对旧页以及新页进行编程。==

As you can probably guess, the direct-mapped FTL has many problems, both in terms of performance as well as reliability.
==正如你大概能猜到的，直接映射的 FTL 在性能和可靠性方面都存在许多问题。==

The performance problems come on each write: the device has to read in the entire block (costly), erase it (quite costly), and then program it (costly).
==性能问题出现在每次写入时：设备必须读入整个块（代价高），擦除它（代价非常高），然后编程（代价高）。==

The end result is severe write amplification (proportional to the number of pages in a block) and as a result, terrible write performance, even slower than typical hard drives with their mechanical seeks and rotational delays.
==最终结果是严重的写入放大（与块中的页数成正比），从而导致糟糕的写入性能，甚至比具有机械寻道和旋转延迟的典型硬盘驱动器还要慢。==

Even worse is the reliability of this approach.
==这种方法的可靠性甚至更糟。==

If file system metadata or user file data is repeatedly overwritten, the same block is erased and programmed, over and over, rapidly wearing it out and potentially losing data.
==如果文件系统元数据或用户文件数据被反复覆盖，同一个块就会被一遍又一遍地擦除和编程，迅速磨损并可能导致数据丢失。==

The direct mapped approach simply gives too much control over wear out to the client workload; if the workload does not spread write load evenly across its logical blocks, the underlying physical blocks containing popular data will quickly wear out.
==直接映射方法仅仅是将磨损的控制权过多地交给了客户端工作负载；如果工作负载没有将其写入负载均匀地分布在逻辑块上，那么包含热门数据的底层物理块将很快磨损。==

For both reliability and performance reasons, a direct-mapped FTL is a bad idea.
==出于可靠性和性能方面的考虑，直接映射的 FTL 是个坏主意。==

**44.7 A Log-Structured FTL**
==**44.7 日志结构 FTL**==

For these reasons, most FTLs today are **log structured**, an idea useful in both storage devices (as we’ll see now) and file systems above them (e.g., in **log-structured file systems**).
==出于这些原因，当今大多数 FTL 都是**日志结构化**的，这一理念在存储设备（正如我们现在所看到的）以及其上层的文件系统（例如在**日志结构文件系统**中）中都非常有用。==

Upon a write to logical block $N$, the device appends the write to the next free spot in the currently-being-written-to block; we call this style of writing **logging**.
==在向逻辑块 $N$ 写入时，设备会将写入内容追加到当前正在写入块的下一个空闲位置；我们称这种写入方式为**日志记录 (logging)**。==

To allow for subsequent reads of block $N$, the device keeps a **mapping table** (in its memory, and persistent, in some form, on the device); this table stores the physical address of each logical block in the system.
==为了允许后续读取块 $N$，设备会维护一个**映射表**（保存在其内存中，并以某种形式持久化存储在设备上）；该表存储了系统中每个逻辑块的物理地址。==

Let’s go through an example to make sure we understand how the basic log-based approach works.
==让我们通过一个例子来确保我们理解了基本的基于日志的方法是如何工作的。==

To the client, the device looks like a typical disk, in which it can read and write 512-byte sectors (or groups of sectors).
==对于客户端来说，该设备看起来像一个典型的磁盘，在其中可以读取和写入 512 字节的扇区（或扇区组）。==

For simplicity, assume that the client is reading or writing 4-KB sized chunks.
==为简单起见，假设客户端读取或写入的是 4 KB 大小的块。==

Let us further assume that the SSD contains some large number of 16-KB sized blocks, each divided into four 4-KB pages; these parameters are unrealistic (flash blocks usually consist of more pages) but will serve our didactic purposes quite well.
==我们进一步假设 SSD 包含大量 16 KB 大小的块，每个块分为四个 4 KB 的页；这些参数虽然不切实际（闪存块通常包含更多页），但非常符合我们的教学目的。==

Assume the client issues the following sequence of operations:
==假设客户端发出以下操作序列：==

• Write(100) with contents a1
==• Write(100)，内容为 a1==

• Write(101) with contents a2
==• Write(101)，内容为 a2==

• Write(2000) with contents b1
==• Write(2000)，内容为 b1==

• Write(2001) with contents b2
==• Write(2001)，内容 for b2==

These **logical block addresses** (e.g., 100) are used by the client of the SSD (e.g., a file system) to remember where information is located.
==这些**逻辑块地址**（例如 100）被 SSD 的客户端（例如文件系统）用来记住信息所在的位置。==

Internally, the device must transform these block writes into the erase and program operations supported by the raw hardware, and somehow record, for each logical block address, which **physical page** of the SSD stores its data.
==在内部，设备必须将这些块写入转换为原始硬件支持的擦除和编程操作，并以某种方式记录每个逻辑块地址由 SSD 的哪个**物理页**存储其数据。==

Assume that all blocks of the SSD are currently not valid, and must be erased before any page can be programmed.
==假设 SSD 的所有块当前都无效，并且在对任何页进行编程之前必须先进行擦除。==

When the first write is received by the SSD (to logical block 100), the FTL decides to write it to physical block 0, which contains four physical pages: 0, 1, 2, and 3.
==当 SSD 收到第一次写入（针对逻辑块 100）时，FTL 决定将其写入物理块 0，该块包含四个物理页：0、1、2 和 3。==

Because the block is not erased, we cannot write to it yet; the device must first issue an erase command to block 0.
==由于该块尚未擦除，我们还不能写入；设备必须先向块 0 发出擦除命令。==

Block 0 is now ready to be programmed.
==块 0 现在已准备好进行编程。==

Most SSDs will write pages in order (i.e., low to high), reducing reliability problems related to **program disturbance**.
==大多数 SSD 会按顺序（即从低到高）写入页，以减少与**编程干扰**相关的可靠性问题。==

The SSD then directs the write of logical block 100 into physical page 0.
==然后，SSD 将逻辑块 100 的写入导向物理页 0。==

But what if the client wants to *read* logical block 100? How can it find where it is?
==但如果客户端想要*读取*逻辑块 100 呢？它如何找到它的位置？==

The SSD must transform a read issued to logical block 100 into a read of physical page 0.
==SSD 必须将向逻辑块 100 发出的读取请求转换为对物理页 0 的读取。==

To accommodate such functionality, when the FTL writes logical block 100 to physical page 0, it records this fact in an **in-memory mapping table**.
==为了实现这种功能，当 FTL 将逻辑块 100 写入物理页 0 时，它会将这一事实记录在一个**内存映射表**中。==

Now you can see what happens when the client writes to the SSD.
==现在你可以看到当客户端向 SSD 写入时会发生什么。==

The SSD finds a location for the write, usually just picking the next free page; it then programs that page with the block’s contents, and records the logical-to-physical mapping in its mapping table.
==SSD 为写入寻找一个位置，通常只是挑选下一个空闲页；然后它用该块的内容对该页进行编程，并在其映射表中记录逻辑到物理的映射。==

Subsequent reads simply use the table to **translate** the logical block address presented by the client into the physical page number required to read the data.
==随后的读取只需使用该表将客户端提供的逻辑块地址**转换**为读取数据所需的物理页号。==

Let’s now examine the rest of the writes in our example write stream: 101, 2000, and 2001.
==现在让我们检查示例写入流中剩余的写入：101、2000 和 2001。==

The log-based approach by its nature improves performance (erases only being required once in a while, and the costly read-modify-write of the direct-mapped approach avoided altogether), and greatly enhances reliability.
==日志结构化方法从本质上提高了性能（只需偶尔进行一次擦除，并完全避免了直接映射方法中代价高昂的“读取-修改-写入”），并且大大增强了可靠性。==

The FTL can now spread writes across all pages, performing what is called **wear leveling** and increasing the lifetime of the device.
==FTL 现在可以将写入分布在所有页上，执行所谓的**磨损均衡**，从而延长设备的寿命。==

**ASIDE: FTL MAPPING INFORMATION PERSISTENCE**
==**旁注：FTL 映射信息的持久化**==

You might be wondering: what happens if the device loses power? Does the in-memory mapping table disappear?
==你可能会想：如果设备断电了会发生什么？内存中的映射表会消失吗？==

Clearly, such information cannot truly be lost, because otherwise the device would not function as a persistent storage device.
==显然，这些信息不能真正丢失，否则设备就无法作为持久化存储设备工作。==

An SSD must have some means of recovering mapping information.
==SSD 必须有某种恢复映射手段的方法。==

The simplest thing to do is to record some mapping information with each page, in what is called an **out-of-band (OOB)** area.
==最简单的方法是在每个页中记录一些映射信息，这被称为**带外 (OOB)** 区域。==

When the device loses power and is restarted, it must reconstruct its mapping table by scanning the OOB areas and reconstructing the mapping table in memory.
==当设备断电并重启时，它必须通过扫描 OOB 区域并在内存中重建映射表来恢复。==

This basic approach has its problems; scanning a large SSD to find all necessary mapping information is slow.
==这种基础方法也有其问题；扫描大型 SSD 以查找所有必要的映射信息速度很慢。==

To overcome this limitation, some higher-end devices use more complex logging and **checkpointing** techniques to speed up recovery.
==为了克服这一局限，一些高端设备使用更复杂的日志记录和**检查点**技术来加速恢复。==

Unfortunately, this basic approach to log structuring has some downsides.
==不幸的是，这种基础的日志结构方法也有一些缺点。==

The first is that overwrites of logical blocks lead to something we call **garbage**, i.e., old versions of data around the drive and taking up space.
==第一个缺点是，逻辑块的覆写会导致产生我们所谓的**垃圾**，即分布在驱动器周围并占用空间的旧版本数据。==

The device has to periodically perform **garbage collection (GC)** to find said blocks and free space for future writes; excessive garbage collection drives up write amplification and lowers performance.
==设备必须定期执行**垃圾回收 (GC)**，以找到这些块并为将来的写入释放空间；过度的垃圾回收会推高写入放大并降低性能。==

The second is high cost of in-memory mapping tables; the larger the device, the more memory such tables need.
==第二个缺点是内存映射表的成本高昂；设备越大，此类表需要的内存就越多。==

**44.8 Garbage Collection**
==**44.8 垃圾回收**==

The first cost of any log-structured approach such as this one is that garbage is created, and therefore **garbage collection** (i.e., dead-block reclamation) must be performed.
==像这样的日志结构化方法的第一项成本就是会产生垃圾，因此必须执行**垃圾回收**（即死块回收）。==

Now, let’s assume that blocks 100 and 101 are written to again, with contents c1 and c2.
==现在，假设对块 100 和 101 再次进行写入，内容为 c1 和 c2。==

The writes are written to the next free pages (in this case, physical pages 4 and 5), and the mapping table is updated accordingly.
==这些写入被写到下一个空闲页（在本例中为物理页 4 和 5），并相应地更新映射表。==

Note that the device must have first erased block 1 to make such programming possible.
==请注意，设备必须首先擦除块 1 才能使此类编程成为可能。==

The problem we have now should be obvious: physical pages 0 and 1, although marked `VALID`, have **garbage** in them, i.e., the old versions of blocks 100 and 101.
==我们现在面临的问题显而易见：物理页 0 和 1 虽然被标记为 `VALID`，但其中包含**垃圾**，即块 100 和 101 的旧版本。==

Because of the log-structured nature of the device, overwrites create garbage blocks, which the device must reclaim to provide free space for new writes to take place.
==由于设备的日志结构特性，覆写会产生垃圾块，设备必须回收这些块，以便为新的写入提供空闲空间。==

The process of finding garbage blocks (also called **dead blocks**) and reclaiming them for future use is called garbage collection, and it is an important component of any modern SSD.
==寻找垃圾块（也称为**死块**）并回收它们以供将来使用的过程被称为垃圾回收，它是任何现代 SSD 的重要组成部分。==

The basic process is simple: find a block that contains one or more garbage pages, read in the **live** (non-garbage) pages from that block, write out those live pages to the log, and (finally) reclaim the entire block for use in writing.
==基本过程很简单：找出一个包含一个或多个垃圾页的块，从该块中读入**活跃**（非垃圾）页，将这些活跃页写回到日志末尾，并（最后）回收整个块以供写入使用。==

Let’s now illustrate with an example.
==现在让我们用一个例子来说明。==

The device decides it wants to reclaim any dead pages within block 0 above.
==设备决定要回收上面块 0 中的任何死页。==

Block 0 has two dead blocks (pages 0 and 1) and two live blocks (pages 2 and 3, which contain blocks 2000 and 2001, respectively).
==块 0 有两个死块（页 0 和 1）和两个活跃块（页 2 和 3，它们分别包含块 2000 和 2001）。==

To do so, the device will:
==为此，设备将：==

• Read live data (pages 2 and 3) from block 0
==• 从块 0 读取活跃数据（页 2 和 3）==

• Write live data to end of the log
==• 将活跃数据写入日志末尾==

• Erase block 0 (freeing it for later usage)
==• 擦除块 0（释放它以供以后使用）==

For the garbage collector to function, there must be enough information within each block to enable the SSD to determine whether each page is live or dead.
==为了使垃圾回收器正常工作，每个块中必须包含足够的信息，使 SSD 能够确定每个页是活跃的还是死掉的。==

One natural way to achieve this end is to store, at some location within each block, information about which logical blocks are stored within each page.
==实现这一目标的一种自然方法是在每个块的某个位置存储有关每个页中存储了哪些逻辑块的信息。==

The device can then use the mapping table to determine whether each page within the block holds live data or not.
==然后，设备可以使用映射表来确定块内的每个页是否持有活跃数据。==

By checking the mapping table (which, before garbage collection, contained 100->4, 101->5, 2000->2, 2001->3), the device can readily determine whether each of the pages within the SSD block holds live information.
==通过检查映射表（在垃圾回收之前，映射表包含 100->4, 101->5, 2000->2, 2001->3），设备可以轻松确定 SSD 块内的每个页是否持有活跃信息。==

As you can see, garbage collection can be expensive, requiring reading and rewriting of live data.
==如你所见，垃圾回收可能代价很高，因为它需要读取和重写活跃数据。==

The ideal candidate for reclamation is a block that consists of only dead pages; in this case, the block can immediately be erased and used for new data, without expensive data migration.
==最理想的回收对象是仅由死页组成的块；在这种情况下，该块可以立即被擦除并用于新数据，而无需昂贵的数据迁移。==

**ASIDE: A NEW STORAGE API KNOWN AS TRIM**
==**旁注：一种名为 TRIM 的新存储 API**==

When we think of hard drives, we usually just think of the most basic interface to read and write them.
==当我们想到硬盘驱动器时，我们通常只会想到读写它们的最基础接口。==

With log-structured SSDs, and indeed, any device that keeps a flexible and changing mapping of logical-to-physical blocks, a new interface is useful, known as the **trim** operation.
==对于日志结构化的 SSD，以及任何保持灵活且不断变化的逻辑到物理块映射的设备，一种名为 **trim** 操作的新接口非常有用。==

The trim operation takes an address (and possibly a length) and simply informs the device that the block(s) specified by the address (and length) have been deleted.
==TRIM 操作接收一个地址（可能还有一个长度），并简单地通知设备，由该地址（和长度）指定的块已被删除。==

The device thus no longer has to track any information about the given address range.
==因此，设备不再需要跟踪有关给定地址范围的任何信息。==

For a standard hard drive, trim isn’t particularly useful, because the drive has a static mapping of block addresses to specific platter, track, and sector(s).
==对于标准的硬盘驱动器，TRIM 并不是特别有用，因为驱动器具有块地址到特定盘片、磁道和扇区的静态映射。==

For a log-structured SSD, however, it is highly useful to know that a block is no longer needed, as the SSD can then remove this information from the FTL and later reclaim the physical space during garbage collection.
==然而，对于日志结构的 SSD，知道一个块不再被需要是非常有用的，因为 SSD 随后可以从 FTL 中删除此信息，并在以后的垃圾回收期间回收物理空间。==

To reduce GC costs, some SSDs **overprovision** the device [A+08]; by adding extra flash capacity, cleaning can be delayed and pushed to the background.
==为了降低垃圾回收 (GC) 成本，一些 SSD 会对设备进行**超额配置 (overprovision)** [A+08]；通过增加额外的闪存容量，清理工作可以延迟并推送到后台进行。==

**44.9 Mapping Table Size**
==**44.9 映射表大小**==

The second cost of log-structuring is the potential for extremely large mapping tables, with one entry for each 4-KB page of the device.
==日志结构化的第二个成本是可能产生极大的映射表，设备的每个 4 KB 页都需要一个表项。==

With a large 1-TB SSD, for example, a single 4-byte entry per 4-KB page results in 1 GB of memory needed by the device, just for these mappings!
==例如，对于一个 1 TB 的大型 SSD，每个 4 KB 页对应一个 4 字节的表项，仅这些映射就需要设备提供 1 GB 的内存！==

Thus, this **page-level** FTL scheme is impractical.
==因此，这种**页级** FTL 方案是不切实际的。==

**Block-Based Mapping**
==**基于块的映射**==

One approach to reduce the costs of mapping is to only keep a pointer per *block* of the device, instead of per page, reducing the amount of mapping information by a factor of $\frac{Size_{block}}{Size_{page}}$.
==一种降低映射成本的方法是仅为设备的每个*块*（而不是每个页）保留一个指针，从而将映射信息量减少到原来的 $\frac{Size_{block}}{Size_{page}}$。==

This **block-level** FTL is akin to having bigger page sizes in a virtual memory system.
==这种**块级** FTL 类似于在虚拟内存系统中使用更大的页大小。==

Unfortunately, using a block-based mapping inside a log-based FTL does not work very well for performance reasons.
==不幸的是，出于性能原因，在日志结构 FTL 中使用基于块的映射效果并不理想。==

The biggest problem arises when a “small write” occurs (i.e., one that is less than the size of a physical block).
==最大的问题出现在发生“小额写入”时（即写入量小于物理块的大小）。==

In this case, the FTL must read a large amount of live data from the old block and copy it into a new one (along with the data from the small write).
==在这种情况下，FTL 必须从旧块中读取大量活跃数据，并将其复制到新块中（连同来自小额写入的数据）。==

This data copying increases write amplification greatly and thus decreases performance.
==这种数据复制大大增加了写入放大，从而降低了性能。==

**Hybrid Mapping**
==**混合映射**==

To enable flexible writing but also reduce mapping costs, many modern FTLs employ a **hybrid mapping** technique.
==为了实现灵活的写入并降低映射成本，许多现代 FTL 采用了**混合映射**技术。==

With this approach, the FTL keeps a few blocks erased and directs all writes to them; these are called **log blocks**.
==通过这种方法，FTL 保持几个块处于已擦除状态，并将所有写入导向它们；这些块被称为**日志块 (log blocks)**。==

Because the FTL wants to be able to write any page to any location within the log block without all the copying required by a pure block-based mapping, it keeps **per-page** mappings for these log blocks.
==由于 FTL 希望能够将任何页写入日志块内的任何位置，而不需要纯基于块的映射所需的所有复制操作，因此它为这些日志块保留了**每页 (per-page)** 映射。==

The FTL thus logically has two types of mapping table in its memory: a small set of per-page mappings in what we’ll call the **log table**, and a larger set of per-block mappings in the **data table**.
==因此，FTL 的内存中逻辑上存在两种类型的映射表：一组较小的每页映射，我们称之为**日志表 (log table)**；以及一组较大的每块映射，称为**数据表 (data table)**。==

When looking for a particular logical block, the FTL will first consult the log table; if the logical block’s location is not found there, the FTL will then consult the data table to find its location.
==在寻找特定的逻辑块时，FTL 会首先咨询日志表；如果在那里没有找到该逻辑块的位置，FTL 随后会咨询数据表来查找其位置。==

The key to the hybrid mapping strategy is keeping the number of log blocks small.
==混合映射策略的关键在于保持日志块的数量较少。==

To keep the number of log blocks small, the FTL has to periodically examine log blocks and **switch** them into blocks that can be pointed to by only a single block pointer.
==为了保持日志块数量较少，FTL 必须定期检查日志块，并将它们**转换 (switch)** 为只能由单个块指针指向的块。==

This switch is accomplished by one of three main techniques: **switch merge**, **partial merge**, or **full merge**.
==这种转换通过三种主要技术之一完成：**交换合并 (switch merge)**、**部分合并 (partial merge)** 或**全合并 (full merge)**。==

**44.10 Wear Leveling**
==**44.10 磨损均衡**==

Finally, a related background activity that modern FTLs must implement is **wear leveling**.
==最后，现代 FTL 必须实现的另一项相关后台活动是**磨损均衡**。==

The basic idea is simple: because multiple erase/program cycles will wear out a flash block, the FTL should try its best to spread that work across all the blocks of the device evenly.
==基本思想很简单：由于多次擦除/编程循环会磨损闪存块，因此 FTL 应尽最大努力将这些工作均匀地分布在设备的所有块上。==

In this manner, all blocks will wear out at roughly the same time, instead of a few “popular” blocks quickly becoming unusable.
==通过这种方式，所有块大致会在同一时间磨损，而不是少数“热门”块迅速变得无法使用。==

**44.11 SSD Performance And Cost**
==**44.11 SSD 性能与成本**==

Unlike hard disk drives, flash-based SSDs have no mechanical components, and in fact are in many ways more similar to DRAM, in that they are “random access” devices.
==与硬盘驱动器不同，基于闪存的 SSD 没有机械部件，事实上在许多方面更类似于 DRAM，因为它们是“随机存取”设备。==

The biggest difference in performance, as compared to disk drives, is realized when performing random reads and writes; while a typical disk drive can only perform a few hundred random I/Os per second, SSDs can do much better.
==与磁盘驱动器相比，最大的性能差异体现在执行随机读写时；虽然典型的磁盘驱动器每秒只能执行几百次随机 I/O，但 SSD 的表现要好得多。==

The reason for such unexpectedly good random-write performance is due to the log-structured design of many SSDs, which transforms random writes into sequential ones and improves performance.
==如此出色的随机写入性能归功于许多 SSD 的日志结构设计，它将随机写入转换为顺序写入并提高了性能。==

So why haven’t SSDs completely replaced hard drives as the storage medium of choice?
==那么，为什么 SSD 还没有完全取代硬盘驱动器成为首选的存储介质呢？==

The answer is simple: **cost**, or more specifically, cost per unit of capacity.
==答案很简单：**成本**，或者更具体地说，是单位容量的成本。==

If performance is the main concern, SSDs are a terrific choice.
==如果性能是主要考虑因素，那么 SSD 是一个极好的选择。==

If, on the other hand, you are assembling a large data center and wish to store massive amounts of information, the large cost difference will drive you towards hard drives.
==另一方面，如果你正在组建一个大型数据中心并希望存储海量信息，巨大的成本差异会让你倾向于选择硬盘驱动器。==

As long as the price gap exists, hard drives are here to stay.
==只要价格差距依然存在，硬盘驱动器就将继续存在。==

**44.12 Summary**
==**44.12 总结**==

Flash-based SSDs are becoming a common presence in laptops, desktops, and servers inside the datacenters that power the world’s economy.
==基于闪存的 SSD 正日益普遍地出现在笔记本电脑、台式机以及驱动全球经济的数据中心服务器中。==

This chapter is just the first step in understanding the state of the art.
==本章仅仅是理解这一前沿技术的第一步。==

Don’t just read academic papers; also read about recent advances in the popular press.
==不要只读学术论文；也要关注大众媒体上关于最新进展的报道。==

Dive in and learn more about this “iceberg” of research on your own.
==深入钻研，自行探索这一研究领域的“冰山”。==

Be careful though; icebergs can sink even the mightiest of ships.
==不过要小心；冰山甚至能撞沉最强大的舰船。==




==### 第一步：数据清洗 | 第二步：句子切分 | 第三步：输出格式==

ASIDE: KEY SSD TERMS
==侧栏：SSD 关键术语==

A flash chip consists of many banks, each of which is organized into **erase blocks** (sometimes just called **blocks**).
==一个闪存芯片由许多 bank 组成，每个 bank 被组织成**擦除块**（有时简称为**块**）。==

Each block is further subdivided into some number of **pages**.
==每个块进一步细分为若干个**页**。==

Blocks are large (128KB–2MB) and contain many pages, which are relatively small (1KB–8KB).
==块的容量很大（128KB–2MB），包含许多相对较小的页（1KB–8KB）。==

To read from flash, issue a read command with an address and length; this allows a client to read one or more pages.
==要从闪存读取，需发出带有地址和长度的读取命令；这允许客户端读取一个或多个页。==

Writing flash is more complex.
==写入闪存则更为复杂。==

First, the client must **erase** the entire block (which deletes all information within the block).
==首先，客户端必须**擦除**整个块（这将删除该块内的所有信息）。==

Then, the client can **program** each page exactly once, thus completing the write.
==然后，客户端可以对每个页进行精确的一次**编程**，从而完成写入。==

A new **trim** operation is useful to tell the device when a particular block (or range of blocks) is no longer needed.
==一种新的 **trim** 操作很有用，它能告知设备何时不再需要特定的块（或块范围）。==

Flash reliability is mostly determined by **wear out**; if a block is erased and programmed too often, it will become unusable.
==闪存的可靠性主要由**磨损**决定；如果一个块被过度频繁地擦除和编程，它将变得不可用。==

A flash-based **solid-state storage device (SSD)** behaves as if it were a normal block-based read/write disk; by using a **flash translation layer (FTL)**, it transforms reads and writes from a client into reads, erases, and programs to underlying flash chips.
==基于闪存的**固态存储设备 (SSD)** 的表现就像一个普通的基于块的读/写磁盘；通过使用**闪存转换层 (FTL)**，它将来自客户端的读取和写入操作转换为对底层闪存芯片的读取、擦除和编程。==

Most FTLs are **log-structured**, which reduces the cost of writing by minimizing erase/program cycles.
==大多数 FTL 都是**日志结构化**的，这通过减少擦除/编程周期来降低写入成本。==

An in-memory translation layer tracks where logical writes were located within the physical medium.
==一个内存中的转换层负责跟踪逻辑写入在物理介质中的位置。==

One key problem with log-structured FTLs is the cost of **garbage collection**, which leads to **write amplification**.
==日志结构化 FTL 的一个关键问题是**垃圾回收**的成本，这会导致**写入放大**。==

Another problem is the size of the mapping table, which can become quite large.
==另一个问题是映射表的大小，它可能会变得非常庞大。==

Using a **hybrid mapping** or just **caching** hot pieces of the FTL are possible remedies.
==使用**混合映射**或仅对 FTL 的热点部分进行**缓存**是可能的补救措施。==

One last problem is **wear leveling**; the FTL must occasionally migrate data from blocks that are mostly read in order to ensure said blocks also receive their share of the erase/program load.
==最后一个问题是**磨损均衡**；FTL 必须偶尔将数据从主要进行读取操作的块中迁移出来，以确保这些块也能分担擦除/编程负载。==

Data Integrity and Protection
==数据完整性和保护==

Specifically, how should a file system or storage system ensure that data is safe, given the unreliable nature of modern storage devices?
==具体来说，考虑到现代存储设备的不稳定性，文件系统或存储系统应该如何确保数据的安全？==

This general area is referred to as **data integrity** or **data protection**.
==这一通用领域被称为**数据完整性**或**数据保护**。==

CRUX: HOW TO ENSURE DATA INTEGRITY
==核心问题：如何确保数据完整性==

How should systems ensure that the data written to storage is protected?
==系统应该如何确保写入存储的数据得到保护？==

What techniques are required?
==需要哪些技术？==

How can such techniques be made efficient, with both low space and time overheads?
==如何使这些技术在低空间和时间开销下保持高效？==

Disk Failure Modes
==磁盘故障模式==

Modern disks will occasionally seem to be mostly working but have trouble successfully accessing one or more blocks.
==现代磁盘偶尔会表现为大部分工作正常，但在成功访问一个或多个块时遇到困难。==

Specifically, two types of single-block failures are common and worthy of consideration: **latent sector errors (LSEs)** and **block corruption**.
==具体而言，有两种类型的单块故障很常见且值得考虑：**潜在扇区错误 (LSE)** 和**块损坏**。==

LSEs arise when a disk sector (or group of sectors) has been damaged in some way.
==当磁盘扇区（或扇区组）以某种方式损坏时，就会产生 LSE。==

In-disk **error correcting codes (ECC)** are used by the drive to determine whether the on-disk bits in a block are good.
==驱动器使用磁盘内的**纠错码 (ECC)** 来确定块中的磁盘位是否完好。==

There are also cases where a disk block becomes **corrupt** in a way not detectable by the disk itself.
==还有一些情况，磁盘块变得**损坏**，且无法被磁盘本身检测到。==

These types of faults are particularly insidious because they are **silent faults**.
==这些类型的故障特别阴险，因为它们是**静默故障**。==

Prabhakaran et al. describes this more modern view of disk failure as the **fail-partial** disk failure model.
==Prabhakaran 等人将这种更现代的磁盘故障观点描述为**部分失效**磁盘故障模型。==

Detecting Corruption: The Checksum
==检测损坏：校验和==

The primary mechanism used by modern storage systems to preserve data integrity is called the **checksum**.
==现代存储系统用于保持数据完整性的主要机制称为**校验和**。==

A checksum is simply the result of a function that takes a chunk of data (say a 4KB block) as input and computes a function over said data, producing a small summary.
==校验和简单来说就是一个函数的结果，该函数将一块数据（例如一个 4KB 的块）作为输入，并对该数据计算一个函数，产生一个小的摘要。==

Common Checksum Functions
==常用校验和函数==

One simple checksum function that some use is based on **exclusive or (XOR)**.
==一种被一些人使用的简单校验和函数是基于**异或 (XOR)** 的。==

Another basic checksum function is addition.
==另一个基础的校验和函数是加法。==

A slightly more complex algorithm is known as the **Fletcher checksum**.
==一种稍微复杂一点的算法被称为 **Fletcher 校验和**。==

Specifically, assume a block $D$ consists of bytes $d_1 ... d_n$.
==具体而言，假设一个块 $D$ 由字节 $d_1 ... d_n$ 组成。==

$s1$ is defined as follows: $s1 = (s1 + d_i) \mod 255$ (computed over all $d_i$).
==$s1$ 的定义如下：$s1 = (s1 + d_i) \mod 255$（对所有 $d_i$ 进行计算）。==

$s2$ in turn is: $s2 = (s2 + s1) \mod 255$ (again over all $d_i$).
==$s2$ 依次为：$s2 = (s2 + s1) \mod 255$（再次对所有 $d_i$ 进行计算）。==

One final commonly-used checksum is known as a **cyclic redundancy check (CRC)**.
==最后一种常用的校验和被称为**循环冗余校验 (CRC)**。==

A New Problem: Misdirected Writes
==一个新问题：误导写入==

The first failure mode of interest is called a **misdirected write**.
==第一个值得关注的故障模式被称为**误导写入**。==

This arises in disk and RAID controllers which write the data to disk correctly, except in the wrong location.
==这发生在磁盘和 RAID 控制器中，它们将数据正确地写入磁盘，只是位置写错了。==

The answer, not surprisingly, is simple: add a little more information to each checksum.
==不出所料，答案很简单：给每个校验和添加一点额外信息。==

In this case, adding a **physical identifier (physical ID)** is quite helpful.
==在这种情况下，添加一个**物理标识符 (physical ID)** 非常有帮助。==

One Last Problem: Lost Writes
==最后一个问题：丢失写入==

Specifically, some modern storage devices also have an issue known as a **lost write**.
==具体来说，一些现代存储设备还存在一种被称为**丢失写入**的问题。==

This occurs when the device informs the upper layer that a write has completed but in fact it never is persisted.
==当设备告知上层写入已完成，但实际上该写入从未被持久化时，就会发生这种情况。==

Some systems add a checksum elsewhere in the system to detect lost writes.
==一些系统在系统的其他位置添加校验和来检测丢失写入。==

For example, Sun’s **Zettabyte File System (ZFS)** includes a checksum in each file system inode and indirect block.
==例如，Sun 的 **Zettabyte 文件系统 (ZFS)** 在每个文件系统索引节点 (inode) 和间接块中都包含一个校验和。==

Scrubbing
==清理（Scrubbing）==

To remedy this problem, many systems utilize **disk scrubbing** of various forms.
==为了补救这个问题，许多系统利用各种形式的**磁盘清理**。==

By periodically reading through *every* block of the system, and checking whether checksums are still valid, the disk system can reduce the chances that all copies of a certain data item become corrupted.
==通过定期读取系统的*每一个*块并检查校验和是否仍然有效，磁盘系统可以降低某个数据项的所有副本都损坏的概率。==

Overheads Of Checksumming
==校验和的开销==

There are two distinct kinds of overheads: space and time.
==存在两种截然不同的开销：空间和时间。==

Space overheads come in two forms: on the disk itself and in the memory of the system.
==空间开销有两种形式：在磁盘本身和在系统的内存中。==

The time overheads induced by checksumming can be quite noticeable.
==由校验和引起的时间开销可能相当明显。==

Minimally, the CPU must compute the checksum over each block, both when the data is stored and when it is accessed.
==至少，CPU 必须在数据存储和访问时对每个块计算校验和。==




**A Dialogue on Distribution**
==**关于分布式的对话**==

**Professor:** And thus we reach our final little piece in the world of operating systems: distributed systems.
==**教授：** 这样我们就来到了操作系统世界的最后一块拼图：分布式系统。==

Since we can’t cover much here, we’ll sneak in a little intro here in the section on persistence, and focus mostly on distributed file systems.
==**教授：** 既然我们无法在此涵盖太多内容，我们就在持久化章节中悄悄加入一点介绍，并主要关注分布式文件系统。==

Hope that is OK!
==**教授：** 希望这没问题！==

**Student:** Sounds OK.
==**学生：** 听起来不错。==

But what is a distributed system exactly, oh glorious and all-knowing professor?
==**学生：** 但是，光荣且无所不知的教授，分布式系统到底是什么呢？==

**Professor:** Well, I bet you know how this is going to go...
==**教授：** 嗯，我敢打赌你知道接下来的路数……==

**Student:** There’s a peach?
==**学生：** 又是关于桃子的比喻？==

**Professor:** Exactly!
==**教授：** 没错！==

But this time, it’s far away from you, and may take some time to get the peach.
==**教授：** 但这一次，桃子离你很远，可能需要一些时间才能拿到它。==

And there are a lot of them!
==**教授：** 而且桃子的数量有很多！==

Even worse, sometimes a peach becomes rotten.
==**教授：** 更糟糕的是，有时桃子会变质。==

But you want to make sure that when anybody bites into a peach, they will get a mouthful of deliciousness.
==**教授：** 但你要确保当任何人咬下一口桃子时，他们都能尝到满口的美味。==

**Student:** This peach analogy is working less and less for me.
==**学生：** 这个桃子的类比对我来说越来越不管用了。==

**Professor:** Come on!
==**教授：** 别这样！==

It’s the last one, just go with it.
==**教授：** 这是最后一个了，就随它去吧。==

**Student:** Fine.
==**学生：** 行吧。==

**Professor:** So anyhow, forget about the peaches.
==**教授：** 那么不管怎样，忘掉桃子吧。==

Building distributed systems is hard, because things fail all the time.
==**教授：** 构建分布式系统很困难，因为故障无处不在。==

Messages get lost, machines go down, disks corrupt data.
==**教授：** 消息会丢失，机器会宕机，磁盘会损坏数据。==

It’s like the whole world is working against you!
==**教授：** 就像整个世界都在与你作对！==

**Student:** But I use distributed systems all the time, right?
==**学生：** 但我一直在使用分布式系统，对吧？==

**Professor:** Yes! You do. And... ?
==**教授：** 是的！你确实在用。然后呢……？==

**Student:** Well, it seems like they mostly work.
==**学生：** 嗯，看起来它们大部分时间都能正常工作。==

After all, when I send a search request to Google, it usually comes back in a snap, with some great results!
==**学生：** 毕竟，当我向 Google 发送搜索请求时，它通常很快就会返回一些很棒的结果！==

Same thing when I use Facebook, Amazon, and so forth.
==**学生：** 当我使用 Facebook、Amazon 等也是一样。==

**Professor:** Yes, it is amazing.
==**教授：** 是的，这确实很神奇。==

And that’s despite all of those failures taking place!
==**教授：** 而且这是在所有这些故障都在发生的情况下实现的！==

Those companies build a huge amount of machinery into their systems so as to ensure that even though some machines have failed, the entire system stays up and running.
==**教授：** 这些公司在系统中构建了大量的机制，以确保即使某些机器发生故障，整个系统仍能保持运行。==

They use a lot of techniques to do this: replication, retry, and various other tricks people have developed over time to detect and recover from failures.
==**教授：** 他们使用了很多技术来实现这一点：复制、重试以及人们随时间推移开发的各种用于检测故障并从中恢复的其他技巧。==

**Student:** Sounds interesting. Time to learn something for real?
==**学生：** 听起来很有趣。是时候学点真本事了吗？==

**Professor:** It does seem so. Let’s get to work!
==**教授：** 看起来确实如此。让我们开始干活吧！==

But first things first ... (bites into peach he has been holding, which unfortunately is rotten)
==**教授：** 但首先……（咬了一口他一直拿着的桃子，不幸的是，它是烂的）==

**Distributed Systems**
==**分布式系统**==

Distributed systems have changed the face of the world.
==**分布式系统已经改变了世界的面貌。**==

When your web browser connects to a web server somewhere else on the planet, it is participating in what seems to be a simple form of a **client/server** distributed system.
==**当你的浏览器连接到地球上其他地方的 Web 服务器时，它正在参与一种看似简单的“客户端/服务器” (client/server) 形式的分布式系统。**==

When you contact a modern web service such as Google or Facebook, you are not just interacting with a single machine, however; behind the scenes, these complex services are built from a large collection (i.e., thousands) of machines, each of which cooperate to provide the particular service of the site.
==**然而，当你接触 Google 或 Facebook 等现代 Web 服务时，你并不仅仅是在与单台机器交互；在幕后，这些复杂的服务是由大量（即数以千计）机器组成的集群构建的，每台机器都协同工作以提供该站点的特定服务。**==

Thus, it should be clear what makes studying distributed systems interesting.
==**因此，研究分布式系统的趣味所在应该是显而易见的。**==

Indeed, it is worthy of an entire class; here, we just introduce a few of the major topics.
==**事实上，它值得专门开设一门课程；在这里，我们只介绍几个主要的主题。**==

A number of new challenges arise when building a distributed system.
==**构建分布式系统时会出现许多新的挑战。**==

The major one we focus on is **failure**; machines, disks, networks, and software all fail from time to time, as we do not (and likely, will never) know how to build “perfect” components and systems.
==**我们关注的主要挑战是“故障” (failure)；机器、磁盘、网络和软件都会不时发生故障，因为我们不知道（而且可能永远也不会知道）如何构建“完美的”组件和系统。**==

However, when we build a modern web service, we’d like it to appear to clients as if it never fails; how can we accomplish this task?
==**然而，当我们构建现代 Web 服务时，我们希望它在客户端看来好像从未发生故障；我们该如何完成这项任务？**==

**THE CRUX: HOW TO BUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL**
==**核心问题：如何构建在组件故障时仍能正常工作的系统**==

How can we build a working system out of parts that don’t work correctly all the time?
==**我们如何利用那些并非始终能正确工作的部件构建出一个可运行的系统？**==

The basic question should remind you of some of the topics we discussed in RAID storage arrays; however, the problems here tend to be more complex, as are the solutions.
==**这个基本问题应该会让你想起我们在 RAID 存储阵列中讨论过的一些话题；然而，这里的问题往往更复杂，解决方案也是如此。**==

Interestingly, while failure is a central challenge in constructing distributed systems, it also represents an opportunity.
==**有趣的是，虽然故障是构建分布式系统的核心挑战，但它也代表着一种机遇。**==

Yes, machines fail; but the mere fact that a machine fails does not imply the entire system must fail.
==**是的，机器会故障；但单台机器故障并不意味着整个系统必须崩溃。**==

By collecting together a set of machines, we can build a system that appears to rarely fail, despite the fact that its components fail regularly.
==**通过将一组机器聚集在一起，我们可以构建一个看似极少发生故障的系统，尽管其组件经常发生故障。**==

This reality is the central beauty and value of distributed systems, and why they underlie virtually every modern web service you use, including Google, Facebook, etc.
==**这一现实是分布式系统的核心美学和价值所在，也是为什么它们成为了你所使用的几乎每一个现代 Web 服务（包括 Google、Facebook 等）的基础。**==

**TIP: COMMUNICATION IS INHERENTLY UNRELIABLE**
==**提示：通信本质上是不可靠的**==

In virtually all circumstances, it is good to view communication as a fundamentally unreliable activity.
==**在几乎所有情况下，将通信视为一种本质上不可靠的行为是很有益处的。**==

Bit corruption, down or non-working links and machines, and lack of buffer space for incoming packets all lead to the same result: packets sometimes do not reach their destination.
==**位损坏、链路或机器宕机或无法工作，以及由于接收数据包的缓冲空间不足，都会导致相同的结果：数据包有时无法到达目的地。**==

To build reliable services atop such unreliable networks, we must consider techniques that can cope with packet loss.
==**为了在如此不可靠的网络之上构建可靠的服务，我们必须考虑能够应对数据包丢失的技术。**==

Other important issues exist as well.
==**此外还存在其他重要问题。**==

System **performance** is often critical; with a network connecting our distributed system together, system designers must often think carefully about how to accomplish their given tasks, trying to reduce the number of messages sent and further make communication as efficient (low latency, high bandwidth) as possible.
==**系统“性能” (performance) 通常至关重要；在将分布式系统连接在一起的网络中，系统设计者必须经常仔细思考如何完成既定任务，设法减少发送的消息数量，并进一步使通信尽可能高效（低延迟、高带宽）。**==

Finally, **security** is also a necessary consideration.
==**最后，“安全性” (security) 也是必须考虑的因素。**==

When connecting to a remote site, having some assurance that the remote party is who they say they are becomes a central problem.
==**当连接到远程站点时，确保对方的身份真实可靠成为了一个核心问题。**==

Further, ensuring that third parties cannot monitor or alter an on-going communication between two others is also a challenge.
==**此外，确保第三方无法监控或篡改两者之间正在进行的通信也是一项挑战。**==

In this introduction, we’ll cover the most basic aspect that is new in a distributed system: **communication**.
==**在此介绍中，我们将涵盖分布式系统中最新颖、最基础的方面：“通信” (communication)。**==

Namely, how should machines within a distributed system communicate with one another?
==**换句话说，分布式系统中的机器应该如何相互通信？**==

We’ll start with the most basic primitives available, messages, and build a few higher-level primitives on top of them.
==**我们将从最基础的原语“消息” (messages) 开始，并在其之上构建一些高级原语。**==

As we said above, failure will be a central focus: how should communication layers handle failures?
==**正如我们上面所说的，故障将是核心关注点：通信层应该如何处理故障？**==

**48.1 Communication Basics**
==**48.1 通信基础**==

The central tenet of modern networking is that communication is fundamentally unreliable.
==**现代网络的中心原则是通信本质上是不可靠的。**==

Whether in the wide-area Internet, or a local-area high-speed network such as Infiniband, packets are regularly lost, corrupted, or otherwise do not reach their destination.
==**无论是在广域互联网，还是在诸如 Infiniband 之类的局域高速网络中，数据包都会经常丢失、损坏或由于其他原因无法到达目的地。**==

There are a multitude of causes for packet loss or corruption.
==**导致数据包丢失或损坏的原因有很多。**==

Sometimes, during transmission, some bits get flipped due to electrical or other similar problems.
==**有时，在传输过程中，由于电气或其他类似问题，某些位会发生翻转。**==

Sometimes, an element in the system, such as a network link or packet router or even the remote host, are somehow damaged or otherwise not working correctly; network cables do accidentally get severed, at least sometimes.
==**有时，系统中的某个元件（如网络链路、数据包路由器甚至远程主机）由于某种原因损坏或无法正常工作；网线确实会发生意外断裂，至少有时会这样。**==

More fundamental however is packet loss due to lack of buffering within a network switch, router, or endpoint.
==**然而，更根本的原因是由于网络交换机、路由器或端点内部缺乏缓冲空间导致的数据包丢失。**==

Specifically, even if we could guarantee that all links worked correctly, and that all the components in the system (switches, routers, end hosts) were up and running as expected, loss is still possible, for the following reason.
==**具体来说，即使我们可以保证所有链路都能正确工作，并且系统中的所有组件（交换机、路由器、终端主机）都按预期运行，丢失仍然是可能的，原因如下。**==

Imagine a packet arrives at a router; for the packet to be processed, it must be placed in memory somewhere within the router.
==**想象一个数据包到达路由器；为了处理该数据包，必须将其放置在路由器内的某个内存位置。**==

If many such packets arrive at once, it is possible that the memory within the router cannot accommodate all of the packets.
==**如果许多此类数据包同时到达，路由器的内存可能无法容纳所有数据包。**==

The only choice the router has at that point is to **drop** one or more of the packets.
==**此时路由器唯一的选择就是“丢弃” (drop) 一个或多个数据包。**==

This same behavior occurs at end hosts as well; when you send a large number of messages to a single machine, the machine’s resources can easily become overwhelmed, and thus packet loss again arises.
==**这种行为在终端主机上也会发生；当你向单台机器发送大量消息时，该机器的资源很容易变得过载，从而再次导致数据包丢失。**==

Thus, packet loss is fundamental in networking.
==**因此，数据包丢失是网络中根本性的问题。**==

The question thus becomes: how should we deal with it?
==**于是问题变成了：我们该如何处理它？**==

**48.2 Unreliable Communication Layers**
==**48.2 不可靠通信层**==

One simple way is this: we don’t deal with it.
==**一种简单的方法是：我们不处理它。**==

Because some applications know how to deal with packet loss, it is sometimes useful to let them communicate with a basic unreliable messaging layer, an example of the **end-to-end argument** one often hears about (see the Aside at end of chapter).
==**因为某些应用程序知道如何处理数据包丢失，所以有时让它们通过基础的不可靠消息层进行通信是有用的，这是人们经常听到的“端到端论点” (end-to-end argument) 的一个例子（见本章末尾的侧边栏）。**==

One excellent example of such an unreliable layer is found in the **UDP/IP** networking stack available today on virtually all modern systems.
==**这种不可靠层的一个极好例子是当今几乎所有现代系统上都可用的 UDP/IP 网络协议栈。**==

To use UDP, a process uses the **sockets** API in order to create a communication endpoint; processes on other machines (or on the same machine) send UDP **datagrams** to the original process (a datagram is a fixed-sized message up to some max size).
==**为了使用 UDP，进程使用套接字 (sockets) API 来创建一个通信端点；其他机器上（或同一台机器上）的进程向原始进程发送 UDP“数据报” (datagrams)（数据报是具有一定最大尺寸限制的固定大小的消息）。**==

UDP is a great example of an unreliable communication layer.
==**UDP 是不可靠通信层的一个典型例子。**==

If you use it, you will encounter situations where packets get lost (dropped) and thus do not reach their destination; the sender is never thus informed of the loss.
==**如果你使用它，你会遇到数据包丢失（被丢弃）从而无法到达目的地的情况；发送方永远不会被告知这种丢失。**==

However, that does not mean that UDP does not guard against any failures at all.
==**然而，这并不意味着 UDP 完全不防范任何故障。**==

For example, UDP includes a **checksum** to detect some forms of packet corruption.
==**例如，UDP 包含一个“校验和” (checksum) 以检测某些形式的数据包损坏。**==

**TIP: USE CHECKSUMS FOR INTEGRITY**
==**提示：使用校验和保证完整性**==

Checksums are a commonly-used method to detect corruption quickly and effectively in modern systems.
==**校验和是现代系统中快速有效地检测损坏的常用方法。**==

A simple checksum is addition: just sum up the bytes of a chunk of data; of course, many other more sophisticated checksums have been created, including basic cyclic redundancy codes (CRCs), the Fletcher checksum, and many others [MK09].
==**一种简单的校验和是加法：只需将一块数据的字节相加；当然，人们已经开发出许多其他更复杂的校验和，包括循环冗余校验 (CRC)、Fletcher 校验和等 [MK09]。**==

In networking, checksums are used as follows.
==**在网络中，校验和的使用方式如下。**==

Before sending a message from one machine to another, compute a checksum over the bytes of the message.
==**在将消息从一台机器发送到另一台之前，计算该消息字节的校验和。**==

Then send both the message and the checksum to the destination.
==**然后将消息和校验和同时发送到目的地。**==

At the destination, the receiver computes a checksum over the incoming message as well; if this computed checksum matches the sent checksum, the receiver can feel some assurance that the data likely did not get corrupted during transmission.
==**在目的地，接收者同样对接收到的消息计算校验和；如果计算出的校验和与发送的校验和匹配，接收者就可以在一定程度上确信数据在传输过程中很可能没有损坏。**==

However, because many applications simply want to send data to a destination and not worry about packet loss, we need more.
==**然而，由于许多应用程序只是想向目的地发送数据而不必担心数据包丢失，我们需要更多的机制。**==

Specifically, we need reliable communication on top of an unreliable network.
==**具体来说，我们需要在不可靠的网络之上建立可靠的通信。**==

**48.3 Reliable Communication Layers**
==**48.3 可靠通信层**==

To build a reliable communication layer, we need some new mechanisms and techniques to handle packet loss.
==**为了构建可靠的通信层，我们需要一些新的机制和技术来处理数据包丢失。**==

The technique that we will use is known as an **acknowledgment**, or **ack** for short.
==**我们将使用的技术被称为“确认” (acknowledgment)，简称为 ack。**==

The idea is simple: the sender sends a message to the receiver; the receiver then sends a short message back to acknowledge its receipt.
==**这个想法很简单：发送方给接收方发送一条消息；接收方随后发回一条简短的消息以确认已收到。**==

When the sender receives an acknowledgment of the message, it can then rest assured that the receiver did indeed receive the original message.
==**当发送方收到消息的确认时，它就可以放心，接收方确实收到了原始消息。**==

However, what should the sender do if it does not receive an acknowledgment?
==**然而，如果发送方没有收到确认，它该怎么办？**==

To handle this case, we need an additional mechanism, known as a **timeout**.
==**为了处理这种情况，我们需要一个额外的机制，称为“超时” (timeout)。**==

When the sender sends a message, the sender now sets a timer to go off after some period of time.
==**当发送方发送消息时，它会设置一个定时器，在一段时间后触发。**==

If, in that time, no acknowledgment has been received, the sender concludes that the message has been lost.
==**如果在该时间内没有收到确认，发送方就会判定消息已丢失。**==

The sender then simply performs a **retry** of the send, sending the same message again with hopes that this time, it will get through.
==**随后，发送方只需进行“重试” (retry) 发送，再次发送相同的消息，并希望这次能够成功通过。**==

For this approach to work, the sender must keep a copy of the message around, in case it needs to send it again.
==**为了使这种方法奏效，发送方必须保留消息的副本，以备需要再次发送时使用。**==

The combination of the timeout and the retry have led some to call the approach **timeout/retry**.
==**超时和重试的结合使得有些人将这种方法称为“超时/重试” (timeout/retry)。**==

Unfortunately, timeout/retry in this form is not quite enough.
==**不幸的是，这种形式的超时/重试还不够完善。**==

Figure 48.5 shows an example of packet loss which could lead to trouble.
==**图 48.5 展示了一个可能导致麻烦的数据包丢失示例。**==

In this example, it is not the original message that gets lost, but the acknowledgment.
==**在这个例子中，丢失的不是原始消息，而是确认信息 (ack)。**==

From the perspective of the sender, the situation seems the same: no ack was received, and thus a timeout and retry are in order.
==**从发送方的角度来看，情况似乎是一样的：没有收到 ack，因此需要进行超时和重试。**==

But from the perspective of the receiver, it is quite different: now the same message has been received twice!
==**但从接收方的角度来看，情况大不相同：现在同一条消息被接收了两次！**==

Thus, when we are aiming for a reliable message layer, we also usually want to guarantee that each message is received **exactly once** by the receiver.
==**因此，当我们的目标是构建可靠的消息层时，通常还希望保证每条消息被接收方“精确接收一次” (exactly once)。**==

To enable the receiver to detect duplicate message transmission, the sender has to identify each message in some unique way, and the receiver needs some way to track whether it has already seen each message before.
==**为了让接收方能够检测到重复的消息传输，发送方必须以某种唯一的方式标识每条消息，而接收方需要某种方法来追踪之前是否已经见过该消息。**==

A simpler approach, requiring little memory, solves this problem, and the mechanism is known as a **sequence counter**.
==**一种需要极少内存的更简单方法可以解决这个问题，该机制被称为“序列计数器” (sequence counter)。**==

With a sequence counter, the sender and receiver agree upon a start value (e.g., 1) for a counter that each side will maintain.
==**通过序列计数器，发送方和接收方约定一个初始值（例如 1），双方各自维护一个计数器。**==

Whenever a message is sent, the current value of the counter is sent along with the message; this counter value ($N$) serves as an ID for the message.
==**每当发送消息时，计数器的当前值会随消息一起发送；该计数器值 ($N$) 充当消息的 ID。**==

After the message is sent, the sender then increments the value (to $N + 1$).
==**消息发送后，发送方随后递增该值（变为 $N + 1$）。**==

The receiver uses its counter value as the expected value for the ID of the incoming message from that sender.
==**接收方使用其自身的计数器值作为预期从该发送方收到的下一条消息的 ID。**==

If the ID of a received message ($N$) matches the receiver’s counter (also $N$), it acks the message and passes it up to the application; in this case, the receiver concludes this is the first time this message has been received.
==**如果接收到的消息 ID ($N$) 与接收方的计数器（也是 $N$）匹配，它会确认该消息并将其传递给应用程序；在这种情况下，接收方判定这是第一次收到该消息。**==

The receiver then increments its counter (to $N + 1$), and waits for the next message.
==**接收方随后递增其计数器（变为 $N + 1$），并等待下一条消息。**==

If the ack is lost, the sender will timeout and re-send message $N$.
==**如果 ack 丢失，发送方将超时并重新发送消息 $N$。**==

This time, the receiver’s counter is higher ($N + 1$), and thus the receiver knows it has already received this message.
==**这一次，接收方的计数器更高（$N + 1$），因此接收方知道它已经收到过这条消息。**==

Thus it acks the message but does **not** pass it up to the application.
==**因此，它会确认该消息，但“不会”将其传递给应用程序。**==

The most commonly used reliable communication layer is known as **TCP/IP**, or just **TCP** for short.
==**最常用的可靠通信层被称为 TCP/IP，简称为 TCP。**==

**TIP: BE CAREFUL SETTING THE TIMEOUT VALUE**
==**提示：小心设置超时值**==

As you can probably guess from the discussion, setting the timeout value correctly is an important aspect of using timeouts to retry message sends.
==**正如你可能从讨论中猜到的那样，正确设置超时值是使用超时重试消息发送的一个重要方面。**==

If the timeout is too small, the sender will re-send messages needlessly, thus wasting CPU time on the sender and network resources.
==**如果超时值太小，发送方会不必要地重发消息，从而浪费发送方的 CPU 时间和网络资源。**==

If the timeout is too large, the sender waits too long to re-send and thus perceived performance at the sender is reduced.
==**如果超时值太大，发送方等待重发的时间过长，从而降低了发送方感知的性能。**==

In a scenario with many clients sending to a single server, packet loss at the server may be an indicator that the server is overloaded.
==**在许多客户端向单个服务器发送请求的场景中，服务器端的数据包丢失可能表明服务器已过载。**==

If true, clients might retry in a different adaptive manner; for example, after the first timeout, a client might increase its timeout value to a higher amount, perhaps twice as high as the original value.
==**如果是这样，客户端可能会以不同的自适应方式进行重试；例如，在第一次超时后，客户端可能会将超时值增加到更高的数值，或许是原始值的两倍。**==

Such an **exponential back-off** scheme avoids situations where resources are being overloaded by an excess of re-sends.
==**这种“指数退避” (exponential back-off) 方案避免了因过度重发而导致资源过载的情况。**==

**48.4 Communication Abstractions**
==**48.4 通信抽象**==

Given a basic messaging layer, we now approach the next question in this chapter: what abstraction of communication should we use when building a distributed system?
==**有了基础的消息层后，我们现在探讨本章的下一个问题：在构建分布式系统时，我们应该使用什么样的通信抽象？**==

One body of work took OS abstractions and extended them to operate in a distributed environment.
==**有一类研究采用了操作系统的抽象，并将其扩展到分布式环境中运行。**==

For example, **distributed shared memory (DSM)** systems enable processes on different machines to share a large, virtual address space [LH89].
==**例如，“分布式共享内存” (DSM) 系统使不同机器上的进程能够共享一个大型虚拟地址空间 [LH89]。**==

This approach is not widely in use today for a number of reasons.
==**由于种种原因，这种方法在今天并未被广泛使用。**==

The largest problem for DSM is how it handles failure.
==**DSM 面临的最大问题是它如何处理故障。**==

Imagine, for example, if a machine fails; what happens to the pages on that machine?
==**想象一下，例如，如果一台机器发生故障，该机器上的页面会发生什么？**==

Dealing with failure when parts of your address space go missing is hard; imagine a linked list where a “next” pointer points into a portion of the address space that is gone. Yikes!
==**当地址空间的一部分消失时，处理故障是非常困难的；想象一个链表，其中的“下一个”指针指向了已经消失的那部分地址空间。哎呀！**==

**48.5 Remote Procedure Call (RPC)**
==**48.5 远程过程调用 (RPC)**==

The most dominant abstraction is based on the idea of a **remote procedure call**, or **RPC** for short [BN84].
==**最主流的抽象是基于“远程过程调用” (remote procedure call) 的理念，简称 RPC [BN84]。**==

Remote procedure call packages all have a simple goal: to make the process of executing code on a remote machine as simple and straightforward as calling a local function.
==**远程过程调用软件包都有一个简单的目标：使在远程机器上执行代码的过程就像调用本地函数一样简单直接。**==

The RPC system generally has two pieces: a **stub generator** (sometimes called a **protocol compiler**), and the **run-time library**.
==**RPC 系统通常包含两个部分：一个“存根生成器” (stub generator)（有时称为协议编译器）和“运行时库” (run-time library)。**==

**Stub Generator**
==**存根生成器**==

The stub generator’s job is simple: to remove some of the pain of packing function arguments and results into messages by automating it.
==**存根生成器的任务很简单：通过自动化来减轻将函数参数和结果打包进消息的痛苦。**==

For the client, a **client stub** is generated, which contains each of the functions specified in the interface.
==**对于客户端，会生成一个“客户端存根” (client stub)，其中包含接口中指定的每个函数。**==

Internally, the code in the client stub does this:
==**在内部，客户端存根中的代码执行以下操作：**==

*   **Create a message buffer.**
==    **创建消息缓冲区。**==
*   **Pack the needed information into the message buffer.** This process is sometimes referred to as the **marshaling** of arguments or the **serialization** of the message.
==    **将所需信息打包进消息缓冲区。** 该过程有时被称为参数的“编组” (marshaling) 或消息的“序列化” (serialization)。==
*   **Send the message to the destination RPC server.**
==    **将消息发送到目标 RPC 服务器。**==
*   **Wait for the reply.** Because function calls are usually **synchronous**, the call will wait for its completion.
==    **等待回复。** 因为函数调用通常是“同步” (synchronous) 的，所以调用将等待其完成。==
*   **Unpack return code and other arguments.** This step is also known as **unmarshaling** or **deserialization**.
==    **解包返回码和其他参数。** 此步骤也称为“解组” (unmarshaling) 或“反序列化” (deserialization)。==
*   **Return to the caller.**
==    **返回给调用者。**==

**Run-Time Library**
==**运行时库**==

The run-time library handles much of the heavy lifting in an RPC system; most performance and reliability issues are handled herein.
==**运行时库负责 RPC 系统中大部分繁重的工作；大多数性能和可靠性问题都在此处处理。**==

One of the first challenges we must overcome is how to locate a remote service. This problem, of **naming**, is a common one in distributed systems.
==**我们必须克服的首要挑战之一是如何定位远程服务。这个“命名” (naming) 问题在分布式系统中很常见。**==

Once a client knows which server it should talk to, the next question is which transport-level protocol should RPC be built upon.
==**一旦客户端知道它应该与哪台服务器通信，接下来的问题是 RPC 应该构建在哪个传输层协议之上。**==

Many RPC packages are built on top of unreliable communication layers, such as UDP.
==**许多 RPC 软件包构建在不可靠的通信层（如 UDP）之上。**==

Doing so enables a more efficient RPC layer, but does add the responsibility of providing reliability to the RPC system.
==**这样做可以实现更高效的 RPC 层，但确实增加了为 RPC 系统提供可靠性的责任。**==

**Aside: THE END-TO-END ARGUMENT**
==**侧边栏：端到端论点**==

The **end-to-end argument** makes the case that the highest level in a system, i.e., usually the application at “the end”, is ultimately the only locale within a layered system where certain functionality can truly be implemented.
==**“端到端论点” (end-to-end argument) 认为，系统中的最高层（即通常位于“端点”的应用程序）最终是分层系统中唯一可以真正实现某些功能的场所。**==

**48.6 Summary**
==**48.6 总结**==

We have seen the introduction of a new topic, distributed systems, and its major issue: how to handle failure which is now a commonplace event.
==**我们已经介绍了分布式系统这一新课题及其核心问题：如何处理现在已成为家常便饭的故障。**==

The key to any distributed system is how you deal with that failure.
==**任何分布式系统的关键都在于你如何应对这种故障。**==

Communication forms the heart of any distributed system.
==**通信构成了任何分布式系统的核心。**==

A common abstraction of that communication is found in remote procedure call (RPC), which enables clients to make remote calls on servers.
==**这种通信的一种常见抽象是远程过程调用 (RPC)，它使客户端能够对服务器进行远程调用。**==




49 Sun’s Network File System (NFS)
==49 Sun 的网络文件系统 (NFS)==

One of the first uses of distributed client/server computing was in the realm of distributed file systems.
==分布式客户端/服务器计算最早的应用之一是在分布式文件系统领域。==

In such an environment, there are a number of client machines and one server (or a few); the server stores the data on its disks, and clients request data through well-formed protocol messages.
==在这种环境中，存在若干客户端机器和一个（或少数几个）服务器；服务器将数据存储在其磁盘上，客户端通过格式良好的协议消息请求数据。==

Figure 49.1 depicts the basic setup.
==图 49.1 描述了这种基本设置。==

As you can see from the picture, the server has the disks, and clients send messages across a network to access their directories and files on those disks.
==正如你从图中看到的，服务器拥有磁盘，客户端通过网络发送消息来访问这些磁盘上的目录和文件。==

Why do we bother with this arrangement? (i.e., why don’t we just let clients use their local disks?)
==我们为什么要费心采用这种安排？（即，为什么我们不直接让客户端使用它们的本地磁盘呢？）==

Well, primarily this setup allows for easy sharing of data across clients.
==嗯，这种设置主要是为了方便客户端之间的数据共享。==

Thus, if you access a file on one machine (Client 0) and then later use another (Client 2), you will have the same view of the file system.
==因此，如果你在其中一台机器（客户端 0）上访问一个文件，稍后使用另一台（客户端 2），你将拥有相同的文件系统视图。==

Your data is naturally shared across these different machines.
==你的数据在这些不同的机器之间自然地共享。==

A secondary benefit is centralized administration; for example, backing up files can be done from the few server machines instead of from the multitude of clients.
==次要的好处是集中管理；例如，备份文件可以从少数几台服务器机器上完成，而不是从众多的客户端上完成。==

Another advantage could be security; having all servers in a locked machine room prevents certain types of problems from arising.
==另一个优点可能是安全性；将所有服务器放在锁定的机房中可以防止某些类型问题的发生。==

CRUX: HOW TO BUILD A DISTRIBUTED FILE SYSTEM
==核心问题：如何构建分布式文件系统==

How do you build a distributed file system?
==你如何构建一个分布式文件系统？==

What are the key aspects to think about?
==需要考虑的关键方面有哪些？==

What is easy to get wrong?
==哪些地方容易出错？==

What can we learn from existing systems?
==我们可以从现有系统中学习到什么？==

49.1 A Basic Distributed File System
==49.1 基础分布式文件系统==

We now will study the architecture of a simplified distributed file system.
==我们现在将研究一个简化的分布式文件系统的架构。==

A simple client/server distributed file system has more components than the file systems we have studied so far.
==一个简单的客户端/服务器分布式文件系统比我们目前研究过的文件系统拥有更多的组件。==

On the client side, there are client applications which access files and directories through the client-side file system.
==在客户端，有通过客户端文件系统访问文件和目录的客户端应用程序。==

A client application issues system calls to the client-side file system (such as `open()`, `read()`, `write()`, `close()`, `mkdir()`, etc.) in order to access files which are stored on the server.
==客户端应用程序向客户端文件系统发出系统调用（如 `open()`、`read()`、`write()`、`close()`、`mkdir()` 等），以便访问存储在服务器上的文件。==

Thus, to client applications, the file system does not appear to be any different than a local (disk-based) file system, except perhaps for performance; in this way, distributed file systems provide transparent access to files, an obvious goal; after all, who would want to use a file system that required a different set of APIs or otherwise was a pain to use?
==因此，对于客户端应用程序来说，除了性能之外，该文件系统看起来与本地（基于磁盘的）文件系统没有任何区别；通过这种方式，分布式文件系统提供了对文件的透明访问，这是一个显而易见的目标；毕竟，谁会想使用一个需要不同 API 集或者使用起来很痛苦的文件系统呢？==

The role of the client-side file system is to execute the actions needed to service those system calls.
==客户端文件系统的作用是执行服务这些系统调用所需的操作。==

For example, if the client issues a `read()` request, the client-side file system may send a message to the server-side file system (or, as it is commonly called, the file server) to read a particular block; the file server will then read the block from disk (or its own in-memory cache), and send a message back to the client with the requested data.
==例如，如果客户端发出 `read()` 请求，客户端文件系统可能会向服务器端文件系统（或者通常称为文件服务器）发送消息以读取特定数据块；文件服务器随后将从磁盘（或其自身的内存缓存）中读取该块，并将包含请求数据的消息发送回客户端。==

The client-side file system will then copy the data into the user buffer supplied to the `read()` system call and thus the request will complete.
==客户端文件系统随后将数据复制到提供给 `read()` 系统调用的用户缓冲区中，从而完成请求。==

Note that a subsequent `read()` of the same block on the client may be cached in client memory or on the client’s disk even; in the best such case, no network traffic need be generated.
==请注意，随后在客户端上对同一块进行的 `read()` 可能会被缓存在客户端内存甚至客户端磁盘中；在最好的情况下，不需要产生任何网络流量。==

Figure 49.2: Distributed File System Architecture
==图 49.2：分布式文件系统架构==

From this simple overview, you should get a sense that there are two important pieces of software in a client/server distributed file system: the client-side file system and the file server.
==通过这个简单的概述，你应该能感觉到客户端/服务器分布式文件系统中有两个重要的软件部分：客户端文件系统和文件服务器。==

Together their behavior determines the behavior of the distributed file system.
==它们的行为共同决定了分布式文件系统的行为。==

Now it’s time to study one particular system: Sun’s Network File System (NFS).
==现在是时候研究一个特定的系统了：Sun 的网络文件系统 (NFS)。==

ASIDE: WHY SERVERS CRASH
==旁注：服务器为什么会崩溃==

Before getting into the details of the NFSv2 protocol, you might be wondering: why do servers crash?
==在深入探讨 NFSv2 协议的细节之前，你可能会想：服务器为什么会崩溃？==

Well, as you might guess, there are plenty of reasons.
==嗯，正如你所料，原因有很多。==

Servers may simply suffer from a power outage (temporarily); only when power is restored can the machines be restarted.
==服务器可能只是遭受了（暂时的）停电；只有当电力恢复后，机器才能重新启动。==

Servers are often comprised of hundreds of thousands or even millions of lines of code; thus, they have bugs (even good software has a few bugs per hundred or thousand lines of code), and thus they eventually will trigger a bug that will cause them to crash.
==服务器通常由数十万甚至数百万行代码组成；因此，它们存在漏洞（即使是好的软件，每百行或千行代码中也会有一些漏洞），因此它们最终会触发一个导致崩溃的漏洞。==

They also have memory leaks; even a small memory leak will cause a system to run out of memory and crash.
==它们还存在内存泄漏；即使是一个很小的内存泄漏也会导致系统耗尽内存并崩溃。==

And, finally, in distributed systems, there is a network between the client and the server; if the network acts strangely (for example, if it becomes partitioned and clients and servers are working but cannot communicate), it may appear as if a remote machine has crashed, but in reality it is just not currently reachable through the network.
==最后，在分布式系统中，客户端和服务器之间存在网络；如果网络表现异常（例如，如果它发生了分区，客户端和服务器都在运行但无法通信），看起来就像远程机器崩溃了，但实际上它只是目前无法通过网络到达。==

49.2 On To NFS
==49.2 转向 NFS==

One of the earliest and quite successful distributed systems was developed by Sun Microsystems, and is known as the Sun Network File System (or NFS) [S86].
==最早且相当成功的分布式系统之一是由 Sun Microsystems 开发的，被称为 Sun 网络文件系统（或 NFS）[S86]。==

In defining NFS, Sun took an unusual approach: instead of building a proprietary and closed system, Sun instead developed an **open protocol** which simply specified the exact message formats that clients and servers would use to communicate.
==在定义 NFS 时，Sun 采取了一种不同寻常的方法：Sun 没有构建专有且封闭的系统，而是开发了一种**开放协议**，该协议只是指定了客户端和服务器用于通信的确切消息格式。==

Different groups could develop their own NFS servers and thus compete in an NFS marketplace while preserving interoperability.
==不同的团体可以开发他们自己的 NFS 服务器，从而在保持互操作性的同时在 NFS 市场中竞争。==

It worked: today there are many companies that sell NFS servers (including Oracle/Sun, NetApp [HLM94], EMC, IBM, and others), and the widespread success of NFS is likely attributed to this “open market” approach.
==它奏效了：今天有许多公司销售 NFS 服务器（包括 Oracle/Sun、NetApp [HLM94]、EMC、IBM 等），NFS 的广泛成功可能归功于这种“开放市场”的方法。==

49.3 Focus: Simple And Fast Server Crash Recovery
==49.3 焦点：简单快速的服务器崩溃恢复==

In this chapter, we will discuss the classic NFS protocol (version 2, a.k.a. NFSv2), which was the standard for many years; small changes were made in moving to NFSv3, and larger-scale protocol changes were made in moving to NFSv4.
==在本章中，我们将讨论经典的 NFS 协议（第 2 版，又名 NFSv2），它曾是多年的标准；在转向 NFSv3 时做了微小的改动，在转向 NFSv4 时进行了更大规模的协议更改。==

However, NFSv2 is both wonderful and frustrating and thus serves as our focus.
==然而，NFSv2 既美妙又令人沮丧，因此它是我们的重点。==

In NFSv2, the main goal in the design of the protocol was **simple and fast server crash recovery**.
==在 NFSv2 中，协议设计的主要目标是**简单快速的服务器崩溃恢复**。==

In a multiple-client, single-server environment, this goal makes a great deal of sense; any minute that the server is down (or unavailable) makes all the client machines (and their users) unhappy and unproductive.
==在多客户端、单服务器的环境中，这个目标非常有意义；服务器宕机（或不可用）的每一分钟都会让所有客户端机器（及其用户）感到不悦且生产力下降。==

Thus, as the server goes, so goes the entire system.
==因此，服务器一旦出故障，整个系统也就瘫痪了。==

49.4 Key To Fast Crash Recovery: Statelessness
==49.4 快速崩溃恢复的关键：无状态性==

This simple goal is realized in NFSv2 by designing what we refer to as a **stateless** protocol.
==在 NFSv2 中，通过设计我们所谓的**无状态**协议来实现这一简单目标。==

The server, by design, does not keep track of anything about what is happening at each client.
==根据设计，服务器不记录每个客户端正在发生的任何事情。==

For example, the server does not know which clients are caching which blocks, or which files are currently open at each client, or the current file pointer position for a file, etc.
==例如，服务器不知道哪些客户端正在缓存哪些数据块，或者每个客户端当前打开了哪些文件，或者文件的当前文件指针位置等。==

Simply put, the server does not track anything about what clients are doing; rather, the protocol is designed to deliver in each protocol request **all the information** that is needed in order to complete the request.
==简单来说，服务器不跟踪客户端正在做的任何事情；相反，协议被设计为在每个协议请求中传递完成该请求所需的**所有信息**。==

If it doesn’t now, this stateless approach will make more sense as we discuss the protocol in more detail below.
==如果现在还不明白，随着我们在下面更详细地讨论该协议，这种无状态方法将变得更有意义。==

For an example of a **stateful** (not stateless) protocol, consider the `open()` system call.
==作为一个**有状态**（非无状态）协议的例子，考虑 `open()` 系统调用。==

Given a pathname, `open()` returns a file descriptor (an integer).
==给定一个路径名，`open()` 返回一个文件描述符（一个整数）。==

This descriptor is used on subsequent `read()` or `write()` requests to access various file blocks, as in this application code:
==该描述符用于随后的 `read()` 或 `write()` 请求以访问各种文件块，如以下应用程序代码所示：==

```c
char buffer[MAX];
int fd = open("foo", O_RDONLY); // get descriptor "fd"
read(fd, buffer, MAX); // read MAX from foo via "fd"
read(fd, buffer, MAX); // read MAX again
...
read(fd, buffer, MAX); // read MAX again
close(fd); // close file
```
Figure 49.3: Client Code: Reading From A File
==图 49.3：客户端代码：从文件中读取==

Now imagine that the client-side file system opens the file by sending a protocol message to the server saying “open the file ’foo’ and give me back a descriptor”.
==现在想象一下，客户端文件系统通过向服务器发送协议消息来打开文件，消息内容为“打开文件 'foo' 并返回给我一个描述符”。==

The file server then opens the file locally on its side and sends the descriptor back to the client.
==文件服务器随后在其本地打开文件，并将描述符发送回客户端。==

On subsequent reads, the client application uses that descriptor to call the `read()` system call; the client-side file system then passes the descriptor in a message to the file server, saying “read some bytes from the file that is referred to by the descriptor I am passing you here”.
==在随后的读取中，客户端应用程序使用该描述符来调用 `read()` 系统调用；客户端文件系统随后在发给文件服务器的消息中传递该描述符，说“从我这里传递给你的描述符所引用的文件中读取一些字节”。==

In this example, the file descriptor is a piece of **shared state** between the client and the server.
==在这个例子中，文件描述符是客户端和服务器之间的一块**共享状态**。==

Shared state, as we hinted above, complicates crash recovery.
==正如我们上面暗示的，共享状态使崩溃恢复变得复杂。==

Imagine the server crashes after the first read completes, but before the client has issued the second one.
==假设服务器在第一次读取完成后、客户端发出第二次读取之前崩溃。==

After the server is up and running again, the client then issues the second read.
==服务器重新启动并运行后，客户端随后发出第二次读取。==

Unfortunately, the server has no idea to which file `fd` is referring; that information was ephemeral (i.e., in memory) and thus lost when the server crashed.
==不幸的是，服务器不知道 `fd` 指的是哪个文件；该信息是瞬态的（即在内存中），因此在服务器崩溃时丢失了。==

To handle this situation, the client and server would have to engage in some kind of **recovery protocol**, where the client would make sure to keep enough information around in its memory to be able to tell the server what it needs to know (in this case, that file descriptor `fd` refers to file `foo`).
==为了处理这种情况，客户端和服务器必须参与某种**恢复协议**，在这种协议中，客户端将确保在其内存中保留足够的信息，以便能够告诉服务器它需要知道的信息（在这种情况下，文件描述符 `fd` 指向文件 `foo`）。==

It gets even worse when you consider the fact that a stateful server has to deal with client crashes.
==当你考虑到有状态服务器必须处理客户端崩溃这一事实时，情况会变得更糟。==

Imagine, for example, a client that opens a file and then crashes.
==例如，想象一个打开文件然后崩溃的客户端。==

The `open()` uses up a file descriptor on the server; how can the server know it is OK to close a given file?
==`open()` 消耗了服务器上的一个文件描述符；服务器如何知道何时可以关闭给定的文件？==

In normal operation, a client would eventually call `close()` and thus inform the server that the file should be closed.
==在正常操作中，客户端最终会调用 `close()`，从而通知服务器应该关闭该文件。==

However, when a client crashes, the server never receives a `close()`, and thus has to notice the client has crashed in order to close the file.
==然而，当客户端崩溃时，服务器永远不会收到 `close()`，因此必须察觉到客户端已崩溃以便关闭文件。==

For these reasons, the designers of NFS decided to pursue a stateless approach: each client operation contains all the information needed to complete the request.
==由于这些原因，NFS 的设计者决定追求一种无状态的方法：每个客户端操作都包含完成请求所需的所有信息。==

No fancy crash recovery is needed; the server just starts running again, and a client, at worst, might have to retry a request.
==不需要复杂的崩溃恢复；服务器只需重新开始运行，而客户端在最坏的情况下可能只需重试请求。==

49.5 The NFSv2 Protocol
==49.5 NFSv2 协议==

We thus arrive at the NFSv2 protocol definition.
==我们因此得出了 NFSv2 协议的定义。==

THE CRUX: HOW TO DEFINE A STATELESS FILE PROTOCOL
==核心问题：如何定义无状态文件协议==

How can we define the network protocol to enable stateless operation?
==我们如何定义网络协议以实现无状态操作？==

Clearly, stateful calls like `open()` can’t be a part of the discussion (as it would require the server to track open files); however, the client application will want to call `open()`, `read()`, `write()`, `close()` and other standard API calls to access files and directories.
==显然，像 `open()` 这样的有状态调用不能成为讨论的一部分（因为它需要服务器跟踪打开的文件）；然而，客户端应用程序将希望调用 `open()`、`read()`、`write()`、`close()` 和其他标准 API 调用来访问文件和目录。==

Thus, as a refined question, how do we define the protocol to both be stateless and support the POSIX file system API?
==因此，作为一个更具体的问题，我们如何定义既是无状态的又能支持 POSIX 文件系统 API 的协议？==

One key to understanding the design of the NFS protocol is understanding the **file handle**.
==理解 NFS 协议设计的关键之一是理解**文件句柄**。==

File handles are used to uniquely describe the file or directory a particular operation is going to operate upon; thus, many of the protocol requests include a file handle.
==文件句柄用于唯一描述特定操作将要操作的文件或目录；因此，许多协议请求都包含一个文件句柄。==

You can think of a file handle as having three important components: a **volume identifier**, an **inode number**, and a **generation number**; together, these three items comprise a unique identifier for a file or directory that a client wishes to access.
==你可以认为文件句柄包含三个重要组成部分：**卷标识符**、**inode 编号**和**生成号**；这三项共同构成了客户端希望访问的文件或目录的唯一标识符。==

The volume identifier informs the server which file system the request refers to (an NFS server can export more than one file system); the inode number tells the server which file within that partition the request is accessing.
==卷标识符通知服务器请求引用的是哪个文件系统（一个 NFS 服务器可以导出多个文件系统）；inode 编号告诉服务器该请求正在访问该分区中的哪个文件。==

Finally, the generation number is needed when reusing an inode number; by incrementing it whenever an inode number is reused, the server ensures that a client with an old file handle can’t accidentally access the newly-allocated file.
==最后，在重用 inode 编号时需要生成号；通过在每次重用 inode 编号时递增它，服务器可以确保持有旧文件句柄的客户端不会意外访问新分配的文件。==

Here is a summary of some of the important pieces of the protocol:
==以下是该协议的一些重要部分的摘要：==

`NFSPROC_GETATTR` file handle
==`NFSPROC_GETATTR` 文件句柄==
returns: attributes
==返回：属性==

`NFSPROC_SETATTR` file handle, attributes
==`NFSPROC_SETATTR` 文件句柄，属性==
returns: attributes
==返回：属性==

`NFSPROC_LOOKUP` directory file handle, name of file/dir to look up
==`NFSPROC_LOOKUP` 目录文件句柄，要查找的文件/目录名称==
returns: file handle, attributes
==返回：文件句柄，属性==

`NFSPROC_READ` file handle, offset, count
==`NFSPROC_READ` 文件句柄，偏移量，计数==
returns: data, attributes
==返回：数据，属性==

`NFSPROC_WRITE` file handle, offset, count, data
==`NFSPROC_WRITE` 文件句柄，偏移量，计数，数据==
returns: attributes
==返回：属性==

`NFSPROC_CREATE` directory file handle, name of file, attributes
==`NFSPROC_CREATE` 目录文件句柄，文件名，属性==
returns: file handle, attributes
==返回：文件句柄，属性==

`NFSPROC_REMOVE` directory file handle, name of file to be removed
==`NFSPROC_REMOVE` 目录文件句柄，要删除的文件名==
returns: –
==返回：–==

`NFSPROC_MKDIR` directory file handle, name of directory, attributes
==`NFSPROC_MKDIR` 目录文件句柄，目录名，属性==
returns: file handle, attributes
==返回：文件句柄，属性==

`NFSPROC_RMDIR` directory file handle, name of directory to be removed
==`NFSPROC_RMDIR` 目录文件句柄，要删除的目录名==
returns: –
==返回：–==

`NFSPROC_READDIR` directory handle, count of bytes to read, cookie
==`NFSPROC_READDIR` 目录句柄，要读取的字节数，cookie==
returns: directory entries, cookie (to get more entries)
==返回：目录条目，cookie（用于获取更多条目）==

Figure 49.4: The NFS Protocol: Examples
==图 49.4：NFS 协议：示例==

We briefly highlight the important components of the protocol.
==我们简要强调该协议的重要组成部分。==

First, the `LOOKUP` protocol message is used to obtain a file handle, which is then subsequently used to access file data.
==首先，`LOOKUP` 协议消息用于获取文件句柄，该句柄随后用于访问文件数据。==

The client passes a directory file handle and name of a file to look up, and the handle to that file (or directory) plus its attributes are passed back to the client from the server.
==客户端传递一个目录文件句柄和要查找的文件名，服务器将该文件（或目录）的句柄及其属性传回给客户端。==

For example, assume the client already has a directory file handle for the root directory of a file system (/) (indeed, this would be obtained through the NFS mount protocol).
==例如，假设客户端已经拥有文件系统根目录 (/) 的目录文件句柄（实际上，这是通过 NFS 挂载协议获取的）。==

If an application running on the client opens the file `/foo.txt`, the client-side file system sends a lookup request to the server, passing it the root file handle and the name `foo.txt`; if successful, the file handle (and attributes) for `foo.txt` will be returned.
==如果运行在客户端上的应用程序打开文件 `/foo.txt`，客户端文件系统会向服务器发送查找请求，并向其传递根文件句柄和文件名 `foo.txt`；如果成功，将返回 `foo.txt` 的文件句柄（和属性）。==

In case you are wondering, attributes are just the metadata that the file system tracks about each file, including fields such as file creation time, last modification time, size, ownership and permissions information, and so forth.
==如果你想知道，属性就是文件系统跟踪的关于每个文件的元数据，包括文件创建时间、最后修改时间、大小、所有权和权限信息等字段。==

Once a file handle is available, the client can issue `READ` and `WRITE` protocol messages on a file to read or write the file, respectively.
==一旦文件句柄可用，客户端就可以对文件发出 `READ` 和 `WRITE` 协议消息，分别用于读取或写入文件。==

The `READ` protocol message requires the protocol to pass along the file handle of the file along with the offset within the file and number of bytes to read.
==`READ` 协议消息要求协议传递文件的文件句柄以及文件内的偏移量和要读取的字节数。==

The server then will be able to issue the read (after all, the handle tells the server which volume and which inode to read from, and the offset and count tells it which bytes of the file to read) and return the data (and up-to-date attributes) to the client.
==服务器随后将能够发出读取指令（毕竟，句柄告诉服务器从哪个卷和哪个 inode 读取，偏移量和计数告诉它读取文件的哪些字节），并将数据（和最新的属性）返回给客户端。==

`WRITE` is handled similarly, except the data is passed from the client to the server, and just a success code (and up-to-date attributes) is returned.
==`WRITE` 的处理方式类似，不同之处在于数据是从客户端传递到服务器的，并且仅返回成功代码（和最新的属性）。==

One last interesting protocol message is the `GETATTR` request; given a file handle, it simply fetches the attributes for that file, including the last modified time of the file.
==最后一个有趣的协议消息是 `GETATTR` 请求；给定一个文件句柄，它只是获取该文件的属性，包括文件的最后修改时间。==

49.6 From Protocol To Distributed File System
==49.6 从协议到分布式文件系统==

Hopefully you are now getting some sense of how this protocol is turned into a file system across the client-side file system and the file server.
==希望你现在已经对如何通过客户端文件系统和文件服务器将此协议转换为文件系统有了一些感觉。==

The client-side file system tracks open files, and generally translates application requests into the relevant set of protocol messages.
==客户端文件系统跟踪打开的文件，并通常将应用程序请求转换为相关的协议消息集。==

The server simply responds to protocol messages, each of which contains all of the information needed to complete the request.
==服务器只是响应协议消息，每条消息都包含完成请求所需的所有信息。==

In the diagram (Figure 49.5), we show what system calls the application makes, and what the client-side file system and file server do in responding to such calls.
==在图表（图 49.5）中，我们展示了应用程序发出的系统调用，以及客户端文件系统和文件服务器在响应此类调用时所做的工作。==

First, notice how the client tracks all relevant **state** for the file access, including the mapping of the integer file descriptor to an NFS file handle as well as the current file pointer.
==首先，注意客户端如何跟踪文件访问的所有相关**状态**，包括整数文件描述符到 NFS 文件句柄的映射以及当前文件指针。==

This enables the client to turn each read request into a properly-formatted read protocol message which tells the server exactly which bytes from the file to read.
==这使得客户端能够将每个读取请求转换为格式正确的读取协议消息，该消息准确地告诉服务器要从文件中读取哪些字节。==

Upon a successful read, the client updates the current file position; subsequent reads are issued with the same file handle but a different offset.
==读取成功后，客户端会更新当前文件位置；随后的读取将使用相同的文件句柄但不同的偏移量发出。==

Second, you may notice where server interactions occur.
==其次，你可能会注意到服务器交互发生在哪里。==

When the file is opened for the first time, the client-side file system sends a `LOOKUP` request message.
==第一次打开文件时，客户端文件系统会发送一条 `LOOKUP` 请求消息。==

Indeed, if a long pathname must be traversed (e.g., `/home/remzi/foo.txt`), the client would send three `LOOKUP`s: one to look up `home` in the directory `/`, one to look up `remzi` in `home`, and finally one to look up `foo.txt` in `remzi`.
==事实上，如果必须遍历一个长路径名（例如 `/home/remzi/foo.txt`），客户端将发送三个 `LOOKUP`：一个在目录 `/` 中查找 `home`，一个在 `home` 中查找 `remzi`，最后在 `remzi` 中查找 `foo.txt`。==

Third, you may notice how each server request has all the information needed to complete the request in its entirety.
==第三，你可能会注意到每个服务器请求都拥有完整完成该请求所需的所有信息。==

This design point is critical to be able to gracefully recover from server failure; it ensures that the server does not need state to be able to respond to the request.
==这个设计点对于能够从服务器故障中优雅地恢复至关重要；它确保服务器不需要状态就能响应请求。==

Figure 49.5: Reading A File: Client-side And File Server Actions
==图 49.5：读取文件：客户端和文件服务器的操作==

TIP: IDEMPOTENCY IS POWERFUL
==提示：幂等性是强大的==

Idempotency is a useful property when building reliable systems.
==幂等性是构建可靠系统时的一个有用属性。==

When an operation can be issued more than once, it is much easier to handle failure of the operation; you can just retry it.
==当一个操作可以发出多次时，处理操作失败就会容易得多；你只需重试即可。==

If an operation is **not** idempotent, life becomes more difficult.
==如果一个操作**不**是幂等的，生活就会变得更加困难。==

49.7 Handling Server Failure With Idempotent Operations
==49.7 使用幂等操作处理服务器故障==

When a client sends a message to the server, it sometimes does not receive a reply.
==当客户端向服务器发送消息时，有时收不到回复。==

There are many possible reasons for this failure to respond.
==这种响应失败可能有许多原因。==

In some cases, the message may be dropped by the network.
==在某些情况下，消息可能会被网络丢弃。==

It is also possible that the server has crashed, and thus is not currently responding to messages.
==服务器也有可能已经崩溃，因此目前无法响应消息。==

In NFSv2, a client handles all of these failures in a single, uniform, and elegant way: it simply **retries** the request.
==在 NFSv2 中，客户端以单一、统一且优雅的方式处理所有这些故障：它只是**重试**请求。==

Specifically, after sending the request, the client sets a timer to go off after a specified time period.
==具体来说，在发送请求后，客户端会设置一个定时器，在指定的时间段后触发。==

If the timer goes off before any reply is received, the client assumes the request has not been processed and resends it.
==如果定时器在收到任何回复之前触发，客户端就会认为请求尚未处理并重新发送。==

The ability of the client to simply retry the request is due to an important property of most NFS requests: they are **idempotent**.
==客户端能够简单地重试请求归功于大多数 NFS 请求的一个重要属性：它们是**幂等**的。==

An operation is called idempotent when the effect of performing the operation multiple times is equivalent to the effect of performing the operation a single time.
==当执行多次操作的效果等同于执行一次操作的效果时，该操作被称为幂等操作。==

For example, “store value to memory” is an idempotent operation.
==例如，“将值存储到内存”是一个幂等操作。==

If, however, you increment a counter three times, it results in a different amount than doing so just once; thus, “increment counter” is not idempotent.
==然而，如果你将计数器增加三次，其结果与仅增加一次不同；因此，“增加计数器”不是幂等的。==

LOOKUP and READ requests are trivially idempotent, as they only read information from the file server and do not update it.
==LOOKUP 和 READ 请求显然是幂等的，因为它们只从文件服务器读取信息而不更新它。==

More interestingly, WRITE requests are also idempotent.
==更有趣的是，WRITE 请求也是幂等的。==

The WRITE message contains the data, the count, and (importantly) the exact offset to write the data to.
==WRITE 消息包含数据、计数以及（重要的）要写入数据的确切偏移量。==

Thus, it can be repeated with the knowledge that the outcome of multiple writes is the same as the outcome of a single one.
==因此，它可以重复进行，因为知道多次写入的结果与单次写入的结果相同。==

Figure 49.6: The Three Types Of Loss
==图 49.6：三种类型的丢失==

Case 1: Request Lost
==案例 1：请求丢失==

Case 2: Server Down
==案例 2：服务器宕机==

Case 3: Reply lost on way back from Server
==案例 3：回复在从服务器返回的途中丢失==

In this way, the client can handle all timeouts in a unified way.
==通过这种方式，客户端可以统一处理所有超时。==

A small aside: some operations are hard to make idempotent.
==一个小旁注：某些操作很难做到幂等。==

For example, when you try to make a directory that already exists, you are informed that the `mkdir` request has failed.
==例如，当你尝试创建一个已经存在的目录时，你会收到 `mkdir` 请求失败的通知。==

Thus, in NFS, if the file server receives a `MKDIR` protocol message and executes it successfully but the reply is lost, the client may repeat it and encounter that failure when in fact the operation at first succeeded.
==因此，在 NFS 中，如果文件服务器收到 `MKDIR` 协议消息并成功执行但回复丢失了，客户端可能会重复该消息并遇到失败，而实际上该操作最初是成功的。==

TIP: PERFECT IS THE ENEMY OF THE GOOD (VOLTAIRE’S LAW)
==提示：完美是优秀的敌人（伏尔泰法则）==

Even when you design a beautiful system, sometimes all the corner cases don’t work out exactly as you might like.
==即使当你设计了一个精美的系统时，有时并非所有的边角案例都能完全如你所愿地解决。==

Take the `mkdir` example above; one could redesign `mkdir` to have different semantics, thus making it idempotent.
==以上面的 `mkdir` 为例；人们可以重新设计 `mkdir` 以具有不同的语义，从而使其具有幂等性。==

The NFS design philosophy covers most of the important cases, and overall makes the system design clean and simple with regards to failure.
==NFS 的设计哲学涵盖了大多数重要案例，并且总体上使系统在处理故障方面保持简洁。==

49.8 Improving Performance: Client-side Caching
==49.8 提高性能：客户端缓存==

Distributed file systems are good for a number of reasons, but sending all read and write requests across the network can lead to a big performance problem.
==分布式文件系统由于许多原因而表现良好，但通过网络发送所有读写请求可能会导致严重的性能问题。==

The answer is **client-side caching**.
==答案是**客户端缓存**。==

The NFS client-side file system caches file data (and metadata) that it has read from the server in client memory.
==NFS 客户端文件系统将从服务器读取的文件数据（和元数据）缓存在客户端内存中。==

The cache also serves as a temporary buffer for writes.
==缓存还充当写入的临时缓冲区。==

Such **write buffering** is useful because it decouples application `write()` latency from actual write performance.
==这种**写缓冲**很有用，因为它将应用程序 `write()` 的延迟与实际的写入性能解耦。==

Adding caching into any sort of system with multiple client caches introduces a big and interesting challenge which we will refer to as the **cache consistency problem**.
==在任何具有多个客户端缓存的系统中添加缓存都会引入一个巨大且有趣的挑战，我们将其称为**缓存一致性问题**。==

49.9 The Cache Consistency Problem
==49.9 缓存一致性问题==

The cache consistency problem is best illustrated with three clients and a single server.
==缓存一致性问题最好通过三个客户端和一个服务器来阐述。==

Figure 49.7: The Cache Consistency Problem
==图 49.7：缓存一致性问题==

There are two subproblems.
==有两个子问题。==

The first subproblem is **update visibility**; when do updates from one client become visible at other clients?
==第一个子问题是**更新可见性**；一个客户端的更新何时对其他客户端可见？==

The second subproblem of cache consistency is a **stale cache**; in this case, C1 still has an old version of a file in its cache.
==缓存一致性的第二个子问题是**陈旧缓存**；在这种情况下，C1 的缓存中仍保留文件的旧版本。==

NFSv2 implementations solve these cache consistency problems in two ways.
==NFSv2 的实现通过两种方式解决这些缓存一致性问题。==

First, to address update visibility, clients implement what is sometimes called **flush-on-close** (a.k.a., **close-to-open**) consistency semantics.
==首先，为了解决更新可见性，客户端实现了有时被称为**关闭时刷新**（又称**从关闭到打开**）的一致性语义。==

Specifically, when a file is written to and subsequently closed by a client application, the client flushes all updates to the server.
==具体来说，当文件被写入并随后由客户端应用程序关闭时，客户端会将所有更新刷新到服务器。==

Second, to address the stale-cache problem, NFSv2 clients first check to see whether a file has changed before using its cached contents.
==其次，为了解决陈旧缓存问题，NFSv2 客户端在使用其缓存内容之前，首先检查文件是否已更改。==

Specifically, before using a cached block, the client-side file system will issue a `GETATTR` request to the server to fetch the file’s attributes.
==具体来说，在应用缓存块之前，客户端文件系统将向服务器发出 `GETATTR` 请求以获取文件的属性。==

If the time-of-modification is more recent than the time that the file was fetched into the client cache, the client **invalidates** the file, thus removing it from the client cache.
==如果修改时间晚于文件被获取到客户端缓存的时间，客户端将**使该文件失效**，从而将其从客户端缓存中移除。==

When the original team at Sun implemented this solution, they realized a new problem; suddenly, the NFS server was flooded with `GETATTR` requests.
==当 Sun 的原团队实现这个解决方案时，他们意识到了一个新问题；突然之间，NFS 服务器充斥着大量的 `GETATTR` 请求。==

To remedy this situation, an **attribute cache** was added to each client.
==为了补救这种情况，每个客户端都添加了一个**属性缓存**。==

The attributes for a particular file were placed in the cache when the file was first accessed, and then would timeout after a certain amount of time (say 3 seconds).
==特定文件的属性在文件首次被访问时放入缓存，然后在一段时间（例如 3 秒）后超时。==

49.11 Implications On Server-Side Write Buffering
==49.11 对服务器端写缓冲的影响==

An NFS server absolutely may **not** return success on a `WRITE` protocol request until the write has been forced to stable storage.
==在写入被强制保存到稳定存储之前，NFS 服务器绝对**不能**对 `WRITE` 协议请求返回成功。==

Yikes! Because the server told the client that the second write was successful before committing it to disk, an old chunk is left in the file, which might be catastrophic.
==哎呀！因为服务器在将第二次写入提交到磁盘之前就告诉客户端写入成功，导致文件中留下了一个旧块，这可能是灾难性的。==

To avoid this problem, NFS servers must commit each write to stable storage before informing the client of success.
==为了避免这个问题，NFS 服务器必须在通知客户端成功之前将每次写入提交到稳定存储。==

ASIDE: INNOVATION BREEDS INNOVATION
==旁注：创新孕育创新==

Probably the most lasting innovation is the **Virtual File System (VFS) / Virtual Node (vnode)** interface.
==也许最持久的创新是**虚拟文件系统 (VFS) / 虚拟节点 (vnode)** 接口。==

The VFS layer includes operations that are done to an entire file system, such as mounting and unmounting.
==VFS 层包括对整个文件系统执行的操作，例如挂载和卸载。==

The vnode layer consists of all operations one can perform on a file, such as `open`, `close`, `read`, `write`.
==vnode 层由可以在文件上执行的所有操作组成，例如 `open`、`close`、`read`、`write`。==

49.12 Summary
==49.12 总结==

NFS is centered around the idea of simple and fast recovery in the face of server failure.
==NFS 的核心思想是在面对服务器故障时实现简单快速的恢复。==

ASIDE: KEY NFS TERMS
==旁注：NFS 关键术语==

• The key to realizing fast and simple crash recovery in NFS is in the design of a **stateless** protocol.
==• 在 NFS 中实现快速简单崩溃恢复的关键在于**无状态**协议的设计。==

• Making requests **idempotent** is a central aspect of the NFS protocol.
==• 使请求具有**幂等性**是 NFS 协议的一个核心方面。==

• Performance concerns dictate the need for client-side **caching** and **write buffering**, but introduces a **cache consistency problem**.
==• 性能考量决定了对客户端**缓存**和**写缓冲**的需求，但这引入了**缓存一致性问题**。==

• NFS implementations provide an engineering solution to cache consistency through a **flush-on-close** approach and an **attribute cache**.
==• NFS 实现通过**关闭时刷新**方法和**属性缓存**为缓存一致性提供了工程解决方案。==

• NFS servers must commit writes to persistent media before returning success.
==• NFS 服务器必须在返回成功之前将写入提交到持久介质。==

• Sun introduced the **VFS/Vnode** interface, enabling multiple file system implementations to coexist.
==• Sun 引入了 **VFS/Vnode** 接口，使多个文件系统实现能够并存。==

50 The Andrew File System (AFS)
==50 Andrew 文件系统 (AFS)==

The Andrew File System was introduced at Carnegie-Mellon University (CMU) in the 1980’s [H+88].
==Andrew 文件系统于 20 世纪 80 年代在卡内基梅隆大学 (CMU) 推出 [H+88]。==

The main goal of this project was simple: **scale**.
==该项目的主要目标很简单：**可扩展性**。==

In NFS, the protocol forces clients to check with the server periodically; frequent checks like this will limit the number of clients a server can respond to.
==在 NFS 中，协议强制客户端定期向服务器检查；像这样频繁的检查会限制服务器能够响应的客户端数量。==

AFS also differs from NFS in that cache consistency is simple and readily understood: when the file is opened, a client will generally receive the latest consistent copy from the server.
==AFS 与 NFS 的不同之处还在于，其缓存一致性简单且易于理解：当文件打开时，客户端通常会从服务器接收最新的、一致的副本。==

50.1 AFS Version 1
==50.1 AFS 第 1 版==

One of the basic tenets of all versions of AFS is **whole-file caching** on the **local disk** of the client machine.
==所有 AFS 版本的基本原则之一是在客户端机器的**本地磁盘**上进行**全文件缓存**。==

When you `open()` a file, the entire file is fetched from the server and stored in a file on your local disk.
==当你 `open()` 一个文件时，整个文件会从服务器获取并存储在你本地磁盘的一个文件中。==

Subsequent application `read()` and `write()` operations are redirected to the local file system; thus, these operations require no network communication and are fast.
==随后的应用程序 `read()` 和 `write()` 操作将被重定向到本地文件系统；因此，这些操作不需要网络通信，速度很快。==

Finally, upon `close()`, the file (if it has been modified) is flushed back to the server.
==最后，在 `close()` 时，文件（如果已被修改）将被刷新回服务器。==

When a client application first calls `open()`, the AFS client-side code (which the AFS designers call **Venus**) would send a **Fetch** protocol message to the server.
==当客户端应用程序首次调用 `open()` 时，AFS 客户端代码（AFS 设计者称之为 **Venus**）会向服务器发送一条 **Fetch** 协议消息。==

The file server (the group of which they called **Vice**) would find the desired file, and ship the entire file back to the client.
==文件服务器（他们称该群体为 **Vice**）会找到所需的文件，并将整个文件运回客户端。==

When finished, the AFS client checks if the file has been modified; if so, it flushes the new version back to the server with a **Store** protocol message.
==完成后，AFS 客户端会检查文件是否已被修改；如果是，它将通过 **Store** 协议消息将新版本刷新回服务器。==




**THE ANDREW FILE SYSTEM (AFS)**
==**Andrew 文件系统 (AFS)**==

**TIP: MEASURE THEN BUILD (PATTERSON’S LAW)**
==**提示：先测量再构建（帕特森定律）**==

One of our advisors, David Patterson (of RISC and RAID fame), used to always encourage us to measure a system and demonstrate a problem before building a new system to fix said problem.
==我们的一位顾问，David Patterson（以 RISC 和 RAID 闻名），过去总是鼓励我们在构建新系统来解决问题之前，先对系统进行测量并证明问题的存在。==

By using experimental evidence, rather than gut instinct, you can turn the process of system building into a more scientific endeavor.
==通过使用实验证据而非直觉，你可以将系统构建的过程转变为一项更具科学性的工作。==

Doing so also has the fringe benefit of making you think about how exactly to measure the system before your improved version is developed.
==这样做还有一个额外的好处，那就是让你在开发改进版本之前，思考究竟该如何测量系统。==

When you do finally get around to building the new system, two things are better as a result: first, you have evidence that shows you are solving a real problem; second, you now have a way to measure your new system in place, to show that it actually improves upon the state of the art.
==当你最终着手构建新系统时，结果会有两方面的提升：首先，你有证据表明你正在解决一个真实存在的问题；其次，你现在拥有一套现成的测量方法来评估新系统，以证明它确实改进了现有技术。==

And thus we call this **Patterson’s Law**.
==因此，我们称之为**帕特森定律**。==

**50.2 Problems with Version 1**
==**50.2 版本 1 存在的问题**==

A few key problems with this first version of AFS motivated the designers to rethink their file system.
==AFS 第一个版本中存在的一些关键问题促使设计者重新思考他们的文件系统。==

To study the problems in detail, the designers of AFS spent a great deal of time measuring their existing prototype to find what was wrong.
==为了详细研究这些问题，AFS 的设计者花费了大量时间测量他们现有的原型，以找出症结所在。==

Such experimentation is a good thing, because **measurement** is the key to understanding how systems work and how to improve them; obtaining concrete, good data is thus a necessary part of systems construction.
==这种实验是一件好事，因为**测量**是理解系统如何工作以及如何改进系统的关键；因此，获取具体、有效的数据是系统构建中必不可少的一部分。==

In their study, the authors found two main problems with AFSv1:
==在研究中，作者发现了 AFSv1 的两个主要问题：==

*   **Path-traversal costs are too high**: When performing a Fetch or Store protocol request, the client passes the entire pathname (e.g., `/home/remzi/notes.txt`) to the server.
==*   **路径遍历开销过高**：在执行 Fetch（获取）或 Store（存储）协议请求时，客户端会将整个路径名（例如 `/home/remzi/notes.txt`）传递给服务器。==

The server, in order to access the file, must perform a full pathname traversal, first looking in the root directory to find `home`, then in `home` to find `remzi`, and so forth, all the way down the path until finally the desired file is located.
==服务器为了访问该文件，必须执行完整的路径名遍历，首先在根目录中查找 `home`，然后在 `home` 中查找 `remzi`，以此类推，沿着路径一直向下，直到最终找到目标文件。==

With many clients accessing the server at once, the designers of AFS found that the server was spending much of its CPU time simply walking down directory paths.
==由于有许多客户端同时访问服务器，AFS 的设计者发现服务器的大部分 CPU 时间都花在遍历目录路径上。==

*   **The client issues too many TestAuth protocol messages**: Much like NFS and its overabundance of GETATTR protocol messages, AFSv1 generated a large amount of traffic to check whether a local file (or its stat information) was valid with the TestAuth protocol message.
==*   **客户端发送了过多的 TestAuth 协议消息**：非常像 NFS 及其过量的 GETATTR 协议消息，AFSv1 产生了大量的通信流量，通过 TestAuth 协议消息来检查本地文件（或其状态信息）是否有效。==

Thus, servers spent much of their time telling clients whether it was OK to use their cached copies of a file.
==因此，服务器花费了大量时间告诉客户端是否可以使用其缓存的文件副本。==

Most of the time, the answer was that the file had not changed.
==大多数情况下，答案是文件并未发生更改。==

There were actually two other problems with AFSv1: load was not balanced across servers, and the server used a single distinct process per client thus inducing context switching and other overheads.
==实际上 AFSv1 还存在另外两个问题：服务器之间的负载不均衡，且服务器为每个客户端使用一个独立的进程，从而导致了上下文切换和其他开销。==

The load imbalance problem was solved by introducing **volumes**, which an administrator could move across servers to balance load; the context-switch problem was solved in AFSv2 by building the server with threads instead of processes.
==负载不均衡问题通过引入**卷 (volumes)** 得到了解决，管理员可以跨服务器移动卷以平衡负载；上下文切换问题在 AFSv2 中通过使用线程而非进程构建服务器得到了解决。==

However, for the sake of space, we focus here on the main two protocol problems above that limited the scale of the system.
==然而，由于篇幅限制，我们在这里重点讨论上述限制系统规模的两个主要协议问题。==

**50.3 Improving the Protocol**
==**50.3 改进协议**==

The two problems above limited the scalability of AFS; the server CPU became the bottleneck of the system, and each server could only service 20 clients without becoming overloaded.
==上述两个问题限制了 AFS 的可扩展性；服务器 CPU 成为了系统的瓶块，每台服务器在不超载的情况下只能为 20 个客户端提供服务。==

Servers were receiving too many TestAuth messages, and when they received Fetch or Store messages, were spending too much time traversing the directory hierarchy.
==服务器接收到了太多的 TestAuth 消息，而在接收到 Fetch 或 Store 消息时，又花费了太多时间遍历目录层级。==

Thus, the AFS designers were faced with a problem:
==因此，AFS 的设计者面临着一个问题：==

**THE CRUX: HOW TO DESIGN A SCALABLE FILE PROTOCOL**
==**症结所在：如何设计可扩展的文件协议**==

How should one redesign the protocol to minimize the number of server interactions, i.e., how could they reduce the number of TestAuth messages?
==应该如何重新设计协议以尽量减少服务器交互次数，即如何减少 TestAuth 消息的数量？==

Further, how could they design the protocol to make these server interactions efficient?
==此外，他们如何设计协议使这些服务器交互变得高效？==

By attacking both of these issues, a new protocol would result in a much more scalable version AFS.
==通过解决这两个问题，新协议将产生一个更具扩展性的 AFS 版本。==

**50.4 AFS Version 2**
==**50.4 AFS 版本 2**==

AFSv2 introduced the notion of a **callback** to reduce the number of client/server interactions.
==AFSv2 引入了**回调 (callback)** 的概念，以减少客户端/服务器之间的交互次数。==

A callback is simply a promise from the server to the client that the server will inform the client when a file that the client is caching has been modified.
==回调简单来说就是服务器对客户端的一个承诺，即当客户端正在缓存的文件被修改时，服务器会通知客户端。==

By adding this **state** to the system, the client no longer needs to contact the server to find out if a cached file is still valid.
==通过向系统添加这种**状态**，客户端不再需要联系服务器来了解缓存文件是否仍然有效。==

Rather, it assumes that the file is valid until the server tells it otherwise; notice the analogy to **polling** versus **interrupts**.
==相反，它假设文件是有效的，直到服务器告知其并非如此；请注意这与**轮询 (polling)** 与**中断 (interrupts)** 之间的类比。==

AFSv2 also introduced the notion of a **file identifier (FID)** (similar to the NFS file handle) instead of pathnames to specify which file a client was interested in.
==AFSv2 还引入了**文件标识符 (FID)** 的概念（类似于 NFS 的文件句柄），而不是使用路径名来指定客户端感兴趣的文件。==

An FID in AFS consists of a volume identifier, a file identifier, and a “uniquifier” (to enable reuse of the volume and file IDs when a file is deleted).
==AFS 中的 FID 由卷标识符、文件标识符和一个“唯一化标识 (uniquifier)”组成（以便在文件删除时能够重用卷和文件 ID）。==

Thus, instead of sending whole pathnames to the server and letting the server walk the pathname to find the desired file, the client would walk the pathname, one piece at a time, caching the results and thus hopefully reducing the load on the server.
==因此，客户端不再将整个路径名发送给服务器并让服务器遍历路径名来查找所需文件，而是亲自遍历路径名，一次处理一段，并缓存结果，从而有望减轻服务器的负载。==

For example, if a client accessed the file `/home/remzi/notes.txt`, and `home` was the AFS directory mounted onto `/` (i.e., `/` was the local root directory, but `home` and its children were in AFS), the client would first Fetch the directory contents of `home`, put them in the local-disk cache, and set up a callback on `home`.
==例如，如果客户端访问文件 `/home/remzi/notes.txt`，且 `home` 是挂载到 `/` 上的 AFS 目录（即 `/` 是本地根目录，但 `home` 及其子目录在 AFS 中），客户端会首先 Fetch（获取）`home` 的目录内容，将其放入本地磁盘缓存，并在 `home` 上设置回调。==

Then, the client would Fetch the directory contents of `remzi`, put it in the local-disk cache, and set up a callback on `remzi`.
==然后，客户端会 Fetch `remzi` 的目录内容，将其放入本地磁盘缓存，并在 `remzi` 上设置回调。==

Finally, the client would Fetch `notes.txt`, cache this regular file in the local disk, set up a callback, and finally return a file descriptor to the calling application.
==最后，客户端会 Fetch `notes.txt`，将此普通文件缓存在本地磁盘中，设置回调，并最终向调用程序返回文件句柄。==

See Figure 50.2 for a summary.
==摘要请参见图 50.2。==

The key difference, however, from NFS, is that with each fetch of a directory or file, the AFS client would establish a callback with the server, thus ensuring that the server would notify the client of a change in its cached state.
==然而，与 NFS 的关键区别在于，每当获取目录或文件时，AFS 客户端都会与服务器建立回调，从而确保服务器会在其缓存状态发生变化时通知客户端。==

The benefit is obvious: although the first access to `/home/remzi/notes.txt` generates many client-server messages (as described above), it also establishes callbacks for all the directories as well as the file `notes.txt`, and thus subsequent accesses are entirely local and require no server interaction at all.
==这样做的好处是显而易见的：虽然第一次访问 `/home/remzi/notes.txt` 会产生许多客户端-服务器消息（如上所述），但它也为所有目录以及文件 `notes.txt` 建立了回调，因此后续访问完全是本地的，根本不需要服务器交互。==

Thus, in the common case where a file is cached at the client, AFS behaves nearly identically to a local disk-based file system.
==因此，在文件缓存于客户端的常见情况下，AFS 的表现几乎与本地基于磁盘的文件系统完全一致。==

If one accesses a file more than once, the second access should be just as fast as accessing a file locally.
==如果多次访问同一个文件，第二次访问的速度应该与在本地访问文件一样快。==

**50.5 Cache Consistency**
==**50.5 缓存一致性**==

When we discussed NFS, there were two aspects of cache consistency we considered: **update visibility** and **cache staleness**.
==当我们讨论 NFS 时，我们考虑了缓存一致性的两个方面：**更新可见性**和**缓存陈旧性**。==

With update visibility, the question is: when will the server be updated with a new version of a file?
==对于更新可见性，问题在于：服务器何时会更新文件的新版本？==

With cache staleness, the question is: once the server has a new version, how long before clients see the new version instead of an older cached copy?
==对于缓存陈旧性，问题在于：一旦服务器有了新版本，客户端需要多长时间才能看到新版本而不是旧的缓存副本？==

Because of callbacks and whole-file caching, the cache consistency provided by AFS is easy to describe and understand.
==由于使用了回调和全文件缓存，AFS 提供的缓存一致性很容易描述和理解。==

There are two important cases to consider: consistency between processes on different machines, and consistency between processes on the same machine.
==有两个重要情况需要考虑：不同机器上的进程之间的一致性，以及同一台机器上的进程之间的一致性。==

Between different machines, AFS makes updates visible at the server and invalidates cached copies at the exact same time, which is when the updated file is closed.
==在不同机器之间，AFS 在更新后的文件关闭时，会同时使更新在服务器上可见并使缓存副本失效。==

A client opens a file, and then writes to it (perhaps repeatedly).
==客户端打开一个文件，然后向其中写入内容（可能会重复写入）。==

When it is finally closed, the new file is flushed to the server (and thus visible).
==当文件最终被关闭时，新文件会被刷新到服务器（从而变得可见）。==

At this point, the server then “breaks” callbacks for any clients with cached copies; the break is accomplished by contacting each client and informing it that the callback it has on the file is no longer valid.
==此时，服务器会“打破 (break)”任何拥有缓存副本的客户端的回调；这种打破是通过联系每个客户端并告知其对该文件的回调不再有效来完成的。==

This step ensures that clients will no longer read stale copies of the file; subsequent opens on those clients will require a re-fetch of the new version of the file from the server (and will also serve to reestablish a callback on the new version of the file).
==这一步确保了客户端将不再读取文件的陈旧副本；这些客户端随后进行的打开操作将需要从服务器重新获取文件的新版本（这同时也会在新版本文件上重新建立回调）。==

AFS makes an exception to this simple model between processes on the same machine.
==对于同一台机器上的进程，AFS 对这种简单的模型做了一个例外处理。==

In this case, writes to a file are immediately visible to other local processes (i.e., a process does not have to wait until a file is closed to see its latest updates).
==在这种情况下，对文件的写入对其他本地进程是立即可见的（即进程不必等到文件关闭就能看到其最新的更新）。==

This makes using a single machine behave exactly as you would expect, as this behavior is based upon typical UNIX semantics.
==这使得在单台机器上的使用表现完全符合预期，因为这种行为是基于典型的 UNIX 语义的。==

Only when switching to a different machine would you be able to detect the more general AFS consistency mechanism.
==只有在切换到不同机器时，你才能察觉到更通用的 AFS 一致性机制。==

There is one interesting cross-machine case that is worthy of further discussion.
==有一个有趣的跨机器案例值得进一步讨论。==

Specifically, in the rare case that processes on different machines are modifying a file at the same time, AFS naturally employs what is known as a **last writer wins** approach (which perhaps should be called **last closer wins**).
==具体来说，在极少数情况下，如果不同机器上的进程同时修改同一个文件，AFS 自然会采用所谓的**最后写入者胜 (last writer wins)** 方法（这也许应该被称为**最后关闭者胜 (last closer wins)**）。==

Specifically, whichever client calls `close()` last will update the entire file on the server last and thus will be the “winning” file, i.e., the file that remains on the server for others to see.
==具体而言，无论哪个客户端最后调用 `close()`，它都会最后更新服务器上的整个文件，从而成为“胜出”的文件，即留在服务器上供他人查看的文件。==

The result is a file that was generated in its entirety either by one client or the other.
==结果是产生了一个要么完全由一个客户端生成，要么完全由另一个客户端生成的文件。==

Note the difference from a block-based protocol like NFS: in NFS, writes of individual blocks may be flushed out to the server as each client is updating the file, and thus the final file on the server could end up as a mix of updates from both clients.
==注意这与像 NFS 这样的基于块的协议的区别：在 NFS 中，当每个客户端更新文件时，单个块的写入可能会被刷新到服务器，因此服务器上的最终文件可能会变成两个客户端更新内容的混合体。==

**50.6 Crash Recovery**
==**50.6 崩溃恢复**==

From the description above, you might sense that crash recovery is more involved than with NFS.
==从上面的描述中，你可能会感觉到崩溃恢复比 NFS 更加复杂。==

You would be right.
==你是对的。==

For example, imagine there is a short period of time where a server (S) is not able to contact a client (C1), for example, while the client C1 is rebooting.
==例如，假设有一小段时间服务器 (S) 无法联系到客户端 (C1)，例如当客户端 C1 正在重启时。==

While C1 is not available, S may have tried to send it one or more callback recall messages.
==当 C1 不可用时，S 可能已经尝试向其发送一个或多个回调撤回消息。==

Because C1 may miss those critical messages when it is rebooting, upon rejoining the system, C1 should treat all of its cache contents as suspect.
==由于 C1 在重启时可能会错过这些关键消息，因此在重新加入系统后，C1 应将其所有缓存内容视为可疑。==

Thus, upon the next access to file F, C1 should first ask the server (with a TestAuth protocol message) whether its cached copy of file F is still valid.
==因此，在下一次访问文件 F 时，C1 应首先向服务器询问（通过 TestAuth 协议消息）其缓存的文件 F 副本是否仍然有效。==

Server recovery after a crash is also more complicated.
==服务器在崩溃后的恢复也更加复杂。==

The problem that arises is that callbacks are kept in memory; thus, when a server reboots, it has no idea which client machine has which files.
==出现的问题是回调保存在内存中；因此，当服务器重启时，它不知道哪台客户端机器拥有哪些文件。==

Thus, upon server restart, each client of the server must realize that the server has crashed and treat all of their cache contents as suspect, and (as above) reestablish the validity of a file before using it.
==因此，在服务器重启时，服务器的每个客户端必须意识到服务器已经崩溃，并将其所有缓存内容视为可疑，并且（如上所述）在利用文件之前重新确立其有效性。==

**50.7 Scale And Performance Of AFSv2**
==**50.7 AFSv2 的规模与性能**==

With the new protocol in place, AFSv2 was measured and found to be much more scalable than the original version.
==随着新协议的实施，经测量发现 AFSv2 比原始版本具有更强的扩展性。==

Indeed, each server could support about 50 clients (instead of just 20).
==事实上，每台服务器可以支持大约 50 个客户端（而不仅仅是 20 个）。==

Let us also gain some perspective on AFS performance by comparing common file-system access scenarios with NFS.
==让我们通过将常见的文件系统访问场景与 NFS 进行比较，来进一步了解 AFS 的性能。==

Figure 50.4 (page 9) shows the results of our qualitative comparison.
==图 50.4（第 9 页）展示了我们定性比较的结果。==

We assume, for the sake of analysis, that an access across the network to the remote server for a file block takes $L_{net}$ time units.
==为了便于分析，我们假设跨网络访问远程服务器获取一个文件块需要 $L_{net}$ 个时间单位。==

Access to local memory takes $L_{mem}$, and access to local disk takes $L_{disk}$.
==访问本地内存需要 $L_{mem}$，访问本地磁盘需要 $L_{disk}$。==

The general assumption is that $L_{net} > L_{disk} > L_{mem}$.
==一般假设是 $L_{net} > L_{disk} > L_{mem}$。==

Second, an interesting difference arises during a large-file sequential re-read (Workload 6).
==其次，在大型文件顺序重新读取（工作负载 6）期间会出现有趣的差异。==

Because AFS has a large local disk cache, it will access the file from there when the file is accessed again.
==由于 AFS 拥有巨大的本地磁盘缓存，当再次访问该文件时，它将从那里访问该文件。==

NFS, in contrast, only can cache blocks in client memory; as a result, if a large file (i.e., a file bigger than local memory) is re-read, the NFS client will have to re-fetch the entire file from the remote server.
==相比之下，NFS 只能在客户端内存中缓存数据块；结果是，如果重新读取一个大文件（即大于本地内存的文件），NFS 客户端将不得不从远程服务器重新获取整个文件。==

Thus, AFS is faster than NFS in this case by a factor of $L_{net} / L_{disk}$, assuming that remote access is indeed slower than local disk.
==因此，在这种情况下，假设远程访问确实比本地磁盘慢，那么 AFS 比 NFS 快了 $L_{net} / L_{disk}$ 倍。==

**50.8 AFS: Other Improvements**
==**50.8 AFS：其他改进**==

AFS provides a true **global namespace** to clients, thus ensuring that all files were named the same way on all client machines.
==AFS 为客户端提供了一个真正的**全局命名空间**，从而确保所有文件在所有客户端机器上的命名方式都相同。==

NFS, in contrast, allows each client to mount NFS servers in any way that they please.
==相比之下，NFS 允许每个客户端以任何他们喜欢的方式挂载 NFS 服务器。==

AFS also takes security seriously, and incorporates mechanisms to authenticate users.
==AFS 还非常重视安全性，并加入了用户身份验证机制。==

AFS also includes facilities for flexible user-managed access control.
==AFS 还包括灵活的、用户管理的访问控制设施。==

**50.9 Summary**
==**50.9 总结**==

AFS shows us how distributed file systems can be built quite differently than what we saw with NFS.
==AFS 向我们展示了分布式文件系统的构建方式可以与我们在 NFS 中看到的截然不同。==

The protocol design of AFS is particularly important; by minimizing server interactions (through whole-file caching and callbacks), each server can support many clients.
==AFS 的协议设计尤为重要；通过（利用全文件缓存和回调）尽量减少服务器交互，每台服务器可以支持许多客户端。==

Perhaps unfortunately, AFS is likely on the decline.
==或许令人遗憾的是，AFS 可能正在走向衰落。==

Because NFS became an open standard, many different vendors supported it.
==因为 NFS 成为了开放标准，许多不同的供应商都支持它。==

**52 A Dialogue on Security**
==**52 关于安全的对话**==

**Professor**: Hello again, student!
==**教授**：你好，同学！==

**Student**: I thought we were done with all this. Will I never be done with this class?
==**学生**：我以为我们已经学完这些了。我永远也上不完这门课了吗？==

**Professor**: That depends on who I am. Some professors want to talk about security and some don’t. Unfortunately for you, given that you’re here, I’m one of those who want to.
==**教授**：这取决于我是谁。有些教授想谈谈安全，有些则不想。对你来说不幸的是，既然你在这儿，我就是那种想谈安全的人。==

**Student**: OK, I suppose we’d better just get on with it.
==**学生**：好吧，我想我们最好直接开始吧。==

**Professor**: That’s the spirit! Let’s say you have a **peach**...
==**教授**：这就对了！假设你有一个**桃子**……==

**Student**: You told me we were at least done with peaches!
==**学生**：你跟我说过我们至少已经告别桃子了！==

**Professor**: When one is discussing security, lies will always be a part of the discussion.
==**教授**：在讨论安全问题时，谎言总是讨论的一部分。==

**Professor**: Anyway, you’ve got a peach. You certainly wouldn’t want to turn around and find someone had stolen your peach, would you?
==**教授**：总之，你有个桃子。你肯定不想一转身发现有人偷了你的桃子，对吧？==

**Student**: Well, I suppose not.
==**学生**：嗯，我想是不想。==

**Professor**: And you probably wouldn’t be any happier if you turned around and discovered someone had swapped out your peach for a **turnip**, either, would you?
==**教授**：如果你一转身发现有人把你的桃子换成了**萝卜**，你大概也不会更高兴，对吧？==

**Student**: I guess not.
==**学生**：我想也是。==

**Professor**: And you also wouldn’t want somebody slapping your hand away every time you reached for your peach, right?
==**教授**：你也不希望每次你去拿桃子时都有人扇开你的手，对吧？==

**Student**: No, that would be pretty rude.
==**学生**：对，那太粗鲁了。==

**Professor**: You wouldn’t want that happening to any of the resources your computer controls, either.
==**教授**：你也不希望这种事发生在计算机控制的任何资源上。==

**Professor**: How can you ensure that secrets remain **confidential**?
==**教授**：你如何确保秘密保持**机密**？==

**Professor**: How can you guarantee the **integrity** of your important data?
==**教授**：你如何保证重要数据的**完整性**？==

**Professor**: How can you ensure that you can use your computer resources when you want to? (**availability**)
==**教授**：你如何确保在你想使用计算机资源时就能使用？（**可用性**）==

**Student**: All this sounds a little like reliability stuff we talked about before...
==**学生**：这听起来有点像我们之前讨论过的可靠性方面的东西……==

**Professor**: Yes and no. Reliability is about accidents. But we’re going a step further. **SOMEBODY WANTS YOUR PEACH!!!!**
==**教授**：是也不是。可靠性是关于意外的。但我们要更进一步。**有人想要你的桃子！！！！**==

**Professor**: When we talk about security, we’re talking about genuine **adversaries**, human adversaries who are trying to make things go wrong for you.
==**教授**：当我们谈论安全时，我们谈论的是真正的**对手**，即试图让你出事的各路人类对手。==

**Professor**: They’re likely to be clever, malevolent, persistent, flexible, and sneaky.
==**教授**：他们很可能聪明、恶毒、固执、灵活且阴险。==

**Student**: This sounds challenging.
==**学生**：这听起来很有挑战性。==

**Professor**: You have no idea... But you will! **YOU WILL!!** (maniacal laughter)
==**教授**：你根本想象不到……但你会明白的！**你会明白的！！**（狂笑）==

**53 Introduction to Operating System Security**
==**53 操作系统安全简介**==

**53.1 Introduction**
==**53.1 引言**==

Security of computing systems is a vital topic whose importance only keeps increasing.
==计算系统的安全是一个至关重要的课题，其重要性在不断增加。==

Operating systems are particularly important from a security perspective. Why?
==从安全角度来看，操作系统尤为重要。为什么？==

To begin with, pretty much everything runs on top of an operating system.
==首先，几乎所有东西都运行在操作系统之上。==

If the software you are running on top of is insecure, what’s above it is going to also be insecure. It’s like building a house on sand.
==如果你运行在其上的软件是不安全的，那么它上面的东西也将是不安全的。这就像在沙地上盖房子。==

Another reason that operating system security is so important is that ultimately all of our software relies on proper behavior of the underlying hardware.
==操作系统安全如此重要的另一个原因是，最终我们所有的软件都依赖于底层硬件的正确行为。==

What has ultimate control of those hardware resources? The operating system.
==谁拥有这些硬件资源的最终控制权？操作系统。==

**CRUX: HOW TO SECURE OS RESOURCES**
==**症结所在：如何保护操作系统资源**==

In the face of multiple possibly concurrent and interacting processes running on the same machine, how can we ensure that the resources each process is permitted to access are exactly those it should access, in exactly the ways we desire?
==面对在同一台机器上运行的多个可能并发且相互交互的进程，我们如何确保每个进程获准访问的资源恰好是它应该访问的，且访问方式完全符合我们的意愿？==

**53.2 What Are We Protecting?**
==**53.2 我们在保护什么？**==

A typical commodity operating system has complete control of all (or almost all) hardware on the machine.
==一个典型的商品操作系统对机器上的所有（或几乎所有）硬件拥有完全的控制权。==

As a result, among the things the OS can do are:
==因此，操作系统可以做的事情包括：==

*   examine or alter any process’s memory
==*   检查或修改任何进程的内存==
*   read, write, delete or corrupt any file
==*   读取、写入、删除或破坏任何文件==
*   change the scheduling or even halt execution of any process
==*   更改调度甚至停止任何进程的执行==

**ASIDE: SECURITY ENCLAVES**
==**旁注：安全飞地**==

Starting in the 1990s, hardware developers began to see a need to keep some hardware isolated, to a degree, from the operating system.
==从 20 世纪 90 年代开始，硬件开发商开始意识到有必要在某种程度上将某些硬件与操作系统隔离。==

**TPM**, or **Trusted Platform Module**, provided assurance that you were booting the version of the operating system you intended to.
==**TPM**（即**受信任平台模块**）提供了保证，确保你正在引导的是你预期的操作系统版本。==

Such hardware elements are called **security enclaves**, since they are meant to allow only safe use of this data, even by the most powerful, trusted code in the system – the operating system itself.
==此类硬件元素被称为**安全飞地**，因为它们旨在只允许安全地使用这些数据，即使是系统中功能最强大、最受信任的代码——操作系统本身也是如此。==

In essence, processes are at the mercy of the operating system.
==本质上，进程受操作系统的摆布。==

Security flaws in your operating system can completely compromise everything about the machine the system runs on.
==操作系统中的安全缺陷可能会完全损害该系统运行机器的一切。==




### 53.3 Security Goals and Policies
==### 53.3 安全目标与策略==

What do we mean when we say we want an operating system, or any system, to be secure?
==当我们说希望一个操作系统，或者任何系统，是“安全”的时，我们到底是什么意思？==

That’s a rather vague statement.
==这其实是一个相当模糊的说法。==

What we really mean is that there are things we would like to happen in the system and things we don’t want to happen, and we’d like a high degree of assurance that we get what we want.
==我们的真实意图是，系统中有些事情是我们希望发生的，有些是不希望发生的，并且我们希望有高度的确定性来保证我们能如愿以偿。==

As in most other aspects of life, we usually end up paying for what we get, so it’s worthwhile to think about exactly what security properties and effects we actually need and then pay only for those, not for other things we don’t need.
==正如生活中的大多数其他方面一样，我们通常需要为所获得的东西付出代价，因此值得去思考我们到底需要哪些安全属性和效果，然后只为这些买单，而不是为不需要的东西付费。==

What this boils down to is that we want to specify the goals we have for the security-relevant behavior of our system and choose defense approaches likely to achieve those goals at a reasonable cost.
==归根结底，我们希望明确系统安全相关行为的目标，并选择能够以合理成本实现这些目标的防御方法。==

Researchers in security have thought about this issue in broad terms for a long time.
==安全领域的研究人员长期以来一直在宏观层面思考这一问题。==

At a high conceptual level, they have defined three big security-related goals that are common to many systems, including operating systems.
==在高级概念层面，他们定义了三个通用的重大安全相关目标，这些目标适用于包括操作系统在内的许多系统。==

They are:
==它们分别是：==

* **Confidentiality** – If some piece of information is supposed to be hidden from others, don’t allow them to find it out.
==* **机密性 (Confidentiality)** —— 如果某条信息本应对他人隐藏，就不要让他们发现它。==

For example, you don’t want someone to learn what your credit card number is – you want that number kept confidential.
==例如，你不希望别人知道你的信用卡号——你希望该号码保持机密。==

* **Integrity** – If some piece of information or component of a system is supposed to be in a particular state, don’t allow an adversary to change it.
==* **完整性 (Integrity)** —— 如果系统的某个信息或组件应该处于特定状态，就不要允许对手更改它。==

For example, if you’ve placed an online order for delivery of one pepperoni pizza, you don’t want a malicious prankster to change your order to 1000 anchovy pizzas.
==例如，如果你在线订购了一份意大利腊肠披萨，你不希望某个恶作剧者将你的订单改为 1000 份鳀鱼披萨。==

One important aspect of integrity is authenticity.
==完整性的一个重要方面是真实性。==

It’s often important to be sure not only that information has not changed, but that it was created by a particular party and not by an adversary.
==通常，我们不仅要确保信息没有被更改，还要确保它是由特定的当事方创建的，而不是由对手创建的。==

* **Availability** – If some information or service is supposed to be available for your own or others’ use, make sure an attacker cannot prevent its use.
==* **可用性 (Availability)** —— 如果某些信息或服务应该供你自己或他人使用，请确保攻击者无法阻止其使用。==

For example, if your business is having a big sale, you don’t want your competitors to be able to block off the streets around your store, preventing your customers from reaching you.
==例如，如果你的商店正在大减价，你不希望竞争对手封锁商店周围的街道，阻止顾客光顾。==

An important extra dimension of all three of these goals is that we want controlled sharing in our systems.
==这三个目标的一个重要额外维度是，我们希望在系统中实现受控共享。==

We share our secrets with some people and not with others.
==我们会与某些人分享秘密，而对其他人保密。==

We allow some people to change our enterprise’s databases, but not just anyone.
==我们允许某些人更改企业的数据库，但不是任何人都可以。==

Some systems need to be made available to a particular set of preferred users (such as those who have paid to play your on-line game) and not to others (who have not).
==某些系统需要对特定的首选用户群（例如那些付费玩在线游戏的人）开放，而对其他人（未付费者）关闭。==

Who’s doing the asking matters a lot, in computers as in everyday life.
==在计算机世界中，就像在日常生活中一样，“谁提出的请求”至关重要。==

Another important aspect of security for computer systems is we often want to be sure that when someone told us something, they cannot later deny that they did so.
==计算机系统安全的另一个重要方面是，我们通常希望确保当某人告诉我们某事时，他们事后无法否认。==

This aspect is often called **non-repudiation**.
==这一方面通常被称为**不可否认性 (non-repudiation)**。==

The harder and more expensive it is for someone to repudiate their actions, the easier it is to hold them to account for those actions, and thus the less likely people are to perform malicious actions.
==一个人抵赖其行为的难度和代价越大，追究其责任就越容易，因此人们实施恶意行为的可能性就越小。==

After all, they might well get caught and will have trouble denying they did it.
==毕竟，他们很可能会被抓住，并且很难否认自己做过。==

These are big, general goals.
==这些都是宏大且通用的目标。==

For a real system, you need to drill down to more detailed, specific goals.
==对于一个真实的系统，你需要深入到更详细、具体的目标。==

In a typical operating system, for example, we might have a confidentiality goal stating that a process’s memory space cannot be arbitrarily read by another process.
==例如，在一个典型的操作系统中，我们可能会有一个机密性目标，即一个进程的内存空间不能被另一个进程任意读取。==

We might have an integrity goal stating that if a user writes a record to a particular file, another user who should not be able to write that file can’t change the record.
==我们可能有一个完整性目标，即如果一个用户向特定文件写入记录，另一个不具备写入该文件权限的用户就不能更改该记录。==

We might have an availability goal stating that one process running on the system cannot hog the CPU and prevent other processes from getting their share of the CPU.
==我们可能有一个可用性目标，即系统中运行的一个进程不能霸占 CPU，从而阻止其他进程获取其应得的 CPU 份额。==

If you think back on what you’ve learned about the process abstraction, memory management, scheduling, file systems, IPC, and other topics from this class, you should be able to think of some other obvious confidentiality, integrity, and availability goals we are likely to want in our operating systems.
==如果你回想一下在本课程中学到的关于进程抽象、内存管理、调度、文件系统、进程间通信（IPC）以及其他主题的知识，你应该能想到我们在操作系统中可能需要的其他一些显而易见的机密性、完整性和可用性目标。==

For any particular system, even goals at this level are not sufficiently specific.
==对于任何特定系统，即使是这种程度的目标也还不够具体。==

The integrity goal alluded to above, where a user’s file should not be overwritten by another user not permitted to do so, gives you a hint about the extra specificity we need in our security goals for a particular system.
==上面提到的完整性目标（即用户的文件不应被未经许可的另一个用户覆盖）为你提示了我们在特定系统的安全目标中需要的额外具体性。==

Maybe there is some user who should be able to overwrite the file, as might be the case when two people are collaborating on writing a report.
==也许有些用户是应该能够覆盖该文件的，比如两个人在协作撰写一份报告。==

But that doesn’t mean an unrelated third user should be able to write that file, if he is not collaborating on the report stored there.
==但这并不意味着一个无关的第三个用户也应该能够写入该文件，如果他没有参与协作那份报告的话。==

We need to be able to specify such detail in our security goals.
==我们需要能够在安全目标中明确这类细节。==

Operating systems are written to be used by many different people with many different needs, and operating system security should reflect that generality.
==操作系统的设计初衷是供许多具有不同需求的人使用，操作系统的安全性也应体现这种通用性。==

What we want in security mechanisms for operating systems is flexibility in describing our detailed security goals.
==我们在操作系统安全机制中追求的是描述详细安全目标的灵活性。==

Ultimately, of course, the operating system software must do its best to enforce those flexible security goals, which implies we’ll need to encode those goals in forms that software can understand.
==当然，操作系统软件最终必须尽力执行这些灵活的安全目标，这意味着我们需要将这些目标编码成软件可以理解的形式。==

We typically must convert our vague understandings of our security goals into highly specific **security policies**.
==我们通常必须将对安全目标模糊的理解转化为高度具体的**安全策略 (security policies)**。==

For example, in the case of the file described above, we might want to specify a policy like ’users A and B may write to file X, but no other user can write it.’
==例如，在上述文件的案例中，我们可能希望指定一条类似“用户 A 和 B 可以写入文件 X，但其他任何用户都不能写入”的策略。==

With that degree of specificity, backed by carefully designed and implemented mechanisms, we can hope to achieve our security goals.
==有了这种程度的具体性，再辅以精心设计和实现的机制，我们就有望实现安全目标。==

Note an important implication for operating system security: in many cases, an operating system will have the mechanisms necessary to implement a desired security policy with a high degree of assurance in its proper application, but only if someone tells the operating system precisely what that policy is.
==注意操作系统安全的一个重要含义：在许多情况下，操作系统拥有实施所需安全策略并保证其正确应用所需的机制，但前提是必须有人准确地告诉操作系统该策略是什么。==

With some important exceptions (like maintaining a process’s address space private unless specifically directed otherwise), the operating system merely supplies general mechanisms that can implement many specific policies.
==除了一些重要的例外（如除非另有明确指示，否则保持进程地址空间的私密性），操作系统仅仅提供通用的机制，这些机制可以实现许多具体的策略。==

Without intelligent design of policies and careful application of the mechanisms, however, what the operating system *should* or *could* do may not be what your operating system *will* do.
==然而，如果没有明智的策略设计和对机制的细致应用，操作系统“应该”或“能够”做的，可能并不是你的操作系统“将会”做的。==

### 53.4 Designing Secure Systems
==### 53.4 设计安全系统==

Few of you will ever build your own operating system, nor even make serious changes to any existing operating system, but we expect many of you will build large software systems of some kind.
==你们中很少有人会去构建自己的操作系统，甚至很少有人会对现有的操作系统进行重大修改，但我们预期你们中的许多人会构建某种大型软件系统。==

Experience of many computer scientists with system design has shown that there are certain design principles that are helpful in building systems with security requirements.
==许多计算机科学家在系统设计方面的经验表明，某些设计原则对于构建具有安全需求的系统非常有帮助。==

These principles were originally laid out by Jerome Saltzer and Michael Schroeder in an influential paper [SS75], though some of them come from earlier observations by others.
==这些原则最初由 Jerome Saltzer 和 Michael Schroeder 在一篇具有影响力的论文 [SS75] 中提出，尽管其中一些源于他人的早期观察。==

While neither the original authors nor later commentators would claim that following them will guarantee that your system is secure, paying attention to them has proven to lead to more secure systems, while you ignore them at your own peril.
==虽然原作者和后来的评论者都不会声称遵循这些原则就能保证系统安全，但事实证明，重视这些原则可以使系统更安全，而忽视它们则需自担风险。==

We’ll discuss them briefly here.
==我们在这里简要讨论一下。==

1. **Economy of mechanism** – This basically means keep your system as small and simple as possible.
==1. **机制最简化 (Economy of mechanism)** —— 这基本上意味着让你的系统尽可能保持小巧和简单。==

Simple systems have fewer bugs and it’s easier to understand their behavior.
==简单的系统 Bug 更少，其行为也更容易理解。==

If you don’t understand your system’s behavior, you’re not likely to know if it achieves its security goals.
==如果你不了解系统的行为，你就很难知道它是否实现了安全目标。==

2. **Fail-safe defaults** – Default to security, not insecurity.
==2. **失效安全默认 (Fail-safe defaults)** —— 默认选择安全，而不是不安全。==

If policies can be set to determine the behavior of a system, have the default for those policies be more secure, not less.
==如果可以通过设置策略来决定系统的行为，请将这些策略的默认值设为更安全的状态。==

3. **Complete mediation** – This is a security term meaning that you should check if an action to be performed meets security policies every single time the action is taken.
==3. **完全中介 (Complete mediation)** —— 这是一个安全术语，意味着每当执行某个动作时，你都应该检查该动作是否符合安全策略。==

4. **Open design** – Assume your adversary knows every detail of your design.
==4. **开放设计 (Open design)** —— 假设你的对手了解你设计的每一个细节。==

If the system can achieve its security goals anyway, you’re in good shape.
==如果在这种情况下系统仍然能实现其安全目标，那么你的状况就很好。==

This principle does not necessarily mean that you actually tell everyone all the details, but base your security on the assumption that the attacker has learned everything.
==这一原则并不一定意味着你真的要把所有细节告诉所有人，而是要基于“攻击者已经掌握了一切”的假设来建立安全性。==

He often has, in practice.
==在实践中，攻击者往往确实做到了这一点。==

5. **Separation of privilege** – Require separate parties or credentials to perform critical actions.
==5. **权限分离 (Separation of privilege)** —— 执行关键操作需要不同的当事方或凭据。==

For example, two-factor authentication, where you use both a password and possession of a piece of hardware to determine identity, is more secure than using either one of those methods alone.
==例如，双因子认证（同时使用密码和持有硬件设备来确定身份）比单独使用其中任何一种方法都更安全。==

6. **Least privilege** – Give a user or a process the minimum privileges required to perform the actions you wish to allow.
==6. **最小权限 (Least privilege)** —— 给予用户或进程执行你希望允许的操作所需的最小权限。==

The more privileges you give to a party, the greater the danger that they will abuse those privileges.
==你赋予当事方的权限越多，他们滥用这些权限的危险就越大。==

Even if you are confident that the party is not malicious, if they make a mistake, an adversary can leverage their error to use their superfluous privileges in harmful ways.
==即使你确信该当事方没有恶意，如果他们犯了错，对手也可以利用其错误，以有害的方式利用那些多余的权限。==

7. **Least common mechanism** – For different users or processes, use separate data structures or mechanisms to handle them.
==7. **最少共享机制 (Least common mechanism)** —— 对于不同的用户或进程，使用独立的数据结构或机制来处理。==

For example, each process gets its own page table in a virtual memory system, ensuring that one process cannot access another’s pages.
==例如，在虚拟内存系统中，每个进程都有自己的页表，确保一个进程无法访问另一个进程的页面。==

8. **Acceptability** – A critical property not dear to the hearts of many programmers.
==8. **可接受性 (Acceptability)** —— 这是一个关键属性，但许多程序员并不在意。==

If your users won’t use it, your system is worthless.
==如果你的用户不愿使用它，你的系统就毫无价值。==

Far too many promising secure systems have been abandoned because they asked too much of their users.
==有太多大有前途的安全系统因为对用户要求过高而被遗弃。==

### 53.5 The Basics of OS Security
==### 53.5 操作系统安全基础==

In a typical operating system, then, we have some set of security goals, centered around various aspects of confidentiality, integrity, and availability.
==因此，在一个典型的操作系统中，我们有一组安全目标，围绕机密性、完整性和可用性的各个方面展开。==

Some of these goals tend to be built in to the operating system model, while others are controlled by the owners or users of the system.
==其中一些目标往往内置于操作系统模型中，而另一些则由系统的所有者或用户控制。==

The built-in goals are those that are extremely common, or must be ensured to make the more specific goals achievable.
==内置目标是那些极其通用的目标，或者是为了使更具体的目标得以实现而必须确保的目标。==

Most of these built-in goals relate to controlling process access to pieces of the hardware.
==这些内置目标大多与控制进程对硬件部分的访问有关。==

That’s because the hardware is shared by all the processes on a system, and unless the sharing is carefully controlled, one process can interfere with the security goals of another process.
==这是因为硬件是由系统上的所有进程共享的，除非这种共享得到严格控制，否则一个进程可能会干扰另一个进程的安全目标。==

Other built-in goals relate to services that the operating system offers, such as file systems, memory management, and interprocess communications.
==其他内置目标与操作系统提供的服务有关，如文件系统、内存管理和进程间通信。==

If these services are not carefully controlled, processes can subvert the system’s security goals.
==如果这些服务没有得到严密控制，进程就可能破坏系统的安全目标。==

Clearly, a lot of system security is going to be related to process handling.
==显然，大量的系统安全都与进程处理有关。==

If the operating system can maintain a clean separation of processes that can only be broken with the operating system’s help, then neither shared hardware nor operating system services can be used to subvert our security goals.
==如果操作系统能够保持进程之间的彻底隔离，且这种隔离只有在操作系统的帮助下才能被打破，那么共享硬件和操作系统服务都无法被用来破坏我们的安全目标。==

That requirement implies that the operating system needs to be careful about allowing use of hardware and of its services.
==这一要求意味着操作系统在允许使用硬件及其服务时需要格外小心。==

For example, the operating system controls virtual memory, which in turn completely controls which physical memory addresses each process can access.
==例如，操作系统控制虚拟内存，而虚拟内存又完全控制每个进程可以访问的物理内存地址。==

Hardware support prevents a process from even naming a physical memory address that is not mapped into its virtual memory space.
==硬件支持甚至能阻止进程说出一个未映射到其虚拟内存空间的物理内存地址。==

System calls offer the operating system another opportunity to provide protection.
==系统调用为操作系统提供了另一个提供保护的机会。==

In most operating systems, processes access system services by making an explicit system call.
==在大多数操作系统中，进程通过发起显式的系统调用来访问系统服务。==

As you have learned, system calls switch the execution mode from the processor’s user mode to its supervisor mode, invoking an appropriate piece of operating system code as they do so.
==正如你所学到的，系统调用将执行模式从处理器的用户模式切换到特权模式（主管模式），并在切换时调用相应的操作系统代码。==

That code can determine which process made the system call and what service the process requested.
==该代码可以确定是哪个进程发起了系统调用，以及该进程请求了什么服务。==

The same mechanism gives the operating system the opportunity to check if the requested service should be allowed under the system’s security policy.
==同样的机制给了操作系统一个机会，去检查请求的服务在系统安全策略下是否应被允许。==

When a process performs a system call, then, the operating system will use the process identifier in the process control block or similar structure to determine the identity of the process.
==因此，当进程执行系统调用时，操作系统将使用进程控制块（PCB）或类似结构中的进程标识符来确定进程的身份。==

The OS can then use **access control mechanisms** to decide if the identified process is **authorized** to perform the requested action.
==然后，操作系统可以使用**访问控制机制 (access control mechanisms)** 来决定识别出的进程是否被**授权 (authorized)** 执行所请求的操作。==

### 53.6 Summary
==### 53.6 总结==

The security of the operating system is vital for both its own and its applications’ sakes.
==操作系统的安全性对于其自身及其应用程序都至关重要。==

Security failures in this software allow essentially limitless bad consequences.
==此类软件中的安全故障基本上会导致无限的严重后果。==

Achieving security in operating systems depends on the security goals one has.
==实现操作系统的安全性取决于所设定的安全目标。==

These goals will typically include goals related to confidentiality, integrity, and availability.
==这些目标通常包括与机密性、完整性和可用性相关的目标。==

As in other areas of operating system design, we handle these varying needs by separating the specific **policies** used by any particular system from the general **mechanisms** used to implement the policies for all systems.
==与操作系统设计的其他领域一样，我们通过将特定系统使用的具体**策略 (policies)** 与用于所有系统实现策略的通用**机制 (mechanisms)** 分离，来处理这些不同的需求。==

---

### 54 Authentication
==### 54 身份认证==

### 54.1 Introduction
==### 54.1 引言==

Context will be everything in operating system decisions on whether to perform some service or to refuse to do so because it will compromise security goals.
==在操作系统决定是否执行某项服务，或因可能危及安全目标而拒绝执行时，上下文（Context）就是一切。==

Perhaps the most important element of that context is who’s doing the asking.
==该上下文中最核心的要素或许就是“谁在请求”。==

In computer security discussions, we often refer to the party asking for something as the **principal**.
==在计算机安全讨论中，我们通常将发起请求的一方称为**主体 (principal)**。==

**Principals** are security-meaningful entities that can request access to resources, such as human users, groups of users, or complex software systems.
==**主体 (Principals)** 是具有安全意义的实体，可以请求访问资源，例如人类用户、用户组或复杂的软件系统。==

The process or other active computing entity performing the request on behalf of a principal is often called its **agent**.
==代表主体执行请求的进程或其他活跃计算实体通常被称为其**代理 (agent)**。==

The request is for access to some particular resource, which we frequently refer to as the **object** of the access request.
==请求是为了访问某个特定资源，我们经常将其称为访问请求的**对象 (object)**。==

Any form of data created and managed by the operating system that keeps track of such access decisions for future reference is often called a **credential**.
==由操作系统创建和管理、用于记录此类访问决策以备将来参考的任何形式的数据，通常被称为**凭据 (credential)**。==

### 54.3 How To Authenticate Users?
==### 54.3 如何认证用户？==

Classically, authenticating the identity of human beings has worked in one of three ways:
==传统上，对人类身份的认证主要通过以下三种方式之一：==

* **Authentication based on what you know**
==* **基于你所知道的认证**==

* **Authentication based on what you have**
==* **基于你所拥有的认证**==

* **Authentication based on what you are**
==* **基于你是什么（生物特征）的认证**==

### 54.4 Authentication By What You Know
==### 54.4 基于你所知道的认证==

Authentication by what you know is most commonly performed by using **passwords**.
==基于“你所知道的”进行的认证最常通过使用**密码 (passwords)** 来完成。==

Interestingly enough, though, our system does not actually need to know the password.
==但有趣的是，我们的系统实际上并不需要知道密码。==

Store a **hash** of the password, not the password itself.
==存储密码的**哈希值 (hash)**，而不是密码本身。==

There is a special class of hashing algorithms called **cryptographic hashes** that make it infeasible to use the hash to figure out what the password is.
==存在一类特殊的哈希算法，称为**密码学哈希 (cryptographic hashes)**，它们使得利用哈希值推算出原密码变得不可行。==

At the time of this writing, **SHA-3** is the US standard for cryptographic hash algorithms, and is a good choice.
==在撰写本文时，**SHA-3** 是美国的密码学哈希算法标准，是一个不错的选择。==

This form of password guessing is called a **dictionary attack**, and it can be highly effective.
==这种形式的密码猜测被称为**字典攻击 (dictionary attack)**，它可能非常有效。==

There’s a simple fix: before hashing a new password and storing it in your password file, generate a big random number (say 32 or 64 bits) and concatenate it to the password.
==有一个简单的解决办法：在对新密码进行哈希处理并存入密码文件之前，生成一个大的随机数（比如 32 位或 64 位），并将其拼接到密码上。==

You typically store the random number (which is called a **salt**) in the password file right next to the hashed password.
==你通常将这个随机数（被称为**盐 (salt)**）存储在密码文件中，紧挨着哈希后的密码。==

Why does this help?
==为什么这样做会有帮助？==

The attacker can no longer create one translation of passwords in the dictionary to their hashes.
==攻击者无法再针对字典中的密码预先生成统一的哈希值映射表。==

If the salt is 32 bits, that’s $2^{32}$ different translations for each word in the dictionary, which makes the approach of pre-computing the translations infeasible.
==如果“盐”是 32 位，那么字典中的每个词就有 $2^{32}$ 种不同的哈希映射，这使得预先计算映射表的方法变得不可行。==

### 54.5 Authentication by What You Have
==### 54.5 基于你所拥有的认证==

Those are both examples of authentication based on what you have, an ID card or a ticket, in these cases.
==在这些案例中，身份证或门票都是基于“你所拥有的”进行认证的例子。==

Some security tokens (sometimes called **dongles**, an unfortunate choice of name) are designed to work that way.
==一些安全令牌（有时被称为**加密狗 (dongles)**，一个不太走运的起名）就是按这种方式设计的。==

### 54.6 Authentication by What You Are
==### 54.6 基于你是什么的认证==

If our operating system can only accurately measure these properties or characteristics, it can distinguish one person from another, solving our authentication problem.
==如果我们的操作系统能够准确测量这些属性或特征，它就能区分不同的人，从而解决我们的认证问题。==



Remember that we’re talking about a computer program (either the OS itself or some separate program it invokes for the purpose) measuring a human characteristic and determining if it belongs to a particular person.
==请记住，我们讨论的是一个计算机程序（操作系统本身或它为此目的调用的某个独立程序）在测量人类特征并确定其是否属于某个特定的人。==

Think about what that entails.
==思考一下这意味着什么。==

What if we plan to use facial recognition with the camera on a smart phone to authenticate the owner of the phone?
==如果我们打算利用智能手机上的摄像头进行人脸识别，以验证手机所有者的身份，该怎么办？==

If we decide it’s the right person, we allow whoever we took the picture of to use the phone.
==如果我们判定是本人，我们就允许被拍照的人使用手机。==

If not, we give them the raspberry (in the cyber sense) and keep them out.
==如果不是，我们就拒绝他们（在网络意义上），并将其拒之门外。==

You should have identified a few challenges here.
==你应该已经在这里发现了一些挑战。==

First, the camera is going to take a picture of someone who is, presumably, holding the phone.
==首先，摄像头将拍摄一张据推测正握着手机的人的照片。==

Maybe it’s the owner, maybe it isn’t.
==可能是所有者，也可能不是。==

That’s the point of taking the picture.
==这正是拍照的目的所在。==

If it isn’t, we should assume whoever it is would like to fool us into thinking that they are the actual owner.
==如果不是，我们应该假设无论对方是谁，都想欺骗我们，让我们认为他们就是真正的所有者。==

What if it’s someone who looks a lot like the right user, but isn’t?
==如果是一个看起来非常像真实用户但又不是本人的人呢？==

What if the person is wearing a mask?
==如果那个人戴着面具呢？==

What if the person holds up a photo of the right user, instead of their own face?
==如果那个人举着真实用户的照片，而不是展示自己的脸呢？==

What if the lighting is dim, or the person isn’t fully facing the camera?
==如果光线昏暗，或者人没有完全面对摄像头呢？==

Alternately, what if it is the right user and the person is not facing the camera, or the lighting is dim, or something else has changed about the person’s look? (e.g., hairstyle)
==或者，如果是真实用户，但此人没有面对摄像头，或者光线昏暗，或者此人的外貌发生了其他变化（例如发型）呢？==

Computer programs don’t recognize faces the way people do.
==计算机程序识别面部的方式与人类不同。==

They do what programs always do with data: they convert it to zeros and ones and process it using some algorithm.
==它们像对待所有数据一样对待照片：将其转换为零和一，并使用某种算法进行处理。==

So that “photo” you took is actually a collection of numbers, indicating shadow and light, shades of color, contrasts, and the like.
==所以你拍摄的那张“照片”实际上是一组数字，表示阴影和光线、色彩深度、对比度等信息。==

OK, now what?
==好，接下来呢？==

Time to decide if it’s the right person’s photo or not!
==是时候决定这是否是正确的人的照片了！==

How?
==如何决定？==

If it were a password, we could have stored the right password (or, better, a hash of the right password) and done a comparison of what got typed in (or its hash) to what we stored.
==如果它是密码，我们可以存储正确的密码（或者更好的是存储正确密码的哈希值），并将输入的内容（或其哈希值）与我们存储的内容进行比对。==

If it’s a perfect match, authenticate.
==如果是完美匹配，则通过身份验证。==

Otherwise, don’t.
==否则，不予通过。==

Can we do the same with this collection of zeros and ones that represent the picture we just took?
==我们能对代表刚才拍摄的照片的这组零和一做同样的事情吗？==

Can we have a picture of the right user stored permanently in some file (also in the form of zeros and ones) and compare the data from the camera to that file?
==我们能否将正确用户的照片永久存储在某个文件中（同样是以零和一的形式），然后将摄像头获取的数据与该文件进行比较？==

Probably not in the same way we compared the passwords.
==可能无法以比较密码的相同方式进行比较。==

Consider one of those factors we just mentioned above: lighting.
==考虑一下我们上面提到的一个因素：光线。==

If the picture we stored in the file was taken under bright lights and the picture coming out of the camera was taken under dim lights, the two sets of zeros and ones are most certainly not going to match.
==如果存储在文件中的照片是在强光下拍摄的，而摄像头输出的照片是在昏暗的光线下拍摄的，那么这两组零和一肯定无法匹配。==

In fact, it’s quite unlikely that two pictures of the same person, taken a second apart under identical conditions, would be represented by exactly the same set of bits.
==事实上，即使是同一个人在相同条件下间隔一秒拍摄的两张照片，也极不可能由完全相同的位集（bits）表示。==

So clearly we can’t do a comparison based on bit-for-bit equivalence.
==所以显然我们不能进行基于位对位等价（bit-for-bit equivalence）的比较。==

Instead, we need to compare based on a higher-level analysis of the two photos, the stored one of the right user and the just-taken one of the person who claims to be that user.
==相反，我们需要基于对两张照片的高级分析进行比较：一张是存储的正确用户的照片，另一张是声称是该用户的、刚刚拍摄的照片。==

Generally this will involve extracting higher-level features from the photos and comparing those.
==通常，这涉及从照片中提取高级特征并对这些特征进行比较。==

We might, for example, try to calculate the length of the nose, or determine the color of the eyes, or make some kind of model of the shape of the mouth.
==例如，我们可能会尝试计算鼻子的长度，或确定眼睛的颜色，或者为嘴巴的形状建立某种模型。==

Then we would compare the same feature set from the two photos.
==然后我们会比较两张照片中相同的特征集。==

Even here, though, an exact match is not too likely.
==不过即使如此，精确匹配的可能性也不大。==

The lighting, for example, might slightly alter the perceived eye color.
==例如，光线可能会稍微改变感知的眼睛颜色。==

So we’ll need to allow some sloppiness in our comparison.
==因此，我们需要在比较中允许一定的冗余度（sloppiness）。==

If the feature match is “close enough,” we authenticate.
==如果特征匹配“足够接近”，我们就通过身份验证。==

If not, we don’t.
==否则，我们不予通过。==

We will look for close matches, not perfect matches, which brings the nose of the camel of tolerances into our authentication tent.
==我们将寻找接近的匹配，而不是完美的匹配，这就在我们的身份验证体系中引入了容差的概念。==

If we are intolerant of all but the closest matches, on some days we will fail to match the real user’s picture to the stored version.
==如果我们除了最接近的匹配之外不容忍任何偏差，那么在某些日子里，我们将无法将真实用户的照片与存储版本相匹配。==

That’s called a **false negative**, since we incorrectly decided not to authenticate.
==这被称为**漏报**（false negative），因为我们错误地决定不通过身份验证。==

If we are too tolerant of differences in measured versus stored data, we will authenticate a user whom is not who they claim to be.
==如果我们对测量数据与存储数据之间的差异过于宽容，我们就会通过一个并非其声称身份的用户的验证。==

That’s a **false positive**, since we incorrectly decided to authenticate.
==这是一个**误报**（false positive），因为我们错误地决定通过身份验证。==

The nature of biometrics is that any implementation will have a characteristic false positive and false negative rate.
==生物识别的本质在于，任何实现都将具有特征性的误报率和漏报率。==

Both are bad, so you’d like both to be low.
==两者都是有害的，所以你希望两者都保持在较低水平。==

For any given implementation of some biometric authentication technique, you can typically tune it to achieve some false positive rate, or tune it to achieve some false negative rate.
==对于某种生物识别认证技术的任何特定实现，你通常可以对其进行调整以达到特定的误报率，或者调整它以达到特定的漏报率。==

But you usually can’t minimize both.
==但你通常无法同时使两者最小化。==

As the false positive rate goes down, the false negative rate goes up, and vice versa.
==随着误报率下降，漏报率就会上升，反之亦然。==

The **sensitivity** describes how close the match must be.
==**灵敏度**（sensitivity）描述了匹配必须达到多接近的程度。==

Figure 54.1 shows the typical relationship between these error rates.
==图 54.1 显示了这些错误率之间的典型关系。==

Note the circle at the point where the two curves cross.
==注意两条曲线相交处的圆圈。==

That point represents the **crossover error rate**, a common metric for describing the accuracy of a biometric.
==该点代表**交叉错误率**（crossover error rate），这是描述生物识别准确性的常用指标。==

It represents an equal tradeoff between the two kinds of errors.
==它代表了两种错误之间的对等权衡。==

It’s not always the case that one tunes a biometric system to hit the crossover error rate, since you might care more about one kind of error than the other.
==人们并不总是将生物识别系统调整到交叉错误率，因为你可能更看重其中一种错误。==

For example, a smart phone that frequently locks its legitimate user out because it doesn’t like today’s fingerprint reading is not going to be popular, while the chances of a thief who stole the phone having a similar fingerprint are low.
==例如，一款因为不喜欢今天的指纹读取而经常将合法用户拒之门外的智能手机是不会受欢迎的，而偷走手机的小偷拥有相似指纹的概率却很低。==

Perhaps low false negatives matter more here.
==在这种情况下，较低的漏报率可能更为重要。==

On the other hand, if you’re opening a bank vault with a retinal scan, requiring the bank manager to occasionally provide a second scan isn’t too bad, while allowing a robber to open the vault with a bogus fake eye would be a disaster.
==另一方面，如果你是用视网膜扫描开启银行金库，要求银行经理偶尔进行第二次扫描并无大碍，而允许强盗用伪造的假眼打开金库则会是一场灾难。==

Low false positives might be better here.
==在这种情况下，较低的误报率可能更好。==

Leaving aside the issues of reliability of authentication using biometrics, another big issue for using human characteristics to authenticate is that many of the techniques for measuring them require special hardware not likely to be present on most machines.
==撇开使用生物识别进行身份验证的可靠性问题不谈，使用人体特征进行身份验证的另一个大问题是，许多测量技术需要大多数机器上不太可能配备的专用硬件。==

Many computers (including smart phones, tablets, and laptops) are likely to have cameras, but embedded devices and server machines probably don’t.
==许多计算机（包括智能手机、平板电脑和笔记本电脑）很可能配有摄像头，但嵌入式设备和服务器机器可能没有。==

Relatively few machines have fingerprint readers, and even fewer are able to measure more exotic biometrics.
==相对较少的机器配有指纹读取器，能够测量更奇特的生物特征的机器就更少了。==

While a few biometric techniques (such as measuring typing patterns) require relatively common hardware that is likely to be present on many machines anyway, there aren’t many such techniques.
==虽然少数生物识别技术（如测量打字模式）需要相对通用的硬件，而这些硬件在许多机器上通常都已经存在，但这类技术的数量并不多。==

Even if a special hardware device is available, the convenience of using them for this purpose can be limiting.
==即使可以使用专用的硬件设备，为了这个目的而使用它们的便利性也可能受到限制。==

One further issue you want to think about when considering using biometric authentication is whether there is any physical gap between where the biometric quantity is measured and where it is checked.
==在考虑使用生物识别身份验证时，你还需要考虑的一个问题是，生物特征值的测量地点与核对地点之间是否存在物理间隙。==

In particular, checking biometric readings provided by an untrusted machine across the network is hazardous.
==特别是，通过网络核对由不受信任的机器提供的生物识别读数是危险的。==

What comes in across the network is simply a pattern of bits spread across one or more messages, whether it represents a piece of a web page, a phoneme in a VoIP conversation, or part of a scanned fingerprint.
==通过网络传输的仅仅是分布在一个或多个消息中的位模式（pattern of bits），无论它代表的是网页片段、VoIP 通话中的音素，还是扫描指纹的一部分。==

Bits are bits, and anyone can create any bit pattern they want.
==位（bits）就是位，任何人都可以创建他们想要的任何位模式。==

If a remote adversary knows what the bit pattern representing your fingerprint looks like, they may not need your finger, or even a fingerprint scanner, to create it and feed it to your machine.
==如果远程对手知道代表你指纹的位模式是什么样的，他们可能不需要你的手指，甚至不需要指纹扫描仪，就可以创建该模式并将其输入你的机器。==

When the hardware performing the scanning is physically attached to your machine, there is less opportunity to slip in a spurious bit pattern that didn’t come from the device.
==当执行扫描的硬件物理连接到你的机器时，插入不属于该设备的伪造位模式的机会就较少。==

When the hardware is on the other side of the world on a machine you have no control over, there is a lot more opportunity.
==当硬件位于世界另一端你无法控制的机器上时，机会就会多得多。==

The point here is to be careful with biometric authentication information provided to you remotely.
==这里的重点是对远程提供给你的生物识别身份验证信息保持警惕。==

In all, it sort of sounds like biometrics are pretty terrible for authentication, but that’s the wrong lesson.
==总而言之，这听起来似乎生物识别在身份验证方面非常糟糕，但这是错误的结论。==

For that matter, previous sections probably made it sound like all methods of authentication are terrible.
==就此而言，前面的章节可能让所有的身份验证方法听起来都很糟糕。==

Certainly none of them are perfect, but your task as a system designer is not to find the perfect authentication mechanism, but to use mechanisms that are well suited to your system and its environment.
==当然，没有哪种方法是完美的，但作为系统设计者，你的任务不是寻找完美的身份验证机制，而是使用非常适合你的系统及其环境的机制。==

A good fingerprint reader built in to a smart phone might do its job quite well.
==内置在智能手机中的优质指纹读取器可能会表现得相当出色。==

A long, unguessable password can provide a decent amount of security.
==长且不可猜测的密码可以提供相当程度的安全性。==

Well-designed smart cards can make it nearly impossible to authenticate yourself without having them in your hand.
==设计良好的智能卡可以让你在没有随身携带的情况下几乎无法通过身份验证。==

And where each type of mechanism fails, you can perhaps correct for that failure by using a second or third authentication mechanism that doesn’t fail in the same cases.
==在每种机制失效的情况下，你也许可以通过使用第二或第三种在相同情况下不会失效的身份验证机制来弥补这种缺陷。==

**54.7 Authenticating Non-Humans**
==**54.7 非人类身份验证**==

No, we’re not talking about aliens or extra-dimensional beings, or even your cat.
==不，我们讨论的不是外星人、异次元生物，甚至也不是你的猫。==

If you think broadly about how computers are used today, you’ll see that there are many circumstances in which no human user is associated with a process that’s running.
==如果你从更广泛的角度思考当今计算机的使用方式，你会发现在许多情况下，正在运行的进程并未与任何人类用户相关联。==

Consider a web server.
==以 Web 服务器为例。==

There really isn’t some human user logged in whose identity should be attached to the web server.
==实际上并没有某个已登录的人类用户需要将其身份附加到 Web 服务器上。==

Or think about embedded devices, such as a smart light bulb.
==或者想想嵌入式设备，比如智能灯泡。==

Nobody logs in to a light bulb, but there is certainly code running there, and quite likely it is process-oriented code.
==没有人会登录灯泡，但那里肯定有代码在运行，而且很可能是面向进程的代码。==

Mechanically, the operating system need not have a problem with the identities of such processes.
==从机制上讲，操作系统不应对此类进程的身份产生问题。==

Simply set up a user called **webserver** or **lightbulb** on the system in question and attach the identity of that “user” to the processes that are associated with running the web server or turning the light bulb on and off.
==只需在相关系统上设置一个名为 **webserver** 或 **lightbulb** 的用户，并将该“用户”的身份附加到与运行 Web 服务器或开关灯泡相关的进程中。==

But that does lead to the question of how you make sure that only real web server processes are tagged with that identity.
==但这也引出了一个问题：你如何确保只有真正的 Web 服务器进程被标记为该身份？==

We wouldn’t want some arbitrary user on the web server machine creating processes that appear to belong to the server, rather than to that user.
==我们不希望 Web 服务器机器上的任何任意用户创建看起来属于服务器、而不是属于该用户的进程。==

One approach is to use passwords for these non-human users, as well.
==一种方法是也为这些非人类用户使用密码。==

Simply assign a password to the web server user.
==只需为 Web 服务器用户分配一个密码。==

When does it get used?
==什么时候使用它？==

When it’s needed, which is when you want to create a process belonging to the web server, but you don’t already have one in existence.
==在需要的时候使用，也就是当你想要创建一个属于 Web 服务器的进程，但目前还没有这类进程时。==

The system administrator could log in as the web server user, creating a command shell and using it to generate the actual processes the server needs to do its business.
==系统管理员可以以 Web 服务器用户的身份登录，创建一个命令行 Shell，并使用它生成服务器业务所需的实际进程。==

As usual, the processes created by this shell process would inherit their parent’s identity, **webserver**, in this case.
==通常情况下，由此 Shell 进程创建的进程将继承其父进程的身份，在这种情况下即为 **webserver**。==

More commonly, we skip the go-between (here, the login) and provide some mechanism whereby the privileged user is permitted to create processes that belongs not to that user, but to some other user such as **webserver**.
==更常见的是，我们跳过中间步骤（这里指登录），并提供某种机制，允许特权用户创建不属于该用户、而是属于其他用户（如 **webserver**）的进程。==

Alternately, we can provide a mechanism that allows a process to change its ownership, so the web server processes would start off under some other user’s identity (such as the system administrator’s) and change their ownership to **webserver**.
==或者，我们可以提供一种允许进程更改其所有权的机制，这样 Web 服务器进程最初在其他用户身份（如系统管理员）下启动，然后将其所有权更改为 **webserver**。==

Yet another approach is to allow a temporary change of process identity, while still remembering the original identity.
==还有一种方法是允许临时更改进程身份，同时仍然记住原始身份。==

(We’ll say more about this last approach in a future chapter.)
==（我们将在以后的章节中详细讨论最后一种方法。）==

Obviously, any of these approaches require strong controls, since they allow one user to create processes belonging to another user.
==显然，这些方法中的任何一种都需要强大的控制，因为它们允许一个用户创建属于另一个用户的进程。==

As mentioned above, passwords are the most common authentication method used to determine if a process can be assigned to one of these non-human users.
==如上所述，密码是用于确定是否可以将进程分配给这些非人类用户的最常用的身份验证方法。==

Sometimes no authentication of the non-human user is required at all, though.
==不过，有时根本不需要对非人类用户进行身份验证。==

Instead, certain other users (like trusted system administrators) are given the right to assign new identities to the processes they create, without providing any further authentication information than their own.
==相反，某些其他用户（如受信任的系统管理员）被赋予将新身份分配给他们创建的进程的权利，除了他们自己的身份验证信息外，无需提供任何进一步的身份验证信息。==

In Linux and other Unix systems, the `sudo` command offers this capability.
==在 Linux 和其他 Unix 系统中，`sudo` 命令提供了这种功能。==

For example, if you type the following:
==例如，如果你输入以下内容：==

`sudo -u webserver apache2`
`sudo -u webserver apache2`

This would indicate that the `apache2` program should be started under the identity of `webserver`, rather than under the identity of whoever ran the `sudo` command.
==这将表明 `apache2` 程序应该在 `webserver` 的身份下启动，而不是在运行 `sudo` 命令的那个人的身份下启动。==

This command might require the user running it to provide their own authentication credentials (for extra certainty that it really is the privileged user asking for it, and not some random visitor accessing the computer during the privileged user’s coffee break), but would not require authentication information associated with `webserver`.
==此命令可能要求运行它的用户提供他们自己的身份验证凭据（以额外确保确实是特权用户在请求，而不是某个在特权用户喝咖啡休息时访问计算机的随机访客），但不需要与 `webserver` 相关的身份验证信息。==

Any sub-processes created by `apache2` would, of course, inherit the identity of `webserver`.
==当然，`apache2` 创建的任何子进程都将继承 `webserver` 的身份。==

We’ll say more about `sudo` in the chapter on access control.
==我们将在访问控制一章中进一步讨论 `sudo`。==

One final identity issue we alluded to earlier is that sometimes we wish to identify not just individual users, but groups of users who share common characteristics, usually security-related characteristics.
==我们之前提到的最后一个身份问题是，有时我们不仅希望识别单个用户，还希望识别共享共同特征（通常是与安全相关的特征）的用户组。==

For example, we might have four or five system administrators, any one of whom is allowed to start up the web server.
==例如，我们可能有四或五名系统管理员，其中任何一人都被允许启动 Web 服务器。==

Instead of associating the privilege with each one individually, it’s advantageous to create a system-meaningful group of users with that privilege.
==与其将特权分别与每个人关联，不如创建一个具有该特权的、对系统有意义的用户组，这样做更有利。==

We would then indicate that the four or five administrators are members of that group.
==然后我们将指明这四或五名管理员是该组的成员。==

This kind of group is another example of a security-relevant principal, since we will make our decisions on the basis of group membership, rather than individual identity.
==这种组是另一个与安全相关的客体（principal）示例，因为我们将根据组成员身份而不是个人身份做出决策。==

When one of the system administrators wished to do something requiring group membership, we would check that he or she was a member.
==当其中一名系统管理员希望执行需要组成员身份的操作时，我们将检查他或她是否是该组的成员。==

We can either associate a group membership with each process, or use the process’s individual identity information as an index into a list of groups that people belong to.
==我们既可以将组成员身份与每个进程关联，也可以使用进程的个人身份信息作为索引进入人们所属的组列表。==

The latter is more flexible, since it allows us to put each user into an arbitrary number of groups.
==后者更灵活，因为它允许我们将每个用户放入任意数量的组中。==

Most modern operating systems, including Linux and Windows, support these kinds of groups, since they provide ease and flexibility in dealing with application of security policies.
==包括 Linux 和 Windows 在内的大多数现代操作系统都支持这类组，因为它们在处理安全策略的应用时提供了便利和灵活性。==

They handle group membership and group privileges in manners largely analogous to those for individuals.
==它们处理组成员身份和组特权的方式在很大程度上类似于处理个人的方式。==

For example, a child process will usually have the same group-related privileges as its parent.
==例如，子进程通常会具有与其父进程相同的与组相关的特权。==

When working with such systems, it’s important to remember that group membership provides a second path by which a user can obtain access to a resource, which has its benefits and its dangers.
==在使用此类系统时，必须记住，组成员身份为用户提供了获取资源访问权限的第二条路径，这既有好处，也有危险。==

**54.8 Summary**
==**54.8 总结**==

If we want to apply security policies to actions taken by processes in our system, we need to know the identity of the processes, so we can make proper decisions.
==如果我们想对系统中进程采取的操作应用安全策略，我们需要知道进程的身份，以便做出正确的决策。==

We start the entire chain of processes by creating a process at boot time belonging to some system user whose purpose is to authenticate users.
==我们在启动时创建一个属于某个系统用户的进程，从而启动整个进程链，该系统用户的目的是对用户进行身份验证。==

They log in, providing authentication information in one or more forms to prove their identity.
==用户登录，以一种或多种形式提供身份验证信息以证明其身份。==

The system verifies their identity using this information and assigns their identity to a new process that allows the user to go about their business, which typically involves running other processes.
==系统利用这些信息验证他们的身份，并将其身份分配给一个允许用户处理业务的新进程，这通常涉及运行其他进程。==

Those other processes will inherit the user’s identity from their parent process.
==这些其他进程将从其父进程继承用户的身份。==

Special secure mechanisms can allow identities of processes to be changed or to be set to something other than the parent’s identity.
==特殊的安全机制可以允许更改进程的身份，或者将其设置为父进程身份以外的其他身份。==

The system can then be sure that processes belong to the proper user and can make security decisions accordingly.
==系统随后可以确信进程属于正确的用户，并据此做出安全决策。==

Historically and practically, the authentication information provided to the system is either something the authenticating user knows (like a password or PIN), something the user has (like a smart card or proof of possession of a smart phone), or something the user is (like the user’s fingerprint or voice scan).
==从历史和实践的角度来看，提供给系统的身份验证信息要么是验证用户知道的内容（如密码或 PIN），要么是用户拥有的内容（如智能卡或智能手机的持有证明），或者是用户本身具备的特征（如用户的指纹或语音扫描）。==

Each of these approaches has its strengths and weaknesses.
==每种方法都有其优缺点。==

A higher degree of security can be obtained by using multi-factor authentication, which requires a user to provide evidence of more than one form, such as requiring both a password and a one-time code that was texted to the user’s smart phone.
==通过使用多因素身份验证可以获得更高程度的安全性，这要求用户提供多种形式的证据，例如同时要求密码和发送到用户智能手机的一次性代码。==

**55 Access Control**
==**55 访问控制**==

**55.1 Introduction**
==**55.1 引言**==

So we know what our security goals are, we have at least a general sense of the security policies we’d like to enforce, and we have some evidence about who is requesting various system services that might (or might not) violate our policies.
==既然我们知道了我们的安全目标，至少对我们想要执行的安全策略有了大致的了解，并且我们掌握了一些关于谁在请求各种可能（或可能不）违反我们策略的系统服务的证据。==

Now we need to take that information and turn it into something actionable, something that a piece of software can perform for us.
==现在我们需要获取这些信息并将其转化为可执行的操作，即软件可以为我们执行的操作。==

There are two important steps here:
==这里有两个重要的步骤：==

1. Figure out if the request fits within our security policy.
==1. 确定该请求是否符合我们的安全策略。==

2. If it does, perform the operation. If not, make sure it isn’t done.
==2. 如果符合，则执行该操作。如果不符合，请确保该操作不被执行。==

The first step is generally referred to as **access control**.
==第一步通常被称为**访问控制**。==

We will determine which system resources or services can be accessed by which parties in which ways under which circumstances.
==我们将决定哪些系统资源或服务可以在什么样的情况下，被哪些方以何种方式访问。==

Basically, it boils down to another of those binary decisions that fit so well into our computing paradigms: yes or no.
==从根本上说，它归结为另一种非常适合我们计算范式的二元决策：是或否。==

But how to make that decision?
==但是如何做出那个决定呢？==

To make the problem more concrete, consider this case.
==为了使问题更加具体，请考虑这种情况。==

User X wishes to read and write file `/var/foo`.
==用户 X 希望读取和写入文件 `/var/foo`。==

Under the covers, this case probably implies that a process being run under the identity of User X issued a system call such as:
==在底层，这种情况可能意味着在用户 X 的身份下运行的进程发出了如下系统调用：==

`open("/var/foo", O_RDWR)`
`open("/var/foo", O_RDWR)`

Note here that we’re not talking about the Linux `open()` call, which is a specific implementation that handles access control a specific way.
==注意，这里我们讨论的不是 Linux 的 `open()` 调用，那是一个以特定方式处理访问控制的特定实现。==

We’re talking about the general idea of how you might be able to control access to a file open system call.
==我们讨论的是如何控制文件打开系统调用访问权限的通用理念。==

Hence the different font, to remind you.
==因此使用了不同的字体以提醒你。==

How should the system handle this request from the process, making sure that the file is not opened if the security policy to be enforced forbids it, but equally making sure that the file is opened if the policy allows it?
==系统应该如何处理来自进程的这个请求，既要确保在执行的安全策略禁止时文件不会被打开，又要同样确保在策略允许时文件会被打开？==

We know that the system call will trap to the operating system, giving it the opportunity to do something to make this decision.
==我们知道系统调用会陷（trap）入操作系统，使其有机会采取行动做出此决策。==

Mechanically, speaking, what should that “something” be?
==从机制上讲，那个“行动”应该是什么？==

**THE CRUX OF THE PROBLEM:**
==**问题的核心：**==

**HOW TO DETERMINE IF AN ACCESS REQUEST SHOULD BE GRANTED?**
==**如何确定访问请求是否应被批准？**==

How can the operating system decide if a particular request made by a particular process belonging to a particular user at some given moment should or should not be granted?
==操作系统如何决定由属于特定用户的特定进程在特定时刻发出的特定请求是否应该被批准？==

What information will be used to make this decision?
==将使用什么信息来做出这个决定？==

How can we set this information to encode the security policies we want to enforce for our system?
==我们如何设置这些信息来对我们想要为系统执行的安全策略进行编码？==

**55.2 Important Aspects Of The Access Control Problem**
==**55.2 访问控制问题的重要方面**==

As usual, the system will run some kind of algorithm to make this decision.
==通常情况下，系统将运行某种算法来做出此决定。==

It will take certain inputs and produce a binary output, a yes-or-no decision on granting access.
==它将获取某些输入并产生二元输出，即关于授予访问权限的“是”或“否”的决定。==

At the high level, access control is usually spoken of in terms of **subjects**, **objects**, and **access**.
==在高级别上，访问控制通常根据**主体**（subjects）、**客体**（objects）和**访问**（access）来讨论。==

A **subject** is the entity that wants to perform the access, perhaps a user or a process.
==**主体**是想要执行访问的实体，可能是一个用户或一个进程。==

An **object** is the thing the subject wants to access, perhaps a file or a device.
==**客体**是主体想要访问的东西，可能是一个文件或一个设备。==

**Access** is some particular mode of dealing with the object, such as reading it or writing it.
==**访问**是处理客体的某种特定模式，例如读取或写入。==

So an access control decision is about whether a particular subject is allowed to perform a particular mode of access on a particular object.
==因此，访问控制决策是关于是否允许特定主体对特定客体执行特定模式的访问。==

We sometimes refer to the process of determining if a particular subject is allowed to perform a particular form of access on a particular object as **authorization**.
==我们有时将确定是否允许特定主体对特定客体执行特定形式的访问的过程称为**授权**（authorization）。==

One relevant issue is when will access control decisions be made?
==一个相关的问题是，何时做出访问控制决策？==

The system must run whatever algorithm it uses every time it makes such a decision.
==系统每次做出此类决策时都必须运行其使用的任何算法。==

The code that implements this algorithm is called a **reference monitor**, and there is an obvious incentive to make sure it is implemented both correctly and efficiently.
==实现此算法的代码被称为**引用监视器**（reference monitor），显然有动机确保它的实现既正确又高效。==

If it’s not correct, you make the wrong access decisions – obviously bad.
==如果不正确，你就会做出错误的访问决策——显然这是有害的。==

Its efficiency is important because it will inject some overhead whenever it is used.
==它的效率很重要，因为无论何时使用它都会引入一些开销。==

Perhaps we wish to minimize these overheads by not checking access control on every possible opportunity.
==也许我们希望通过不针对每个可能的时机检查访问控制来最小化这些开销。==

On the other hand, remember that principle of complete mediation we introduced a couple of chapters back?
==另一方面，还记得我们在几章前介绍的全中介（complete mediation）原则吗？==

That principle said we should check security conditions every time someone asked for something.
==该原则指出，每当有人请求某样东西时，我们都应该检查安全状况。==

Clearly, we’ll need to balance costs against security benefits.
==显然，我们需要在成本和安全效益之间进行权衡。==

But if we can find some beneficial special cases where we can achieve low cost without compromising security, we can possibly manage to avoid trading off one for the other, at least in those cases.
==但如果我们能找到一些有益的特殊情况，在这些情况下我们可以在不损害安全性的情况下实现低成本，那么我们或许能够避免在这两者之间进行取舍，至少在这些情况下是这样。==

One way to do so is to give subjects objects that belong only to them.
==实现这一点的一种方法是给主体只属于它们自己的客体。==

If the object is inherently theirs, by its very nature and unchangeably so, the system can let the subject (a process, in the operating system case) access it freely.
==如果客体本质上属于它们，由于其性质且不可更改，系统就可以让主体（在操作系统的情况下为进程）自由地访问它。==

Virtualization allows us to create virtual objects of this kind.
==虚拟化允许我们创建这种类型的虚拟客体。==

Virtual memory is an excellent example.
==虚拟内存就是一个绝佳的例子。==

A process is allowed to access its virtual memory freely, with no special operating system access control check at the moment the process tries to use it.
==进程被允许自由访问其虚拟内存，在进程尝试使用它时，操作系统无需进行特殊的访问控制检查。==

A good thing, too, since otherwise we would need to run our access control algorithm on every process memory reference, which would lead to a ridiculously slow system.
==这也是件好事，否则我们就需要对每一次进程内存引用运行访问控制算法，这将导致系统速度极其缓慢。==

We can play similar virtualization tricks with peripheral devices.
==我们也可以对外部设备玩类似的虚拟化花招。==

If a process is given access to some virtual device, which is actually backed up by a real physical device controlled by the OS, and if no other process is allowed to use that device, the operating system need not check for access control every time the process wants to use it.
==如果一个进程被赋予了访问某个虚拟设备的权限，而该设备实际上是由操作系统控制的真实物理设备支撑的，并且如果不允许其他进程使用该设备，那么操作系统就不必在进程每次想要使用它时都进行访问控制检查。==

For example, a process might be granted control of a GPU based on an initial access control decision, after which the process can write to the GPU’s memory or issue instructions directly to it without further intervention by the OS.
==例如，一个进程可能会基于初始访问控制决策被授予 GPU 的控制权，之后该进程就可以写入 GPU 的内存或直接向其发出指令，而无需操作系统的进一步干预。==

Of course, as discussed earlier, virtualization is mostly an operating-system provided illusion.
==当然，如前所述，虚拟化主要是操作系统提供的一种幻觉。==

Processes share memory, devices, and other computing resources.
==进程共享内存、设备和其他计算资源。==

What appears to be theirs alone is actually shared, with the operating system running around behind the scenes to keep the illusion going, sometimes assisted by special hardware.
==那些看起来属于它们独有的东西实际上是共享的，操作系统在后台运行以维持这种幻觉，有时还辅以专门的硬件。==

That means the operating system, without the direct knowledge and participation of the applications using the virtualized resource, still has to make sure that only proper forms of access to it are allowed.
==这意味着操作系统在不让使用虚拟化资源的应用程序直接知晓和参与的情况下，仍然必须确保只允许对其进行正确的访问形式。==

So merely relying on virtualization to ensure proper access just pushes the problem down to protecting the virtualization functionality of the OS.
==因此，仅仅依靠虚拟化来确保正确的访问，只是将问题下移到了保护操作系统的虚拟化功能上。==

Even if we leave that issue aside, sooner or later we have to move past cheap special cases and deal with the general problem.
==即使撇开这个问题不谈，我们迟早也必须跨越廉价的特殊情况，去处理一般性问题。==

Subject X wants to read and write object `/tmp/foo`.
==主体 X 想要读取和写入客体 `/tmp/foo`。==

Maybe it’s allowable, maybe it isn’t.
==也许这是允许的，也许不是。==

Now what?
==现在怎么办？==

Computer scientists have come up with two basic approaches to solving this question, relying on different data structures and different methods of making the decision.
==计算机科学家已经提出了解决这个问题的两种基本方法，它们依赖于不同的数据结构和不同的决策方法。==

One is called **access control lists** and the other is called **capabilities**.
==一种被称为**访问控制列表**（access control lists），另一种被称为**权标/能力**（capabilities）。==

It’s actually a little inaccurate to claim that computer scientists came up with these approaches, since they’ve been in use in non-computer contexts for millennia.
==声称计算机科学家发明了这些方法实际上有点不准确，因为它们已经在非计算机语境中使用了数千年。==

Let’s look at them in a more general perspective before we consider operating system implementations.
==在考虑操作系统实现之前，让我们从更普遍的角度来看待它们。==

Let’s say we want to start an exclusive nightclub (called, perhaps, Chez Andrea) restricted to only the best operating system researchers and developers.
==假设我们想创办一家高级夜总会（也许叫 Chez Andrea），仅限最优秀的操作系统研究人员和开发人员进入。==

We don’t want to let any of those database or programming language people slip in, so we’ll need to make sure only our approved customers get through the door.
==我们不想让任何数据库或编程语言领域的人溜进来，所以我们需要确保只有经过我们批准的客户才能进门。==

How might we do that?
==我们该如何做到这一点？==

One way would be to hire a massive intimidating bouncer who has a list of all the approved members.
==一种方法是雇佣一名身材魁梧、令人生畏的保镖，他手里有一份所有获批会员的名单。==

When someone wants to enter the club, they would prove their identity to the bouncer, and the bouncer would see if they were on the list.
==当有人想要进入俱乐部时，他们会向保镖证明自己的身份，保镖会查看他们是否在名单上。==

If it was Linus Torvalds or Barbara Liskov, the bouncer would let them in, but would keep out the hoi polloi networking folks who had failed to distinguish themselves in operating systems.
==如果是 Linus Torvalds 或 Barbara Liskov，保镖会让他们进去，但会将那些在操作系统领域没能出人头地的普通网络技术人员拒之门外。==

Another approach would be to put a really great lock on the door of the club and hand out keys to that lock to all of our OS buddies.
==另一种方法是在俱乐部的门上装一把非常高级的锁，并向我们所有的操作系统伙伴发放该锁的钥匙。==

If Jerome Saltzer wanted to get in to Chez Andrea, he’d merely pull out his key and unlock the door.
==如果 Jerome Saltzer 想进入 Chez Andrea，他只需要掏出钥匙开门即可。==

If some computer architects with no OS chops wanted to get in, they wouldn’t have a key and thus would be stuck outside.
==如果一些没有操作系统功底的计算机架构师想进来，他们就没有钥匙，因此会被困在门外。==

Compared to the other approach, we’d save on the salary of the bouncer, though we would have to pay for the locks and keys.
==与另一种方法相比，我们可以节省保镖的薪水，尽管我们必须支付锁和钥匙的费用。==

As new luminaries in the OS field emerge who we want to admit, we’ll need new keys for them, and once in a while we may make a mistake and hand out a key to someone who doesn’t deserve it, or a member might lose a key, in which case we need to make sure that key no longer opens the club door.
==随着我们想要接纳的操作系统领域新泰斗的出现，我们需要为他们提供新钥匙，而且偶尔我们可能会犯错，把钥匙发给不该给的人，或者会员可能会丢失钥匙，在这种情况下，我们需要确保该钥匙不再能打开俱乐部的门。==

The same ideas can be used in computer systems.
==同样的理念可以应用于计算机系统。==

Early computer scientists decided to call the approach that’s kind of like locks and keys a **capability-based system**, while the approach based on the bouncer and the list of those to admit was called an **access control list system**.
==早期的计算机科学家决定将那种有点像锁和钥匙的方法称为**基于权标（能力）的系统**（capability-based system），而将基于保镖和准入名单的方法称为**访问控制列表系统**（access control list system）。==

Capabilities are thus like keys, or tickets to a movie, or tokens that let you ride a subway.
==因此，权标就像钥匙、电影票或让你乘坐地铁的代币。==

Access control lists are thus like, well, lists.
==访问控制列表就像，嗯，列表。==

How does this work in an operating system?
==这在操作系统中是如何运作的？==

If you’re using capabilities, when a process belonging to user X wants to read and write file `/tmp/foo`, it hands a capability specific to that file to the system.
==如果你使用的是权标，当属于用户 X 的进程想要读取和写入文件 `/tmp/foo` 时，它会向系统提交一个针对该文件的权标。==

(And precisely what, you may ask, is a capability in this context? Good question! We’ll get to that.)
==（你可能会问，在这种情况下权标究竟是什么？好问题！我们稍后会讨论。）==

If you’re using access control lists (**ACLs**, for short), the system looks up user X on an ACL associated with `/tmp/foo`, only allowing the access if the user is on the list.
==如果你使用的是访问控制列表（简称 **ACL**），系统会在与 `/tmp/foo` 关联的 ACL 中查找用户 X，只有当用户在列表中时才允许访问。==

In either case, the check can be made at the moment the access (an `open()` call, in our example) is requested.
==在任何一种情况下，都可以在请求访问（在我们的示例中是 `open()` 调用）的时刻进行检查。==

The check is made after trapping to the operating system, but before the access is actually permitted, with an early exit and error code returned if the access control check fails.
==检查是在陷（trap）入操作系统之后、但在实际允许访问之前进行的，如果访问控制检查失败，则会提前退出并返回错误代码。==

At a high level, these two options may not sound very different, but when you start thinking about the algorithm you’ll need to run and the data structures required to support that algorithm, you’ll quickly see that there are major differences.
==在高级别上，这两个选项听起来可能没有太大区别，但当你开始思考需要运行的算法以及支持该算法所需的数据结构时，你很快就会发现存在重大差异。==

Let’s walk through each in turn.
==让我们依次介绍每一个。==




==### 第一步：数据清洗 & 第二步：句子切分 & 第三步：输出格式==

**ASIDE: THE ANDROID ACCESS CONTROL MODEL**
==**侧记：Android 访问控制模型**==

The Android system is one of the leading software platforms for today’s mobile computing devices, especially smart phones.
==Android 系统是当今移动计算设备（尤其是智能手机）领先的软件平台之一。==

These devices pose different access control challenges than classic server computers, or even personal desktop computers or laptops.
==与传统的服务器计算机，甚至个人台式机或笔记本电脑相比，这些设备提出了不同的访问控制挑战。==

Their functionality is based on the use of many relatively small independent applications, commonly called apps, that are downloaded, installed, and run on a device belonging to only a single user.
==它们的功能基于许多相对较小的独立应用程序（通常称为 app）的使用，这些程序在仅属于单个用户的设备上下载、安装和运行。==

Thus, there is no issue of protecting multiple users on one machine from each other.
==因此，不存在保护一台机器上的多个用户免受彼此侵害的问题。==

If one used a standard access control model, these apps would run under that user’s identity.
==如果使用标准的访问控制模型，这些 app 将在该用户的身份下运行。==

But apps are developed by many entities, and some may be malicious.
==但是 app 是由许多实体开发的，其中一些可能是恶意的。==

Further, most apps have no legitimate need for most of the resources on the device.
==此外，大多数 app 对设备上的大多数资源并没有合法的需求。==

If they are granted too many privileges, a malicious app can access the phone owner’s contacts, make phone calls, or buy things over the network, among many other undesirable behaviors.
==如果它们被授予过多的权限，恶意 app 可能会访问手机所有者的联系人、拨打电话或通过网络购买物品，以及许多其他不良行为。==

The principle of least privilege implies that we should not give apps the full privileges belonging to owner, but they must have some privileges if they are to do anything interesting.
==最小权限原则意味着我们不应该给予 app 属于所有者的全部权限，但如果它们要执行任何有意义的操作，就必须拥有某些权限。==

Android runs on top of a version of Linux, and an application’s access limitations are achieved in part by generating a new user ID for each installed app.
==Android 运行在 Linux 的一个版本之上，应用程序的访问限制部分是通过为每个安装的 app 生成一个新的用户 ID（UID）来实现的。==

The app runs under that ID and its accesses can be controlled on that basis.
==该 app 在该 ID 下运行，其访问权限可以基于此进行控制。==

However, the Android middleware offers additional facilities for controlling access.
==然而，Android 中间件提供了额外的设施来控制访问。==

Application developers define accesses required by their app.
==应用程序开发人员定义其 app 所需的访问权限。==

When a user considers installing an app on their device, they are shown what permissions it requires.
==当用户考虑在他们的设备上安装 app 时，系统会向他们显示该 app 所需的权限。==

The user can either grant the app those permissions, not install the app, or limit its permissions, though the latter choice may also limit app utility.
==用户可以选择授予该 app 这些权限、不安装该 app 或限制其权限，尽管后一种选择也可能限制 app 的实用性。==

Also, the developer specifies ways in which other apps can communicate with the new app.
==此外，开发人员还会指定其他 app 与该新 app 通信的方式。==

The data structure used to encode this access information is called a **permission label**.
==用于编码此访问信息的数据结构称为**权限标签**。==

An app’s permission labels (both what it can access and what it provides to others) are set at app design time, and encoded into a particular Android system at the moment the app is installed on that machine.
==app 的权限标签（包括它可以访问的内容以及它为他人提供的内容）在 app 设计阶段设定，并在 app 安装到该机器的那一刻编码到特定的 Android 系统中。==

Permission labels are thus like capabilities, since possession of them by the app allows the app to do something, while lacking a label prevents the app from doing that thing.
==因此，权限标签类似于权能（capabilities），因为 app 拥有它们就可以执行某些操作，而缺少标签则会阻止 app 执行该操作。==

An app’s set of permission labels is set statically at install time.
==app 的权限标签集在安装时静态设定。==

The user can subsequently change those permissions, although limiting them may damage app functionality.
==用户随后可以更改这些权限，尽管限制它们可能会损害 app 的功能。==

Permission labels are a form of mandatory access control.
==权限标签是强制访问控制（MAC）的一种形式。==

The Android security model is discussed in detail by Enck et al. [E+09].
==Enck 等人 [E+09] 详细讨论了 Android 安全模型。==

The Android security approach is interesting, but not perfect.
==Android 的安全方法很有趣，但并不完美。==

In particular, users are not always aware of the implications of granting an application access to something, and, faced with the choice of granting the access or not being able to effectively use the app, they will often grant it.
==特别是，用户并不总是意识到授予应用程序访问权限的后果，面对“授予访问权限”或“无法有效使用 app”的选择时，他们通常会选择授予。==

This behavior can be problematic, if the app is malicious.
==如果该 app 是恶意的，这种行为可能会带来问题。==

If desired, the owner can alter that initial ACL, but experience shows that users rarely do.
==如果需要，所有者可以更改初始 ACL，但经验表明用户很少这样做。==

This tendency demonstrates the importance of properly chosen defaults.
==这种倾向证明了正确选择默认设置的重要性。==

Here, as in many other places in an operating system, a theoretically changeable or tunable setting will, in practice, be used unaltered by almost everyone almost always.
==在这里，正如操作系统中的许多其他地方一样，理论上可以更改或调整的设置，在实践中几乎总是被几乎所有人原封不动地使用。==

However, while many will never touch access controls on their resources, for an important set of users and systems these controls are of vital importance to achieve their security goals.
==然而，虽然许多人永远不会触动其资源的访问控制，但对于一组重要的用户和系统来说，这些控制对于实现其安全目标至关重要。==

Even if you mostly rely on defaults, many software installation packages use some degree of care in setting access controls on executables and configuration files they create.
==即使你主要依赖默认设置，许多软件安装包在为它们创建的可执行文件和配置文件设置访问控制时也会表现出一定程度的谨慎。==

Generally, you should exercise caution in fiddling around with access controls in your system.
==通常，你在摆弄系统中的访问控制时应保持谨慎。==

If you don’t know what you’re doing, you might expose sensitive information or allow attackers to alter critical system settings.
==如果你不知道自己在做什么，你可能会暴露敏感信息或允许攻击者更改关键的系统设置。==

If you tighten existing access controls, you might suddenly cause a bunch of daemon programs running in the background to stop working.
==如果你收紧现有的访问控制，你可能会突然导致一堆在后台运行的守护进程停止工作。==

One practical issue that many large institutions discovered when trying to use standard access control methods to implement their security policies is that people performing different roles within the organization require different privileges.
==许多大型机构在尝试使用标准访问控制方法实施其安全策略时发现的一个实际问题是，在组织内履行不同角色的人员需要不同的权限。==

For example, in a hospital, all doctors might have a set of privileges not given to all pharmacists, who themselves have privileges not given to the doctors.
==例如，在医院中，所有医生可能拥有一组不给予所有药剂师的权限，而药剂师本身也拥有不给予医生的权限。==

Organizing access control on the basis of such roles and then assigning particular users to the roles they are allowed to perform makes implementation of many security policies easier.
==基于此类角色组织访问控制，然后将特定用户分配给他们被允许履行的角色，可以使许多安全策略的实施变得更加容易。==

This approach is particularly valuable if certain users are permitted to switch roles depending on the task they are currently performing, since then one need not worry about setting or changing the individual’s access permissions on the fly, but simply switch their role from one to another.
==如果允许某些用户根据当前执行的任务切换角色，这种方法就特别有价值，因为这样就不必担心随时设置或更改个人的访问权限，而只需将他们的角色从一个切换到另一个即可。==

Usually they will hold the role’s permission only as long as they maintain that role.
==通常，他们只有在维持该角色期间才持有该角色的权限。==

Once they exit the particular role (perhaps to enter a different role with different privileges), they lose the privileges of the role they exit.
==一旦他们退出特定角色（也许是为了进入具有不同权限的其他角色），他们就会失去所退出角色的权限。==

This observation led to the development of **Role-Based Access Control**, or **RBAC**.
==这一观察结果导致了**基于角色的访问控制**（即 **RBAC**）的发展。==

The core ideas had been around for some time before they were more formally laid out in a research paper by Ferraiolo and Kuhn [FK92].
==核心理念已经存在了一段时间，直到 Ferraiolo 和 Kuhn [FK92] 的一篇研究论文更正式地阐述了它们。==

Now RBAC is in common use in many organizations, particularly large ones.
==现在 RBAC 在许多组织中得到了普遍应用，特别是大型组织。==

Large organizations face more serious management challenges than small ones, so approaches like RBAC that allow groups of users to be dealt with in one operation can significantly ease the management task.
==大型组织面临比小型组织更严重的管理挑战，因此像 RBAC 这样允许通过一次操作处理用户组的方法可以显著减轻管理任务。==

For example, if a company determines that all programmers should be granted access to a new library that has been developed, but accountants should not, RBAC would achieve this effect with a single operation that assigns the necessary privilege to the Programmer role.
==例如，如果一家公司决定所有程序员都应被授予访问新开发库的权限，而会计师则不应获得，RBAC 就可以通过将必要权限分配给“程序员”角色的单次操作来实现这一效果。==

If a programmer is promoted to a management position for which access to the library is unnecessary, the company can merely remove the Programmer role from the set of roles the manager could take on.
==如果一名程序员被提升到不需要访问该库的管理职位，公司只需从该经理可以担任的角色集中移除“程序员”角色即可。==

Such restrictions do not necessarily imply that you suspect your accountants of being dishonest and prone to selling your secret library code to competitors.
==此类限制并不一定意味着你怀疑你的会计师不诚实并倾向于将你的秘密库代码出售给竞争对手。==

Remember the principle of least privilege: when you give someone access to something, you are relying not just on their honesty, but on their caution.
==记住最小权限原则：当你给予某人访问某物的权限时，你不仅依赖于他们的诚实，还依赖于他们的谨慎。==

If accountants can’t access the library at all, then neither malice nor carelessness on their part can lead to an accountant’s privileges leaking your library code.
==如果会计师根本无法访问该库，那么他们的恶意或疏忽都不会导致会计师的权限泄露你的库代码。==

Least privilege is not just a theoretically good idea, but a vital part of building secure systems in the real world.
==最小权限不仅是一个理论上的好主意，而且是构建现实世界安全系统的重要组成部分。==

**56.1 Introduction**
==**56.1 引言**==

In previous chapters, we’ve discussed clarifying your security goals, determining your security policies, using authentication mechanisms to identify principals, and using access control mechanisms to enforce policies concerning which principals can access which computer resources in which ways.
==在之前的章节中，我们讨论了明确安全目标、确定安全策略、使用身份验证机制识别主体，以及使用访问控制机制执行有关哪些主体可以以何种方式访问哪些计算机资源的策略。==

While we identified a number of shortcomings and problems inherent in all of these elements of securing your system, if we regard those topics as covered, what’s left for the operating system to worry about, from a security perspective?
==虽然我们发现了保护系统安全的这些要素中固有的许多缺点和问题，但如果我们认为这些主题已经讲完了，从安全的角度来看，操作系统还有什么需要担心的呢？==

Why isn’t that everything?
==为什么那不是全部？==

There are a number of reasons why we need more.
==我们需要更多手段的原因有很多。==

Of particular importance: not everything is controlled by the operating system.
==特别重要的一点是：并非所有内容都受操作系统控制。==

But perhaps you respond, you told me the operating system is all-powerful!
==但也许你会回应说，你告诉过我操作系统是无所不能的！==

Not really.
==并不完全是这样。==

It has substantial control over a limited domain – the hardware on which it runs, using the interfaces of which it is given control.
==它对有限的领域拥有实质性的控制权——即它所运行的硬件，使用的是它被赋予控制权的接口。==

It has no real control over what happens on other machines, nor what happens if one of its pieces of hardware is accessed via some mechanism outside the operating system’s control.
==它无法真正控制其他机器上发生的事情，也无法控制如果其硬件组件之一通过操作系统控制之外的某种机制被访问时会发生什么。==

But how can we expect the operating system to protect something when the system does not itself control access to that resource?
==但是，当系统本身不控制对该资源的访问时，我们如何期望操作系统保护某些东西呢？==

The answer is to prepare the resource for trouble in advance.
==答案是提前为资源可能遇到的麻烦做好准备。==

In essence, we assume that we are going to lose the data, or that an opponent will try to alter it improperly.
==从本质上讲，我们假设我们会丢失数据，或者对手会尝试不当地更改数据。==

And we take steps to ensure that such actions don’t cause us problems.
==我们采取措施确保此类操作不会给我们带来问题。==

The key observation is that if an opponent cannot understand the data in the form it is obtained, our secrets are safe.
==关键的观察结果是，如果对手无法理解所获数据形式的含义，我们的秘密就是安全的。==

Further, if the attacker cannot understand it, it probably can’t be altered, at least not in a controllable way.
==此外，如果攻击者无法理解它，它可能就无法被更改，至少不能以可控的方式更改。==

If the attacker doesn’t know what the data means, how can it be changed into something the attacker prefers?
==如果攻击者不知道数据的含义，又怎么能将其更改为攻击者喜欢的样子呢？==

The core technology we’ll use is **cryptography**, a set of techniques to convert data from one form to another, in controlled ways with expected outcomes.
==我们将使用的核心技术是**密码学**，这是一组以可控方式将数据从一种形式转换为另一种形式并产生预期结果的技术。==

We will convert the data from its ordinary form into another form using cryptography.
==我们将使用密码学将数据从普通形式转换为另一种形式。==

If we do it right, the opponent will not be able to determine what the original data was by examining the protected form.
==如果我们做得正确，对手将无法通过检查受保护的形式来确定原始数据是什么。==

**56.2 Cryptography**
==**56.2 密码学**==

The basic idea behind cryptography is to take a piece of data and use an algorithm (often called a **cipher**), usually augmented with a second piece of information (which is called a **key**), to convert the data into a different form.
==密码学背后的基本思想是获取一段数据，并使用一种算法（通常称为**加密算法**或**置换**），通常辅以第二段信息（称为**密钥**），将数据转换为不同的形式。==

The new form should look nothing like the old one, but, typically, we want to be able to run another algorithm, again augmented with a second piece of information, to convert the data back to its original form.
==新形式看起来应该与旧形式完全不同，但通常我们希望能够运行另一种算法，同样辅以第二段信息，将数据转换回其原始形式。==

Let’s formalize that just a little bit.
==让我们将其稍加形式化。==

We start with data $P$ (which we usually call the **plaintext**), a key $K$, and an encryption algorithm $E()$.
==我们从数据 $P$（我们通常称之为**明文**）、密钥 $K$ 和加密算法 $E()$ 开始。==

We end up with $C$, the altered form of $P$, which we usually call the **ciphertext**:
==我们最终得到 $C$，即 $P$ 的改变形式，我们通常称之为**密文**：==

$C = E(P, K)$ (56.1)
$C = E(P, K)$ (56.1)

The reverse transformation takes $C$, which we just produced, a decryption algorithm $D()$, and the key $K$:
==反向转换采用我们刚刚生成的 $C$、解密算法 $D()$ 和密钥 $K$：==

$P = D(C, K)$ (56.2)
$P = D(C, K)$ (56.2)

It sounds like we’ve thrown away half our protection, since now the cryptography’s benefit relies entirely on the secrecy of the key.
==听起来我们好像丢掉了一半的保护，因为现在密码学的益处完全取决于密钥的保密性。==

Precisely.
==确实如此。==

Let’s say that again in all caps, since it’s so important that you really need to remember it: **THE CRYPTOGRAPHY’S BENEFIT RELIES ENTIRELY ON THE SECRECY OF THE KEY.**
==让我们用全大写再重复一遍，因为它是如此重要，你真的需要记住它：**密码学的益处完全取决于密钥的保密性。**==

**56.4 Cryptographic Hashes**
==**56.4 密码哈希**==

So to be particularly careful, we can use a **cryptographic hash** to ensure integrity.
==因此，为了特别小心，我们可以使用**密码哈希**来确保完整性。==

Cryptographic hashes are a special category of hash functions with several important properties:
==密码哈希是一类特殊的哈希函数，具有几个重要的属性：==

* It is computationally infeasible to find two inputs that will produce the same hash value.
==* 在计算上无法找到两个能产生相同哈希值的输入。==

* Any change to an input will result in an unpredictable change to the resulting hash value.
==* 对输入的任何更改都将导致生成的哈希值发生不可预测的更改。==

* It is computationally infeasible to infer any properties of the input based only on the hash value.
==* 仅根据哈希值在计算上无法推断出输入的任何属性。==

To formalize it a bit, to perform a cryptographic hash we take a plaintext $P$ and a hashing algorithm $H()$.
==为了将其形式化，执行密码哈希时，我们采用明文 $P$ 和哈希算法 $H()$。==

$S = H(P)$ (56.5)
$S = H(P)$ (56.5)

**56.7 At-Rest Data Encryption**
==**56.7 静态数据加密**==

One common general use of at-rest data encryption is called **full disk encryption**.
==静态数据加密的一个常见通用用途称为**全盘加密**。==

This usually means that the entire contents (or almost the entire contents) of the storage device are encrypted.
==这通常意味着存储设备的全部内容（或几乎全部内容）都被加密。==

Windows BitLocker and Apple’s FileVault are examples of software-based full disk encryption.
==Windows BitLocker 和 Apple 的 FileVault 是基于软件的全盘加密示例。==




The data remains decrypted as long as it is stored anywhere in the machine’s memory, including in shared buffers or user address space.
==只要数据存储在机器内存中的任何地方，包括共享缓冲区或用户地址空间中，它就会保持解密状态。==

When new data is to be sent to the device, it is first encrypted.
==当新数据要发送到设备时，会先对其进行加密。==

The data is never placed on the storage device in decrypted form.
==数据绝不会以解密形式放置在存储设备上。==

After the initial request to obtain the decryption key is performed, encryption and decryption are totally transparent to users and applications.
==在执行获取解密密钥的初始请求后，加密和解密对用户和应用程序来说是完全透明的。==

They never see the data in encrypted form and are not asked for the key again, until the machine reboots.
==直到机器重启之前，他们绝不会看到加密形式的数据，也不会被再次要求提供密钥。==

Cryptography is a computationally expensive operation, particularly if performed in software.
==密码学是一项计算开销巨大的操作，特别是在通过软件执行时。==

There will be overhead associated with performing software-based **full disk encryption**.
==执行基于软件的**全盘加密**会产生相关的开销。==

Reports of the amount of overhead vary, but a few percent extra latency for disk-heavy operations is common.
==关于开销大小的报告各不相同，但对于磁盘密集型操作，增加几个百分点的延迟是很常见的。==

For operations making less use of the disk, the overhead may be imperceptible.
==对于较少使用磁盘的操作，开销可能是察觉不到的。==

For hardware-based full disk encryption, the rated speed of the disk drive will be achieved, which may or may not be slower than a similar model not using full disk encryption.
==对于基于硬件的全盘加密，可以达到磁盘驱动器的额定速度，这可能比不使用全盘加密的类似型号慢，也可能不慢。==

**What does this form of encryption protect against?**
==**这种形式的加密能防范什么？**==

* It offers no extra protection against users trying to access data they should not be allowed to see.
==* 它对于试图访问其无权查看的数据的用户不提供额外保护。==

Either the standard access control mechanisms that the operating system provides work (and such users can’t get to the data because they lack access permissions) or they don’t (in which case such users will be given equal use of the decryption key as anyone else).
==要么操作系统提供的标准访问控制机制有效（此类用户因缺乏访问权限而无法获取数据），要么失效（在这种情况下，此类用户将获得与其他人同等的解密密钥使用权）。==

* It does not protect against flaws in applications that divulge data.
==* 它不能防范泄露数据的应用程序漏洞。==

Such flaws will permit attackers to pose as the user, so if the user can access the unencrypted data, so can the attacker.
==此类漏洞将允许攻击者冒充用户，因此如果用户可以访问未加密的数据，攻击者也可以。==

For example, it offers little protection against buffer overflows or SQL injections.
==例如，它对于缓冲区溢出或 SQL 注入几乎没有保护作用。==

* It does not protect against dishonest privileged users on the system, such as a system administrator.
==* 它不能防范系统中的不诚实特权用户，例如系统管理员。==

Administrator’s privileges may allow the admin to pose as the user who owns the data or to install system components that provide access to the user’s data; thus, the admin could access decrypted copies of the data on request.
==管理员的权限可能允许其冒充拥有数据的用户，或安装能够访问用户数据的系统组件；因此，管理员可以根据需要访问数据的解密副本。==

* It does not protect against security flaws in the OS itself.
==* 它不能防范操作系统本身的安全性漏洞。==

Once the key is provided, it is available (directly in memory, or indirectly by asking the hardware to use it) to the operating system, whether that OS is trustworthy and secure or compromised and insecure.
==一旦提供了密钥，操作系统就可以获取它（直接在内存中，或间接通过要求硬件使用它），无论该操作系统是可信安全的，还是已被攻破且不安全的。==

So what benefit does this form of encryption provide?
==那么这种形式的加密能提供什么好处呢？==

Consider this situation.
==考虑这种情况。==

If a hardware device storing data is physically moved from one machine to another, the OS on the other machine is not obligated to honor the access control information stored on the device.
==如果存储数据的硬件设备被物理地从一台机器移动到另一台机器，另一台机器上的操作系统没有义务遵守存储在设备上的访问控制信息。==

In fact, it need not even use the same file system to access that device.
==事实上，它甚至不需要使用相同的文件系统来访问该设备。==

For example, it can treat the device as merely a source of raw data blocks, rather than an organized file system.
==例如，它可以将该设备仅仅视为原始数据块的来源，而不是一个有组织的文件系统。==

So any access control information associated with files on the device might be ignored by the new operating system.
==因此，与设备上的文件相关的任何访问控制信息都可能被新的操作系统忽略。==

However, if the data on the device is encrypted via full disk encryption, the new machine will usually be unable to obtain the encryption key.
==然而，如果设备上的数据通过全盘加密进行了加密，新机器通常将无法获得加密密钥。==

It can access the raw blocks, but they are encrypted and cannot be decrypted without the key.
==它可以访问原始数据块，但它们是加密的，没有密钥就无法解密。==

This benefit would be useful if the hardware in question was stolen and moved to another machine, for example.
==例如，如果相关硬件被盗并转移到另一台机器，这种优势就会非常有用。==

This situation is a very real possibility for mobile devices, which are frequently lost or stolen.
==对于经常丢失或被盗的移动设备来说，这种情况极有可能发生。==

Disk drives are sometimes resold, and data belonging to the former owner (including quite sensitive data) has been found on them by the re-purchaser.
==磁盘驱动器有时会被转售，而重新购买者曾在其上发现属于前任所有者的数据（包括非常敏感的数据）。==

These are important cases where full disk encryption provides real benefits.
==在这些重要案例中，全盘加密提供了实实在在的好处。==

For other forms of encryption of data at rest, the system must still address the issues of how much is encrypted, how to obtain the key, and when to encrypt and decrypt the data, with different types of protection resulting depending on how these questions are addressed.
==对于其他形式的静态数据加密，系统仍必须解决加密多少内容、如何获取密钥以及何时加密和解密数据的问题，根据这些问题的处理方式，会产生不同类型的保护效果。==

Generally, such situations require that some software ensures that the unencrypted form of the data is no longer stored anywhere, including caches, and that the cryptographic key is not available to those who might try to illicitly access the data.
==通常，此类情况要求某些软件确保数据的未加密形式不再存储在任何地方（包括缓存中），并且加密密钥对于那些可能试图非法访问数据的人不可用。==

There are relatively few circumstances where such protection is of value, but there are a few common examples:
==此类保护具有价值的情况相对较少，但有几个常见的例子：==

* **Archiving data** that might need to be copied and must be preserved, but need not be used.
==* **归档数据**：这些数据可能需要复制且必须保存，但不需要使用。==

In this case, the data can be encrypted at the time of its creation, and perhaps never decrypted, or only decrypted under special circumstances under the control of the data’s owner.
==在这种情况下，数据可以在创建时进行加密，并且可能永远不解密，或者仅在数据所有者控制下的特殊情况下解密。==

If the machine was uncompromised when the data was first encrypted and the key is not permanently stored on the system, the encrypted data is fairly safe.
==如果在数据首次加密时机器未被攻破，且密钥没有永久存储在系统中，那么加密数据是相当安全的。==

Note, however, that if the key is lost, you will never be able to decrypt the archived data.
==但请注意，如果密钥丢失，你将永远无法解密归档数据。==

* **Storing sensitive data in a cloud computing facility**, a variant of the previous example.
==* **在云计算设施中存储敏感数据**：这是前一个例子的变体。==

If one does not completely trust the cloud computing provider (or one is uncertain of how careful that provider is – remember, when you trust another computing element, you’re trusting not only its honesty, but also its carefulness and correctness), encrypting the data before sending it to the cloud facility is wise.
==如果不完全信任云计算提供商（或者不确定该提供商有多细心——请记住，当你信任另一个计算元素时，你不仅是在信任它的诚实，还在信任它的细心和正确性），那么在将数据发送到云设施之前对其进行加密是明智的。==

Many cloud backup products include this capability.
==许多云备份产品都包含此功能。==

In this case, the cryptography and key use occur before moving the data to the untrusted system, or after it is recovered from that system.
==在这种情况下，密码操作和密钥使用发生在将数据移动到不可信系统之前，或者从该系统恢复数据之后。==

* **User-level encryption performed through an application.**
==* **通过应用程序执行的用户级加密。**==

For example, a user might choose to encrypt an email message, with any stored version of it being in encrypted form.
==例如，用户可能会选择加密一封电子邮件，其任何存储版本都将以加密形式存在。==

In this case, the cryptography will be performed by the application, and the user will do something to make a cryptographic key available to the application.
==在这种情况下，密码操作将由应用程序执行，用户将执行某些操作使加密密钥对应用程序可用。==

Ideally, that application will ensure that the unencrypted form of the data and the key used to encrypt it are no longer readily available after encryption is completed.
==理想情况下，该应用程序将确保在加密完成后，数据的未加密形式和用于加密它的密钥不再易于获取。==

Remember, however, that while the key exists, the operating system can obtain access to it without your application knowing.
==但请记住，只要密钥存在，操作系统就可以在你的应用程序不知情的情况下获取它的访问权限。==

One important special case for encrypting selected data at rest is a **password vault** (also known as a **key ring**), which we discussed in the authentication chapter.
==针对特定静态数据进行加密的一个重要特例是**密码保险箱**（也称为**钥匙环**），我们在身份验证章节中讨论过它。==

Typical users interact with many remote sites that require them to provide passwords (authentication based on “what you know”, remember?).
==典型用户会与许多远程站点交互，这些站点要求他们提供密码（还记得基于“你所知道的信息”的身份验证吗？）。==

The best security is achieved if one uses a different password for each site, but doing so places a burden on the human user, who generally has a hard time remembering many passwords.
==如果为每个站点使用不同的密码，可以实现最佳的安全性，但这样做会给人类用户带来负担，他们通常很难记住许多密码。==

A solution is to encrypt all the different passwords and store them on the machine, indexed by the site they are used for.
==一种解决方案是加密所有不同的密码并将它们存储在机器上，并按所使用的站点进行索引。==

When one of the passwords is required, it is decrypted and provided to the site that requires it.
==当需要其中一个密码时，会将其解密并提供给需要的站点。==

For password vaults and all such special cases, the system must have some way of obtaining the required key whenever data needs to be encrypted or decrypted.
==对于密码保险箱和所有此类特例，系统必须有某种方法在需要加密或解密数据时获取所需的密钥。==

If an attacker can obtain the key, the cryptography becomes useless, so safe storage of the key becomes critical.
==如果攻击者能够获得密钥，密码学就会变得毫无用处，因此密钥的安全存储变得至关重要。==

Typically, if the key is stored in unencrypted form anywhere on the computer in question, the encrypted data is at risk, so well designed encryption systems tend not to do so.
==通常，如果密钥以未加密的形式存储在相关计算机的任何位置，加密数据就会面临风险，因此设计良好的加密系统往往不会这样做。==

For example, in the case of password vaults, the key used to decrypt the passwords is not stored in the machine’s stable storage.
==例如，在密码保险箱的情况下，用于解密密码的密钥不会存储在机器的稳定存储器中。==

It is obtained by asking the user for it when required, or asking for a passphrase used to derive the key.
==它是通过在需要时向用户询问，或询问用于派生密钥的密码短语来获取的。==

The key is then used to decrypt the needed password.
==然后使用该密钥来解密所需的密码。==

Maximum security would suggest destroying the key as soon as this decryption was performed (remember the principle of **least privilege**?), but doing so would imply that the user would have to re-enter the key each time a password was needed (remember the principle of **acceptability**?).
==最大程度的安全性建议在执行解密后立即销毁密钥（还记得**最小特权**原则吗？），但这样做意味着每当需要密码时，用户都必须重新输入密钥（还记得**可接受性**原则吗？）。==

A compromise between usability and security is reached, in most cases, by remembering the key after first entry for a significant period of time, but only keeping it in RAM.
==在大多数情况下，通过在首次输入后将其记住一段相当长的时间，但仅保存在 RAM 中，可以在可用性和安全性之间达成折中。==

When the user logs out, or the system shuts down, or the application that handles the password vault (such as a web browser) exits, the key is “forgotten.”
==当用户注销、系统关闭或处理密码保险箱的应用程序（如网络浏览器）退出时，密钥就会被“遗忘”。==

This approach is reminiscent of single sign-on systems, where a user is asked for a password when the system is first accessed, but is not required to re-authenticate again until logging out.
==这种方法让人联想到单点登录系统，用户在首次访问系统时会被要求输入密码，但在注销之前不需要再次重新认证。==

It has the same disadvantages as those systems, such as permitting an unattended terminal to be used by unauthorized parties to use someone else’s access permissions.
==它具有与这些系统相同的缺点，例如允许无人看管的终端被未经授权的方利用，从而使用他人的访问权限。==

Both have the tremendous advantage that they don’t annoy their users so much that they are abandoned in favor of systems offering no security whatsoever.
==两者的巨大优势在于，它们不会让用户感到如此厌烦，以至于被弃用而转向完全不提供安全性的系统。==

### 56.8 Cryptographic Capabilities
==### 56.8 密码学能力（Capabilities）==

Remember from our chapter on access control that **capabilities** had the problem that we could not leave them in users’ hands, since then users could forge them and grant themselves access to anything they wanted.
==记得我们在访问控制章节中提到过，**能力（capabilities）**存在一个问题，即我们不能将它们留在用户手中，因为那时用户可以伪造它们并授予自己访问任何内容的权限。==

Cryptography can be used to create unforgeable capabilities.
==密码学可以用来创建不可伪造的能力。==

A trusted entity could use cryptography to create a sufficiently long and securely encrypted data structure that indicated that the possessor was allowed to have access to a particular resource.
==受信任的实体可以使用密码学创建一个足够长且安全加密的数据结构，该结构表明持有者被允许访问特定资源。==

This data structure could then be given to a user, who would present it to the owner of the matching resource to obtain access.
==然后可以将此数据结构交给用户，用户将其呈现给匹配资源的所有者以获得访问权限。==

The system that actually controlled the resource must be able to check the validity of the data structure before granting access, but would not need to maintain an access control list.
==实际控制资源的系统必须能够在授予访问权限之前检查数据结构的有效性，但不需要维护访问控制列表。==

Such cryptographic capabilities could be created either with symmetric or public key cryptography.
==此类密码学能力既可以使用对称加密创建，也可以使用公钥加密创建。==

With symmetric cryptography, both the creator of the capability and the system checking it would need to share the same key.
==使用对称加密时，能力的创建者和检查能力的系统都需要共享同一个密钥。==

This option is most feasible when both of those entities are the same system, since otherwise it requires moving keys around between the machines that need to use the keys, possibly at high speed and scale, depending on the use scenario.
==当这两个实体是同一个系统时，此选项最为可行，否则需要在需要使用密钥的机器之间移动密钥，根据使用场景的不同，可能需要高速且大规模地进行。==

One might wonder why the single machine would bother creating a cryptographic capability to allow access, rather than simply remembering that the user had passed an access check, but there are several possible reasons.
==人们可能会想，为什么单台机器要费力创建密码学能力来允许访问，而不是简单地记住用户已经通过了访问检查，但这有几个可能的原因。==

For example, if the machine controlling the resource worked with vast numbers of users, keeping track of the access status for each of them would be costly and complex, particularly in a distributed environment where the system needed to worry about failures and delays.
==例如，如果控制资源的机器处理大量用户，跟踪每个用户的访问状态将是昂贵且复杂的，特别是在系统需要担心故障和延迟的分布式环境中。==

Or if the system wished to give transferable rights to the access, as it might if the principal might move from machine to machine, it would be more feasible to allow the capability to move with the principal and be used from any location.
==或者，如果系统希望赋予可转移的访问权（例如当主体可能在机器之间移动时），那么允许能力随主体移动并在任何位置使用将更加可行。==

Symmetric cryptographic capabilities also make sense when all of the machines creating and checking them are inherently trusted and key distribution is not problematic.
==当所有创建和检查对称密码学能力的机器都是固有的受信任机器，且密钥分发不成问题时，这种能力也是有意义的。==

If public key cryptography is used to create the capabilities, then the creator and the resource controller need not be co-located and the trust relationships need not be as strong.
==如果使用公钥加密来创建这些能力，那么创建者和资源控制器不需要位于同一地点，信任关系也不需要那么强。==

The creator of the capability needs one key (typically the secret key) and the controller of the resource needs the other.
==能力的创建者需要一个密钥（通常是私钥），资源的控制器需要另一个密钥。==

If the content of the capability is not itself secret, then a true public key can be used, with no concern over who knows it.
==如果能力的内容本身不是秘密，那么可以使用真正的公钥，而无需担心谁知道它。==

If secrecy (or at least some degree of obscurity) is required, what would otherwise be a public key can be distributed only to the limited set of entities that would need to check the capabilities.
==如果需要保密（或至少是一定程度的模糊性），那么本应公开的密钥可以仅分发给需要检查这些能力的有限实体集。==

A resource manager could create a set of credentials (indicating which principal was allowed to use what resources, in what ways, for what period of time) and then encrypt them with a private key.
==资源管理器可以创建一组凭据（表明哪个主体被允许以何种方式、在什么时间段内使用什么资源），然后用私钥对其进行加密。==

Any one else can validate those credentials by decrypting them with the manager’s public key.
==任何其他人都可以通过使用管理器的公钥对其进行解密来验证这些凭据。==

As long as only the resource manager knows the private key, no one can forge capabilities.
==只要只有资源管理器知道私钥，就没有人能伪造能力。==

As suggested above, such cryptographic capabilities can hold a good deal of information, including expiration times, identity of the party who was given the capability, and much else.
==如上所述，此类密码学能力可以包含大量信息，包括到期时间、获得能力的方的身份等等。==

Since strong cryptography will ensure integrity of all such information, the capability can be relied upon.
==由于强大的密码学将确保所有此类信息的完整性，因此该能力是可以被信赖的。==

This feature allows the creator of the capability to prevent arbitrary copying and sharing of the capability, at least to a certain extent.
==此特性允许能力的创建者在一定程度上防止能力的随意复制和共享。==

For example, a cryptographic capability used in a network context can be tied to a particular IP address, and would only be regarded as valid if the message carrying it came from that address.
==例如，在网络环境中使用的密码学能力可以与特定的 IP 地址绑定，并且只有在携带它的消息来自该地址时才被视为有效。==

Many different encryption schemes can be used.
==可以使用许多不同的加密方案。==

The important point is that the encrypted capabilities must be long enough that it is computationally infeasible to find a valid capability by brute force enumeration or random guessing (e.g., the number of invalid bit patterns is $10^{15}$ times larger than the number of valid bit patterns).
==重要的一点是，加密的能力必须足够长，以至于通过穷举枚举或随机猜测来找到一个有效的能力在计算上是不可行的（例如，无效位模式的数量比有效位模式的数量大 $10^{15}$ 倍）。==

### 56.9 Summary
==### 56.9 总结==

Cryptography can offer certain forms of protection for data even when that data is no longer in a system’s custody.
==即使数据不再处于系统的监管之下，密码学也可以为数据提供某些形式的保护。==

These forms of protection include secrecy, integrity, and authentication.
==这些保护形式包括机密性、完整性和身份验证。==

Cryptography achieves such protection by converting the data’s original bit pattern into a different bit pattern, using an algorithm called a **cipher**.
==密码学通过使用一种称为**密码（cipher）**的算法，将数据的原始位模式转换为不同的位模式来实现此类保护。==

In most cases, the transformation can be reversed to obtain the original bit pattern.
==在大多数情况下，这种转换是可以逆转的，以获得原始位模式。==

**Symmetric ciphers** use a single secret key shared by all parties with rights to access the data.
==**对称密码**使用由所有有权访问数据的方共享的单个密钥。==

**Asymmetric ciphers** use one key to encrypt the data and a second key to decrypt the data, with one of the keys kept secret and the other commonly made public.
==**非对称密码**使用一个密钥加密数据，使用第二个密钥解密数据，其中一个密钥保密，另一个通常公开。==

**Cryptographic hashes**, on the other hand, do not allow reversal of the cryptography and do not require the use of keys.
==另一方面，**密码学哈希**不允许逆转加密过程，也不需要使用密钥。==

Strong ciphers make it computationally infeasible to obtain the original bit pattern without access to the required key.
==强大的密码使得在没有获取所需密钥的情况下，通过计算获取原始位模式是不可行的。==

For symmetric and asymmetric ciphers, this implies that only holders of the proper key can obtain the cipher’s benefits.
==对于对称和非对称密码，这意味着只有持有正确密钥的人才能获得密码带来的好处。==

Since cryptographic hashes have no key, this implies that no one should be able to obtain the original bit pattern from the hash.
==由于密码学哈希没有密钥，这意味着任何人都无法从哈希值中获取原始位模式。==

For operating systems, the obvious situations in which cryptography can be helpful are when data is sent to another machine, or when hardware used to store the data might be accessed without the intervention of the operating system.
==对于操作系统，密码学显然在以下情况中很有帮助：当数据被发送到另一台机器时，或者当用于存储数据的硬件可能在没有操作系统干预的情况下被访问时。==

In the latter case, data can be encrypted on the device (using either hardware or software), and decrypted as it is delivered to the operating system.
==在后一种情况下，数据可以在设备上加密（使用硬件或软件），并在交付给操作系统时解密。==

Ciphers are generally not secret, but rather are widely known and studied standards.
==密码通常不是秘密，而是广为人知且经过研究的标准。==

A cipher’s ability to protect data thus relies entirely on **key secrecy**.
==因此，密码保护数据的能力完全取决于**密钥的秘密性**。==

If attackers can learn, deduce, or guess the key, all protection is lost.
==如果攻击者能够获悉、推导或猜测出密钥，所有的保护都会丧失。==

Thus, extreme care in key selection and maintaining key secrecy is required if one relies on cryptography for protection.
==因此，如果依靠密码学进行保护，则需要极其谨慎地选择密钥并保持密钥的秘密性。==

A good principle is to store keys in as few places as possible, for as short a duration as possible, available to as few parties as possible.
==一个好的原则是将密钥存储在尽可能少的地方，持续时间尽可能短，并仅对尽可能少的方可用。==

### Chapter 57: Distributed System Security
==### 第 57 章：分布式系统安全==

### 57.1 Introduction
==### 57.1 简介==

An operating system can only control its own machine’s resources.
==一个操作系统只能控制它自己机器的资源。==

Thus, operating systems will have challenges in providing security in distributed systems, where more than one machine must cooperate.
==因此，在必须有多台机器协作的分布式系统中，操作系统在提供安全性方面将面临挑战。==

There are two large problems:
==存在两个大问题：==

* The other machines in the distributed system might not properly implement the security policies you want, or they might be adversaries impersonating trusted partners.
==* 分布式系统中的其他机器可能没有正确执行你想要的策略，或者它们可能是冒充受信任伙伴的敌手。==

We cannot control remote systems, but we still have to be able to trust validity of the credentials and capabilities they give us.
==我们无法控制远程系统，但我们仍必须能够信任它们提供给我们的凭据和能力的有效性。==

* Machines in a distributed system communicate across a network that none of them fully control and that, generally, cannot be trusted.
==* 分布式系统中的机器通过一个没有任何机器能完全控制且通常不可信的网络进行通信。==

Adversaries often have equal access to that network and can forge, copy, replay, alter, destroy, and delay our messages, and generally interfere with our attempts to use the network.
==敌手通常具有同等的网络访问权限，可以伪造、复制、重放、更改、销毁和延迟我们的消息，并通常会干扰我们使用网络的尝试。==

As suggested earlier, cryptography will be the major tool we use here, but we also said cryptography was hard to get right.
==如前所述，密码学将是我们在这里使用的主要工具，但我们也说过密码学很难做得正确。==

That makes it sound like the perfect place to use carefully designed standard tools, rather than to expect everyone to build their own.
==这听起来像是一个使用精心设计的标准工具的完美场所，而不是期望每个人都构建自己的工具。==

That’s precisely correct.
==这完全正确。==

**THE CRUX: HOW TO PROTECT DISTRIBUTED SYSTEM OPERATIONS**
==**核心问题：如何保护分布式系统操作**==

How can we secure a system spanning more than one machine?
==我们如何保护跨越多个机器的系统安全？==

What tools are available to help us protect such systems?
==有哪些工具可以帮助我们保护此类系统？==

How do we use them properly?
==我们如何正确使用它们？==

What are the areas in using the tools that require us to be careful and thoughtful?
==在使用这些工具时，哪些领域需要我们谨慎周到地考虑？==

### 57.2 The Role of Authentication
==### 57.2 身份验证的作用==

How can we handle our uncertainty about whether our partners in a distributed system are going to enforce our security policies?
==我们如何处理关于分布式系统中的伙伴是否会执行我们的安全策略的不确定性？==

In most cases, we can’t do much.
==在大多数情况下，我们无能为力。==

At best, we can try to arrange to agree on policies and hope everyone follows through on those agreements.
==充其量，我们可以尝试商定策略，并希望每个人都能履行这些协议。==

There are some special cases where we can get high-quality evidence that our partners have behaved properly, but that’s not easy, in general.
==在某些特殊情况下，我们可以获得伙伴行为得当的高质量证据，但总的来说这并不容易。==

For example, how can we know that they are using full disk encryption, or that they have carefully wiped an encryption key we are finished using, or that they have set access controls on the local copies of their files properly?
==例如，我们如何知道他们是否正在使用全盘加密，或者他们是否小心地擦除了我们使用完毕的加密密钥，或者他们是否正确地设置了其文件本地副本的访问控制？==

They can say they did, but how can we know?
==他们可以声称自己做了，但我们如何确定呢？==

Generally, we can’t.
==通常情况下，我们无法确定。==

But you’re used to that.
==但你已经习惯了。==

In the real world, your friends and relatives know some secrets about you, and they might have keys to get into your home, and if you loan them your car you’re fairly sure you’ll get it back.
==在现实世界中，你的朋友和亲戚知道你的一些秘密，他们可能有进入你家的钥匙，如果你把车借给他们，你相当确定能拿回来。==

That’s not so much because you have perfect mechanisms to prevent those trusted parties from behaving badly, but because you are pretty sure they won’t.
==这与其说是你有完美的机制来防止这些受信任的方行为不端，不如说是你非常确定他们不会。==

If you’re wrong, perhaps you can detect that they haven’t behaved well and take compensating actions (like changing your locks or calling the police to report your car stolen).
==如果你错了，也许你可以发现他们表现不好并采取补偿措施（比如换锁或报警报案车辆被盗）。==

We’ll need to rely on similar approaches in distributed computer systems.
==在分布式计算机系统中，我们需要依赖类似的方法。==

We will simply have to trust that some parties will behave well.
==我们只能相信某些方会表现良好。==

In some cases, we can detect when they don’t and adjust our trust in the parties accordingly, and maybe take other compensating actions.
==在某些情况下，我们可以检测到他们何时没有表现良好，并相应地调整我们对这些方的信任，或许还会采取其他补偿措施。==

Of course, in the cyber world, our actions are at a distance over a network, and all we see are bits going out and coming in on the network.
==当然，在网络世界中，我们的行为是通过网络远距离进行的，我们看到的只是网络上流出和流入的比特。==

For a trust-based solution to work, we have to be quite sure that the bits we send out can be verified by our buddies as truly coming from us, and we have to be sure that the bits coming in really were created by them.
==为了使基于信任的解决方案奏效，我们必须非常确定我们发送出去的比特可以被伙伴验证为真实来自我们，并且我们必须确定进入的比特确实是由他们创建的。==

That’s a job for **authentication**.
==这就是**身份验证（authentication）**的工作。==

As suggested in the earlier authentication chapter, when working over a network, we need to authenticate based on a bundle of bits.
==正如之前的身份验证章节所建议的，在通过网络工作时，我们需要基于一串比特来进行身份验证。==

Most commonly, we use a form of authentication based on **what you know**.
==最常见的是，我们使用一种基于“你所知道的信息”的身份验证形式。==

Now, think back to the earlier chapters.
==现在，回想一下之前的章节。==

What might someone running on a remote operating system know that no one else knows?
==在远程操作系统上运行的某人可能知道哪些别人不知道的事情？==

How about a password? How about a private key?
==密码怎么样？私钥怎么样？==

Most of our distributed system authentication will rely on one of these two elements.
==我们的大多数分布式系统身份验证将依赖于这两个元素之一。==

Either you require the remote machine to provide you with a password, or you require it to provide evidence using a private key stored only on that machine.
==要么你要求远程机器向你提供密码，要么你要求它使用仅存储在该机器上的私钥提供证据。==

In each case, you need to know something to check the authentication: either the password (or, better, a cryptographic hash of the password plus a salt) or the public key.
==在每种情况下，你都需要知道一些信息来检查身份验证：要么是密码（或者更好的是，密码加盐后的密码学哈希值），要么是公钥。==

When is each appropriate?
==每种方式在何时适用？==

Passwords tend to be useful if there are a vast number of parties who need to authenticate themselves to one party.
==如果有大量的方需要向一个方进行身份验证，密码往往很有用。==

Public keys tend to be useful if there’s one party who needs to authenticate himself to a vast number of parties.
==如果有一个方需要向大量的方证明自己的身份，公钥往往很有用。==

Why?
==为什么？==

With a password, the authentication provides evidence that somebody knows a password.
==使用密码时，身份验证提供了某人知道密码的证据。==

If you want to know exactly who that is (which is usually important), only the party authenticating and the party checking can know it.
==如果你想确切地知道那个人是谁（这通常很重要），只有进行身份验证的方和进行检查的方能知道它。==

With a public key, many parties can know the key, but only one party who knows the matching private key can authenticate himself.
==使用公钥时，许多方都可以知道该密钥，但只有知道匹配私钥的那一方才能验证自己的身份。==

So we tend to use both mechanisms, but for different cases.
==因此，我们倾向于使用这两种机制，但针对不同的情况。==

When a web site authenticates itself to a user, it’s done with PK cryptography.
==当网站向用户验证其身份时，它是通过公钥（PK）加密完成的。==

By distributing one single public key (to vast numbers of users), the web site can be authenticated by all its users.
==通过分发一个单一的公钥（给大量用户），该网站可以被其所有用户验证。==

The web site need not bother keeping separate authentication information to authenticate itself to each user.
==该网站无需费力为每个用户保存单独的身份验证信息来验证自己。==

When that user authenticates itself to the web site, it’s done with a password.
==当该用户向网站验证身份时，是通过密码完成的。==

Each user must be separately authenticated to the web site, so we require a unique piece of identifying information for that user, preferably something that’s easy for a person to use.
==每个用户必须单独向网站进行身份验证，因此我们要求为该用户提供唯一的标识信息，最好是易于人们使用的信息。==

Setting up and distributing public keys is hard, while setting up individual passwords is relatively easy.
==设置和分发公钥很困难，而设置个人密码相对容易。==

How, practically, do we use each of these authentication mechanisms in a distributed system?
==在分布式系统中，我们实际上如何使用这些身份验证机制？==

If we want a remote partner to authenticate itself via passwords, we will require it to provide us with that password, which we will check.
==如果我们希望远程伙伴通过密码进行身份验证，我们将要求它向我们提供该密码，我们将对其进行检查。==

We’ll need to encrypt the transport of the password across the network if we do that; otherwise anyone eavesdropping on the network (which is easy for many wireless networks) will readily learn passwords sent unencrypted.
==如果我们这样做，我们需要对密码在网络上的传输进行加密；否则，任何在网络上偷听的人（这在许多无线网络中很容易做到）都会轻而易举地获知未加密发送的密码。==

Encrypting the password will require that we already have either a shared symmetric key or our partner’s public key.
==加密密码将要求我们已经拥有共享的对称密钥或伙伴的公钥。==

Let’s concentrate now on how we get that public key, either to use it directly or set up the cryptography to protect the password in transit.
==现在让我们集中讨论如何获取该公钥，无论是直接使用它，还是设置加密以保护传输中的密码。==

We’ll spend the rest of the chapter on securing the network connection, but please don’t forget that even if you secure the network perfectly, you still face the major security challenge of the uncontrolled site you’re interacting with on the other side of the network.
==我们将在本章的其余部分讨论如何保护网络连接，但请不要忘记，即使你完美地保护了网络，你仍然面临着与网络另一端进行交互的、不受控制的站点的重大安全挑战。==

If your compromised partner attacks you, it will offer little consolation that the attack was authenticated and encrypted.
==如果你的伙伴已被攻破并攻击你，那么攻击是经过身份验证和加密的这一点并不能提供多少安慰。==

### 57.3 Public Key Authentication For Distributed Systems
==### 57.3 分布式系统的公钥身份验证==

The public key doesn’t need to be secret, but we need to be sure it really belongs to our partner.
==公钥不需要保密，但我们需要确保它确实属于我们的伙伴。==

If we have a face-to-face meeting, our partner can directly give us a public key in some form or another, in which case we can be pretty sure it’s the right one.
==如果我们进行面对面的会面，我们的伙伴可以直接以某种形式给我们一个公钥，在这种情况下，我们可以非常确定它是正确的。==

That’s limiting, though, since we often interact with partners whom we never see face to face.
==不过这是有局限性的，因为我们经常与从未见过面的伙伴互动。==

For that matter, whose “face” belongs to Amazon or Google?
==就此而言，亚马逊或谷歌的“脸”属于谁？==

Fortunately, we can use the fact that secrecy isn’t required to simply create a bunch of bits containing the public key.
==幸运的是，我们可以利用不需要保密这一事实，简单地创建一串包含公钥的比特。==

Anyone who gets a copy of the bits has the key.
==任何获得这些比特副本的人都拥有该密钥。==

But how do they know for sure whose key it is?
==但他们如何确定这是谁的密钥呢？==

What if some other trusted party known to everyone who needs to authenticate our partner used their own public key to cryptographically sign that bunch of bits, verifying that they do indeed belong to our partner?
==如果某个其他受信任的方（每个需要验证我们伙伴身份的人都认识它）使用他们自己的公钥对这串比特进行加密签名，验证它们确实属于我们的伙伴，那会怎么样呢？==

If we could check that signature, we could then be sure that bunch of bits really does represent our partner’s public key, at least to the extent that we trust that third party who did the signature.
==如果我们能检查该签名，我们就能确定这串比特确实代表了我们伙伴的公钥，至少在我们信任进行签名的第三方的范围内是如此。==

This technique is how we actually authenticate web sites and many other entities on the Internet.
==这种技术就是我们实际上如何在互联网上验证网站和许多其他实体身份的方法。==

Every time you browse the web or perform any other web-based activity, you use it.
==每次你浏览网页或执行任何其他基于网络的操作时，你都会使用它。==

The signed bundle of bits is called a **certificate**.
==这一组经过签名的比特被称为**证书（certificate）**。==

Essentially, it contains information about the party that owns the public key, the public key itself, and other information, such as an expiration date.
==从本质上讲，它包含有关拥有公钥的方的信息、公钥本身以及其他信息（如到期日期）。==

The entire set of information, including the public key, is run through a cryptographic hash, and the result is encrypted with the trusted third party’s private key, digitally signing the certificate.
==整个信息集（包括公钥）通过密码学哈希运行，结果用受信任第三方的私钥加密，从而对证书进行数字签名。==

If you obtain a copy of the certificate, and can check the signature, you can learn someone else’s public key, even if you have never met or had any direct interaction with them.
==如果你获得了证书副本并能检查签名，你就可以获知他人的公钥，即使你从未见过他们或与他们有过任何直接互动。==

In certain ways, it’s a beautiful technology that empowers the whole Internet.
==在某些方面，这是一项赋予整个互联网力量的优美技术。==

Let’s briefly go through an example, to solidify the concepts.
==让我们简要地看一个例子，以巩固这些概念。==

Let’s say Frobazz Inc. wants to obtain a certificate for its public key, which is $K_F$.
==假设 Frobazz 公司想为其公钥 $K_F$ 获取一份证书。==

Frobazz Inc. pays big bucks to Acmesign Co., a widely trusted company whose business it is to sell certificates, to obtain a certificate signed by AcmeSign.
==Frobazz 公司向 Acmesign 公司支付巨资，以获得由 AcmeSign 签名的证书。Acmesign 是一家被广泛信任的公司，其业务就是出售证书。==

Such companies are commonly called **Certificate Authorities**, or **CAs**, since they create authoritative certificates trusted by many parties.
==这类公司通常被称为**证书颁发机构**或 **CA**，因为它们创建被多方信任的权威证书。==

Acmesign checks up on Frobazz Inc. to ensure that the people asking for the certificate actually are legitimate representatives of Frobazz.
==Acmesign 会对 Frobazz 公司进行调查，以确保请求证书的人确实是 Frobazz 的合法代表。==

Acmesign then makes very, very sure that the public key it’s about to embed in a certificate actually is the one that Frobazz wants to use.
==然后，Acmesign 会非常非常确定它即将嵌入证书的公钥确实是 Frobazz 想要使用的那个。==

Assuming it is, Acmesign runs a cryptographic hashing algorithm (perhaps SHA-3) on Frobazz’s name, public key $K_F$, and other information, producing hash $H_F$.
==假设确实如此，Acmesign 对 Frobazz 的名称、公钥 $K_F$ 和其他信息运行密码学哈希算法（可能是 SHA-3），生成哈希值 $H_F$。==

Acmesign then encrypts $H_F$ with its own private key, $P_A$, producing digital signature $S_F$.
==然后，Acmesign 用它自己的私钥 $P_A$ 加密 $H_F$，生成数字签名 $S_F$。==

Finally, Acmesign combines all the information used to produce $H_F$, plus Acmesign’s own identity and the signature $S_F$, into the certificate $C_F$, which it hands over to Frobazz, presumably in exchange for money.
==最后，Acmesign 将所有用于生成 $H_F$ 的信息，加上 Acmesign 自身的身份和签名 $S_F$，组合成证书 $C_F$，并将其交给 Frobazz，大概是为了换取金钱。==

Remember, $C_F$ is just some bits.
==记住，$C_F$ 仅仅是一些比特。==

Now Frobazz Inc. wants to authenticate itself over the Internet to one of its customers.
==现在 Frobazz 公司想要通过互联网向其客户之一验证自己的身份。==

If the customer already has Frobazz’s public key, we can use public key authentication mechanisms directly.
==如果客户已经拥有 Frobazz 的公钥，我们可以直接使用公钥身份验证机制。==

If the customer does not have the public key, Frobazz sends $C_F$ to the customer.
==如果客户没有该公钥，Frobazz 会将 $C_F$ 发送给客户。==

The customer examines the certificate, sees that it was generated by Acmesign using, say, SHA-3, and runs the same information that Acmesign hashed (all of which is in the certificate itself) through SHA-3, producing $H_F'$.
==客户检查该证书，看到它是 Acmesign 使用（比如）SHA-3 生成的，并将 Acmesign 哈希过的相同信息（所有这些信息都在证书本身中）通过 SHA-3 运行，产生 $H_F'$。==

Then the customer uses Acmesign’s public key to decrypt $S_F$ (also in the certificate), obtaining $H_F$.
==然后客户使用 Acmesign 的公钥来解密 $S_F$（也在证书中），获得 $H_F$。==

If all is well, $H_F$ equals $H_F'$, and now the customer knows that the public key in the certificate is indeed Frobazz’s.
==如果一切正常，$H_F$ 等于 $H_F'$，现在客户知道证书中的公钥确实是 Frobazz 的。==

Public key-based authentication can proceed.
==基于公钥的身份验证可以继续进行。==

If the two hashes aren’t exactly the same, the customer knows that something fishy is going on and will not accept the certificate.
==如果这两个哈希值不完全相同，客户就知道有些不对劲，并且不会接受该证书。==

There are some wonderful properties about this approach to learning public keys.
==这种获取公钥的方法有一些奇妙的特性。==

First, note that the signing authority (Acmesign, in our example) did not need to participate in the process of the customer checking the certificate.
==首先，请注意，签署机构（在我们例子中是 Acmesign）不需要参与客户检查证书的过程。==

In fact, Frobazz didn’t really, either.
==事实上，Frobazz 也不需要。==

The customer can get the certificate from literally anywhere and obtain the same degree of assurance of its validity.
==客户几乎可以从任何地方获取该证书，并获得同等程度的有效性保证。==

Second, it only needs to be done once per customer.
==其次，每个客户只需要操作一次。==

After obtaining the certificate and checking it, the customer has the public key that is needed.
==在获取并检查证书后，客户就拥有了所需的公钥。==

From that point onward, the customer can simply store it and use it.
==从那时起，客户只需存储并使用它即可。==

If, for whatever reason, it gets lost, the customer can either extract it again from the certificate (if that has been saved), or go through the process of obtaining the certificate again.
==如果由于某种原因丢失了它，客户可以从证书中再次提取（如果证书已保存），或者重新执行获取证书的过程。==

Third, the customer had no need to trust the party claiming to be Frobazz until that identity had been proven by checking the certificate.
==第三，在通过检查证书证明其身份之前，客户不需要信任声称是 Frobazz 的那一方。==

The customer can proceed with caution until the certificate checks out.
==在证书通过检查之前，客户可以谨慎行事。==

Assuming you’ve been paying attention for the last few chapters, you should be saying to yourself, “now, wait a minute, isn’t there a chicken-and-egg problem here?”
==假设你一直在关注过去的几个章节，你应该会对自己说：“现在，等等，这里难道没有一个‘先有鸡还是先有蛋’的问题吗？”==

We’ll learn Frobazz’s public key by getting a certificate for it.
==我们将通过获取 Frobazz 公钥的证书来获知它。==

The certificate will be signed by Acmesign.
==证书将由 Acmesign 签名。==

We’ll check the signature by knowing Acmesign’s public key.
==我们将通过获知 Acmesign 的公钥来检查签名。==

But where did we get Acmesign’s key?
==但我们从哪里得到 Acmesign 的密钥呢？==

We really hope you did have that head-scratching moment and asked yourself that question, because if you did, you understand the true nature of the Internet authentication problem.
==我们真心希望你确实有过那个抓耳挠腮的时刻并问过自己这个问题，因为如果你问了，你就理解了互联网身份验证问题的本质。==

Ultimately, we’ve got to bootstrap it.
==归根结底，我们必须对其进行引导（bootstrap）。==

You’ve got to somehow or other obtain a public key for somebody that you trust.
==你必须以某种方式获得你所信任的某人的公钥。==

Once you do, if it’s the right public key for the right kind of party, you can then obtain a lot of other public keys.
==一旦你这样做了，如果它是适合那类方的正确公钥，你就可以接着获取许多其他公钥。==

But without something to start from, you can’t do much of anything.
==但如果没有一个起始点，你什么也做不了。==

Where do you get that primal public key?
==你从哪里得到那个原始公钥呢？==

Most commonly, it comes in a piece of software you obtain and install.
==最常见的情况是，它包含在你获取并安装的一款软件中。==

The one you use most often is probably your browser, which typically comes with the public keys for several hundred trusted authorities.
==你最常使用的可能就是你的浏览器，它通常自带了数百个受信任机构的公钥。==

Whenever you go to a new web site that cares about security, it provides you with a certificate containing that site’s public key, and signed by one of those trusted authorities pre-configured into your browser.
==每当你访问一个注重安全的新网站时，它会向你提供一个包含该网站公钥的证书，该证书由预先配置在浏览器中的受信任机构之一签名。==

You use the pre-configured public key of that authority to verify that the certificate is indeed proper, after which you know the public key of that web site.
==你使用该机构预先配置的公钥来验证证书是否确实正当，之后你就知道了该网站的公钥。==

From that point onward, you can use the web site’s public key to authenticate it.
==从那时起，你就可以使用该网站的公钥来验证其身份。==

Anyone can create a certificate, not just those trusted CAs, either by getting one from someone whose business it is to issue certificates or simply by creating one from scratch, following a certificate standard (**X.509** is the most commonly used certificate standard).
==任何人都可以创建证书，而不仅仅是那些受信任的 CA，既可以从专门从事证书颁发业务的人那里获取，也可以按照证书标准（**X.509** 是最常用的证书标准）从头开始创建一个。==

The necessary requirement: the party being authenticated and the parties performing the authentication must all trust whoever created the certificate.
==必要条件：被验证身份的方和执行身份验证的方必须都信任证书的创建者。==

If they don’t trust that party, why would they believe the certificate is correct?
==如果他们不信任那一方，他们为什么要相信证书是正确的呢？==

If you are building your own distributed system, you can create your own certificates from a machine you (and other participants in the system) trust and can handle the bootstrapping issue by carefully hand-installing the certificate signing machine’s public key wherever it needs to be.
==如果你正在构建自己的分布式系统，你可以从你（以及系统中的其他参与者）信任的机器上创建自己的证书，并可以通过在需要的地方小心地手动安装证书签名机器的公钥来处理引导问题。==

There are a number of existing software packages for creating certificates, and, as usual with critical cryptographic software, you’re better off using an existing, trusted implementation rather than coding up one of your own.
==有许多现成的软件包用于创建证书，而且与关键的密码学软件一样，你最好使用现有的、受信任的实现，而不是自己编写代码。==

One example you might want to look at is **PGP** (available in both supported commercial versions and compatible but less supported free versions), but there are others.
==一个你可能想参考的例子是 **PGP**（有受支持的商业版本和兼容但受支持较少的免费版本），当然还有其他选择。==

If you are working with a fixed number of machines and you can distribute the public key by hand in some reasonable way, you can dispense entirely with certificates.
==如果你在固定数量的机器上工作，并且可以通过某种合理的方式手动分发公钥，你就可以完全不需要证书。==

Remember, the only point of a PK certificate is to distribute the public key, so if your public keys are already where they need to be, you don’t need certificates.
==记住，公钥证书的唯一目的就是分发公钥，因此如果你的公钥已经在需要的地方了，你就不需要证书。==

OK, one way or another you’ve obtained the public key you need to authenticate some remote machine. Now what?
==好了，通过某种方式，你已经获得了验证某台远程机器身份所需的公钥。接下来怎么办？==

Well, anything they send you encrypted with their private key will only decrypt with their public key, so anything that decrypts properly with the public key must have come from them, right?
==嗯，他们用私钥加密后发送给你的任何内容，都只能用他们的公钥解密，所以任何能用公钥正确解密的内容一定来自他们，对吧？==

Yes, it must have come from them at some point, but it’s possible for an adversary to have made a copy of a legitimate message the site sent at some point in the past and then send it again it at some future date.
==是的，它一定在某个时间点来自他们，但敌手有可能复制了该站点在过去某个时间点发送的一条合法消息，然后在未来的某个日期再次发送。==

Depending on exactly what’s going on, that could cause trouble, since you may take actions based on that message that the legitimate site did not ask for.
==取决于具体发生了什么，这可能会导致麻烦，因为你可能会根据该消息采取合法站点并未要求的行动。==

So usually we take measures to ensure that we’re not being subjected to a **replay attack**.
==因此，我们通常会采取措施，确保我们不会受到**重放攻击**。==

Such measures generally involve ensuring that each encrypted message contains unique information not in any other message.
==此类措施通常涉及确保每条加密消息都包含其他任何消息中都不存在的唯一信息。==

This feature is built in properly to standard cryptographic protocols, so if you follow our advice and use one of those, you will get protection from such replay attacks.
==此功能已正确内置于标准密码学协议中，因此如果你遵循我们的建议并使用其中之一，你将获得针对此类重放攻击的保护。==

If you insist on building your own cryptography, you’ll need to learn a good deal more about this issue and will have to apply that knowledge very carefully.
==如果你坚持构建自己的加密算法，你需要更多地了解这个问题，并且必须非常谨慎地应用这些知识。==

Also, public key cryptography is expensive.
==此外，公钥加密开销很大。==

We want to stop using it as soon as possible, but we also want to continue to get authentication guarantees.
==我们希望尽快停止使用它，但我们也希望继续获得身份验证保证。==

We’ll see how to do that when we discuss **SSL** and **TLS**.
==我们将在讨论 **SSL** 和 **TLS** 时看到如何做到这一点。==

### 57.4 Password Authentication For Distributed Systems
==### 57.4 分布式系统的密码身份验证==

The other common option to authenticate in distributed systems is to use a password.
==分布式系统中另一种常见的身份验证选项是使用密码。==

As noted above, that will work best in situations where only two parties need to deal with any particular password: the party being authenticated and the authenticating party.
==如前所述，这在只有两个方需要处理任何特定密码的情况下效果最好：被验证身份的方和进行身份验证的方。==

They make sense when an individual user is authenticating himself to a site that hosts many users, such as when you log in to Amazon.
==当个人用户向托管许多用户的站点验证自己的身份时（例如当你登录亚马逊时），这种方式是有意义的。==

They don’t make sense when that site is trying to authenticate itself to an individual user, such as when a web site claiming to be Amazon wants to do business with you.
==当该站点试图向个人用户验证自己的身份时（例如当一个自称是亚马逊的网站想要和你做生意时），这种方式就没有意义了。==

Public key authentication works better there.
==在这种情况下，公钥身份验证的效果更好。==

How do we properly handle password authentication over the network, when it is a reasonable choice?
==当密码身份验证是一个合理的选择时，我们如何正确处理网络上的密码身份验证？==

The password is usually associated with a particular user ID, so the user provides that ID and password to the site requiring authentication.
==密码通常与特定的用户 ID 相关联，因此用户向需要身份验证的站点提供该 ID 和密码。==

That typically happens over a network, and typically we cannot guarantee that networks provide confidentiality.
==这通常发生在网络上，而通常我们无法保证网络能提供机密性。==

If our password is divulged to someone else, they’ll be able to pose as us, so we must add confidentiality to this cross-network authentication, generally by encrypting at least the password itself (though encrypting everything involved is better).
==如果我们的密码泄露给了别人，他们就能冒充我们，所以我们必须为这种跨网络身份验证增加机密性，通常通过至少加密密码本身（尽管加密涉及的所有内容更好）。==

So a typical interchange with Alice trying to authenticate herself to Frobazz Inc.’s web site would involve the site requesting a user ID and password and Alice providing both, but encrypting them before sending them over the network.
==因此，爱丽丝尝试向 Frobazz 公司的网站验证身份的典型交互将涉及：网站请求用户 ID 和密码，爱丽丝提供这两者，但在通过网络发送之前对其进行加密。==

The obvious question you should ask is, encrypting them with what key?
==你应该问的一个显而易见的问题是：用什么密钥加密它们？==

Well, if Frobazz authenticated itself to Alice using PK, as discussed above, Alice can encrypt her user ID and password with Frobazz’s public key.
==既然 Frobazz 已经按照上述方式使用公钥加密（PK）向爱丽丝验证了身份，爱丽丝就可以用 Frobazz 的公钥加密她的用户 ID 和密码。==

Frobazz Inc., having the matching private key, will be able to check them, but nobody else can read them.
==Frobazz 公司拥有匹配的私钥，能够检查它们，但其他任何人都无法读取。==

In actuality, there are various reasons why this alone would not suffice, including replay attacks, as mentioned above.
==实际上，出于各种原因（包括上述提到的重放攻击），仅靠这一点是不够的。==

But we can and do use Frobazz’s private key to set up cryptography that will protect Alice’s password in transit.
==但我们可以且确实利用 Frobazz 的私钥建立起加密机制，以保护爱丽丝在传输过程中的密码。==

We’ll discuss the details in the section on SSL/TLS.
==我们将在关于 SSL/TLS 的部分讨论具体细节。==

We discussed issues of password choice and management in the chapter on authentication, and those all apply in the networking context.
==我们在身份验证章节中讨论了密码选择和管理的问题，这些问题同样适用于网络环境。==

Otherwise, there’s not that much more to say about how we’ll use passwords, other than to note that after the remote site has verified the password, what does it actually know?
==除此之外，关于我们将如何使用密码，没有更多可说的了，只需注意在远程站点验证密码后，它实际上知道了什么？==

That the site or user who sent the password knows it, and, to the strength of the password, that site or user is who it claims to be.
==它知道发送密码的站点或用户知道该密码，并且根据密码的强度，确定该站点或用户就是它所声称的那个。==

But what about future messages that come in, supposedly from that site?
==但是，此后收到的据称来自该站点的消息又该如何处理呢？==

Remember, anyone can create any message they want, so if all we do is verify that the remote site sent us the right password, all we know is that particular message is authentic.
==请记住，任何人都可以创建他们想要的任何消息，因此如果我们只是验证远程站点发送了正确的密码，我们所知道的仅仅是那条特定消息是真实的。==

We don’t want to have to include the password on every message we send, just as we don’t want to use PK to encrypt every message we send.
==我们不想在发送的每条消息中都包含密码，就像我们不想使用公钥加密（PK）来加密发送的每条消息一样。==

We will use both authentication techniques to establish initial authenticity, then use something else to tie that initial authenticity to subsequent interactions.
==我们将同时使用这两种身份验证技术来建立初始真实性，然后使用其他手段将这种初始真实性与随后的交互联系起来。==

Let’s move right along to SSL/TLS to talk about how we do that.
==让我们直接进入 SSL/TLS 部分，讨论如何实现这一点。==

### 57.5 SSL/TLS
### 57.5 SSL/TLS

We saw in an earlier chapter that a standard method of communicating between processes in modern systems is the **socket**.
==我们在之前的章节中看到，现代系统中进程间通信的标准方法是**套接字（socket）**。==

That’s equally true when the processes are on different machines.
==当进程位于不同的机器上时，这一点同样适用。==

So a natural way to add cryptographic protection to communications crossing unprotected networks is to add cryptographic features to sockets.
==因此，为跨越不受保护网络的通信添加加密保护的一种自然方式是为套接字添加加密功能。==

That’s precisely what **SSL** (the **Secure Socket Layer**) was designed to do, many years ago.
==这正是多年前设计 **SSL**（**安全套接层**）的目的。==

Unfortunately, SSL did not get it quite right.
==不幸的是，SSL 做得并不完全正确。==

That’s because it’s pretty darn hard to get it right, not because the people who designed and built it were careless.
==这是因为要把它做对真的很难，而不是因为设计和构建它的人粗心大意。==

They learned from their mistakes and created a new version of encrypted sockets called **Transport Layer Security** (**TLS**).
==他们从错误中吸取教训，创建了一个新版本的加密套接字，称为**传输层安全**（**TLS**）。==

You will frequently hear people talk about using SSL.
==你会经常听到人们谈论使用 SSL。==

They are usually treating it as a shorthand for SSL/TLS.
==他们通常将其视为 SSL/TLS 的简写。==

SSL, formally, is insecure and should never be used for anything.
==从正式意义上讲，SSL 是不安全的，绝不应被用于任何用途。==

Use TLS.
==请使用 TLS。==

The only exception is that some very old devices might run software that doesn’t support TLS.
==唯一的例外是，一些非常旧的设备可能运行不支持 TLS 的软件。==

In that case, it’s better to use SSL than nothing.
==在这种情况下，使用 SSL 总比什么都不用强。==

We’ll adopt the same shorthand as others from here on, since it’s ubiquitous.
==从现在起，我们将采用和其他人一样的简写，因为这无处不在。==

The concept behind SSL is simple: move encrypted data through an ordinary socket.
==SSL 背后的概念很简单：通过普通套接字传输加密数据。==

You set up a socket, set up a special structure to perform whatever cryptography you want, and hook the output of that structure to the input of the socket.
==你设置一个套接字，设置一个特殊的结构来执行你想要的任何加密操作，并将该结构的输出挂接到套接字的输入。==

You reverse the process on the other end.
==在另一端执行相反的过程。==

What’s simple in concept is rather laborious in execution, with a number of steps required to achieve the desired result.
==概念上简单的事情在执行中相当费力，需要经过若干步骤才能达到预期的结果。==

There are further complications due to the general nature of SSL.
==由于 SSL 的通用性质，还存在进一步的复杂性。==

The technology is designed to support a variety of cryptographic operations and many different ciphers, as well as multiple methods to perform key exchange and authentication between the sender and receiver.
==该技术旨在支持各种密码操作和许多不同的密码，以及在发送者和接收者之间执行密钥交换和身份验证的多种方法。==

The process of adding SSL to your program is intricate, requiring the use of particular libraries and a sequence of calls into those libraries to set up a correct SSL connection.
==将 SSL 添加到程序的过程错综复杂，需要使用特定的库，并按顺序调用这些库来建立正确的 SSL 连接。==

We will not go through those operations step by step here, but you will need to learn about them to make proper use of SSL.
==我们在这里不会逐步讲解这些操作，但你需要了解它们才能正确使用 SSL。==

Their purpose is, for the most part, to allow a wide range of generality both in the cryptographic options SSL supports and the ways you use those options in your program.
==它们的目的在很大程度上是为了在 SSL 支持的密码选项以及你在程序中使用这些选项的方式上提供广泛的通用性。==

For example, these setup calls would allow you to create one set of SSL connections using AES-128 and another using AES-256, if that’s what you needed to do.
==例如，如果你需要这样做，这些设置调用将允许你创建一组使用 AES-128 的 SSL 连接和另一组使用 AES-256 的 SSL 连接。==

One common requirement for setting up an SSL connection that we will go through in a bit more detail is how to securely distribute whatever cryptographic key you will use for the connection you are setting up.
==我们将稍微详细介绍建立 SSL 连接的一个共同要求，即如何安全地分发你将用于正在建立的连接的任何加密密钥。==

Best cryptographic practice calls for you to use a brand new key to encrypt the bulk of your data for each connection you set up.
==最佳密码学实践要求你为建立的每个连接使用一个全新的密钥来加密大部分数据。==

You will use public/private keys for authentication many times, but as we discussed earlier, you need to use symmetric cryptography to encrypt the data once you have authenticated your partner, and you want a fresh key for that.
==你会多次使用公钥/私钥进行身份验证，但正如我们之前讨论的，一旦你验证了伙伴的身份，你就需要使用对称加密来加密数据，并且你需要为此使用一个新密钥。==

Even if you are running multiple simultaneous SSL connections with the same partner, you want a different symmetric key for each connection.
==即使你与同一个伙伴运行多个并发的 SSL 连接，你也希望为每个连接使用不同的对称密钥。==

So what do you need to do to set up a new SSL connection?
==那么，建立一个新的 SSL 连接需要做些什么呢？==

We won’t go through all of the gory details, but, in essence, SSL needs to bootstrap a secure connection based (usually) on asymmetric cryptography when no usable symmetric key exists.
==我们不会详细讲解所有细节，但本质上，当不存在可用的对称密钥时，SSL 需要（通常）基于非对称加密来引导一个安全连接。==

The very first step is to start a negotiation between the client and the server.
==第一步是在客户端和服务器之间开始协商。==

Each party might only be able to handle particular ciphers, secure hashes, key distribution strategies, or authentication schemes, based on what version of SSL they have installed, how it’s configured, and how the programs that set up the SSL connection on each side were written.
==根据所安装的 SSL 版本、配置方式以及各方建立 SSL 连接的程序的编写方式，每一方可能只能处理特定的密码、安全哈希、密钥分发策略或身份验证方案。==

In the most common cases, the negotiation will end in both sides finding some acceptable set of ciphers and techniques that hit a balance between security and performance.
==在最常见的情况下，协商将以双方找到一组可接受的密码和技术而告终，这些密码和技术在安全性和性能之间达到了平衡。==

For example, they might use RSA with 2048 bit keys for asymmetric cryptography, some form of a **Diffie-Hellman key exchange** mechanism to establish a new symmetric key, SHA-3 to generate secure hashes for integrity, and AES with 256 bit keys for bulk encryption.
==例如，他们可能会使用 2048 位密钥的 RSA 进行非对称加密，使用某种形式的 **Diffie-Hellman 密钥交换**机制建立新的对称密钥，使用 SHA-3 生成用于完整性的安全哈希，并使用 256 位密钥的 AES 进行批量加密。==

A modern installation of SSL might support 50 or more different combinations of these options.
==现代 SSL 安装可能会支持 50 种或更多这些选项的组合。==

In some cases, it may be important for you to specify which of these many combinations are acceptable for your system, but often most of them will do, in which case you can let SSL figure out which to use for each connection without worrying about it yourself.
==在某些情况下，指定这些众多组合中哪些对于你的系统是可接受的可能很重要，但通常其中大多数都可以，在这种情况下，你可以让 SSL 为每个连接决定使用哪一个，而无需自己操心。==

The negotiation will happen invisibly and SSL will get on with its main business: authenticating at least the server (optionally the client), creating and distributing a new symmetric key, and running the communication through the chosen cipher using that key.
==协商将隐形发生，SSL 将继续其主要业务：至少验证服务器（可选验证客户端），创建并分发一个新的对称密钥，并使用该密钥通过选定的密码运行通信。==

We can use Diffie-Hellman key exchange to create the key (and SSL frequently does), but we need to be sure who we are sharing that key with.
==我们可以使用 Diffie-Hellman 密钥交换来创建密钥（SSL 经常这样做），但我们需要确定我们在与谁共享该密钥。==

SSL offers a number of possibilities for doing so.
==SSL 为此提供了多种可能性。==

The most common method is for the client to obtain a certificate containing the server’s public key (typically by having the server send it to the client) and to use the public key in that certificate to verify the authenticity of the server’s messages.
==最常用的方法是客户端获取一个包含服务器公钥的证书（通常是由服务器将其发送给客户端），并使用该证书中的公钥来验证服务器消息的真实性。==

It is possible for the client to obtain the certificate through some other means, though less common.
==客户端有可能通过其他手段获取证书，尽管这种情况较少见。==

Note that having the server send the certificate is every bit as secure (or insecure) as having the client obtain the certificate through other means.
==请注意，让服务器发送证书与让客户端通过其他手段获取证书同样安全（或不安全）。==

Certificate security is not based on the method used to transport it, but on the cryptography embedded in the certificate.
==证书的安全性并非基于传输它的方法，而是基于嵌入在证书中的密码学。==

With the certificate in hand (however the client got it), the Diffie-Hellman key exchange can now proceed in an authenticated fashion.
==一旦拿到证书（无论客户端是如何得到的），Diffie-Hellman 密钥交换现在就可以以经过身份验证的方式进行。==

### ASIDE: DIFFIE-HELLMAN KEY EXCHANGE
==### 旁注：DIFFIE-HELLMAN 密钥交换==

What if you want to share a secret key between two parties, but they can only communicate over an insecure channel, where eavesdroppers can hear anything they say?
==如果你想在两方之间共享一个密钥，但他们只能通过不安全的通道通信，偷听者可以听到他们说的任何话，该怎么办？==

You might think this is an impossible problem to solve, but you’d be wrong.
==你可能认为这是一个无法解决的问题，但你错了。==

Two extremely smart cryptographers named Whitfield Diffie and Martin Hellman solved this problem years ago, and their solution is in common use.
==两位极其聪明的密码学家 Whitfield Diffie 和 Martin Hellman 多年前就解决了这个问题，他们的解决方案已被广泛使用。==

It’s called **Diffie-Hellman key exchange**.
==它被称为 **Diffie-Hellman 密钥交换**。==

Here’s how it works.
==以下是它的工作原理。==

Let’s say Alice and Bob want to share a secret key, but currently don’t share anything, other than the ability to send each other messages.
==假设爱丽丝和鲍勃想要共享一个密钥，但目前除了互相发送消息的能力外，没有任何共享的东西。==

First, they agree on two numbers, $n$ (a large prime number) and $g$ (which is primitive mod $n$).
==首先，他们商定两个数字，$n$（一个大质数）和 $g$（它是 $n$ 的原根）。==

They can use the insecure channel to do this, since $n$ and $g$ don’t need to be secret.
==他们可以使用不安全通道来做这件事，因为 $n$ 和 $g$ 不需要保密。==

Alice chooses a large random integer, say $x$, calculates $X = g^x \pmod n$, and sends $X$ to Bob.
==爱丽丝选择一个大的随机整数，假设为 $x$，计算 $X = g^x \pmod n$，并将 $X$ 发送给鲍勃。==

Bob independently chooses a large random integer, say $y$, calculates $Y = g^y \pmod n$, and sends $Y$ to Alice.
==鲍勃独立选择一个大的随机整数，假设为 $y$，计算 $Y = g^y \pmod n$，并将 $Y$ 发送给爱丽丝。==

The eavesdroppers can hear $X$ and $Y$, but since Alice and Bob didn’t send $x$ or $y$, the eavesdroppers don’t know those values.
==偷听者可以听到 $X$ 和 $Y$，但由于爱丽丝和鲍勃没有发送 $x$ 或 $y$，偷听者不知道这些值。==

It’s important that Alice and Bob keep $x$ and $y$ secret.
==重要的是爱丽丝和鲍勃必须对 $x$ 和 $y$ 保密。==

Alice now computes $k = Y^x \pmod n$, and Bob computes $k = X^y \pmod n$.
==爱丽丝现在计算 $k = Y^x \pmod n$，鲍勃计算 $k = X^y \pmod n$。==

Alice and Bob get the same value $k$ from these computations.
==爱丽丝和鲍勃通过这些计算得到相同的 $k$ 值。==

Why?
==为什么？==

Well, $Y^x \pmod n = (g^y \pmod n)^x \pmod n$, which in turn equals $g^{yx} \pmod n$.
==嗯，$Y^x \pmod n = (g^y \pmod n)^x \pmod n$，这又等于 $g^{yx} \pmod n$。==

$X^y \pmod n = (g^x \pmod n)^y \pmod n = g^{xy} \pmod n$, which is the same thing Alice got.
==$X^y \pmod n = (g^x \pmod n)^y \pmod n = g^{xy} \pmod n$，这与爱丽丝得到的结果相同。==

Nothing magic there, that’s just how exponentiation and modulus arithmetic work.
==这没有什么神奇的，这只是指数运算和模运算的工作原理。==

So $k$ is the same in both calculations and is known to both Alice and Bob.
==因此，在两次计算中 $k$ 是相同的，并且爱丽丝和鲍勃都知道它。==

What about those eavesdroppers?
==那些偷听者呢？==

They know $g$, $n$, $X$, and $Y$, but not $x$ or $y$.
==他们知道 $g$、$n$、$X$ 和 $Y$，但不知道 $x$ 或 $y$。==

They can compute $k' = XY \pmod n$, but that is not equal to the $k$ Alice and Bob calculated.
==他们可以计算 $k' = XY \pmod n$，但这不等于爱丽丝和鲍勃计算出的 $k$。==

They do have approaches to derive $x$ or $y$, which would give them enough information to obtain $k$, but those approaches require them either to perform a calculation for every possible value of $n$ (which is why you want $n$ to be very large) or to compute a discrete logarithm.
==他们确实有推导 $x$ 或 $y$ 的方法，这将给他们足够的信息来获得 $k$，但这些方法要求他们要么对 $n$ 的每个可能值执行计算（这就是为什么你希望 $n$ 非常大），要么计算离散对数。==

Computing a discrete logarithm is a solvable problem, but it’s computationally infeasible for large numbers.
==计算离散对数是一个可解决的问题，但对于大数来说，它在计算上是不可行的。==

So if the prime $n$ is large (and meets other properties), the eavesdroppers are out of luck.
==因此，如果质数 $n$ 很大（并符合其他属性），偷听者就没戏了。==

How large? 600 digit primes should be good enough.
==多大？600 位的质数应该足够了。==

Neat, no?
==很巧妙，不是吗？==

But there is a fly in the ointment, when one considers using Diffie-Hellman over a network.
==但是，当考虑在网络上使用 Diffie-Hellman 时，美中不足的是。==

It ensures that you securely share a key with someone, but gives you no assurance of who you’re sharing the key with.
==它确保你与某人安全地共享密钥，但它无法保证你在与谁共享密钥。==

Maybe Alice is sharing the key with Bob, as she thinks and hopes, but maybe she’s sharing it with Mallory, who posed as Bob and injected his own $Y$.
==也许爱丽丝正在与她认为并希望的鲍勃共享密钥，但也可能她正在与 Mallory 共享，后者冒充鲍勃并注入了他自己的 $Y$。==

Since we usually care who we’re in secure communication with, we typically augment Diffie-Hellman with an authentication mechanism to provide the assurance of our partner’s identity.
==由于我们通常关心我们在与谁进行安全通信，因此我们通常会用身份验证机制来增强 Diffie-Hellman，以提供对伙伴身份的保证。==

The server will sign its Diffie-Hellman messages with its private key, which will allow the client to determine that its partner in this key exchange is the correct server.
==服务器将使用其私钥签署其 Diffie-Hellman 消息，这将允许客户端确定其在该密钥交换中的伙伴是正确的服务器。==

Typically, the client does not provide (or even have) its own certificate, so it cannot sign its Diffie-Hellman messages.
==通常，客户端不提供（甚至没有）自己的证书，因此它不能签署其 Diffie-Hellman 消息。==

This implies that when SSL’s Diffie-Hellman key exchange completes, typically the client is pretty sure who the server is, but the server has no clue about the client’s identity.
==这意味着当 SSL 的 Diffie-Hellman 密钥交换完成时，客户端通常很确定服务器是谁，但服务器对客户端的身份一无所知。==

Recalling our discussion earlier in this chapter, it actually isn’t a problem for the server to be unsure about the client’s identity at this point, in many cases.
==回想本章早些时候的讨论，在许多情况下，服务器在此时不确定客户端的身份实际上并不是问题。==

As we stated earlier, the client will probably want to use a password to authenticate itself, not a public key extracted from a certificate.
==如前所述，客户端可能希望使用密码来验证自己的身份，而不是从证书中提取公钥。==

As long as the server doesn’t permit the client to do anything requiring trust before the server obtains and checks the client’s password, the server probably doesn’t care who the client is, anyway.
==只要在服务器获取并检查客户端密码之前，服务器不允许客户端执行任何需要信任的操作，那么服务器可能根本不在乎客户端是谁。==

Many servers offer some services to anonymous clients (such as providing them with publicly available information), so as long as they can get a password from the client before proceeding to more sensitive subjects, there is no security problem.
==许多服务器向匿名客户端提供某些服务（例如向其提供公开可用的信息），因此只要在进入更敏感的话题之前能从客户端获得密码，就不存在安全问题。==

The server can ask the client for a user ID and password later, at any point after the SSL connection is established.
==在 SSL 连接建立后的任何时间点，服务器都可以稍后要求客户端提供用户 ID 和密码。==

Since creating the SSL connection sets up a symmetric key, the exchange of ID and password can be protected with that key.
==由于创建 SSL 连接会建立对称密钥，因此 ID 和密码的交换可以用该密钥进行保护。==

A final word about SSL/TLS: it’s a protocol, not a software package.
==最后关于 SSL/TLS 的一句话：它是一种协议，而不是一个软件包。==

There are multiple different software packages that implement this protocol.
==有多个不同的软件包实现了这一协议。==

Ideally, if they all implement the protocol properly, they all interact correctly.
==理想情况下，如果它们都正确地实现了协议，它们都能正确地进行交互。==

However, they use different code to implement the protocol.
==然而，它们使用不同的代码来实现协议。==

As a result, software flaws in one implementation of SSL/TLS might not be present in other implementations.
==因此，SSL/TLS 的一种实现中的软件漏洞可能不会出现在其他实现中。==

For example, the **Heartbleed** attack was based on implementation details of OpenSSL, but was not present in other implementations, such as the version of SSL/TLS found in Microsoft’s Windows operating system.
==例如，**Heartbleed** 攻击是基于 OpenSSL 的实现细节，但并没有出现在其他实现中，例如微软 Windows 操作系统中的 SSL/TLS 版本。==

It is also possible that the current protocol definition of SSL/TLS contains protocol flaws that would be present in any compliant implementation.
==当前的 SSL/TLS 协议定义本身也有可能包含协议缺陷，这些缺陷会出现在任何合规的实现中。==

If you hear of a security problem involving SSL, determine whether it is a **protocol flaw** or an **implementation flaw** before taking further action.
==如果你听说涉及 SSL 的安全问题，在采取进一步行动之前，请先确定它是**协议缺陷**还是**实现缺陷**。==

### 57.6 Other Authentication Approaches
==### 57.6 其他身份验证方法==

While passwords and public keys are the most common ways to authenticate a remote user or machines, there are other options.
==虽然密码和公钥是验证远程用户或机器身份最常用的方式，但还有其他选择。==

One such option is used all the time.
==其中一种选择一直在被使用。==

After you have authenticated yourself to a web site by providing a password, the web site will continue to assume that the authentication is valid.
==在你通过提供密码向网站验证了身份后，该网站将继续假设该验证是有效的。==

It won’t ask for your password every time you click a link or perform some other interaction with it.
==当你点击链接或与其进行其他交互时，它不会每次都询问你的密码。==

Imagine how much of a pain it would be if you had to provide your password every time you wanted to do anything.
==想象一下，如果你想做任何事都必须提供密码，那将是多么痛苦。==

In such cases, the site you are working with has chosen to make a security tradeoff.
==在这种情况下，你正在使用的站点选择做出了安全性权衡。==

It verified your identity at some time in the past using your password and then relies on another method to authenticate you in the future.
==它在过去的某个时间使用你的密码验证了你的身份，然后依靠另一种方法在未来对你进行身份验证。==

A common method is to use **web cookies**.
==一种常见的方法是使用 **Web Cookie**。==

Web cookies are pieces of data that a web site sends to a client with the intention that the client stores that data and send it back again whenever the client next communicates with the server.
==Web Cookie 是网站发送给客户端的一段数据，目的是让客户端存储该数据，并在下次与服务器通信时将其发回。==

With proper use of cryptography, a server that has verified the password of a client can create a web cookie that securely stores the client’s identity.
==通过正确使用加密技术，验证了客户端密码的服务器可以创建一个安全存储客户端身份的 Web Cookie。==

When the client communicates with the server again, the web browser automatically includes the cookie in the request, which allows the server to verify the client’s identity without asking for a password again.
==当客户端再次与服务器通信时，浏览器会自动在请求中包含该 Cookie，这允许服务器在不再次询问密码的情况下验证客户端的身份。==

If you spend a few minutes thinking about this authentication approach, you might come up with some possible security problems associated with it.
==如果你花几分钟思考这种身份验证方法，你可能会想到一些与其相关的潜在安全问题。==

The people designing this technology have dealt with some of these problems, like preventing an eavesdropper from simply using a cookie that was copied as it went across the network.
==设计这项技术的人已经处理了其中的一些问题，比如防止偷听者直接使用在网络传输过程中复制的 Cookie。==

However, there are other security problems that can’t be solved with these kinds of cookies, but could have been solved if you required the user to provide the password every time.
==然而，还有一些安全问题无法通过这类 Cookie 解决，但如果你要求用户每次都提供密码，这些问题本可以解决。==

Is it better to make life simpler for your user by not asking for a password except when absolutely necessary, or is it better to provide your user with improved security by frequently requiring proof of identity?
==是通过除非绝对必要否则不询问密码来让用户的生活更简单，还是通过频繁要求身份证明来为用户提供更高的安全性，哪种更好？==

The point isn’t that there is one correct answer to this question, but that you need to think about such questions in the design of your system.
==重点不在于这个问题有一个正确的答案，而在于你在设计系统时需要思考此类问题。==

There are other authentication options.
==还有其他身份验证选项。==

One example is what is called a **challenge/response protocol**.
==一个例子是所谓的**挑战/响应协议**。==

The remote machine sends you a challenge, typically in the form of a number.
==远程机器向你发送一个挑战，通常是一个数字的形式。==

To authenticate yourself, you must perform some operation on the challenge that produces a response.
==为了验证身份，你必须对该挑战执行某种操作以产生响应。==

This should be an operation that only the authentic party can perform, so it probably relies on the use of a secret that party knows, but no one else does.
==这应该是一个只有真实方才能执行的操作，因此它可能依赖于使用该方知道而其他人不知道的秘密。==

The secret is applied to the challenge, producing the response, which is sent to the server.
==将秘密应用于挑战，产生响应，并发送给服务器。==

A different challenge is sent every time, requiring a different response, so attackers gain no advantage by listening to and copying down old challenges and responses.
==每次发送的挑战都不同，需要不同的响应，因此攻击者通过偷听并复制旧的挑战和响应无法获得任何优势。==

Another authentication option is to use an **authentication server**.
==另一个身份验证选项是使用**身份验证服务器**。==

In essence, you talk to a server that you trust and that trusts you.
==从本质上讲，你与一个你信任且信任你的服务器交谈。==

The party you wish to authenticate to must also trust the server.
==你希望向其验证身份的那一方也必须信任该服务器。==

The authentication server vouches for your identity in some secure form, usually involving cryptography.
==身份验证服务器以某种安全形式（通常涉及加密）为你的身份提供担保。==

Since the party you wish to communicate with trusts the authentication server, it now trusts that you are who you claim to be.
==由于你希望通信的那一方信任身份验证服务器，它现在也信任你就是你所声称的那个人。==

Online versions are more responsive to changes in security conditions than offline versions like CAs.
==在线版本比像 CA 这样的离线版本对安全状况的变化反应更迅速。==

**Kerberos** is one example of such an online authentication server.
==**Kerberos** 就是这种在线身份验证服务器的一个例子。==

### 57.7 Some Higher Level Tools
==### 57.7 一些高级工具==

In some cases, we can achieve desirable security effects by working at a higher level.
==在某些情况下，我们可以通过在更高级别上工作来达到理想的安全效果。==

**HTTPS** (the cryptographically protected version of the HTTP protocol) and **SSH** (a competitor to SSL most often used to set up secure sessions with remote computers) are two good examples.
==**HTTPS**（HTTP 协议的加密保护版本）和 **SSH**（SSL 的竞争对手，最常用于建立与远程计算机的安全会话）是两个很好的例子。==

### HTTPS
### HTTPS

HTTP, the protocol that supports the World Wide Web, does not have its own security features.
==HTTP（支持万维网的协议）本身没有安全特性。==

Rather than come up with a fresh implementation of security for HTTP, however, HTTPS takes the existing HTTP definition and connects it to SSL/TLS.
==然而，HTTPS 并没有为 HTTP 提出一套全新的安全实现，而是采用了现有的 HTTP 定义并将其连接到 SSL/TLS。==

SSL takes care of establishing a secure connection, including authenticating the web server using the certificate approach discussed earlier and establishing a new symmetric encryption key known only to the client and server.
==SSL 负责建立安全连接，包括使用前面讨论的证书方法验证 Web 服务器，并建立一个新的仅为客户端和服务器所知的对称加密密钥。==

To a large extent, HTTPS is simply HTTP passed through an SSL connection.
==在很大程度上，HTTPS 只是通过 SSL 连接传输的 HTTP。==

HTTPS makes direct use of a high quality transport security tool, thus replacing an insecure transport with a highly secure transport at very little development cost.
==HTTPS 直接利用高质量的传输安全工具，从而以极低的开发成本将不安全的传输替换为高度安全的传输。==

HTTPS obviously depends heavily on authentication, since we want to be sure we aren’t communicating with malicious web sites.
==HTTPS 显然非常依赖身份验证，因为我们要确保不与恶意网站进行通信。==

HTTPS uses certificates for that purpose.
==HTTPS 为此目的使用证书。==

Remember, however, what a certificate actually tells you, assuming it checks out: that at some moment in time the signing authority thought it was a good idea to vouch that a particular public key belongs to a particular party.
==然而，请记住证书实际上告诉你了什么（假设它通过了检查）：即在某个时间点，签署机构认为担保某个特定公钥属于某个特定方是一个好主意。==

There is no implication that the party is good or evil, that the matching private key is still secret, or even that the certificate signing authority itself is secure and uncompromised.
==这并不暗示该方是善是恶，也不暗示匹配的私钥仍然保密，甚至不暗示证书签署机构本身是安全且未被攻破的。==

Remember also that HTTPS only vouches for authenticity.
==还请记住，HTTPS 仅担保真实性。==

An authenticated web site using HTTPS can still launch an attack on your client.
==使用 HTTPS 的经过身份验证的网站仍然可以对你的客户端发起攻击。==



a push towards servers insisting on HTTPS, and refusing to talk to clients who can’t or won’t speak HTTPS.
==推动服务器强制使用 HTTPS，并拒绝与不能或不愿使用 HTTPS 的客户端进行通信。==

This approach is called HSTS (HTTP Strict Transport Security).
==这种方法被称为 HSTS（HTTP 严格传输安全）。==

HSTS is an option for a web site.
==HSTS 是网站的一个选项。==

If the web site decides it will support HSTS, all interactions with it will be cryptographically secured for any client.
==如果网站决定支持 HSTS，那么与任何客户端的所有交互都将通过加密方式获得安全保障。==

Clients who can’t or won’t accept HTTPS will not be allowed to interact with such a web site.
==不能或不愿接受 HTTPS 的客户端将不被允许与该网站进行交互。==

HSTS is used by a number of major web sites, including Google’s `google.com` domain, but is far from ubiquitous as of 2020.
==许多主流网站都在使用 HSTS，包括 Google 的 `google.com` 域名，但截至 2020 年，它还远未达到普及的程度。==

While HTTPS is primarily intended to help secure web browsing, it is sometimes used to secure other kinds of communications.
==虽然 HTTPS 的主要目的是为了帮助保护网页浏览的安全，但它有时也用于保护其他类型的通信。==

Some developers have leveraged HTTP for purposes rather different than standard web browsing, and, for them, using HTTPS to secure their communications is both natural and cheap.
==一些开发人员利用 HTTP 实现了与标准网页浏览截然不同的目的，对他们来说，使用 HTTPS 来保护通信既自然又廉价。==

However, you can only use HTTPS to secure your system if you commit to using HTTP as your application protocol, and HTTP was intended primarily to support a human-based activity.
==然而，只有当你承诺使用 HTTP 作为应用层协议时，才能使用 HTTPS 来保护系统，而 HTTP 最初主要是为了支持基于人类的行为而设计的。==

HTTP messages, for example, are typically encoded in ASCII and include substantial headers designed to support web browsing needs.
==例如，HTTP 消息通常以 ASCII 编码，并包含大量旨在支持网页浏览需求的头部信息。==

You may be able to achieve far greater efficiency of your application by using SSL, rather than HTTPS.
==通过直接使用 SSL 而不是 HTTPS，你可能会让应用程序获得更高的效率。==

Or you can use SSH.
==或者，你可以使用 SSH。==

**SSH**
**SSH**

SSH stands for **Secure Shell** which accurately describes the original purpose of the program.
==SSH 代表**安全外壳**（Secure Shell），这准确地描述了该程序的最初用途。==

SSH is available on Linux and other Unix systems, and to some extent on Windows systems.
==SSH 可用于 Linux 和其他 Unix 系统，在某种程度上也适用于 Windows 系统。==

SSH was envisioned as a secure remote shell, but it has been developed into a more general tool for allowing secure interactions between computers.
==SSH 最初被设想为一个安全的远程外壳，但后来它被发展成一个更通用的工具，用于允许计算机之间进行安全的交互。==

Most commonly this shell is used for command line interfaces, but SSH can support many other forms of secure remote interactions.
==最常见的情况是，这个外壳用于命令行界面，但 SSH 也可以支持许多其他形式的安全远程交互。==

For example, it can be used to protect remote X Windows sessions.
==例如，它可以用来保护远程 X Windows 会话。==

Generally, TCP ports can be forwarded through SSH, providing a powerful method to protect interactions between remote systems.
==通常，TCP 端口可以通过 SSH 进行转发，从而提供一种强大的方法来保护远程系统之间的交互。==

SSH addresses many of the same problems seen by SSL, often in similar ways.
==SSH 解决了许多与 SSL 相同的问题，而且通常采用类似的方法。==

Remote users must be authenticated, shared encryption keys must be established, integrity must be checked, and so on.
==远程用户必须经过身份验证，必须建立共享加密密钥，必须检查完整性，等等。==

SSH typically relies on public key cryptography and certificates to authenticate remote servers.
==SSH 通常依靠公钥加密和证书来对远程服务器进行身份验证。==

Clients frequently do not have their own certificates and private keys, in which case providing a user ID and password is permitted.
==客户端通常没有自己的证书和私私钥，在这种情况下，允许提供用户 ID 和密码。==

SSH supports other options for authentication not based on certificates or passwords, such as the use of authentication servers (such as Kerberos).
==SSH 支持不基于证书或密码的其他身份验证选项，例如使用身份验证服务器（如 Kerberos）。==

Various ciphers (both for authentication and for symmetric encryption) are supported, and some form of negotiation is required between the client and the server to choose a suitable set.
==SSH 支持各种加密算法（用于身份验证和对称加密），并且客户端和服务器之间需要进行某种形式的协商，以选择一套合适的方案。==

A typical use of SSH provides a good example of a common general kind of network security vulnerability called a **man-in-the-middle attack**.
==SSH 的一个典型应用提供了一个常见的通用网络安全漏洞示例，称为**中间人攻击**（man-in-the-middle attack）。==

This kind of attack occurs when two parties think they are communicating directly, but actually are communicating through a malicious third party without knowing it.
==这种攻击发生在两个当事方认为他们正在直接通信，但实际上是在不知情的情况下通过一个恶意第三方进行通信时。==

That third party sees all of the messages passed between them, and can alter such messages or inject new messages without their knowledge.
==该第三方可以看到在他们之间传递的所有消息，并且可以在他们不知情的情况下修改这些消息或注入新消息。==

Well-designed network security tools are immune to man-in-the-middle attacks of many types, but even a good tool like SSH can sometimes be subject to them.
==设计良好的网络安全工具对多种类型的中间人攻击具有免疫力，但即使是像 SSH 这样优秀的工具，有时也可能遭受此类攻击。==

If you use SSH much, you might have encountered an example yourself.
==如果你经常使用 SSH，你可能自己就遇到过一个例子。==

When you first use SSH to log into a remote machine you’ve never logged into before, you probably don’t have the public key associated with that remote machine.
==当你第一次使用 SSH 登录一台以前从未登录过的远程机器时，你可能没有与该远程机器关联的公钥。==

How do you get it?
==你如何得到它？==

Often, not through a certificate or any other secure means, but simply by asking the remote site to send it to you.
==通常不是通过证书或任何其他安全手段，而只是通过请求远程站点将其发送给你。==

Then you have its public key and away you go, securely authenticating that machine and setting up encrypted communications.
==然后你就拥有了它的公钥，接着就可以对该机器进行安全认证并建立加密通信。==

But what if there’s a man in the middle when you first attempt to log into the remote machine?
==但是，当你第一次尝试登录远程机器时，如果中间有一个人（中间人）怎么办？==

In that case, when the remote machine sends you its public key, the man in the middle can discard the message containing the correct public key and substitute one containing his own public key.
==在这种情况下，当远程机器向你发送其公钥时，中间人可以丢弃包含正确公钥的消息，并替换为包含他自己公钥的消息。==

Now you think you have the public key for the remote server, but you actually have the public key of the man in the middle.
==现在你认为你拥有了远程服务器的公钥，但实际上你拥有的是中间人的公钥。==

That means the man in the middle can pose as the remote server and you’ll never be the wiser.
==这意味着中间人可以伪装成远程服务器，而你永远不会察觉。==

The folks who designed SSH were well aware of this problem, and if you ever do use SSH this way, up will pop a message warning you of the danger and asking if you want to go ahead despite the risk.
==SSH 的设计者们深知这个问题，如果你真的以这种方式使用 SSH，就会弹出一个消息，警告你存在危险，并询问你是否要在冒风险的情况下继续。==

Folk wisdom suggests that everyone always says “yes, go ahead” when they get this message, including network security professionals.
==民间智慧表明，每个人在收到这条消息时总是会说“是的，继续”，包括网络安全专业人员也是如此。==

For that matter, folk wisdom suggests that all messages warning a user of the possibility of insecure actions are always ignored, which should suggest to you just how much security benefit will arise from adding such confirmation messages to your system.
==就此而言，民间智慧还表明，所有警告用户可能存在不安全行为的消息总是被忽略，这应该会让你意识到在系统中添加此类确认消息能产生多少安全效益。==

SSH is not built on SSL, but is a separate implementation.
==SSH 不是构建在 SSL 之上的，而是一个独立的实现。==

As a result, the two approaches each have their own bugs, features, and uses.
==因此，这两种方法各有其自身的错误、特性和用途。==

A security flaw found in SSH will not necessarily have any impact on SSL, and vice versa.
==在 SSH 中发现的安全缺陷不一定会对 SSL 产生任何影响，反之亦然。==

**Summary**
==**总结**==

Distributed systems are critical to modern computing, but are difficult to secure.
==分布式系统对现代计算至关重要，但很难保证其安全性。==

The cornerstone of providing distributed system security tends to be ensuring that the insecure network connecting system components does not introduce new security problems.
==提供分布式系统安全性的基石往往是确保连接系统组件的不安全网络不会引入新的安全问题。==

Messages sent between the components are encrypted and authenticated, protecting their privacy and integrity, and offering exclusive access to the distributed service to the intended users.
==在组件之间发送的消息经过加密和身份验证，从而保护了它们的隐私性和完整性，并为预期用户提供对分布式服务的专享访问权限。==

Standard tools like SSL/TLS and public keys distributed through X.509 certificates are used to provide these security services.
==SSL/TLS 等标准工具以及通过 X.509 证书分发的公钥被用于提供这些安全服务。==

Passwords are often used to authenticate remote human users.
==密码通常用于对远程人类用户进行身份验证。==

Symmetric cryptography is used for transport of most data, since it is cheaper than asymmetric cryptography.
==对称加密用于大多数数据的传输，因为它比非对称加密更廉价。==

Often, symmetric keys are not shared by system participants before the communication starts, so the first step in the protocol is typically exchanging a symmetric key.
==通常，系统参与者在通信开始前不会共享对称密钥，因此协议的第一步通常是交换对称密钥。==

As discussed in previous chapters, key secrecy is critical in proper use of cryptography, so care is required in the key distribution process.
==正如前几章所述，密钥保密性在密码学的正确使用中至关重要，因此在密钥分发过程中需要格外小心。==

**Diffie-Hellman key exchange** is commonly used, but it still requires authentication to ensure that only the intended participants know the key.
==**Diffie-Hellman 密钥交换**是常用的方法，但它仍然需要身份验证，以确保只有预期的参与者知道密钥。==

As mentioned in earlier chapters, building your own cryptographic solutions is challenging and often leads to security failures.
==正如前几章提到的，构建自己的加密解决方案具有挑战性，并且经常导致安全故障。==

A variety of tools, including SSL/TLS, SSH, and HTTPS, have already tackled many of the challenging problems and made good progress in overcoming them.
==包括 SSL/TLS、SSH 和 HTTPS 在内的各种工具已经解决了许多具有挑战性的问题，并在克服这些问题方面取得了良好进展。==

These tools can be used to build other systems, avoiding many of the pitfalls of building cryptography from scratch.
==这些工具可以用来构建其他系统，从而避免了从头开始构建加密系统的许多陷阱。==

However, proper use of even the best security tools depends on an understanding of the tool’s purpose and limitations, so developing deeper knowledge of the way such tools can be integrated into one’s system is vital to using them to their best advantage.
==然而，即使是最好的安全工具，其正确使用也取决于对工具用途和局限性的理解，因此，深入了解此类工具如何集成到个人系统中的知识，对于发挥其最大优势至关重要。==

Remember that these tools only make limited security guarantees.
==请记住，这些工具仅提供有限的安全保证。==

They do not provide the same assurance that an operating system gets when it performs actions locally on hardware under its direct control.
==它们无法提供像操作系统在受其直接控制的硬件上本地执行操作时所获得的那种保证。==

Thus, even when using good authentication and encryption tools properly, a system designer is well advised to think carefully about the implications of performing actions requested by a remote site, or providing sensitive information to that site.
==因此，即使正确使用了良好的身份验证和加密工具，系统设计者最好也要仔细考虑执行远程站点请求的操作或向该站点提供敏感信息的后果。==

What happens beyond the boundary of the machine the OS controls is always uncertain and thus risky.
==在操作系统控制的机器边界之外发生的事情总是具有不确定性，因此也充满风险。==

**A Dialogue on Virtual Machine Monitors**
==**关于虚拟机监控器的对话**==

Student: So now we’re stuck in the Appendix, huh?
==学生：所以我们现在被困在附录里了，是吗？==

Professor: Yes, just when you thought things couldn’t get any worse.
==教授：是的，就在你认为情况不会变得更糟的时候。==

Student: Well, what are we going to talk about?
==学生：好吧，我们要谈论什么？==

Professor: An old topic that has been reborn: **virtual machine monitors**, also known as **hypervisors**.
==教授：一个重获新生的老话题：**虚拟机监控器**（virtual machine monitors），也被称为**管理程序**（hypervisors）。==

Student: Oh, like VMware? That’s cool; I’ve used that kind of software before.
==学生：噢，就像 VMware 那样吗？那很酷；我以前用过那种软件。==

Professor: Cool indeed. We’ll learn how VMMs add yet another layer of virtualization into systems, this one beneath the OS itself! Crazy and amazing stuff, really.
==教授：确实很酷。我们将学习 VMM 如何在系统中增加另一层虚拟化，而这一层就在操作系统本身之下！真的是既疯狂又神奇的东西。==

Student: Sounds neat. Why not include this in the earlier part of the book, then, on virtualization? Shouldn’t it really go there?
==学生：听起来很棒。那为什么不把这部分放在书的前面关于虚拟化的部分呢？它难道不应该放在那里吗？==

Professor: That’s above our pay grade, I’m afraid. But my guess is this: there is already a lot of material there.
==教授：恐怕那超出了我们的职权范围。但我的猜测是：那里已经有很多材料了。==

By moving this small aside on VMMs into the appendix, a particular instructor can choose whether to include it or skip it.
==通过将关于 VMM 的这段简短插叙移到附录中，特定的教师可以自行选择是包含它还是跳过它。==

But I do think it should be included, because if you can understand how VMMs work, then you really understand virtualization quite well.
==但我确实认为它应该被包含进去，因为如果你能理解 VMM 的工作原理，那么你就真正很好地理解了虚拟化。==

Student: Alright then, let’s get to work!
==学生：那好吧，让我们开始工作吧！==

**Virtual Machine Monitors**
==**虚拟机监控器**==

**B.1 Introduction**
==**B.1 引言**==

Years ago, IBM sold expensive mainframes to large organizations, and a problem arose: what if the organization wanted to run different operating systems on the machine at the same time?
==多年前，IBM 向大型机构出售昂贵的大型机，随后出现了一个问题：如果该机构想在同一台机器上同时运行不同的操作系统怎么办？==

Some applications had been developed on one OS, and some on others, and thus the problem.
==有些应用程序是在一个操作系统上开发的，有些是在其他系统上开发的，问题由此产生。==

As a solution, IBM introduced yet another level of indirection in the form of a **virtual machine monitor (VMM)** (also called a **hypervisor**) [G74].
==作为解决方案，IBM 引入了另一种形式的间接层，即**虚拟机监控器 (VMM)**（也称为**管理程序**）[G74]。==

Specifically, the monitor sits between one or more operating systems and the hardware and gives the illusion to each running OS that it controls the machine.
==具体来说，监控器位于一个或多个操作系统与硬件之间，并给每个运行中的操作系统一种它控制着机器的错觉。==

Behind the scenes, however, the monitor actually is in control of the hardware, and must multiplex running OSes across the physical resources of the machine.
==然而，在幕后，监控器实际上控制着硬件，并且必须在机器的物理资源上多路复用运行中的操作系统。==

Indeed, the VMM serves as an operating system for operating systems, but at a much lower level; the OS must still think it is interacting with the physical hardware.
==事实上，VMM 充当了操作系统的操作系统，但处于更低的层级；操作系统必须仍然认为它正在与物理硬件进行交互。==

Thus, **transparency** is a major goal of VMMs.
==因此，**透明性**是 VMM 的一个主要目标。==

Thus, we find ourselves in a funny position: the OS has thus far served as the master illusionist, tricking unsuspecting applications into thinking they have their own private CPU and a large virtual memory, while secretly switching between applications and sharing memory as well.
==因此，我们发现自己处于一个滑稽的境地：到目前为止，操作系统一直充当着首席幻术师，欺骗那些毫无戒心的应用程序，让它们以为自己拥有私有的 CPU 和巨大的虚拟内存，而实际上却在秘密地在应用程序之间进行切换并共享内存。==

Now, we have to do it again, but this time underneath the OS, who is used to being in charge.
==现在，我们要再来一次，但这次是在习惯于发号施令的操作系统之下。==

How can the VMM create this illusion for each OS running on top of it?
==VMM 如何为运行在其上的每个操作系统创造这种错觉？==

**THE CRUX: HOW TO VIRTUALIZE THE MACHINE UNDERNEATH THE OS**
==**关键点：如何在操作系统之下实现机器虚拟化**==

The virtual machine monitor must transparently virtualize the machine underneath the OS; what are the techniques required to do so?
==虚拟机监控器必须透明地虚拟化操作系统之下的机器；实现这一目标需要哪些技术？==

**B.2 Motivation: Why VMMs?**
==**B.2 动机：为什么需要 VMM？**==

Today, VMMs have become popular again for a multitude of reasons.
==今天，由于多种原因，VMM 再次流行起来。==

**Server consolidation** is one such reason.
==**服务器整合**就是原因之一。==

In many settings, people run services on different machines which run different operating systems (or even OS versions), and yet each machine is lightly utilized.
==在许多环境下，人们在不同的机器上运行服务，这些机器运行着不同的操作系统（甚至不同的操作系统版本），但每台机器的利用率都很低。==

In this case, virtualization enables an administrator to **consolidate** multiple OSes onto fewer hardware platforms, and thus lower costs and ease administration.
==在这种情况下，虚拟化使管理员能够将多个操作系统**整合**到更少的硬件平台上，从而降低成本并简化管理。==

Virtualization has also become popular on desktops, as many users wish to run one operating system (say Linux or Mac OS X) but still have access to native applications on a different platform (say Windows).
==虚拟化在桌面端也变得流行起来，因为许多用户希望运行一个操作系统（比如 Linux 或 Mac OS X），但仍然能够访问另一个平台（比如 Windows）上的原生应用程序。==

This type of improvement in **functionality** is also a good reason.
==这种**功能性**方面的改进也是一个很好的理由。==

Another reason is testing and debugging.
==另一个原因是测试和调试。==

While developers write code on one main platform, they often want to debug and test it on the many different platforms that they deploy the software to in the field.
==虽然开发人员在一个主平台上编写代码，但他们通常希望在将来部署软件的许多不同平台上进行调试和测试。==

Thus, virtualization makes it easy to do so, by enabling a developer to run many operating system types and versions on just one machine.
==因此，虚拟化通过使开发人员能够在仅一台机器上运行许多操作系统类型和版本，从而轻松实现了这一点。==

This resurgence in virtualization began in earnest the mid-to-late 1990’s, and was led by a group of researchers at Stanford headed by Professor Mendel Rosenblum.
==这场虚拟化的复兴始于 20 世纪 90 年代中后期，由斯坦福大学教授 Mendel Rosenblum 领导的一组研究人员发起。==

His group’s work on Disco [B+97], a virtual machine monitor for the MIPS processor, was an early effort that revived VMMs and eventually led that group to the founding of VMware [V98], now a market leader in virtualization technology.
==他的小组在 Disco [B+97]（一个针对 MIPS 处理器的虚拟机监控器）上的工作是复兴 VMM 的早期尝试，并最终促使该小组创立了 VMware [V98]，该公司现在是虚拟化技术的市场领导者。==

In this chapter, we will discuss the primary technology underlying Disco and through that window try to understand how virtualization works.
==在本章中，我们将讨论 Disco 背后的主要技术，并通过这个窗口尝试了解虚拟化是如何工作的。==

**B.3 Virtualizing the CPU**
==**B.3 CPU 虚拟化**==

To run a **virtual machine** (e.g., an OS and its applications) on top of a virtual machine monitor, the basic technique that is used is **limited direct execution**, a technique we saw before when discussing how the OS virtualizes the CPU.
==要在虚拟机监控器上运行**虚拟机**（例如，操作系统及其应用程序），使用的基本技术是**受限直接执行**（limited direct execution），这是我们之前在讨论操作系统如何虚拟化 CPU 时见过的技术。==

Thus, when we wish to “boot” a new OS on top of the VMM, we simply jump to the address of the first instruction and let the OS begin running.
==因此，当我们希望在 VMM 上“启动”一个新操作系统时，我们只需跳转到第一条指令的地址，让操作系统开始运行即可。==

It is as simple as that (well, almost).
==就是这么简单（好吧，几乎如此）。==

Assume we are running on a single processor, and that we wish to multiplex between two virtual machines, that is, between two OSes and their respective applications.
==假设我们运行在单处理器上，并且希望在两个虚拟机之间进行多路复用，即在两个操作系统及其各自的应用程序之间进行切换。==

In a manner quite similar to an operating system switching between running processes (a **context switch**), a virtual machine monitor must perform a **machine switch** between running virtual machines.
==以一种与操作系统在运行进程之间切换（**上下文切换**）非常相似的方式，虚拟机监控器必须在运行中的虚拟机之间执行**机器切换**。==

Thus, when performing such a switch, the VMM must save the entire machine state of one OS (including registers, PC, and unlike in a context switch, any privileged hardware state), restore the machine state of the to-be-run VM, and then jump to the PC of the to-be-run VM and thus complete the switch.
==因此，在执行此类切换时，VMM 必须保存一个操作系统的整个机器状态（包括寄存器、PC，并且与上下文切换不同，还包括任何特权硬件状态），恢复待运行虚拟机的机器状态，然后跳转到待运行虚拟机的 PC，从而完成切换。==

Note that the to-be-run VM’s PC may be within the OS itself (i.e., the system was executing a system call) or it may simply be within a process that is running on that OS (i.e., a user-mode application).
==请注意，待运行虚拟机的 PC 可能位于操作系统本身内部（即系统正在执行系统调用），或者可能只是位于该操作系统上运行的一个进程中（即用户模式应用程序）。==

We get into some slightly trickier issues when a running application or OS tries to perform some kind of **privileged operation**.
==当正在运行的应用程序或操作系统尝试执行某种**特权操作**时，我们会遇到一些稍微棘手的问题。==

For example, on a system with a software-managed TLB, the OS will use special privileged instructions to update the TLB with a translation before restarting an instruction that suffered a TLB miss.
==例如，在具有软件管理 TLB 的系统上，操作系统将使用特殊的特权指令，在重新启动发生 TLB 未命中的指令之前，使用转换后的地址更新 TLB。==

In a virtualized environment, the OS cannot be allowed to perform privileged instructions, because then it controls the machine rather than the VMM beneath it.
==在虚拟化环境中，不能允许操作系统执行特权指令，因为这样它就控制了机器，而不是其下的 VMM。==

Thus, the VMM must somehow intercept attempts to perform privileged operations and thus retain control of the machine.
==因此，VMM 必须以某种方式拦截执行特权操作的尝试，从而保持对机器的控制。==

A simple example of how a VMM must interpose on certain operations arises when a running process on a given OS tries to make a system call.
==当给定操作系统上的运行进程尝试进行系统调用时，就会出现 VMM 必须干预某些操作的简单例子。==

For example, the process may be trying to call `open()` on a file, or may be calling `read()` to get data from it, or may be calling `fork()` to create a new process.
==例如，进程可能正尝试对文件调用 `open()`，或者可能正在调用 `read()` 以从中获取数据，或者可能正在调用 `fork()` 以创建一个新进程。==

In a system without virtualization, a system call is achieved with a special instruction; on MIPS, it is a **trap** instruction, and on x86, it is the `int` (an interrupt) instruction with the argument `0x80`.
==在没有虚拟化的系统中，系统调用是通过一条特殊指令实现的；在 MIPS 上，它是一条**陷阱**（trap）指令，在 x86 上，它是带有参数 `0x80` 的 `int`（中断）指令。==

Here is the `open` library call on FreeBSD [B00] (recall that your C code first makes a library call into the C library, which then executes the proper assembly sequence to actually issue the trap instruction and make a system call):
==这是 FreeBSD [B00] 上的 `open` 库调用（回想一下，你的 C 代码首先调用 C 库，然后 C 库执行适当的汇编序列，以实际发出陷阱指令并进行系统调用）：==

```assembly
open:
    push dword mode
    push dword flags
    push dword path
    mov eax, 5
    push eax
    int 80h
```

On UNIX-based systems, `open()` takes just three arguments: `int open(char *path, int flags, mode_t mode)`.
==在基于 UNIX 的系统上，`open()` 仅需三个参数：`int open(char *path, int flags, mode_t mode)`。==

You can see in the code above how the `open()` library call is implemented: first, the arguments get pushed onto the stack (`mode`, `flags`, `path`), then a `5` gets pushed onto the stack, and then `int 80h` is called, which transfers control to the kernel.
==你可以在上面的代码中看到 `open()` 库调用是如何实现的：首先，参数被压入栈中（`mode`、`flags`、`path`），然后将 `5` 压入栈中，接着调用 `int 80h`，这会将控制权转移给内核。==

The `5`, if you were wondering, is the pre-agreed upon convention between user-mode applications and the kernel for the `open()` system call in FreeBSD; different system calls would place different numbers onto the stack (in the same position) before calling the trap instruction `int` and thus making the system call.
==如果你感到好奇，这里的 `5` 是 FreeBSD 中用户模式应用程序与内核之间针对 `open()` 系统调用预先约定的规范；不同的系统调用在调用陷阱指令 `int` 并进行系统调用之前，会将不同的数字放置在栈的同一位置。==

When a trap instruction is executed, as we’ve discussed before, it usually does a number of interesting things.
==正如我们之前讨论过的，当执行陷阱指令时，它通常会做一些有趣的事情。==

Most important in our example here is that it first transfers control (i.e., changes the PC) to a well-defined **trap handler** within the operating system.
==在我们这里的例子中，最重要的一点是它首先将控制权转移（即更改 PC）到操作系统中定义的**陷阱处理程序**（trap handler）。==

The OS, when it is first starting up, establishes the address of such a routine with the hardware (also a privileged operation) and thus upon subsequent traps, the hardware knows where to start running code to handle the trap.
==操作系统在最初启动时，向硬件登记此类例程的地址（这也是一种特权操作），因此在随后的陷阱发生时，硬件知道从哪里开始运行代码来处理陷阱。==

At the same time of the trap, the hardware also does one other crucial thing: it changes the mode of the processor from **user mode** to **kernel mode**.
==在发生陷阱的同时，硬件还做了另一件至关重要的事：它将处理器的模式从**用户模式**更改为**内核模式**。==

In user mode, operations are restricted, and attempts to perform privileged operations will lead to a trap and likely the termination of the offending process; in kernel mode, on the other hand, the full power of the machine is available, and thus all privileged operations can be executed.
==在用户模式下，操作受到限制，尝试执行特权操作将导致陷阱，并可能终止违规进程；另一方面，在内核模式下，可以使用机器的所有功能，因此可以执行所有特权操作。==

Thus, in a traditional setting (again, without virtualization), the flow of control would be like what you see in Figure B.1.
==因此，在传统环境下（同样是没有虚拟化），控制流将如图 B.1 所示。==

On a virtualized platform, things are a little more interesting.
==在虚拟化平台上，情况变得更有趣一些。==

When an application running on an OS wishes to perform a system call, it does the exact same thing: executes a trap instruction with the arguments carefully placed on the stack (or in registers).
==当运行在操作系统上的应用程序希望执行系统调用时，它会执行完全相同的操作：执行陷阱指令，并将参数仔细地放置在栈中（或寄存器中）。==

However, it is the VMM that controls the machine, and thus the VMM who has installed a trap handler that will first get executed in kernel mode.
==然而，由于是 VMM 控制着机器，因此是 VMM 安装的陷阱处理程序将首先在内核模式下执行。==

So what should the VMM do to handle this system call?
==那么，VMM 应该如何处理这个系统调用呢？==

The VMM doesn’t really know **how** to handle the call; after all, it does not know the details of each OS that is running and therefore does not know what each call should do.
==VMM 实际上并不知道**如何**处理该调用；毕竟，它不知道每个正在运行的操作系统的细节，因此也就不知道每个调用应该做什么。==

What the VMM does know, however, is **where** the OS’s trap handler is.
==然而，VMM 确实知道操作系统的陷阱处理程序在**哪里**。==

It knows this because when the OS booted up, it tried to install its own trap handlers; when the OS did so, it was trying to do something privileged, and therefore trapped into the VMM; at that time, the VMM recorded the necessary information (i.e., where this OS’s trap handlers are in memory).
==它知道这一点是因为当操作系统启动时，它尝试安装自己的陷阱处理程序；当操作系统这样做时，它正在尝试执行特权操作，因此陷入了 VMM；那时，VMM 记录了必要的信息（即该操作系统的陷阱处理程序在内存中的位置）。==

Now, when the VMM receives a trap from a user process running on the given OS, it knows exactly what to do: it jumps to the OS’s trap handler and lets the OS handle the system call as it should.
==现在，当 VMM 接收到来自运行在给定操作系统上的用户进程的陷阱时，它确切地知道该怎么做：它跳转到该操作系统的陷阱处理程序，让操作系统按其应有的方式处理系统调用。==

When the OS is finished, it executes some kind of privileged instruction to return from the trap (`rett` on MIPS, `iret` on x86), which again bounces into the VMM, which then realizes that the OS is trying to return from the trap and thus performs a real return-from-trap and thus returns control to the user and puts the machine back in user mode.
==当操作系统完成处理后，它会执行某种特权指令以从陷阱返回（MIPS 上为 `rett`，x86 上为 `iret`），这再次弹回到 VMM 中，VMM 随后意识到操作系统正尝试从陷阱返回，从而执行真正的“从陷阱返回”操作，进而将控制权返回给用户并将机器重新置于用户模式。==

The entire process is depicted in Figures B.2 and B.3, both for the normal case without virtualization and the case with virtualization.
==整个过程在图 B.2 和 B.3 中进行了描述，分别针对没有虚拟化的正常情况和有虚拟化的情况。==

As you can see from the figures, a lot more has to take place when virtualization is going on.
==正如你从图中看到的，在进行虚拟化时，必须发生更多的事情。==

Certainly, because of the extra jumping around, virtualization might indeed slow down system calls and thus could hurt performance.
==当然，由于额外的跳转，虚拟化确实可能会减慢系统调用速度，从而可能损害性能。==

You might also notice that we have one remaining question: what mode should the OS run in?
==你可能还会注意到，我们还有一个遗留问题：操作系统应该在什么模式下运行？==

It can’t run in kernel mode, because then it would have unrestricted access to the hardware.
==它不能在内核模式下运行，因为那样它将拥有对硬件的无限制访问权限。==

Thus, it must run in some less privileged mode than before, be able to access its own data structures, and simultaneously prevent access to its data structures from user processes.
==因此，它必须在比以前特权更低的某种模式下运行，既能访问自己的数据结构，同时又要防止用户进程访问其数据结构。==

In the Disco work, Rosenblum and colleagues handled this problem quite neatly by taking advantage of a special mode provided by the MIPS hardware known as **supervisor mode**.
==在 Disco 的工作中，Rosenblum 及其同事利用 MIPS 硬件提供的一种称为**主管模式**（supervisor mode）的特殊模式，非常巧妙地处理了这个问题。==

When running in this mode, one still doesn’t have access to privileged instructions, but one can access a little more memory than when in user mode; the OS can use this extra memory for its data structures and all is well.
==在这种模式下运行时，虽然仍然无法访问特权指令，但可以访问比用户模式稍多一点的内存；操作系统可以将这些额外的内存用于其数据结构，一切进展顺利。==

On hardware that doesn’t have such a mode, one has to run the OS in user mode and use memory protection (page tables and TLBs) to protect OS data structures appropriately.
==在没有这种模式的硬件上，必须在用户模式下运行操作系统，并使用内存保护（页表和 TLB）来适当地保护操作系统数据结构。==

In other words, when switching into the OS, the monitor would have to make the memory of the OS data structures available to the OS via page-table protections; when switching back to the running application, the ability to read and write the kernel would have to be removed.
==换句话说，当切换到操作系统时，监控器必须通过页表保护使操作系统的内存数据结构对操作系统可用；当切换回运行中的应用程序时，必须移除读取和写入内核的能力。==

**B.4 Virtualizing Memory**
==**B.4 内存虚拟化**==

You should now have a basic idea of how the processor is virtualized: the VMM acts like an OS and schedules different virtual machines to run, and some interesting interactions occur when privilege levels change.
==你现在应该对处理器是如何虚拟化的有了基本了解：VMM 表现得像一个操作系统，调度不同的虚拟机运行，并且在特权级别改变时会发生一些有趣的交互。==

But we have left out a big part of the equation: how does the VMM virtualize memory?
==但我们遗漏了方程中的一个重要部分：VMM 如何虚拟化内存？==

Each OS normally thinks of physical memory as a linear array of pages, and assigns each page to itself or user processes.
==每个操作系统通常将物理内存视为线性的页面数组，并将每个页面分配给自身或用户进程。==

The OS itself, of course, already virtualizes memory for its running processes, such that each process has the illusion of its own private address space.
==当然，操作系统本身已经为其运行中的进程虚拟化了内存，使得每个进程都拥有自己私有地址空间的错觉。==

Now we must add another layer of virtualization, so that multiple OSes can share the actual physical memory of the machine, and we must do so transparently.
==现在我们必须增加另一层虚拟化，以便多个操作系统可以共享机器的实际物理内存，并且我们必须透明地做到这一点。==

This extra layer of virtualization makes “physical” memory a virtualization on top of what the VMM refers to as **machine memory**, which is the real physical memory of the system.
==这额外的一层虚拟化使得“物理”内存成为构建在 VMM 所称的**机器内存**（machine memory）之上的虚拟化，而机器内存才是系统的真实物理内存。==

Thus, we now have an additional layer of indirection: each OS maps virtual-to-physical addresses via its per-process page tables; the VMM maps the resulting physical mappings to underlying machine addresses via its per-OS page tables.
==因此，我们现在有了一个额外的间接层：每个操作系统通过其每个进程的页表将虚拟地址映射到物理地址；VMM 通过其每个操作系统的页表将产生的物理映射映射到底层的机器地址。==

Figure B.4 depicts this extra level of indirection.
==图 B.4 描绘了这一额外的间接层。==

In the figure, there is just a single virtual address space with four pages, three of which are valid (0, 2, and 3).
==在图中，只有一个拥有四个页面的虚拟地址空间，其中三个页面是有效的（0、2 和 3）。==

The OS uses its page table to map these pages to three underlying physical frames (10, 3, and 8, respectively).
==操作系统使用其页表将这些页面映射到三个底层的物理页框（分别是 10、3 和 8）。==

Underneath the OS, the VMM performs a further level of indirection, mapping PFNs 3, 8, and 10 to machine frames 6, 10, and 5 respectively.
==在操作系统之下，VMM 执行了进一步的间接映射，分别将物理页框号（PFN）3、8 和 10 映射到机器页框 6、10 和 5。==

To understand how this works a little better, let’s recall how **address translation** works in a modern paged system.
==为了更好地理解这是如何工作的，让我们回顾一下在现代分页系统中**地址转换**是如何工作的。==

Specifically, let’s discuss what happens on a system with a software-managed TLB during address translation.
==具体来说，让我们讨论在具有软件管理 TLB 的系统上，地址转换期间会发生什么。==

Assume a user process generates an address; by definition, the process generates a **virtual address**, as its address space has been virtualized by the OS.
==假设一个用户进程生成了一个地址；根据定义，该进程生成的是一个**虚拟地址**，因为其地址空间已被操作系统虚拟化。==

As you know by now, it is the role of the OS, with help from the hardware, to turn this into a **physical address** and thus be able to fetch the desired contents from physical memory.
==正如你现在所知道的，操作系统的角色是在硬件的帮助下，将其转换为**物理地址**，从而能够从物理内存中获取所需的内容。==

Assume we have a 32-bit virtual address space and a 4-KB page size.
==假设我们有一个 32 位虚拟地址空间和 4-KB 的页面大小。==

Thus, our 32-bit address is chopped into two parts: a 20-bit virtual page number (VPN), and a 12-bit offset.
==因此，我们的 32 位地址被切分为两部分：20 位的虚拟页号（VPN）和 12 位的偏移量。==

The role of the OS, with help from the hardware TLB, is to translate the VPN into a valid physical page frame number (PFN) and thus produce a fully-formed physical address which can be sent to physical memory to fetch the proper data.
==操作系统的角色是在硬件 TLB 的帮助下，将 VPN 转换为有效的物理页框号（PFN），从而生成一个完整的物理地址，该地址可以发送到物理内存以获取正确的数据。==

In the common case, we expect the TLB to handle the translation in hardware, thus making the translation fast.
==在通常情况下，我们希望 TLB 在硬件中处理转换，从而使转换速度更快。==

When a TLB miss occurs (at least, on a system with a software-managed TLB), the OS must get involved to service the miss, as depicted here in Figure B.5.
==当发生 TLB 未命中时（至少在具有软件管理 TLB 的系统上），操作系统必须介入以处理该未命中，如图 B.5 所示。==

As you can see, a TLB miss causes a trap into the OS, which handles the fault by looking up the VPN in the page table and installing the translation in the TLB.
==如你所见，TLB 未命中会导致陷入操作系统，操作系统通过在页表中查找 VPN 并将转换后的映射安装到 TLB 中来处理该故障。==

With a virtual machine monitor underneath the OS, however, things again get a little more interesting.
==然而，在操作系统下方有了虚拟机监控器后，情况再次变得更有趣了。==

When a process makes a virtual memory reference and misses in the TLB, it is not the OS TLB miss handler that runs; rather, it is the VMM TLB miss handler, as the VMM is the true privileged owner of the machine.
==当一个进程进行虚拟内存引用并在 TLB 中未命中时，运行的不是操作系统的 TLB 未命中处理程序；相反，是 VMM 的 TLB 未命中处理程序在运行，因为 VMM 才是机器真正的特权所有者。==

However, in the normal case, the VMM TLB handler doesn’t know how to handle the TLB miss, so it immediately jumps into the OS TLB miss handler; the VMM knows the location of this handler because the OS, during “boot”, tried to install its own trap handlers.
==然而，在正常情况下，VMM TLB 处理程序不知道如何处理 TLB 未命中，因此它立即跳转到操作系统的 TLB 未命中处理程序中；VMM 知道此处理程序的位置，因为操作系统在“启动”期间尝试安装了自己的陷阱处理程序。==

The OS TLB miss handler then runs, does a page table lookup for the VPN in question, and tries to install the VPN-to-PFN mapping in the TLB.
==操作系统的 TLB 未命中处理程序随后运行，对相关的 VPN 进行页表查找，并尝试在 TLB 中安装 VPN 到 PFN 的映射。==

However, doing so is a privileged operation, and thus causes another trap into the VMM.
==然而，这样做是一项特权操作，因此会导致再次陷入 VMM。==

At this point, the VMM plays its trick: instead of installing the OS’s VPN-to-PFN mapping, the VMM installs its desired VPN-to-MFN mapping.
==此时，VMM 耍了一个花招：它不安装操作系统的 VPN 到 PFN 映射，而是安装它所期望的 VPN 到 MFN（机器页框号）映射。==

After doing so, the system eventually gets back to the user-level code, which retries the instruction, and results in a TLB hit, fetching the data from the machine frame where the data resides.
==执行完此操作后，系统最终会返回到用户级代码，该代码会重试指令，并导致 TLB 命中，从而从数据所在的机器页框中获取数据。==

**ASIDE: PARA-VIRTUALIZATION**
==**插叙：半虚拟化**==

In many situations, it is good to assume that the OS cannot be modified in order to work better with virtual machine monitors.
==在许多情况下，最好假设操作系统不能被修改，以便更好地与虚拟机监控器协作。==

However, this is not always the case, and when the OS can be modified, it may run more efficiently on top of a VMM.
==然而，情况并非总是如此，当操作系统可以被修改时，它在 VMM 上运行的效率可能会更高。==

Running a modified OS to run on a VMM is generally called **para-virtualization** [WSG02], as the virtualization provided by the VMM isn’t a complete one, but rather a partial one requiring OS changes to operate effectively.
==为了在 VMM 上运行而运行经过修改的操作系统，通常被称为**半虚拟化**（para-virtualization）[WSG02]，因为 VMM 提供的虚拟化不是完全的虚拟化，而是需要修改操作系统才能有效运行的部分虚拟化。==

Research shows that a properly-designed para-virtualized system, with just the right OS changes, can be made to be nearly as efficient a system without a VMM [BD+03].
==研究表明，通过恰当的操作系统修改，一个设计良好的半虚拟化系统可以变得几乎与没有 VMM 的系统一样高效 [BD+03]。==

**B.5 The Information Gap**
==**B.5 信息鸿沟**==

Just like the OS doesn’t know too much about what application programs really want, the VMM often doesn’t know too much about what the OS is doing or wanting.
==就像操作系统不太了解应用程序真正想要什么一样，VMM 往往也不太了解操作系统正在做什么或想要什么。==

This lack of knowledge, sometimes called the **information gap** between the VMM and the OS, can lead to various inefficiencies [B+97].
==这种知识的匮乏，有时被称为 VMM 与操作系统之间的**信息鸿沟**（information gap），可能会导致各种效率低下 [B+97]。==

For example, an OS, when it has nothing else to run, will sometimes go into an **idle loop** just spinning and waiting for the next interrupt to occur:
==例如，一个操作系统在没有其他任务可运行时，有时会进入一个**空闲循环**（idle loop），只是在那里轮询并等待下一个中断的发生：==

```c
while (1)
    ; // the idle loop
```

It makes sense to spin like this if the OS is in charge of the entire machine and thus knows there is nothing else that needs to run.
==如果操作系统掌控着整台机器，并且知道没有其他任务需要运行，那么像这样轮询是有意义的。==

However, when a VMM is running underneath two different OSes, one in the idle loop and one usefully running user processes, it would be useful for the VMM to know that one OS is idle so it can give more CPU time to the OS doing useful work.
==然而，当 VMM 在两个不同的操作系统之下运行时，如果其中一个处于空闲循环，另一个正在有用地运行用户进程，那么让 VMM 知道其中一个操作系统处于空闲状态将非常有用，这样它就可以将更多的 CPU 时间分配给那个正在做有用工作的操作系统。==

Another example arises with **demand zeroing** of pages.
==另一个例子出现在页面的**请求置零**（demand zeroing）中。==

Most operating systems zero a physical frame before mapping it into a process’s address space.
==大多数操作系统在将物理页框映射到进程地址空间之前会将其置零。==

The reason for doing so is simple: security.
==这样做的主因很简单：安全。==

If the OS gave one process a page that another had been using without zeroing it, an information leak across processes could occur.
==如果操作系统将一个进程使用过的页面在不置零的情况下分配给另一个进程，可能会发生进程间的信息泄漏。==

Unfortunately, the VMM must zero pages that it gives to each OS, for the same reason, and thus many times a page will be zeroed twice.
==不幸的是，出于同样的原因，VMM 也必须将其交给每个操作系统的页面置零，因此很多时候一个页面会被置零两次。==

**B.6 Summary**
==**B.6 总结**==

Virtualization is in a renaissance.
==虚拟化正处于复兴时期。==

For a multitude of reasons, users and administrators want to run multiple OSes on the same machine at the same time.
==出于多种原因，用户和管理员希望在同一台机器上同时运行多个操作系统。==

The key is that VMMs generally provide this service **transparently**; the OS above has little clue that it is not actually controlling the hardware of the machine.
==关键在于 VMM 通常**透明地**提供这种服务；其上的操作系统几乎不知道自己实际上并没有控制机器的硬件。==

The key method that VMMs use to do so is to extend the notion of **limited direct execution**.
==VMM 实现这一目标的关键方法是扩展**受限直接执行**的概念。==

By setting up the hardware to enable the VMM to interpose on key events (such as traps), the VMM can completely control how machine resources are allocated while preserving the illusion that the OS requires.
==通过设置硬件以使 VMM 能够干预关键事件（如陷阱），VMM 可以完全控制机器资源的分配方式，同时保持操作系统所需的错觉。==

You might have noticed some similarities between what the OS does for processes and what the VMM does for OSes.
==你可能已经注意到操作系统为进程所做的与 VMM 为操作系统所做的之间有一些相似之处。==

They both virtualize the hardware after all, and hence do some of the same things.
==毕竟它们都对硬件进行了虚拟化，因此执行了一些相同的操作。==

However, there is one key difference: with the OS virtualization, a number of new abstractions and nice interfaces are provided; with VMM-level virtualization, the abstraction is identical to the hardware (and thus not very nice).
==然而，有一个关键的区别：通过操作系统虚拟化，提供了一系列新的抽象和友好的接口；而通过 VMM 级虚拟化，抽象与硬件完全一致（因此并不怎么友好）。==

**Monitors (Deprecated)**
==**管程（已弃用）**==

Around the time concurrent programming was becoming a big deal, object-oriented programming was also gaining ground.
==在大约并发编程变得重要的时候，面向对象编程也在取得进展。==

Not surprisingly, people started to think about ways to merge synchronization into a more structured programming environment.
==不出所料，人们开始思考将同步合并到更结构化的编程环境中的方法。==

One such approach that emerged was the **monitor**.
==其中一种出现的方法就是**管程**（monitor）。==

First described by Per Brinch Hansen [BH73] and later refined by Tony Hoare [H74], the idea behind a monitor is quite simple.
==管程的概念最早由 Per Brinch Hansen [BH73] 描述，后来由 Tony Hoare [H74] 完善，其背后的想法非常简单。==

Consider the following pretend monitor written in C++ notation:
==考虑以下用 C++ 符号编写的假想管程：==

```cpp
monitor class account {
private:
    int balance = 0;
public:
    void deposit(int amount) {
        balance = balance + amount;
    }
    void withdraw(int amount) {
        balance = balance - amount;
    }
};
```

Note: this is a “pretend” class because C++ does not support monitors, and hence the `monitor` keyword does not exist.
==注意：这是一个“假想”类，因为 C++ 不支持管程，因此不存在 `monitor` 关键字。==

However, Java does support monitors, with what are called **synchronized** methods.
==然而，Java 确实支持管程，它使用的是所谓的**同步**（synchronized）方法。==

In this example, you may notice we have our old friend the account and some routines to deposit and withdraw an amount from the balance.
==在这个例子中，你可能会注意到我们有老朋友“账户”，以及一些用于从余额中存款和取款的例程。==

As you also may notice, these are **critical sections**; if they are called by multiple threads concurrently, you have a race condition and the potential for an incorrect outcome.
==你可能也注意到了，这些是**临界区**；如果它们被多个线程并发调用，你就会遇到竞态条件，并可能产生错误的结果。==



MONITORS (DEPRECATED)
==管程（已弃用）==

In a monitor class, you don’t get into trouble, though, because the monitor guarantees that **only one thread can be active within the monitor at a time**.
==不过，在管程类中，你不会遇到麻烦，因为管程保证了**一次只能有一个线程在管程内活动**。==

Thus, our above example is a perfectly safe and working piece of code; multiple threads can call deposit() or withdraw() and know that mutual exclusion is preserved.
==因此，我们上面的例子是一个完全安全且有效的代码片段；多个线程可以调用 deposit() 或 withdraw()，并知道互斥性得到了维护。==

How does the monitor do this?
==管程是如何做到这一点的？==

Simple: with a lock.
==很简单：通过锁。==

Whenever a thread tries to call a monitor routine, it implicitly tries to acquire the monitor lock.
==每当线程尝试调用管程例程时，它都会隐式地尝试获取管程锁。==

If it succeeds, then it will be able to call into the routine and run the method’s code.
==如果成功，它将能够进入该例程并运行方法代码。==

If it does not, it will block until the thread that is in the monitor finishes what it is doing.
==如果不成功，它将阻塞，直到管程中的线程完成其操作。==

Thus, if we wrote a C++ class that looked like the following, it would accomplish the exact same goal as the monitor class above:
==因此，如果我们编写一个如下所示的 C++ 类，它将实现与上述管程类完全相同的目标：==

```cpp
class account {
private:
    int balance = 0;
    pthread_mutex_t monitor;

public:
    void deposit(int amount) {
        pthread_mutex_lock(&monitor);
        balance = balance + amount;
        pthread_mutex_unlock(&monitor);
    }
    void withdraw(int amount) {
        pthread_mutex_lock(&monitor);
        balance = balance - amount;
        pthread_mutex_unlock(&monitor);
    }
};
```

Figure D.2: A C++ Class that acts like a Monitor
==图 D.2：一个表现得像管程的 C++ 类==

Thus, as you can see from this example, the monitor isn’t doing too much for you automatically.
==因此，正如你从这个例子中看到的，管程并没有自动为你做太多事情。==

Basically, it is just acquiring a lock and releasing it.
==基本上，它只是获取锁并释放锁。==

By doing so, we achieve what the monitor requires: only one thread will be active within deposit() or withdraw(), as desired.
==通过这样做，我们实现了管程的要求：正如预期的那样，在 deposit() 或 withdraw() 中一次只会有一个活跃线程。==

D.1 Why Bother with Monitors?
==D.1 为什么要费心使用管程？==

You might wonder why monitors were invented at all, instead of just using explicit locking.
==你可能会奇怪为什么要发明管程，而不是直接使用显式锁。==

At the time, object-oriented programming was just coming into fashion.
==当时，面向对象编程正开始流行。==

Thus, the idea was to gracefully blend some of the key concepts in concurrent programming with some of the basic approaches of object orientation.
==因此，当时的初衷是将并发编程中的一些关键概念与面向对象的一些基本方法优雅地融合在一起。==

Nothing more than that.
==仅此而已。==

D.2 Do We Get More Than Automatic Locking?
==D.2 除了自动加锁，我们还能得到更多吗？==

```cpp
monitor class BoundedBuffer {
private:
    int buffer[MAX];
    int fill, use;
    int fullEntries = 0;
    cond_t empty;
    cond_t full;

public:
    void produce(int element) {
        if (fullEntries == MAX) // line P0
            wait(&empty);       // line P1
        buffer[fill] = element; // line P2
        fill = (fill + 1) % MAX; // line P3
        fullEntries++;          // line P4
        signal(&full);          // line P5
    }

    int consume() {
        if (fullEntries == 0)   // line C0
            wait(&full);        // line C1
        int tmp = buffer[use];  // line C2
        use = (use + 1) % MAX;  // line C3
        fullEntries--;          // line C4
        signal(&empty);         // line C5
        return tmp;             // line C6
    }
}
```

Figure D.3: Producer/Consumer with Monitors and Hoare Semantics
==图 D.3：使用管程和 Hoare 语义的生产者/消费者问题==

Back to business.
==言归正传。==

As we know from our discussion of semaphores, just having locks is not quite enough.
==正如我们从关于信号量的讨论中所知，仅仅有锁是不够的。==

For example, to implement the producer/consumer solution, we previously used semaphores to both put threads to sleep when waiting for a condition to change (e.g., a producer waiting for a buffer to be emptied), as well as to wake up a thread when a particular condition has changed (e.g., a consumer signaling that it has indeed emptied a buffer).
==例如，为了实现生产者/消费者解决方案，我们之前使用信号量来使等待条件改变的线程进入睡眠状态（例如，生产者等待缓冲区变空），并在特定条件发生改变时唤醒线程（例如，消费者发出信号表示它确实清空了一个缓冲区）。==

Monitors support such functionality through an explicit construct known as a **condition variable**.
==管程通过一种称为**条件变量**的显式结构来支持这种功能。==

Let’s take a look at the producer/consumer solution, here written with monitors and condition variables.
==让我们来看看生产者/消费者的解决方案，这里是用管程和条件变量编写的。==

In this monitor class, we have two routines, produce() and consume().
==在这个管程类中，我们有两个例程：produce() 和 consume()。==

A producer thread would repeatedly call produce() to put data into the bounded buffer, while a consumer() would repeatedly call consume().
==生产者线程会重复调用 produce() 将数据放入有界缓冲区，而消费者则会重复调用 consume()。==

The example is a modern paraphrase of Hoare’s solution [H74].
==该示例是对 Hoare 解决方案 [H74] 的现代改写。==

You should notice some similarities between this code and the semaphore-based solution in the previous note.
==你应该会注意到这段代码与前一篇笔记中基于信号量的解决方案之间的一些相似之处。==

One major difference is how condition variables must be used in concert with an explicit **state variable**.
==一个主要的区别是条件变量必须与显式的**状态变量**配合使用。==

In this case, the integer `fullEntries` determines whether a producer or consumer must wait, depending on its state.
==在这种情况下，整数 `fullEntries` 根据其状态决定生产者或消费者是否必须等待。==

Semaphores, in contrast, have an internal numeric value which serves this same purpose.
==相比之下，信号量具有一个起相同作用的内部数值。==

Thus, condition variables must be paired with some kind of external state value in order to achieve the same end.
==因此，条件变量必须与某种外部状态值配对，才能达到相同的目的。==

The most important aspect of this code, however, is the use of the two condition variables, empty and full, and the respective wait() and signal() calls that employ them.
==然而，这段代码最重要的方面是使用了两个条件变量 empty 和 full，以及使用它们的相应 wait() 和 signal() 调用。==

These operations do exactly what you might think: wait() blocks the calling thread on a given condition; signal() wakes one waiting thread that is waiting on the condition.
==这些操作正如你所想的那样：wait() 在给定条件下阻塞调用线程；signal() 唤醒一个在该条件下等待的线程。==

However, there are some subtleties in how these calls operate; understanding the semantics of these calls is critically important to understanding why this code works.
==然而，这些调用的运作方式存在一些细微差别；理解这些调用的语义对于理解这段代码为何有效至关重要。==

In what researchers in operating systems call **Hoare semantics** (yes, a somewhat unfortunate name), the signal() immediately wakes one waiting thread and runs it.
==在操作系统研究人员所谓的 **Hoare 语义**（是的，一个有点令人遗憾的名字）中，signal() 会立即唤醒一个等待线程并运行它。==

Thus, the monitor lock, which is implicitly held by the running thread, immediately is transferred to the woken thread which then runs until it either blocks or exits the monitor.
==因此，由正在运行的线程隐式持有的管程锁会立即转移到被唤醒的线程，然后该线程一直运行，直到它阻塞或退出管程。==

Note that there may be more than one thread waiting; signal() only wakes one waiting thread and runs it, while the others must wait for a subsequent signal.
==请注意，可能不止一个线程在等待；signal() 仅唤醒一个等待线程并运行它，而其他线程必须等待后续的信号。==

A simple example will help us understand this code better.
==一个简单的例子将帮助我们更好地理解这段代码。==

Imagine there are two threads, one a producer and the other a consumer.
==想象有两个线程，一个是生产者，另一个是消费者。==

The consumer gets to run first, and calls consume(), only to find that fullEntries = 0 (C0), as there is nothing in the buffer yet.
==消费者先运行并调用 consume()，结果发现 fullEntries = 0 (C0)，因为缓冲区中还没有任何东西。==

Thus, it calls wait(&full) (C1), and waits for a buffer to be filled.
==因此，它调用 wait(&full) (C1)，并等待缓冲区被填充。==

The producer then runs, finds it doesn’t have to wait (P0), puts an element into the buffer (P2), increments the fill index (P3) and the fullEntries count (P4), and calls signal(&full) (P5).
==然后生产者运行，发现它不需要等待 (P0)，将一个元素放入缓冲区 (P2)，增加填充索引 (P3) 和 fullEntries 计数 (P4)，并调用 signal(&full) (P5)。==

In Hoare semantics, the producer does not continue running after the signal; rather, the signal immediately transfers control to the waiting consumer, which returns from wait() (C1) and immediately consumes the element produced by the producer (C2) and so on.
==在 Hoare 语义中，生产者在发出信号后不会继续运行；相反，该信号立即将控制权转移给等待的消费者，消费者从 wait() (C1) 返回并立即消耗生产者生产的元素 (C2)，依此类推。==

Only after the consumer returns will the producer get to run again and return from the produce() routine.
==只有在消费者返回后，生产者才会再次运行并从 produce() 例程返回。==

D.3 Where Theory Meets Practice
==D.3 理论与实践的交汇点==

Tony Hoare, who wrote the solution above and came up with the exact semantics for signal() and wait(), was a theoretician.
==编写上述解决方案并提出 signal() 和 wait() 精确语义的 Tony Hoare 是一位理论家。==

Clearly a smart guy, too; he came up with quicksort after all [H61].
==显然他也是个聪明人；毕竟他发明了快速排序 [H61]。==

However, the semantics of signaling and waiting, as it turns out, were not ideal for a real implementation.
==然而，事实证明，信号通知（signaling）和等待（waiting）的语义对于实际实现来说并不理想。==

As the old saying goes, in theory, there is no difference between theory and practice, but in practice, there is.
==正如俗话所说，在理论上，理论和实践没有区别，但在实践中，确实有区别。==

OLD SAYING: THEORY VS. PRACTICE
==俗话说：理论与实践==

The old saying is “in theory, there is no difference between theory and practice, but in practice, there is.”
==俗话说得好：“在理论上，理论和实践没有区别，但在实践中，确实有区别。”==

Of course, only practitioners tell you this; a theory person could undoubtedly prove that it is not true.
==当然，只有实践者会告诉你这一点；理论家无疑可以证明这不是真的。==

A few years later, Butler Lampson and David Redell of Xerox PARC were building a concurrent language known as **Mesa**, and decided to use monitors as their basic concurrency primitive [LR80].
==几年后，施乐帕克研究中心（Xerox PARC）的 Butler Lampson 和 David Redell 正在开发一种名为 **Mesa** 的并发语言，并决定使用管程作为其基本的并发原语 [LR80]。==

They were well-known systems researchers, and they soon found that Hoare semantics, while more amenable to proofs, were hard to realize in a real system.
==他们是著名的系统研究员，很快发现 Hoare 语义虽然更利于证明，但在实际系统中很难实现。==

In particular, to build a working monitor implementation, Lampson and Redell decided to change the meaning of signal() in a subtle but critical way.
==特别地，为了构建一个可用的管程实现，Lampson 和 Redell 决定以一种微妙但关键的方式改变 signal() 的含义。==

The signal() routine now was just considered a **hint** [L83]; it would move a single waiting thread from the blocked state to a runnable state, but it would not run it immediately.
==signal() 例程现在仅被视为一种**提示** [L83]；它会将一个等待线程从阻塞状态移动到可运行状态，但不会立即运行它。==

Rather, the signaling thread would retain control until it exited the monitor and was descheduled.
==相反，发出信号的线程将保留控制权，直到它退出管程并被取消调度。==

D.4 Oh Oh, A Race
==D.4 噢，竞态条件==

Given these new **Mesa semantics**, let us again reexamine the code above.
==鉴于这些新的 **Mesa 语义**，让我们再次重新检查上面的代码。==

Imagine again a consumer (consumer 1) who enters the monitor and finds the buffer empty and thus waits (C1).
==再次想象一个消费者（消费者 1）进入管程，发现缓冲区为空，因此进入等待状态 (C1)。==

Now the producer comes along and fills the buffer and signals that a buffer has been filled, moving the waiting consumer from blocked on the full condition variable to ready.
==现在生产者出现了，填充了缓冲区并发出信号表示缓冲区已满，将被阻塞在 full 条件变量上的等待消费者移动到就绪状态。==

The producer keeps running for a while, and eventually gives up the CPU.
==生产者继续运行一段时间，最终放弃 CPU。==

But Houston, we have a problem.
==但是，休斯顿，我们遇到了一个问题。==

Can you see it?
==你能看出来吗？==

Imagine a different consumer (consumer 2) now calls into the consume() routine; it will find a full buffer, consume it, and return, setting fullEntries to 0 in the meanwhile.
==想象另一个消费者（消费者 2）现在调用 consume() 例程；它会发现缓冲区已满，将其消耗掉，然后返回，同时将 fullEntries 设置为 0。==

Can you see the problem yet?
==你看到问题了吗？==

Well, here it comes.
==好了，问题来了。==

Our old friend consumer 1 now finally gets to run, and returns from wait(), expecting a buffer to be full (C1...); unfortunately, this is no longer true, as consumer 2 snuck in and consumed the buffer before consumer 1 had a chance to consume it.
==我们的老朋友消费者 1 现在终于开始运行，并从 wait() 返回，期望缓冲区是满的 (C1...)；不幸的是，这不再属实，因为消费者 2 在消费者 1 有机会消耗缓冲区之前就偷偷潜入并消耗掉了它。==

Thus, the code doesn’t work, because in the time between the signal() by the producer and the return from wait() by consumer 1, the condition has changed.
==因此，代码无法工作，因为在生产者发出 signal() 和消费者 1 从 wait() 返回之间的时间里，条件发生了变化。==

This timeline illustrates the problem:
==这个时间轴说明了该问题：==

Figure D.4: Why the Code doesn’t work with Hoare Semantics
==图 D.4：为什么代码在 Hoare 语义下无法正常工作（注：此处原文标题似有误，应指 Mesa 语义下旧代码失效）==

Fortunately, the switch from Hoare semantics to Mesa semantics requires only a small change by the programmer to realize a working solution.
==幸运的是，从 Hoare 语义切换到 Mesa 语义只需要程序员做一点小改动就能实现可行的解决方案。==

Specifically, when woken, a thread should **recheck the condition** it was waiting on; because signal() is only a hint, it is possible that the condition has changed (even multiple times) and thus may not be in the desired state when the waiting thread runs.
==具体来说，当被唤醒时，线程应该**重新检查它正在等待的条件**；因为 signal() 只是一个提示，条件有可能已经改变（甚至多次改变），因此当等待线程运行时，它可能不在所需的状态。==

In our example, two lines of code must change, lines P0 and C0:
==在我们的例子中，必须更改两行代码，即 P0 行和 C0 行：==

```cpp
void produce(int element) {
    while (fullEntries == MAX) // line P0 (CHANGED IF->WHILE)
        wait(&empty);          // line P1
    // ...
}

int consume() {
    while (fullEntries == 0)   // line C0 (CHANGED IF->WHILE)
        wait(&full);           // line C1
    // ...
}
```

Figure D.5: Producer/Consumer with Monitors and Mesa Semantics
==图 D.5：使用管程和 Mesa 语义的生产者/消费者问题==

Not too hard after all.
==毕竟不算太难。==

Because of the ease of this implementation, virtually any system today that uses condition variables with signaling and waiting uses Mesa semantics.
==由于这种实现的简便性，今天几乎所有使用带有信号通知和等待的条件变量的系统都使用 Mesa 语义。==

Thus, if you remember nothing else at all from this class, you can just remember: **always recheck the condition after being woken!**
==因此，如果你在这门课中什么都没记住，你只需记住：**唤醒后务必重新检查条件！**==

Put in even simpler terms, **use while loops and not if statements** when checking conditions.
==用更简单的话来说，在检查条件时，**使用 while 循环而不是 if 语句**。==

Note that this is always correct, even if somehow you are running on a system with Hoare semantics; in that case, you would just needlessly retest the condition an extra time.
==请注意，这始终是正确的，即使你是在具有 Hoare 语义的系统上运行；在这种情况下，你只是不必要地多测试了一次条件。==

D.5 Peeking Under The Hood A Bit
==D.5 深入了解底层实现==

To understand a bit better why Mesa semantics are easier to implement, let’s understand a little more about the implementation of Mesa monitors.
==为了更好地理解为什么 Mesa 语义更容易实现，让我们进一步了解 Mesa 管程的实现。==

In their work [LR80], Lampson and Redell describe three different types of queues that a thread can be a part of at a given time: the **ready queue**, a **monitor lock queue**, and a **condition variable queue**.
==在他们的著作 [LR80] 中，Lampson 和 Redell 描述了线程在给定时间可以参与的三种不同类型的队列：**就绪队列**、**管程锁队列**和**条件变量队列**。==

Note that a program might have multiple monitor classes and multiple condition variable instances; there is a queue per instance of said items.
==请注意，一个程序可能有多个管程类和多个条件变量实例；上述每个实例都有一个队列。==

D.6 Other Uses Of Monitors
==D.6 管程的其他用途==

In their paper on Mesa, Lampson and Redell also point out a few places where a different kind of signaling is needed.
==在关于 Mesa 的论文中，Lampson 和 Redell 还指出了一些需要不同类型信号的场景。==

For example, consider the following memory allocator (Figure D.7).
==例如，考虑以下内存分配器（图 D.7）。==

```cpp
monitor class allocator {
    int available; // how much memory is available?
    cond_t c;

    void *allocate(int size) {
        while (size > available)
            wait(&c);
        available -= size;
        return a_chunk_of_memory;
    }

    void free(void *pointer, int size) {
        available += size;
        signal(&c);
    }
};
```

Figure D.7: A Simple Memory Allocator
==图 D.7：一个简单的内存分配器==

Imagine two threads call allocate.
==想象有两个线程调用 allocate。==

The first calls allocate(20) and the second allocate(10).
==第一个调用 allocate(20)，第二个调用 allocate(10)。==

No memory is available, and thus both threads call wait() and block.
==没有可用内存，因此两个线程都调用 wait() 并阻塞。==

Some time later, a different thread comes along and calls free(p, 15), and thus frees up 15 bytes of memory.
==一段时间后，另一个线程出现并调用 free(p, 15)，从而释放了 15 字节的内存。==

It then signals that it has done so.
==然后它发出信号表示已完成释放。==

Unfortunately, it wakes the thread waiting for 20 bytes; that thread rechecks the condition, finds that only 15 bytes are available, and calls wait() again.
==不幸的是，它唤醒了等待 20 字节的线程；该线程重新检查条件，发现只有 15 字节可用，于是再次调用 wait()。==

The thread that could have benefited from the free of 15 bytes, i.e., the thread that called allocate(10), is not woken.
==本可以从释放的 15 字节中受益的线程，即调用 allocate(10) 的线程，却没有被唤醒。==

Lampson and Redell suggest a simple solution to this problem.
==Lampson 和 Redell 为这个问题提出了一个简单的解决方案。==

Instead of a signal() which wakes a single waiting thread, they employ a **broadcast()** which wakes *all* waiting threads.
==他们不使用唤醒单个等待线程的 signal()，而是采用 **broadcast()** 唤醒*所有*等待线程。==

D.7 Using Monitors To Implement Semaphores
==D.7 使用管程实现信号量==

You can probably see a lot of similarities between monitors and semaphores.
==你可能会发现管程和信号量之间有很多相似之处。==

Not surprisingly, you can use one to implement the other.
==不出所料，你可以用其中一个来实现另一个。==

D.8 Monitors in the Real World
==D.8 现实世界中的管程==

We already mentioned above that we were using “pretend” monitors; C++ has no such concept.
==我们上面已经提到我们使用的是“假装的”管程；C++ 没有这样的概念。==

We now show how to make a monitor-like C++ class, and how Java uses synchronized methods to achieve a similar end.
==我们现在展示如何创建一个类似管程的 C++ 类，以及 Java 如何使用 synchronized 方法来实现类似的目标。==

A Java Monitor
==Java 管程==

Interestingly, the designers of Java decided to use monitors as they thought they were a graceful way to add synchronization primitives into a language.
==有趣的是，Java 的设计者决定使用管程，因为他们认为这是将同步原语添加到语言中的一种优雅方式。==

To use them, you just add the keyword **synchronized** to the method or set of methods that you wish to use as a monitor.
==要使用它们，你只需将关键字 **synchronized** 添加到你希望作为管程使用的方法或方法集中。==

D.9 Summary
==D.9 总结==

We have seen the introduction of monitors, a structuring concept developed by Brinch Hansen and subsequently Hoare in the early seventies.
==我们已经看到了管程的引入，这是由 Brinch Hansen 以及随后的 Hoare 在七十年代初开发的一种结构化概念。==

When running inside the monitor, a thread implicitly holds a monitor lock, and thus prevents other threads from entering the monitor, allowing the ready construction of mutual exclusion.
==在管程内部运行时，线程隐式持有管程锁，从而防止其他线程进入管程，从而可以轻松构建互斥。==

We also have seen the introduction of explicit condition variables, which allow threads to signal() and wait() much like we saw with semaphores in the previous note.
==我们也看到了显式条件变量的引入，它们允许线程进行 signal() 和 wait()，就像我们在前一篇笔记中看到的信号量一样。==

The semantics of signal() and wait() are critical; because all modern systems implement **Mesa semantics**, a recheck of the condition that the thread went to sleep on is required for correct execution.
==signal() 和 wait() 的语义至关重要；因为所有现代系统都实现 **Mesa 语义**，为了正确执行，需要重新检查线程进入睡眠状态时所依据的条件。==

Thus, signal() is just a **hint** that something has changed; it is the responsibility of the woken thread to make sure the conditions are right for its continued execution.
==因此，signal() 只是一个表明某些情况已发生变化的**提示**；被唤醒的线程有责任确保条件适合其继续执行。==




`hw.c` has been modified more recently than `hw.o` has been created, `make` will know that `hw.o` is out of date and should be generated anew; in that case, it will execute the command line, `gcc -O -Wall -c hw.c`, which generates `hw.o`.
==`hw.c` 的修改时间比 `hw.o` 的创建时间更近，`make` 将会知道 `hw.o` 已经过期并应重新生成；在这种情况下，它将执行命令行 `gcc -O -Wall -c hw.c`，从而生成 `hw.o`。==

Thus, if you are compiling a large program, `make` will know which object files need to be re-generated based on their dependencies, and will only do the necessary amount of work to recreate the executable.
==因此，如果你正在编译一个大型程序，`make` 将根据它们的依赖关系知道哪些对象文件需要重新生成，并且只会进行重建可执行文件所需的必要工作。==

Also note that `hw.o` will be created in the case that it does not exist at all.
==另外请注意，如果 `hw.o` 根本不存在，它也会被创建。==

Continuing along, `helper.o` may also be regenerated or created, based on the same criteria as defined above.
==继续往下看，`helper.o` 也可能根据上述定义的相同标准被重新生成或创建。==

When both of the object files have been created, `make` is now ready to execute the command to create the final executable, and goes back and does so: `gcc -o hw hw.o helper.o -lm`.
==当两个对象文件都创建好后，`make` 就可以执行创建最终可执行文件的命令，并返回执行：`gcc -o hw hw.o helper.o -lm`。==

Up until now, we’ve been ignoring the `clean` target in the makefile.
==到目前为止，我们一直忽略了 makefile 中的 `clean` 目标。==

To use it, you have to ask for it explicitly.
==要使用它，你必须显式地请求它。==

Type `prompt> make clean`
==输入 `prompt> make clean`==

This will execute the command on the command line.
==这将执行命令行上的命令。==

Because there are no prerequisites for the `clean` target, typing `make clean` will always result in the command(s) being executed.
==由于 `clean` 目标没有先决条件，输入 `make clean` 总是会导致命令被执行。==

In this case, the `clean` target is used to remove the object files and executable, quite handy if you wish to rebuild the entire program from scratch.
==在这种情况下，`clean` 目标用于删除对象文件和可执行文件，如果你想从头开始重新构建整个程序，这非常方便。==

Now you might be thinking, “well, this seems OK, but these makefiles sure are cumbersome!”
==现在你可能会想，“嗯，这看起来还行，但这些 makefile 确实很麻烦！”==

And you’d be right — if they always had to be written like this.
==你是对的——如果它们总是必须这样写的话。==

Fortunately, there are a lot of shortcuts that make `make` even easier to use.
==幸运的是，有很多快捷方式可以让 `make` 变得更容易使用。==

For example, this makefile has the same functionality but is a little nicer to use:
==例如，这个 makefile 具有相同的功能，但使用起来更舒服一些：==

```makefile
# specify all source files here
SRCS = hw.c helper.c
# specify target here (name of executable)
TARG = hw
# specify compiler, compile flags, and needed libs
CC = gcc
OPTS = -Wall -O
LIBS = -lm

# this translates .c files in src list to .o’s
OBJS = $(SRCS:.c=.o)

# all is not really needed, but is used to generate the target
all: $(TARG)

# this generates the target executable
$(TARG): $(OBJS)
    $(CC) -o $(TARG) $(OBJS) $(LIBS)

# this is a generic rule for .o files
%.o: %.c
    $(CC) $(OPTS) -c $< -o $@

# and finally, a clean line
clean:
    rm -f $(OBJS) $(TARG)
```
```makefile
==# 在此处指定所有源文件==
SRCS = hw.c helper.c
==# 在此处指定目标（可执行文件的名称）==
TARG = hw
==# 指定编译器、编译标志和所需的库==
CC = gcc
OPTS = -Wall -O
LIBS = -lm

==# 这将源列表中的 .c 文件转换为 .o 文件==
OBJS = $(SRCS:.c=.o)

==# all 并不是真正需要的，但用于生成目标==
all: $(TARG)

==# 这将生成目标可执行文件==
$(TARG): $(OBJS)
    $(CC) -o $(TARG) $(OBJS) $(LIBS)

==# 这是一个用于 .o 文件的通用规则==
%.o: %.c
    $(CC) $(OPTS) -c $< -o $@

==# 最后是一个 clean 行==
clean:
    rm -f $(OBJS) $(TARG)
```

Though we won’t go into the details of `make` syntax, as you can see, this makefile can make your life somewhat easier.
==虽然我们不会深入探讨 `make` 语法的细节，但正如你所看到的，这个 makefile 可以让你的生活更轻松一些。==

For example, it allows you to easily add new source files into your build, simply by adding them to the `SRCS` variable at the top of the makefile.
==例如，它允许你通过简单地将新源文件添加到 makefile 顶部的 `SRCS` 变量中，轻松地将它们添加到你的构建中。==

You can also easily change the name of the executable by changing the `TARG` line, and the compiler, flags, and library specifications are all easily modified.
==你还可以通过更改 `TARG` 行来轻松更改可执行文件的名称，并且编译器、标志和库规范也都很容易修改。==

One final word about `make`: figuring out a target’s prerequisites is not always trivial, especially in large and complex programs.
==关于 `make` 的最后一句话：确定一个目标的先决条件并不总是那么容易，特别是在大型和复杂的程序中。==

Not surprisingly, there is another tool that helps with this, called `makedepend`.
==不出所料，还有另一个工具可以提供帮助，叫做 `makedepend`。==

Read about it on your own and see if you can incorporate it into a makefile.
==你自己阅读相关内容，看看是否可以将其整合到 makefile 中。==

**F.7 Debugging**
==**F.7 调试**==

Finally, after you have created a good build environment, and a correctly compiled program, you may find that your program is buggy.
==最后，在你创建了一个良好的构建环境并正确编译了程序之后，你可能会发现你的程序存在漏洞（bug）。==

One way to fix the problem(s) is to think really hard — this method is sometimes successful, but often not.
==解决问题的一种方法是苦思冥想——这种方法有时会成功，但通常不会。==

The problem is a lack of information; you just don’t know exactly what is going on within the program, and therefore cannot figure out why it is not behaving as expected.
==问题在于缺乏信息；你只是不确切知道程序内部发生了什么，因此无法弄清楚它为什么没有按预期运行。==

Fortunately, there is some help: `gdb`, the GNU debugger.
==幸运的是，有一些帮助工具：`gdb`，即 GNU 调试器。==

Let’s take the following buggy code, saved in the file `buggy.c`, and compiled into the executable `buggy`.
==让我们以保存为文件 `buggy.c` 并编译为可执行文件 `buggy` 的以下有漏洞的代码为例。==

```c
#include <stdio.h>
struct Data {
    int x;
};

int
main(int argc, char *argv[])
{
    struct Data *p = NULL;
    printf("%d\n", p->x);
}
```
```c
#include <stdio.h>
struct Data {
    int x;
};

int
main(int argc, char *argv[])
{
    struct Data *p = NULL;
    printf("%d\n", p->x);
}
```

In this example, the main program dereferences the variable `p` when it is `NULL`, which will lead to a segmentation fault.
==在这个例子中，主程序在变量 `p` 为 `NULL` 时对其进行解引用，这将导致段错误（segmentation fault）。==

Of course, this problem should be easy to fix by inspection, but in a more complex program, finding such a problem is not always easy.
==当然，这个问题通过检查应该很容易修复，但在更复杂的程序中，发现这样的问题并不总是那么容易。==

To prepare yourself for a debugging session, recompile your program and make sure to pass the `-g` flag to each compile line.
==为了准备调试会话，请重新编译你的程序，并确保在每个编译行中都传递 `-g` 标志。==

This includes extra debugging information in your executable that will be useful during your debugging session.
==这会在你的可执行文件中包含额外的调试信息，这些信息在调试会话期间非常有用。==

Also, don’t turn on optimization (`-O`); though this may work, it may also lead to confusion during debugging.
==此外，不要开启优化（`-O`）；虽然这可能可行，但也可能在调试过程中导致混乱。==

After re-compiling with `-g`, you are ready to use the debugger.
==使用 `-g` 重新编译后，你就可以使用调试器了。==

Fire up `gdb` at the command prompt as follows: `prompt> gdb buggy`
==在命令提示符下启动 `gdb`，如下所示：`prompt> gdb buggy`==

This puts you inside an interactive session with the debugger.
==这将使你进入与调试器的交互式会话。==

Note that you can also use the debugger to examine “core” files that were produced during bad runs, or to attach to an already-running program; read the documentation to learn more about this.
==请注意，你还可以使用调试器检查在运行失败时产生的“核心（core）”文件，或者附加到一个已经在运行的程序；阅读文档以了解更多相关信息。==

Once inside, you may see something like this:
==进入后，你可能会看到类似这样的内容：==

```text
prompt> gdb buggy
GNU gdb ...
Copyright 2008 Free Software Foundation, Inc.
(gdb)
```
```text
prompt> gdb buggy
GNU gdb ...
Copyright 2008 Free Software Foundation, Inc.
(gdb)
```

The first thing you might want to do is to go ahead and run the program.
==你可能想做的第一件事就是继续运行程序。==

To do this, simply type `run` at `gdb` command prompt.
==为此，只需在 `gdb` 命令提示符下输入 `run`。==

In this case, this is what you might see:
==在这种情况下，你可能会看到以下内容：==

```text
(gdb) run
Starting program: buggy

Program received signal SIGSEGV, Segmentation fault.
0x8048433 in main (argc=1, argv=0xbffff844) at buggy.c:19
19 printf("%d\n", p->x);
```
```text
(gdb) run
Starting program: buggy

==程序接收到信号 SIGSEGV，段错误。==
0x8048433 in main (argc=1, argv=0xbffff844) at buggy.c:19
19 printf("%d\n", p->x);
```

As you can see from the example, in this case, `gdb` immediately pinpoints where the problem occurred; a “segmentation fault” was generated at the line where we tried to dereference `p`.
==正如你从例子中看到的，在这种情况下，`gdb` 立即指出了问题发生的位置；在我们尝试对 `p` 进行解引用的那一行产生了一个“段错误”。==

This just means that we accessed some memory that we weren’t supposed to access.
==这仅仅意味着我们访问了一些本不该访问的内存。==

At this point, the astute programmer can examine the code, and say “aha! it must be that `p` does not point to anything valid, and thus should not be dereferenced!”, and then go ahead and fix the problem.
==此时，精明的程序员可以检查代码并说：“啊哈！一定是 `p` 没有指向任何有效的东西，因此不应该被解引用！”，然后着手修复问题。==

However, if you didn’t know what was going on, you might want to examine some variable.
==然而，如果你不知道发生了什么，你可能想检查某些变量。==

`gdb` allows you to do this interactively during the debug session.
==`gdb` 允许你在调试会话期间交互式地执行此操作。==

```text
(gdb) print p
$1 = (struct Data *) 0x0
```
```text
(gdb) print p
$1 = (struct Data *) 0x0
```

By using the `print` primitive, we can examine `p`, and see both that it is a pointer to a struct of type `Data`, and that it is currently set to `NULL` (or zero, or hex zero which is shown here as “0x0”).
==通过使用 `print` 原语，我们可以检查 `p`，并看到它是一个指向 `Data` 类型结构体的指针，并且当前被设置为 `NULL`（或零，或此处显示为“0x0”的十六进制零）。==

Finally, you can also set breakpoints within your program to have the debugger stop the program at a certain routine.
==最后，你还可以在程序中设置断点，让调试器在某个例程处停止程序。==

After doing this, it is often useful to step through the execution (one line at a time), and see what is happening.
==完成此操作后，逐步执行（一次一行）并查看发生了什么是很有用的。==

```text
(gdb) break main
Breakpoint 1 at 0x8048426: file buggy.c, line 17.
(gdb) run
Starting program: /homes/hacker/buggy

Breakpoint 1, main (argc=1, argv=0xbffff844) at buggy.c:17
17 struct Data *p = NULL;
(gdb) next
19 printf("%d\n", p->x);
(gdb) next

Program received signal SIGSEGV, Segmentation fault.
0x8048433 in main (argc=1, argv=0xbffff844) at buggy.c:19
19 printf("%d\n", p->x);
```
```text
(gdb) break main
==断点 1 位于 0x8048426：文件 buggy.c，第 17 行。==
(gdb) run
==正在启动程序：/homes/hacker/buggy==

==断点 1，main (argc=1, argv=0xbffff844) 位于 buggy.c:17==
17 struct Data *p = NULL;
(gdb) next
19 printf("%d\n", p->x);
(gdb) next

==程序接收到信号 SIGSEGV，段错误。==
0x8048433 in main (argc=1, argv=0xbffff844) at buggy.c:19
19 printf("%d\n", p->x);
```

In the example above, a breakpoint is set at the `main()` routine; thus, when we run the program, the debugger almost immediately stops execution at `main`.
==在上面的例子中，在 `main()` 例程处设置了一个断点；因此，当我们运行程序时，调试器几乎立即在 `main` 处停止执行。==

At that point in the example, a `next` command is issued, which executes the next source-level command.
==在例子的那个点上，发出了一个 `next` 命令，它执行下一个源码级命令。==

Both `next` and `step` are useful ways to advance through a program — read about them in the documentation for more details.
==`next` 和 `step` 都是在程序中推进的有用方式——阅读文档以获取更多细节。==

This discussion really does not do `gdb` justice; it is a rich and flexible debugging tool, with many more features than can be described in the limited space here.
==这里的讨论确实有失公允，没能充分展现 `gdb` 的强大；它是一个功能丰富且灵活的调试工具，其功能远多于在此有限空间内所能描述的。==

Read more about it on your own and become an expert in your copious spare time.
==在你的大量业余时间里多读读相关内容，成为一名专家。==

**F.8 Documentation**
==**F.8 文档**==

To learn a lot more about all of these things, you have to do two things: the first is to use these tools, and the second is to read more about them on your own.
==要了解有关所有这些内容的更多信息，你必须做两件事：第一是使用这些工具，第二是自己阅读更多关于它们的信息。==

One way to find out more about `gcc`, `gmake`, and `gdb` is to read their `man` pages; type `man gcc`, `man gmake`, or `man gdb` at your command prompt.
==了解更多关于 `gcc`、`gmake` 和 `gdb` 的一种方法是阅读它们的 `man` 手册页；在命令提示符下输入 `man gcc`、`man gmake` 或 `man gdb`。==

You can also use `man -k` to search the `man` pages for keywords, though that doesn’t always work as well as it might; googling is probably a better approach here.
==你还可以使用 `man -k` 在 `man` 手册页中搜索关键字，尽管这并不总是那么有效；在这里，谷歌搜索可能是更好的方法。==

One tricky thing about `man` pages: typing `man XXX` may not result in the thing you want, if there is more than one thing called `XXX`.
==`man` 手册页中一件棘手的事情：如果有一个以上的东西叫 `XXX`，输入 `man XXX` 可能不会得到你想要的东西。==

For example, if you are looking for the `kill()` system call `man` page, and if you just type `man kill` at the prompt, you will get the wrong `man` page, because there is a command-line program called `kill`.
==例如，如果你正在寻找 `kill()` 系统调用的 `man` 手册页，如果你只是在提示符下输入 `man kill`，你会得到错误的 `man` 手册页，因为有一个名为 `kill` 的命令行程序。==

`Man` pages are divided into sections, and by default, `man` will return the `man` page in the lowest section that it finds, which in this case is section 1.
==`Man` 手册页被分为不同的章节，默认情况下，`man` 会返回它找到的最低章节中的 `man` 手册页，在本例中是第 1 节。==

Note that you can tell which `man` page you got by looking at the top of the page: if you see `kill(2)`, you know you are in the right `man` page in Section 2, where system calls live.
==请注意，你可以通过查看页面顶部来判断你得到的是哪个 `man` 手册页：如果你看到 `kill(2)`，你就知道你是在第 2 节中正确的 `man` 手册页里，那是系统调用所在的地方。==

Type `man man` to learn more about what is stored in each of the different sections of the `man` pages.
==输入 `man man` 以了解更多关于 `man` 手册页每个不同章节中存储内容的信息。==

Also note that `man -a kill` can be used to cycle through all of the different `man` pages named “kill”.
==另请注意，`man -a kill` 可用于循环查看所有名为“kill”的不同 `man` 手册页。==

`Man` pages are useful for finding out a number of things.
==`Man` 手册页对于查寻许多事情都很有用。==

In particular, you will often want to look up what arguments to pass to a library call, or what header files need to be included to use a library call.
==特别地，你经常会想要查找传递给库调用的参数，或者使用库调用需要包含哪些头文件。==

All of this should be available in the `man` page.
==所有这些都应该可以在 `man` 手册页中找到。==

For example, if you look up the `open()` system call, you will see:
==例如，如果你查找 `open()` 系统调用，你会看到：==

```text
SYNOPSIS
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>

int open(const char *path, int oflag, /* mode_t mode */...);
```
```text
==大纲==
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>

int open(const char *path, int oflag, /* mode_t mode */...);
```

That tells you to include the headers `sys/types.h`, `sys/stat.h`, and `fcntl.h` in order to use the `open` call.
==这告诉你要包含头文件 `sys/types.h`、`sys/stat.h` 和 `fcntl.h` 才能使用 `open` 调用。==

It also tells you about the parameters to pass to `open`, namely a string called `path`, and integer flag `oflag`, and an optional argument to specify the `mode` of the file.
==它还告诉你传递给 `open` 的参数，即一个名为 `path` 的字符串、整数标志 `oflag`，以及一个用于指定文件 `mode`（模式）的可选参数。==

If there were any libraries you needed to link with to use the call, it would tell you that here too.
==如果使用该调用需要链接任何库，它也会在这里告诉你。==

`Man` pages require some effort to use effectively.
==有效使用 `man` 手册页需要一些努力。==

They are often divided into a number of standard sections.
==它们通常被分为若干标准章节。==

The main body will describe how you can pass different parameters in order to have the function behave differently.
==正文将描述你如何传递不同的参数，以便让函数表现出不同的行为。==

One particularly useful section is called the **RETURN VALUES** part of the `man` page, and it tells you what the function will return under success or failure.
==一个特别有用的部分是 `man` 手册页中名为 **RETURN VALUES**（返回值）的部分，它会告诉你函数在成功或失败时会返回什么。==

From the `open()` `man` page again:
==再次引用 `open()` 的 `man` 手册页：==

```text
RETURN VALUES
Upon successful completion, the open() function opens the file and return a non-negative integer representing the lowest numbered unused file descriptor. Otherwise, -1 is returned, errno is set to indicate the error, and no files are created or modified.
```
```text
==返回值==
==成功完成后，open() 函数将打开文件并返回一个非负整数，代表编号最低的未使用文件描述符。否则，返回 -1，设置 errno 以指示错误，并且不创建或修改任何文件。==
```

Thus, by checking what `open` returns, you can see if the `open` succeeded or not.
==因此，通过检查 `open` 的返回值，你可以查看 `open` 是否成功。==

If it didn’t, `open` (and many standard library routines) will set a global variable called `errno` to a value to tell you about the error.
==如果没成功，`open`（以及许多标准库例程）将把一个名为 `errno` 的全局变量设置为一个值，以告知你错误信息。==

See the **ERRORS** section of the `man` page for more details.
==有关更多详细信息，请参阅 `man` 手册页的 **ERRORS**（错误）部分。==

Another thing you might want to do is to look for the definition of a structure that is not specified in the `man` page itself.
==你可能想做的另一件事是寻找 `man` 手册页本身未指定的结构体定义。==

For example, the `man` page for `gettimeofday()` has the following synopsis:
==例如，`gettimeofday()` 的 `man` 手册页有以下大纲：==

```text
SYNOPSIS
#include <sys/time.h>
int gettimeofday(struct timeval *restrict tp, void *restrict tzp);
```
```text
==大纲==
#include <sys/time.h>
int gettimeofday(struct timeval *restrict tp, void *restrict tzp);
```

From this page, you can see that the time is put into a structure of type `timeval`, but the `man` page may not tell you what fields that struct has!
==从这个页面你可以看到，时间被放入一个 `timeval` 类型的结构体中，但 `man` 手册页可能不会告诉你该结构体有哪些字段！==

(In this case, it does, but you may not always be so lucky)
==（在这种情况下它确实告诉了，但你并不总是那么幸运）==

Thus, you may have to hunt for it.
==因此，你可能不得不去寻找它。==

All include files are found under the directory `/usr/include`, and thus you can use a tool like `grep` to look for it.
==所有包含文件都位于 `/usr/include` 目录下，因此你可以使用像 `grep` 这样的工具来寻找它。==

For example, you might type: `prompt> grep ’struct timeval’ /usr/include/sys/*.h`
==例如，你可以输入：`prompt> grep ’struct timeval’ /usr/include/sys/*.h`==

This lets you look for the definition of the structure in all files that end with `.h` in `/usr/include/sys`.
==这让你在 `/usr/include/sys` 中所有以 `.h` 结尾的文件里寻找该结构体的定义。==

Unfortunately, this may not always work, as that include file may include others which are found elsewhere.
==不幸的是，这并不总是有效，因为该包含文件可能包含在其他地方找到的其他文件。==

A better way to do this is to use a tool at your disposal, the compiler.
==一个更好的方法是使用你可以支配的工具——编译器。==

Write a program that includes the header `time.h`, let’s say called `main.c`.
==编写一个包含头文件 `time.h` 的程序，假设叫做 `main.c`。==

Then, instead of compiling it, use the compiler to invoke the preprocessor.
==然后，不要编译它，而是使用编译器来调用预处理器。==

The preprocessor processes all the directives in your file, such as `#define` commands and `#include` commands.
==预处理器处理文件中的所有指令，例如 `#define` 命令和 `#include` 命令。==

To do this, type `gcc -E main.c`.
==为此，输入 `gcc -E main.c`。==

The result of this is a C file that has all of the needed structures and prototypes in it, including the definition of the `timeval` struct.
==其结果是一个包含所有所需结构体和原型的 C 文件，包括 `timeval` 结构体的定义。==

Probably an even better way to find these things out: google.
==寻找这些东西可能还有一个更好的方法：谷歌。==

You should always google things you don’t know about — it’s amazing how much you can learn simply by looking it up!
==你总是应该谷歌搜索你不了解的事情——仅仅通过查阅就能学到这么多东西，真是令人惊讶！==

**Info Pages**
==**Info 页面**==

Also quite useful in the hunt for documentation are the `info` pages, which provide much more detailed documentation on many GNU tools.
==在寻找文档时，`info` 页面也相当有用，它提供了关于许多 GNU 工具的更为详尽的文档。==

You can access the `info` pages by running the program `info`, or via emacs, the preferred editor of hackers, by executing `Meta-x info`.
==你可以通过运行程序 `info` 来访问 `info` 页面，或者通过黑客首选的编辑器 emacs 执行 `Meta-x info` 来访问。==

A program like `gcc` has hundreds of flags, and some of them are surprisingly useful to know about.
==像 `gcc` 这样的程序有数百个标志，了解其中的一些标志出奇地有用。==

`gmake` has many more features that will improve your build environment.
==`gmake` 有更多可以改善你构建环境的功能。==

Finally, `gdb` is quite a sophisticated debugger.
==最后，`gdb` 是一个相当复杂的调试器。==

Read the `man` and `info` pages, try out features that you hadn’t tried before, and become a power user of your programming tools.
==阅读 `man` 和 `info` 页面，尝试你以前没有尝试过的功能，并成为你编程工具的高级用户。==

**F.9 Suggested Readings**
==**F.9 建议阅读**==

Other than the `man` and `info` pages, there are a number of useful books out there.
==除了 `man` 和 `info` 页面外，还有许多有用的书籍。==

Note that a lot of this information is available for free on-line; however, sometimes having something in book form seems to make it easier to learn.
==请注意，这些信息有很多可以在网上免费获得；然而，有时拥有书籍形式的东西似乎更容易学习。==

Also, always look for O’Reilly books on topics you are interested in; they are almost always of high quality.
==此外，始终寻找你感兴趣的主题的 O’Reilly 书籍；它们的质量几乎总是很高。==

*   **“The C Programming Language”**, by Brian Kernighan and Dennis Ritchie. This is the definitive C book to have.
    *   ==**《C 程序设计语言》**，由 Brian Kernighan 和 Dennis Ritchie 著。这是必不可少的权威 C 语言书籍。==

*   **“Managing Projects with make”**, by Andrew Oram and Steve Talbott. A reasonable and short book on make.
    *   ==**《使用 make 管理项目》**，由 Andrew Oram 和 Steve Talbott 著。一本合理且简短的关于 make 的书。==

*   **“Debugging with GDB: The GNU Source-Level Debugger”**, by Richard M. Stallman, Roland H. Pesch. A little book on using GDB.
    *   ==**《使用 GDB 调试：GNU 源码级调试器》**，由 Richard M. Stallman, Roland H. Pesch 著。一本关于使用 GDB 的小书。==

*   **“Advanced Programming in the UNIX Environment”**, by W. Richard Stevens and Steve Rago. Stevens wrote some excellent books, and this is a must for UNIX hackers.
   *   ==**《UNIX 环境高级编程》**，由 W. Richard Stevens 和 Steve Rago 著。Stevens 写了一些优秀的著作，这是 UNIX 黑客的必读书籍。==

*   ==**“Expert C Programming”**, by Peter Van der Linden. A lot of the useful tips about compilers, etc., above are stolen directly from here.
   *   ==**《C 专家编程》**，由 Peter Van der Linden 著。上面关于编译器等许多有用的提示直接“窃取”自这本书。==

**G Laboratory: Systems Projects**
==**G 实验室：系统项目**==

This chapter presents some ideas for systems projects.
==本章介绍了一些系统项目的想法。==

We usually do about six or seven projects in a 15-week semester, meaning one every two weeks or so.
==我们通常在一个 15 周的学期里做大约六七个项目，这意味着大约每两周做一个。==

The first few are usually done by a single student, and the last few in groups of size two.
==前几个通常由单个学生完成，最后几个由两人一组完成。==

Each semester, the projects follow this same outline; however, we vary the details to keep it interesting and make “sharing” of code across semesters more challenging.
==每学期，项目都遵循相同的框架；然而，我们会改变细节以保持其趣味性，并使跨学期的代码“共享”更具挑战性。==

**G.1 Intro Project**
==**G.1 入门项目**==

The first project is an introduction to systems programming.
==第一个项目是系统编程简介。==

Typical assignments have been to write some variant of the `sort` utility, with different constraints.
==典型的作业是编写 `sort` 工具的一些变体，并带有不同的约束。==

To complete the project, one must get familiar with some system calls (and their return error codes), use a few simple data structures, and not much else.
==要完成该项目，必须熟悉一些系统调用（及其返回错误代码），使用一些简单的数据结构，仅此而已。==

**G.2 UNIX Shell**
**G.2 UNIX Shell**

In this project, students build a variant of a UNIX shell.
==在这个项目中，学生们构建一个 UNIX shell 的变体。==

Students learn about process management as well as how mysterious things like pipes and redirects actually work.
==学生们学习进程管理，以及像管道和重定向这样神秘的东西实际上是如何工作的。==

**G.3 Memory-allocation Library**
==**G.3 内存分配库**==

This project explores how a chunk of memory is managed, by building an alternative memory-allocation library (like `malloc()` and `free()` but with different names).
==该项目通过构建一个替代内存分配库（类似于 `malloc()` 和 `free()` 但名称不同）来探索如何管理一块内存。==

The project teaches students how to use `mmap()` to get a chunk of anonymous memory, and then about pointers in great detail in order to build a simple (or perhaps, more complex) free list to manage the space.
==该项目教学生如何使用 `mmap()` 获取一块匿名内存，然后非常详细地学习指针，以便构建一个简单的（或者可能更复杂的）空闲列表来管理空间。==

**G.4 Intro to Concurrency**
==**G.4 并发入门**==

This project introduces concurrent programming with POSIX threads.
==该项目介绍了使用 POSIX 线程进行并发编程。==

Build some simple thread-safe libraries: a list, hash table, and some more complicated data structures are good exercises in adding locks to real-world code.
==构建一些简单的线程安全库：列表、哈希表和一些更复杂的数据结构是在现实代码中添加锁的良好练习。==

**G.5 Concurrent Web Server**
==**G.5 并发 Web 服务器**==

This project explores the use of concurrency in a real-world application.
==该项目探索了并发在现实应用中的使用。==

Students take a simple web server (or build one) and add a thread pool to it, in order to serve requests concurrently.
==学生们使用一个简单的 Web 服务器（或构建一个）并向其中添加一个线程池，以便并发地处理请求。==

**G.6 File System Checker**
==**G.6 文件系统检查器**==

This project explores on-disk data structures and their consistency.
==该项目探索了磁盘上的数据结构及其一致性。==

Students build a simple file system checker.
==学生们构建一个简单的文件系统检查器。==

**H Laboratory: xv6 Projects**
==**H 实验室：xv6 项目**==

This chapter presents some ideas for projects related to the `xv6` kernel.
==本章介绍了一些与 `xv6` 内核相关的项目想法。==

The kernel is available from MIT and is quite fun to play with; doing these projects also make the in-class material more directly relevant to the projects.
==该内核可从 MIT 获得，玩起来很有趣；做这些项目也使得课堂材料与项目的关联更加直接。==

**H.1 Intro Project**
==**H.1 入门项目**==

The introduction adds a simple system call to `xv6`.
==入门项目向 `xv6` 添加一个简单的系统调用。==

Many variants are possible, including a system call to count how many system calls have taken place, or other information-gathering calls.
==许多变体都是可能的，包括统计发生了多少次系统调用的系统调用，或其他信息收集调用。==

**H.2 Processes and Scheduling**
==**H.2 进程与调度**==

Students build a more complicated scheduler than the default round robin.
==学生们构建一个比默认轮询调度更复杂的调度器。==

Many variants are possible, including a Lottery scheduler or multi-level feedback queue.
==许多变体都是可能的，包括彩票调度器或多级反馈队列。==

**H.3 Intro to Virtual Memory**
==**H.3 虚拟内存入门**==

The basic idea is to add a new system call that, given a virtual address, returns the translated physical address.
==基本思路是添加一个新的系统调用，给定一个虚拟地址，返回转换后的物理地址。==

This lets students see how the virtual memory system sets up page tables without doing too much hard work.
==这让学生们可以看到虚拟内存系统如何在不进行太多艰苦工作的情况下设置页表。==

**H.4 Copy-on-write Mappings**
==**H.4 写时复制映射**==

This project adds the ability to perform a lightweight `fork()`, called `vfork()`, to `xv6`.
==该项目向 `xv6` 添加了执行轻量级 `fork()`（称为 `vfork()`）的能力。==

This new call doesn’t simply copy the mappings but rather sets up copy-on-write mappings to shared pages.
==这个新调用不只是简单地复制映射，而是为共享页面设置写时复制映射。==

**H.11 File System Checker**
==**H.11 文件系统检查器**==

Students build a simple file system checker for the `xv6` file system.
==学生们为 `xv6` 文件系统构建一个简单的文件系统检查器。==

Students learn about what makes a file system consistent and how exactly to check for it.
==学生们学习什么使文件系统保持一致，以及究竟如何对其进行检查。==

